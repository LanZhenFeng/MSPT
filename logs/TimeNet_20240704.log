Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.5077978
	speed: 0.1236s/iter; left time: 380.6996s
	iters: 200, epoch: 1 | loss: 0.3612643
	speed: 0.0937s/iter; left time: 279.3412s
	iters: 300, epoch: 1 | loss: 0.2336917
	speed: 0.0832s/iter; left time: 239.5869s
Epoch: 1 cost time: 31.638867139816284
Epoch: 1, Steps: 318 | Train Loss: 0.4990888 Vali Loss: 0.2687268 Test Loss: 0.2518326
Validation loss decreased (inf --> 0.268727).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2173458
	speed: 0.0750s/iter; left time: 207.2702s
	iters: 200, epoch: 2 | loss: 0.2550483
	speed: 0.0979s/iter; left time: 260.6707s
	iters: 300, epoch: 2 | loss: 0.2408290
	speed: 0.0879s/iter; left time: 225.3846s
Epoch: 2 cost time: 27.691322565078735
Epoch: 2, Steps: 318 | Train Loss: 0.2378992 Vali Loss: 0.2329124 Test Loss: 0.2203284
Validation loss decreased (0.268727 --> 0.232912).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2345697
	speed: 0.1022s/iter; left time: 249.8222s
	iters: 200, epoch: 3 | loss: 0.2391940
	speed: 0.0951s/iter; left time: 223.0925s
	iters: 300, epoch: 3 | loss: 0.2093594
	speed: 0.0876s/iter; left time: 196.7367s
Epoch: 3 cost time: 30.214820623397827
Epoch: 3, Steps: 318 | Train Loss: 0.2145473 Vali Loss: 0.2197849 Test Loss: 0.2111993
Validation loss decreased (0.232912 --> 0.219785).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1777622
	speed: 0.0733s/iter; left time: 155.8234s
	iters: 200, epoch: 4 | loss: 0.2076332
	speed: 0.0579s/iter; left time: 117.2824s
	iters: 300, epoch: 4 | loss: 0.2262300
	speed: 0.0922s/iter; left time: 177.5779s
Epoch: 4 cost time: 23.75246000289917
Epoch: 4, Steps: 318 | Train Loss: 0.2060161 Vali Loss: 0.2182951 Test Loss: 0.2093228
Validation loss decreased (0.219785 --> 0.218295).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2687032
	speed: 0.0812s/iter; left time: 146.8396s
	iters: 200, epoch: 5 | loss: 0.1933338
	speed: 0.0984s/iter; left time: 168.1070s
	iters: 300, epoch: 5 | loss: 0.2114874
	speed: 0.0718s/iter; left time: 115.4744s
Epoch: 5 cost time: 27.188284158706665
Epoch: 5, Steps: 318 | Train Loss: 0.2020482 Vali Loss: 0.2158398 Test Loss: 0.2083661
Validation loss decreased (0.218295 --> 0.215840).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1515787
	speed: 0.1021s/iter; left time: 152.1957s
	iters: 200, epoch: 6 | loss: 0.2239861
	speed: 0.0742s/iter; left time: 103.1484s
	iters: 300, epoch: 6 | loss: 0.2514190
	speed: 0.0871s/iter; left time: 112.4477s
Epoch: 6 cost time: 26.886465311050415
Epoch: 6, Steps: 318 | Train Loss: 0.2007350 Vali Loss: 0.2157129 Test Loss: 0.2068530
Validation loss decreased (0.215840 --> 0.215713).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.3156638
	speed: 0.0774s/iter; left time: 90.8433s
	iters: 200, epoch: 7 | loss: 0.2126773
	speed: 0.0831s/iter; left time: 89.1337s
	iters: 300, epoch: 7 | loss: 0.1454333
	speed: 0.0723s/iter; left time: 70.3517s
Epoch: 7 cost time: 23.901348114013672
Epoch: 7, Steps: 318 | Train Loss: 0.1999479 Vali Loss: 0.2154146 Test Loss: 0.2067089
Validation loss decreased (0.215713 --> 0.215415).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2103359
	speed: 0.0986s/iter; left time: 84.3425s
	iters: 200, epoch: 8 | loss: 0.2006307
	speed: 0.0738s/iter; left time: 55.6890s
	iters: 300, epoch: 8 | loss: 0.1942107
	speed: 0.0641s/iter; left time: 41.9685s
Epoch: 8 cost time: 25.19952940940857
Epoch: 8, Steps: 318 | Train Loss: 0.1987529 Vali Loss: 0.2149890 Test Loss: 0.2066706
Validation loss decreased (0.215415 --> 0.214989).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1423190
	speed: 0.0966s/iter; left time: 51.8704s
	iters: 200, epoch: 9 | loss: 0.1529240
	speed: 0.0331s/iter; left time: 14.4511s
	iters: 300, epoch: 9 | loss: 0.2146145
	speed: 0.0848s/iter; left time: 28.5856s
Epoch: 9 cost time: 23.4882173538208
Epoch: 9, Steps: 318 | Train Loss: 0.1983875 Vali Loss: 0.2151032 Test Loss: 0.2066376
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2454946
	speed: 0.0594s/iter; left time: 13.0026s
	iters: 200, epoch: 10 | loss: 0.2315827
	speed: 0.0856s/iter; left time: 10.1849s
	iters: 300, epoch: 10 | loss: 0.2177556
	speed: 0.0668s/iter; left time: 1.2683s
Epoch: 10 cost time: 21.550389051437378
Epoch: 10, Steps: 318 | Train Loss: 0.1986587 Vali Loss: 0.2141388 Test Loss: 0.2066174
Validation loss decreased (0.214989 --> 0.214139).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.45341384410858154, mae:0.5288776755332947, rmse:0.6733601093292236, mape:0.01888509839773178, mspe:0.0005830716690979898, rse:0.4718143045902252, r2_score:0.732470868388495, acc:0.9811149016022682
corr: [36.65793  36.95301  36.97885  37.219704 37.112644 37.118874 37.293503
 37.520428 37.601704 37.547695]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3801989
	speed: 0.0917s/iter; left time: 282.3831s
	iters: 200, epoch: 1 | loss: 0.3098021
	speed: 0.0412s/iter; left time: 122.9628s
	iters: 300, epoch: 1 | loss: 0.4362741
	speed: 0.0421s/iter; left time: 121.4183s
Epoch: 1 cost time: 18.189074754714966
Epoch: 1, Steps: 318 | Train Loss: 0.4665766 Vali Loss: 0.2899345 Test Loss: 0.2748718
Validation loss decreased (inf --> 0.289934).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.3089074
	speed: 0.0640s/iter; left time: 176.7433s
	iters: 200, epoch: 2 | loss: 0.1892241
	speed: 0.0680s/iter; left time: 181.0843s
	iters: 300, epoch: 2 | loss: 0.2553063
	speed: 0.0600s/iter; left time: 153.7935s
Epoch: 2 cost time: 19.43789005279541
Epoch: 2, Steps: 318 | Train Loss: 0.2778368 Vali Loss: 0.2579656 Test Loss: 0.2455029
Validation loss decreased (0.289934 --> 0.257966).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2722037
	speed: 0.0763s/iter; left time: 186.4610s
	iters: 200, epoch: 3 | loss: 0.2440697
	speed: 0.0640s/iter; left time: 150.0556s
	iters: 300, epoch: 3 | loss: 0.1865717
	speed: 0.0480s/iter; left time: 107.8635s
Epoch: 3 cost time: 19.354416131973267
Epoch: 3, Steps: 318 | Train Loss: 0.2529796 Vali Loss: 0.2391339 Test Loss: 0.2333588
Validation loss decreased (0.257966 --> 0.239134).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2863797
	speed: 0.0733s/iter; left time: 155.9288s
	iters: 200, epoch: 4 | loss: 0.2501284
	speed: 0.0511s/iter; left time: 103.6026s
	iters: 300, epoch: 4 | loss: 0.2263116
	speed: 0.0561s/iter; left time: 108.0301s
Epoch: 4 cost time: 19.735734939575195
Epoch: 4, Steps: 318 | Train Loss: 0.2412485 Vali Loss: 0.2333852 Test Loss: 0.2267681
Validation loss decreased (0.239134 --> 0.233385).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1879982
	speed: 0.0813s/iter; left time: 147.0630s
	iters: 200, epoch: 5 | loss: 0.2148905
	speed: 0.0479s/iter; left time: 81.9179s
	iters: 300, epoch: 5 | loss: 0.2061981
	speed: 0.0654s/iter; left time: 105.3062s
Epoch: 5 cost time: 20.609743118286133
Epoch: 5, Steps: 318 | Train Loss: 0.2347432 Vali Loss: 0.2288308 Test Loss: 0.2221254
Validation loss decreased (0.233385 --> 0.228831).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2810613
	speed: 0.0668s/iter; left time: 99.5602s
	iters: 200, epoch: 6 | loss: 0.2433633
	speed: 0.0331s/iter; left time: 46.0196s
	iters: 300, epoch: 6 | loss: 0.2825223
	speed: 0.0386s/iter; left time: 49.8491s
Epoch: 6 cost time: 14.266897678375244
Epoch: 6, Steps: 318 | Train Loss: 0.2319616 Vali Loss: 0.2269330 Test Loss: 0.2211987
Validation loss decreased (0.228831 --> 0.226933).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1777882
	speed: 0.0660s/iter; left time: 77.4048s
	iters: 200, epoch: 7 | loss: 0.2093166
	speed: 0.0499s/iter; left time: 53.5197s
	iters: 300, epoch: 7 | loss: 0.1952986
	speed: 0.0566s/iter; left time: 55.0687s
Epoch: 7 cost time: 18.095831155776978
Epoch: 7, Steps: 318 | Train Loss: 0.2296055 Vali Loss: 0.2269201 Test Loss: 0.2202095
Validation loss decreased (0.226933 --> 0.226920).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.3054423
	speed: 0.0715s/iter; left time: 61.1225s
	iters: 200, epoch: 8 | loss: 0.2132707
	speed: 0.0550s/iter; left time: 41.5048s
	iters: 300, epoch: 8 | loss: 0.1996516
	speed: 0.0467s/iter; left time: 30.5975s
Epoch: 8 cost time: 18.35771155357361
Epoch: 8, Steps: 318 | Train Loss: 0.2292027 Vali Loss: 0.2262070 Test Loss: 0.2195995
Validation loss decreased (0.226920 --> 0.226207).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2531539
	speed: 0.0637s/iter; left time: 34.1900s
	iters: 200, epoch: 9 | loss: 0.2724993
	speed: 0.0619s/iter; left time: 27.0628s
	iters: 300, epoch: 9 | loss: 0.1705255
	speed: 0.0372s/iter; left time: 12.5197s
Epoch: 9 cost time: 16.967169523239136
Epoch: 9, Steps: 318 | Train Loss: 0.2288462 Vali Loss: 0.2257040 Test Loss: 0.2193573
Validation loss decreased (0.226207 --> 0.225704).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2400939
	speed: 0.0470s/iter; left time: 10.2943s
	iters: 200, epoch: 10 | loss: 0.1857344
	speed: 0.0365s/iter; left time: 4.3435s
	iters: 300, epoch: 10 | loss: 0.2300908
	speed: 0.0789s/iter; left time: 1.4997s
Epoch: 10 cost time: 16.91807723045349
Epoch: 10, Steps: 318 | Train Loss: 0.2286687 Vali Loss: 0.2263455 Test Loss: 0.2192248
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.4813710153102875, mae:0.5549411773681641, rmse:0.6938090920448303, mape:0.019853021949529648, mspe:0.0006250312435440719, rse:0.4859136939048767, r2_score:0.7057711898862258, acc:0.9801469780504704
corr: [37.635265 37.786144 37.570442 37.806995 37.72318  37.9628   37.733727
 37.945217 37.80073  37.958565 38.090977 38.176254 38.39526  38.18972
 38.360184]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3954719
	speed: 0.0735s/iter; left time: 226.5133s
	iters: 200, epoch: 1 | loss: 0.2989163
	speed: 0.0560s/iter; left time: 166.8707s
	iters: 300, epoch: 1 | loss: 0.2124656
	speed: 0.0824s/iter; left time: 237.4750s
Epoch: 1 cost time: 21.795135021209717
Epoch: 1, Steps: 318 | Train Loss: 0.4923412 Vali Loss: 0.2763873 Test Loss: 0.2629000
Validation loss decreased (inf --> 0.276387).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2886947
	speed: 0.1082s/iter; left time: 298.9282s
	iters: 200, epoch: 2 | loss: 0.2691713
	speed: 0.0877s/iter; left time: 233.4398s
	iters: 300, epoch: 2 | loss: 0.2587795
	speed: 0.0850s/iter; left time: 217.8654s
Epoch: 2 cost time: 29.04843521118164
Epoch: 2, Steps: 318 | Train Loss: 0.2608763 Vali Loss: 0.2361900 Test Loss: 0.2447722
Validation loss decreased (0.276387 --> 0.236190).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2960669
	speed: 0.1001s/iter; left time: 244.7089s
	iters: 200, epoch: 3 | loss: 0.2437556
	speed: 0.1105s/iter; left time: 259.1503s
	iters: 300, epoch: 3 | loss: 0.2285155
	speed: 0.0962s/iter; left time: 215.8896s
Epoch: 3 cost time: 31.834384202957153
Epoch: 3, Steps: 318 | Train Loss: 0.2410159 Vali Loss: 0.2228568 Test Loss: 0.2383634
Validation loss decreased (0.236190 --> 0.222857).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2120449
	speed: 0.1109s/iter; left time: 235.7879s
	iters: 200, epoch: 4 | loss: 0.3109505
	speed: 0.1074s/iter; left time: 217.6359s
	iters: 300, epoch: 4 | loss: 0.2312391
	speed: 0.0874s/iter; left time: 168.5027s
Epoch: 4 cost time: 32.60718321800232
Epoch: 4, Steps: 318 | Train Loss: 0.2329461 Vali Loss: 0.2182779 Test Loss: 0.2399033
Validation loss decreased (0.222857 --> 0.218278).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2595710
	speed: 0.0897s/iter; left time: 162.2158s
	iters: 200, epoch: 5 | loss: 0.2722620
	speed: 0.0974s/iter; left time: 166.4453s
	iters: 300, epoch: 5 | loss: 0.2361845
	speed: 0.1054s/iter; left time: 169.5603s
Epoch: 5 cost time: 30.462685346603394
Epoch: 5, Steps: 318 | Train Loss: 0.2291457 Vali Loss: 0.2153168 Test Loss: 0.2387341
Validation loss decreased (0.218278 --> 0.215317).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2419741
	speed: 0.1161s/iter; left time: 173.1644s
	iters: 200, epoch: 6 | loss: 0.1954655
	speed: 0.1085s/iter; left time: 150.9075s
	iters: 300, epoch: 6 | loss: 0.3501547
	speed: 0.1142s/iter; left time: 147.4145s
Epoch: 6 cost time: 35.10221719741821
Epoch: 6, Steps: 318 | Train Loss: 0.2272722 Vali Loss: 0.2130551 Test Loss: 0.2392344
Validation loss decreased (0.215317 --> 0.213055).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1966673
	speed: 0.1039s/iter; left time: 121.8938s
	iters: 200, epoch: 7 | loss: 0.1976856
	speed: 0.0854s/iter; left time: 91.6361s
	iters: 300, epoch: 7 | loss: 0.1801411
	speed: 0.0998s/iter; left time: 97.0877s
Epoch: 7 cost time: 30.79831600189209
Epoch: 7, Steps: 318 | Train Loss: 0.2267997 Vali Loss: 0.2133533 Test Loss: 0.2397180
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2270603
	speed: 0.1066s/iter; left time: 91.1296s
	iters: 200, epoch: 8 | loss: 0.1839262
	speed: 0.1022s/iter; left time: 77.1276s
	iters: 300, epoch: 8 | loss: 0.2946888
	speed: 0.1044s/iter; left time: 68.3741s
Epoch: 8 cost time: 32.48527383804321
Epoch: 8, Steps: 318 | Train Loss: 0.2262885 Vali Loss: 0.2130201 Test Loss: 0.2399963
Validation loss decreased (0.213055 --> 0.213020).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1653045
	speed: 0.1082s/iter; left time: 58.0866s
	iters: 200, epoch: 9 | loss: 0.2265245
	speed: 0.1036s/iter; left time: 45.2765s
	iters: 300, epoch: 9 | loss: 0.2037607
	speed: 0.0938s/iter; left time: 31.6250s
Epoch: 9 cost time: 32.41476511955261
Epoch: 9, Steps: 318 | Train Loss: 0.2257766 Vali Loss: 0.2142204 Test Loss: 0.2400839
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2138685
	speed: 0.1107s/iter; left time: 24.2342s
	iters: 200, epoch: 10 | loss: 0.2802265
	speed: 0.0756s/iter; left time: 8.9948s
	iters: 300, epoch: 10 | loss: 0.2425721
	speed: 0.0758s/iter; left time: 1.4409s
Epoch: 10 cost time: 27.87732219696045
Epoch: 10, Steps: 318 | Train Loss: 0.2255882 Vali Loss: 0.2132216 Test Loss: 0.2400477
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.5266624093055725, mae:0.5706024169921875, rmse:0.7257151007652283, mape:0.02037421055138111, mspe:0.0006741368561051786, rse:0.5079649686813354, r2_score:0.6886969878494134, acc:0.9796257894486189
corr: [38.167946 37.548973 37.76405  37.611736 37.687004 37.442173 37.32992
 37.12294  37.283012 37.250782 37.26897  37.297054 37.22163  37.015415
 36.892918 36.78317  36.901123 36.599373 36.99083  37.330765]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4219940
	speed: 0.1039s/iter; left time: 320.2251s
	iters: 200, epoch: 1 | loss: 0.2448778
	speed: 0.0767s/iter; left time: 228.6257s
	iters: 300, epoch: 1 | loss: 0.3364530
	speed: 0.0710s/iter; left time: 204.5540s
Epoch: 1 cost time: 26.554996728897095
Epoch: 1, Steps: 318 | Train Loss: 0.4312306 Vali Loss: 0.2771015 Test Loss: 0.2470102
Validation loss decreased (inf --> 0.277101).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2563195
	speed: 0.0937s/iter; left time: 258.8846s
	iters: 200, epoch: 2 | loss: 0.2812950
	speed: 0.0712s/iter; left time: 189.6262s
	iters: 300, epoch: 2 | loss: 0.2854964
	speed: 0.0856s/iter; left time: 219.5041s
Epoch: 2 cost time: 26.528447151184082
Epoch: 2, Steps: 318 | Train Loss: 0.2506461 Vali Loss: 0.2614631 Test Loss: 0.2389552
Validation loss decreased (0.277101 --> 0.261463).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2114304
	speed: 0.0849s/iter; left time: 207.6099s
	iters: 200, epoch: 3 | loss: 0.2240200
	speed: 0.0929s/iter; left time: 217.9269s
	iters: 300, epoch: 3 | loss: 0.1877610
	speed: 0.0752s/iter; left time: 168.7561s
Epoch: 3 cost time: 27.002333641052246
Epoch: 3, Steps: 318 | Train Loss: 0.2330808 Vali Loss: 0.2574915 Test Loss: 0.2406727
Validation loss decreased (0.261463 --> 0.257491).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2586557
	speed: 0.0864s/iter; left time: 183.8099s
	iters: 200, epoch: 4 | loss: 0.2015159
	speed: 0.0742s/iter; left time: 150.3306s
	iters: 300, epoch: 4 | loss: 0.2090420
	speed: 0.0704s/iter; left time: 135.7184s
Epoch: 4 cost time: 24.084460020065308
Epoch: 4, Steps: 318 | Train Loss: 0.2271408 Vali Loss: 0.2549243 Test Loss: 0.2382696
Validation loss decreased (0.257491 --> 0.254924).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1619118
	speed: 0.0869s/iter; left time: 157.1454s
	iters: 200, epoch: 5 | loss: 0.2264648
	speed: 0.0827s/iter; left time: 141.2936s
	iters: 300, epoch: 5 | loss: 0.2381930
	speed: 0.0563s/iter; left time: 90.6615s
Epoch: 5 cost time: 23.96847367286682
Epoch: 5, Steps: 318 | Train Loss: 0.2241868 Vali Loss: 0.2541010 Test Loss: 0.2383835
Validation loss decreased (0.254924 --> 0.254101).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2198034
	speed: 0.0883s/iter; left time: 131.6041s
	iters: 200, epoch: 6 | loss: 0.2133861
	speed: 0.0686s/iter; left time: 95.4622s
	iters: 300, epoch: 6 | loss: 0.2287624
	speed: 0.0871s/iter; left time: 112.4303s
Epoch: 6 cost time: 25.83483910560608
Epoch: 6, Steps: 318 | Train Loss: 0.2230133 Vali Loss: 0.2540122 Test Loss: 0.2383837
Validation loss decreased (0.254101 --> 0.254012).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1958389
	speed: 0.0928s/iter; left time: 108.9010s
	iters: 200, epoch: 7 | loss: 0.1908567
	speed: 0.0862s/iter; left time: 92.4916s
	iters: 300, epoch: 7 | loss: 0.2487826
	speed: 0.0733s/iter; left time: 71.2911s
Epoch: 7 cost time: 26.344581127166748
Epoch: 7, Steps: 318 | Train Loss: 0.2225627 Vali Loss: 0.2524238 Test Loss: 0.2381725
Validation loss decreased (0.254012 --> 0.252424).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2293448
	speed: 0.0797s/iter; left time: 68.1764s
	iters: 200, epoch: 8 | loss: 0.2565181
	speed: 0.0866s/iter; left time: 65.3846s
	iters: 300, epoch: 8 | loss: 0.1716857
	speed: 0.0674s/iter; left time: 44.1713s
Epoch: 8 cost time: 23.802477598190308
Epoch: 8, Steps: 318 | Train Loss: 0.2224179 Vali Loss: 0.2535316 Test Loss: 0.2381594
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2670024
	speed: 0.0864s/iter; left time: 46.3705s
	iters: 200, epoch: 9 | loss: 0.2365535
	speed: 0.0849s/iter; left time: 37.0855s
	iters: 300, epoch: 9 | loss: 0.2107957
	speed: 0.0679s/iter; left time: 22.8835s
Epoch: 9 cost time: 24.970577001571655
Epoch: 9, Steps: 318 | Train Loss: 0.2219751 Vali Loss: 0.2514518 Test Loss: 0.2386248
Validation loss decreased (0.252424 --> 0.251452).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1914099
	speed: 0.0872s/iter; left time: 19.0870s
	iters: 200, epoch: 10 | loss: 0.1823909
	speed: 0.0936s/iter; left time: 11.1385s
	iters: 300, epoch: 10 | loss: 0.2134191
	speed: 0.0912s/iter; left time: 1.7332s
Epoch: 10 cost time: 28.662314653396606
Epoch: 10, Steps: 318 | Train Loss: 0.2214914 Vali Loss: 0.2526102 Test Loss: 0.2385927
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.5236527919769287, mae:0.5680801868438721, rmse:0.7236385941505432, mape:0.020294129848480225, mspe:0.0006736404611729085, rse:0.5061731338500977, r2_score:0.694942416740831, acc:0.9797058701515198
corr: [37.10214  37.1318   37.133133 37.209396 37.180855 37.31635  37.17946
 37.313614 37.316624 37.419067 37.32343  37.56559  37.474194 37.14757
 37.35256  37.225655 37.266773 37.231346 37.400208 37.60846  37.405224
 37.393475 37.854164 37.7003   37.992157]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4299647
	speed: 0.1034s/iter; left time: 317.5947s
	iters: 200, epoch: 1 | loss: 0.2592629
	speed: 0.0801s/iter; left time: 238.0288s
	iters: 300, epoch: 1 | loss: 0.3312840
	speed: 0.0805s/iter; left time: 231.0435s
Epoch: 1 cost time: 27.732163429260254
Epoch: 1, Steps: 317 | Train Loss: 0.4401359 Vali Loss: 0.2465729 Test Loss: 0.3062332
Validation loss decreased (inf --> 0.246573).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2856844
	speed: 0.0792s/iter; left time: 217.9884s
	iters: 200, epoch: 2 | loss: 0.2208782
	speed: 0.0879s/iter; left time: 233.3693s
	iters: 300, epoch: 2 | loss: 0.2563108
	speed: 0.0750s/iter; left time: 191.6197s
Epoch: 2 cost time: 25.85141134262085
Epoch: 2, Steps: 317 | Train Loss: 0.2452116 Vali Loss: 0.2180900 Test Loss: 0.3343073
Validation loss decreased (0.246573 --> 0.218090).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2109893
	speed: 0.0836s/iter; left time: 203.7017s
	iters: 200, epoch: 3 | loss: 0.2386338
	speed: 0.0739s/iter; left time: 172.7252s
	iters: 300, epoch: 3 | loss: 0.2199850
	speed: 0.0741s/iter; left time: 165.8377s
Epoch: 3 cost time: 24.516415119171143
Epoch: 3, Steps: 317 | Train Loss: 0.2279147 Vali Loss: 0.2112802 Test Loss: 0.3520926
Validation loss decreased (0.218090 --> 0.211280).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1930025
	speed: 0.0850s/iter; left time: 180.1170s
	iters: 200, epoch: 4 | loss: 0.1849798
	speed: 0.0849s/iter; left time: 171.5628s
	iters: 300, epoch: 4 | loss: 0.2024871
	speed: 0.0615s/iter; left time: 117.9996s
Epoch: 4 cost time: 25.08046317100525
Epoch: 4, Steps: 317 | Train Loss: 0.2232069 Vali Loss: 0.2125976 Test Loss: 0.3303671
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2143287
	speed: 0.0958s/iter; left time: 172.7113s
	iters: 200, epoch: 5 | loss: 0.2338907
	speed: 0.0745s/iter; left time: 126.9221s
	iters: 300, epoch: 5 | loss: 0.2210857
	speed: 0.0629s/iter; left time: 100.8985s
Epoch: 5 cost time: 24.67881751060486
Epoch: 5, Steps: 317 | Train Loss: 0.2197367 Vali Loss: 0.2104070 Test Loss: 0.3333350
Validation loss decreased (0.211280 --> 0.210407).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1884587
	speed: 0.0979s/iter; left time: 145.5384s
	iters: 200, epoch: 6 | loss: 0.2247144
	speed: 0.0924s/iter; left time: 128.0801s
	iters: 300, epoch: 6 | loss: 0.2717211
	speed: 0.0562s/iter; left time: 72.2980s
Epoch: 6 cost time: 26.21351957321167
Epoch: 6, Steps: 317 | Train Loss: 0.2198361 Vali Loss: 0.2105920 Test Loss: 0.3234915
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2334169
	speed: 0.0869s/iter; left time: 101.5562s
	iters: 200, epoch: 7 | loss: 0.2427254
	speed: 0.0838s/iter; left time: 89.5791s
	iters: 300, epoch: 7 | loss: 0.2362448
	speed: 0.0876s/iter; left time: 84.8679s
Epoch: 7 cost time: 27.190059423446655
Epoch: 7, Steps: 317 | Train Loss: 0.2182089 Vali Loss: 0.2164287 Test Loss: 0.3225477
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1843171
	speed: 0.0913s/iter; left time: 77.7950s
	iters: 200, epoch: 8 | loss: 0.2477642
	speed: 0.0781s/iter; left time: 58.7314s
	iters: 300, epoch: 8 | loss: 0.2295620
	speed: 0.0834s/iter; left time: 54.4011s
Epoch: 8 cost time: 26.949400186538696
Epoch: 8, Steps: 317 | Train Loss: 0.2161941 Vali Loss: 0.2105786 Test Loss: 0.3224108
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.7314904928207397, mae:0.6604695320129395, rmse:0.8552721738815308, mape:0.023510843515396118, mspe:0.0009270896553061903, rse:0.5978190898895264, r2_score:0.5094491374601474, acc:0.9764891564846039
corr: [34.799553 35.333767 35.27501  35.451954 35.469864 35.796654 35.891846
 36.16661  36.129078 36.457497 36.601624 36.131683 36.668793 36.70212
 36.36869  36.737583 35.69625  35.847343 35.478573 34.83147  35.666035
 35.72526  34.884167 36.180714 35.755108 35.498714 35.718086 35.33288
 34.974834 34.629803]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4684001
	speed: 0.1199s/iter; left time: 369.2715s
	iters: 200, epoch: 1 | loss: 0.3506200
	speed: 0.0822s/iter; left time: 245.1405s
	iters: 300, epoch: 1 | loss: 0.2537333
	speed: 0.0909s/iter; left time: 261.8455s
Epoch: 1 cost time: 30.7765691280365
Epoch: 1, Steps: 318 | Train Loss: 0.4613434 Vali Loss: 0.2654969 Test Loss: 0.2435665
Validation loss decreased (inf --> 0.265497).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2154082
	speed: 0.1039s/iter; left time: 287.1707s
	iters: 200, epoch: 2 | loss: 0.2495267
	speed: 0.1158s/iter; left time: 308.4862s
	iters: 300, epoch: 2 | loss: 0.2475274
	speed: 0.1087s/iter; left time: 278.4825s
Epoch: 2 cost time: 35.09057807922363
Epoch: 2, Steps: 318 | Train Loss: 0.2489866 Vali Loss: 0.2198947 Test Loss: 0.2038537
Validation loss decreased (0.265497 --> 0.219895).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2569728
	speed: 0.1174s/iter; left time: 287.1187s
	iters: 200, epoch: 3 | loss: 0.2230451
	speed: 0.1133s/iter; left time: 265.6084s
	iters: 300, epoch: 3 | loss: 0.2066902
	speed: 0.1110s/iter; left time: 249.1088s
Epoch: 3 cost time: 36.39563703536987
Epoch: 3, Steps: 318 | Train Loss: 0.2140228 Vali Loss: 0.2033434 Test Loss: 0.1873839
Validation loss decreased (0.219895 --> 0.203343).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1723503
	speed: 0.0991s/iter; left time: 210.8024s
	iters: 200, epoch: 4 | loss: 0.2065537
	speed: 0.1109s/iter; left time: 224.8956s
	iters: 300, epoch: 4 | loss: 0.2068514
	speed: 0.1117s/iter; left time: 215.2660s
Epoch: 4 cost time: 33.97878360748291
Epoch: 4, Steps: 318 | Train Loss: 0.2026357 Vali Loss: 0.2028141 Test Loss: 0.1831090
Validation loss decreased (0.203343 --> 0.202814).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2830180
	speed: 0.0967s/iter; left time: 174.8452s
	iters: 200, epoch: 5 | loss: 0.2230978
	speed: 0.0972s/iter; left time: 166.1909s
	iters: 300, epoch: 5 | loss: 0.2086649
	speed: 0.1090s/iter; left time: 175.3195s
Epoch: 5 cost time: 32.14201259613037
Epoch: 5, Steps: 318 | Train Loss: 0.1974199 Vali Loss: 0.1996071 Test Loss: 0.1800086
Validation loss decreased (0.202814 --> 0.199607).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1480469
	speed: 0.1110s/iter; left time: 165.5225s
	iters: 200, epoch: 6 | loss: 0.2128797
	speed: 0.1057s/iter; left time: 147.0111s
	iters: 300, epoch: 6 | loss: 0.2394250
	speed: 0.1019s/iter; left time: 131.5728s
Epoch: 6 cost time: 33.592512130737305
Epoch: 6, Steps: 318 | Train Loss: 0.1960061 Vali Loss: 0.1996439 Test Loss: 0.1789801
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.3064865
	speed: 0.1046s/iter; left time: 122.6835s
	iters: 200, epoch: 7 | loss: 0.2362262
	speed: 0.1130s/iter; left time: 121.3000s
	iters: 300, epoch: 7 | loss: 0.1328687
	speed: 0.1193s/iter; left time: 116.0777s
Epoch: 7 cost time: 35.8719322681427
Epoch: 7, Steps: 318 | Train Loss: 0.1946589 Vali Loss: 0.1991804 Test Loss: 0.1786481
Validation loss decreased (0.199607 --> 0.199180).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2095399
	speed: 0.1282s/iter; left time: 109.6196s
	iters: 200, epoch: 8 | loss: 0.1890770
	speed: 0.0949s/iter; left time: 71.6131s
	iters: 300, epoch: 8 | loss: 0.1812153
	speed: 0.1034s/iter; left time: 67.7450s
Epoch: 8 cost time: 33.96196150779724
Epoch: 8, Steps: 318 | Train Loss: 0.1937606 Vali Loss: 0.1995621 Test Loss: 0.1785655
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1289677
	speed: 0.1100s/iter; left time: 59.0630s
	iters: 200, epoch: 9 | loss: 0.1468160
	speed: 0.1034s/iter; left time: 45.1762s
	iters: 300, epoch: 9 | loss: 0.2102233
	speed: 0.1045s/iter; left time: 35.2161s
Epoch: 9 cost time: 33.72935628890991
Epoch: 9, Steps: 318 | Train Loss: 0.1930960 Vali Loss: 0.1999287 Test Loss: 0.1784147
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2364881
	speed: 0.1170s/iter; left time: 25.6309s
	iters: 200, epoch: 10 | loss: 0.2254202
	speed: 0.1076s/iter; left time: 12.8081s
	iters: 300, epoch: 10 | loss: 0.1959244
	speed: 0.1100s/iter; left time: 2.0900s
Epoch: 10 cost time: 34.63443636894226
Epoch: 10, Steps: 318 | Train Loss: 0.1932254 Vali Loss: 0.1988067 Test Loss: 0.1783531
Validation loss decreased (0.199180 --> 0.198807).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.391388863325119, mae:0.4955815374851227, rmse:0.6256107687950134, mape:0.017720477655529976, mspe:0.0005066462326794863, rse:0.43835699558258057, r2_score:0.7796597950609716, acc:0.98227952234447
corr: [37.822426 37.7734   37.603287 37.658016 37.72174  37.30464  37.494984
 37.752262 37.610847 37.908226]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3613183
	speed: 0.1148s/iter; left time: 353.7305s
	iters: 200, epoch: 1 | loss: 0.2851735
	speed: 0.0779s/iter; left time: 232.1903s
	iters: 300, epoch: 1 | loss: 0.3402781
	speed: 0.0845s/iter; left time: 243.3703s
Epoch: 1 cost time: 29.634344577789307
Epoch: 1, Steps: 318 | Train Loss: 0.4167997 Vali Loss: 0.2322874 Test Loss: 0.2082206
Validation loss decreased (inf --> 0.232287).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2147748
	speed: 0.0777s/iter; left time: 214.6292s
	iters: 200, epoch: 2 | loss: 0.1502299
	speed: 0.0891s/iter; left time: 237.3765s
	iters: 300, epoch: 2 | loss: 0.1833367
	speed: 0.0874s/iter; left time: 223.9465s
Epoch: 2 cost time: 26.852092266082764
Epoch: 2, Steps: 318 | Train Loss: 0.2205233 Vali Loss: 0.2128339 Test Loss: 0.1839263
Validation loss decreased (0.232287 --> 0.212834).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1877332
	speed: 0.0946s/iter; left time: 231.1790s
	iters: 200, epoch: 3 | loss: 0.1753247
	speed: 0.0806s/iter; left time: 188.9560s
	iters: 300, epoch: 3 | loss: 0.1882745
	speed: 0.0807s/iter; left time: 181.1207s
Epoch: 3 cost time: 27.422647953033447
Epoch: 3, Steps: 318 | Train Loss: 0.2042153 Vali Loss: 0.2054357 Test Loss: 0.1779782
Validation loss decreased (0.212834 --> 0.205436).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2597169
	speed: 0.0808s/iter; left time: 171.8431s
	iters: 200, epoch: 4 | loss: 0.1912349
	speed: 0.0777s/iter; left time: 157.5067s
	iters: 300, epoch: 4 | loss: 0.1961119
	speed: 0.0932s/iter; left time: 179.5418s
Epoch: 4 cost time: 26.637190103530884
Epoch: 4, Steps: 318 | Train Loss: 0.1983883 Vali Loss: 0.2044685 Test Loss: 0.1752327
Validation loss decreased (0.205436 --> 0.204469).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1714545
	speed: 0.0876s/iter; left time: 158.4845s
	iters: 200, epoch: 5 | loss: 0.1970069
	speed: 0.0827s/iter; left time: 141.3683s
	iters: 300, epoch: 5 | loss: 0.2050318
	speed: 0.0748s/iter; left time: 120.3794s
Epoch: 5 cost time: 25.244314432144165
Epoch: 5, Steps: 318 | Train Loss: 0.1959771 Vali Loss: 0.2029807 Test Loss: 0.1734826
Validation loss decreased (0.204469 --> 0.202981).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2244045
	speed: 0.0984s/iter; left time: 146.6827s
	iters: 200, epoch: 6 | loss: 0.1849066
	speed: 0.0508s/iter; left time: 70.6650s
	iters: 300, epoch: 6 | loss: 0.2532642
	speed: 0.0808s/iter; left time: 104.3054s
Epoch: 6 cost time: 24.750930547714233
Epoch: 6, Steps: 318 | Train Loss: 0.1945339 Vali Loss: 0.2036849 Test Loss: 0.1746468
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1529670
	speed: 0.0942s/iter; left time: 110.4706s
	iters: 200, epoch: 7 | loss: 0.1940667
	speed: 0.0882s/iter; left time: 94.6627s
	iters: 300, epoch: 7 | loss: 0.1685555
	speed: 0.0844s/iter; left time: 82.1040s
Epoch: 7 cost time: 28.315762758255005
Epoch: 7, Steps: 318 | Train Loss: 0.1940248 Vali Loss: 0.2024800 Test Loss: 0.1736298
Validation loss decreased (0.202981 --> 0.202480).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2384453
	speed: 0.1030s/iter; left time: 88.0874s
	iters: 200, epoch: 8 | loss: 0.2141479
	speed: 0.0867s/iter; left time: 65.4693s
	iters: 300, epoch: 8 | loss: 0.1662261
	speed: 0.0917s/iter; left time: 60.0419s
Epoch: 8 cost time: 30.1324565410614
Epoch: 8, Steps: 318 | Train Loss: 0.1938458 Vali Loss: 0.2017898 Test Loss: 0.1731164
Validation loss decreased (0.202480 --> 0.201790).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2106747
	speed: 0.0947s/iter; left time: 50.8605s
	iters: 200, epoch: 9 | loss: 0.2094176
	speed: 0.0815s/iter; left time: 35.6227s
	iters: 300, epoch: 9 | loss: 0.1640669
	speed: 0.0915s/iter; left time: 30.8447s
Epoch: 9 cost time: 27.698630332946777
Epoch: 9, Steps: 318 | Train Loss: 0.1935949 Vali Loss: 0.2012568 Test Loss: 0.1730071
Validation loss decreased (0.201790 --> 0.201257).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2049454
	speed: 0.0864s/iter; left time: 18.9231s
	iters: 200, epoch: 10 | loss: 0.1401643
	speed: 0.0845s/iter; left time: 10.0496s
	iters: 300, epoch: 10 | loss: 0.1934470
	speed: 0.0786s/iter; left time: 1.4940s
Epoch: 10 cost time: 26.20868492126465
Epoch: 10, Steps: 318 | Train Loss: 0.1934086 Vali Loss: 0.2028361 Test Loss: 0.1729469
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.3796573281288147, mae:0.48310208320617676, rmse:0.6161633729934692, mape:0.017284125089645386, mspe:0.0004927506088279188, rse:0.431534081697464, r2_score:0.7820830095022081, acc:0.9827158749103546
corr: [37.373177 37.431137 37.340096 37.41187  37.435654 37.524143 37.725418
 37.812557 37.5849   37.7282   37.49561  37.522823 37.576614 37.608776
 37.386395]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4065227
	speed: 0.1165s/iter; left time: 358.9502s
	iters: 200, epoch: 1 | loss: 0.2967777
	speed: 0.0864s/iter; left time: 257.5850s
	iters: 300, epoch: 1 | loss: 0.2053920
	speed: 0.0926s/iter; left time: 266.7368s
Epoch: 1 cost time: 30.910436630249023
Epoch: 1, Steps: 318 | Train Loss: 0.4726847 Vali Loss: 0.2661033 Test Loss: 0.2470726
Validation loss decreased (inf --> 0.266103).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2639317
	speed: 0.0921s/iter; left time: 254.3972s
	iters: 200, epoch: 2 | loss: 0.2562635
	speed: 0.0777s/iter; left time: 206.8383s
	iters: 300, epoch: 2 | loss: 0.2429833
	speed: 0.0725s/iter; left time: 185.7604s
Epoch: 2 cost time: 25.61740803718567
Epoch: 2, Steps: 318 | Train Loss: 0.2518367 Vali Loss: 0.2155852 Test Loss: 0.2140192
Validation loss decreased (0.266103 --> 0.215585).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2791731
	speed: 0.0861s/iter; left time: 210.6082s
	iters: 200, epoch: 3 | loss: 0.2356756
	speed: 0.0768s/iter; left time: 179.9894s
	iters: 300, epoch: 3 | loss: 0.2366417
	speed: 0.0791s/iter; left time: 177.6454s
Epoch: 3 cost time: 25.983701705932617
Epoch: 3, Steps: 318 | Train Loss: 0.2290660 Vali Loss: 0.2071134 Test Loss: 0.2037623
Validation loss decreased (0.215585 --> 0.207113).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2250437
	speed: 0.0656s/iter; left time: 139.5804s
	iters: 200, epoch: 4 | loss: 0.2691654
	speed: 0.0846s/iter; left time: 171.4233s
	iters: 300, epoch: 4 | loss: 0.2281112
	speed: 0.0789s/iter; left time: 152.0637s
Epoch: 4 cost time: 24.663496732711792
Epoch: 4, Steps: 318 | Train Loss: 0.2206027 Vali Loss: 0.2006979 Test Loss: 0.2029332
Validation loss decreased (0.207113 --> 0.200698).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2516727
	speed: 0.0976s/iter; left time: 176.5913s
	iters: 200, epoch: 5 | loss: 0.2627800
	speed: 0.0724s/iter; left time: 123.6728s
	iters: 300, epoch: 5 | loss: 0.2233470
	speed: 0.0807s/iter; left time: 129.7876s
Epoch: 5 cost time: 26.523818016052246
Epoch: 5, Steps: 318 | Train Loss: 0.2172041 Vali Loss: 0.1980057 Test Loss: 0.2004968
Validation loss decreased (0.200698 --> 0.198006).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2354595
	speed: 0.0905s/iter; left time: 134.9941s
	iters: 200, epoch: 6 | loss: 0.1852457
	speed: 0.0796s/iter; left time: 110.7108s
	iters: 300, epoch: 6 | loss: 0.3353260
	speed: 0.0761s/iter; left time: 98.2342s
Epoch: 6 cost time: 25.953689336776733
Epoch: 6, Steps: 318 | Train Loss: 0.2152184 Vali Loss: 0.1958608 Test Loss: 0.1998351
Validation loss decreased (0.198006 --> 0.195861).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1766049
	speed: 0.0908s/iter; left time: 106.5176s
	iters: 200, epoch: 7 | loss: 0.1787124
	speed: 0.0806s/iter; left time: 86.4413s
	iters: 300, epoch: 7 | loss: 0.1686659
	speed: 0.0665s/iter; left time: 64.6706s
Epoch: 7 cost time: 24.101954221725464
Epoch: 7, Steps: 318 | Train Loss: 0.2143350 Vali Loss: 0.1962331 Test Loss: 0.1996045
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2108826
	speed: 0.0880s/iter; left time: 75.2267s
	iters: 200, epoch: 8 | loss: 0.1835677
	speed: 0.0743s/iter; left time: 56.1158s
	iters: 300, epoch: 8 | loss: 0.2599449
	speed: 0.0859s/iter; left time: 56.2380s
Epoch: 8 cost time: 26.62093448638916
Epoch: 8, Steps: 318 | Train Loss: 0.2140436 Vali Loss: 0.1965410 Test Loss: 0.1992089
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1568938
	speed: 0.0858s/iter; left time: 46.0851s
	iters: 200, epoch: 9 | loss: 0.2142080
	speed: 0.0833s/iter; left time: 36.4000s
	iters: 300, epoch: 9 | loss: 0.1856466
	speed: 0.0758s/iter; left time: 25.5338s
Epoch: 9 cost time: 25.904979705810547
Epoch: 9, Steps: 318 | Train Loss: 0.2137503 Vali Loss: 0.1971971 Test Loss: 0.1991324
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.43853044509887695, mae:0.5255833864212036, rmse:0.662216305732727, mape:0.018810970708727837, mspe:0.0005681656766682863, rse:0.4635189175605774, r2_score:0.7468173143482857, acc:0.9811890292912722
corr: [38.190998 37.74313  37.85169  37.639084 38.221745 37.690926 37.653404
 37.525406 37.700066 37.76958  37.746696 38.039963 38.013004 37.72034
 37.75356  37.787    38.102303 38.128624 38.26282  38.236225]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3620767
	speed: 0.1084s/iter; left time: 333.9033s
	iters: 200, epoch: 1 | loss: 0.2055197
	speed: 0.0794s/iter; left time: 236.7345s
	iters: 300, epoch: 1 | loss: 0.3245768
	speed: 0.0780s/iter; left time: 224.7518s
Epoch: 1 cost time: 28.0334951877594
Epoch: 1, Steps: 318 | Train Loss: 0.3949514 Vali Loss: 0.2475149 Test Loss: 0.2276078
Validation loss decreased (inf --> 0.247515).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2418847
	speed: 0.0954s/iter; left time: 263.5354s
	iters: 200, epoch: 2 | loss: 0.2622877
	speed: 0.0836s/iter; left time: 222.6823s
	iters: 300, epoch: 2 | loss: 0.2753698
	speed: 0.0844s/iter; left time: 216.4188s
Epoch: 2 cost time: 28.042441606521606
Epoch: 2, Steps: 318 | Train Loss: 0.2335981 Vali Loss: 0.2281648 Test Loss: 0.2065465
Validation loss decreased (0.247515 --> 0.228165).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1861253
	speed: 0.0909s/iter; left time: 222.1799s
	iters: 200, epoch: 3 | loss: 0.2048843
	speed: 0.0807s/iter; left time: 189.2645s
	iters: 300, epoch: 3 | loss: 0.1728598
	speed: 0.0856s/iter; left time: 192.2615s
Epoch: 3 cost time: 27.74292230606079
Epoch: 3, Steps: 318 | Train Loss: 0.2182807 Vali Loss: 0.2247271 Test Loss: 0.2016546
Validation loss decreased (0.228165 --> 0.224727).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2189304
	speed: 0.0845s/iter; left time: 179.6896s
	iters: 200, epoch: 4 | loss: 0.1995560
	speed: 0.0799s/iter; left time: 161.9702s
	iters: 300, epoch: 4 | loss: 0.2075613
	speed: 0.0835s/iter; left time: 160.9253s
Epoch: 4 cost time: 26.528724670410156
Epoch: 4, Steps: 318 | Train Loss: 0.2125511 Vali Loss: 0.2231201 Test Loss: 0.1996292
Validation loss decreased (0.224727 --> 0.223120).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1515125
	speed: 0.0912s/iter; left time: 165.0586s
	iters: 200, epoch: 5 | loss: 0.2126421
	speed: 0.0828s/iter; left time: 141.5767s
	iters: 300, epoch: 5 | loss: 0.2252046
	speed: 0.0796s/iter; left time: 128.1032s
Epoch: 5 cost time: 26.84509515762329
Epoch: 5, Steps: 318 | Train Loss: 0.2102569 Vali Loss: 0.2224948 Test Loss: 0.1980599
Validation loss decreased (0.223120 --> 0.222495).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2243952
	speed: 0.0843s/iter; left time: 125.7163s
	iters: 200, epoch: 6 | loss: 0.2048662
	speed: 0.0823s/iter; left time: 114.5201s
	iters: 300, epoch: 6 | loss: 0.2108919
	speed: 0.0800s/iter; left time: 103.3069s
Epoch: 6 cost time: 26.215138912200928
Epoch: 6, Steps: 318 | Train Loss: 0.2093721 Vali Loss: 0.2233295 Test Loss: 0.1976955
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1998170
	speed: 0.0839s/iter; left time: 98.4562s
	iters: 200, epoch: 7 | loss: 0.1772412
	speed: 0.0833s/iter; left time: 89.4037s
	iters: 300, epoch: 7 | loss: 0.2342498
	speed: 0.0939s/iter; left time: 91.3940s
Epoch: 7 cost time: 27.92587447166443
Epoch: 7, Steps: 318 | Train Loss: 0.2088123 Vali Loss: 0.2219855 Test Loss: 0.1968615
Validation loss decreased (0.222495 --> 0.221985).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2229005
	speed: 0.0976s/iter; left time: 83.4388s
	iters: 200, epoch: 8 | loss: 0.2325535
	speed: 0.0868s/iter; left time: 65.5343s
	iters: 300, epoch: 8 | loss: 0.1628101
	speed: 0.0843s/iter; left time: 55.2026s
Epoch: 8 cost time: 28.364646196365356
Epoch: 8, Steps: 318 | Train Loss: 0.2087288 Vali Loss: 0.2226566 Test Loss: 0.1968767
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2497948
	speed: 0.0947s/iter; left time: 50.8807s
	iters: 200, epoch: 9 | loss: 0.2336867
	speed: 0.0844s/iter; left time: 36.9005s
	iters: 300, epoch: 9 | loss: 0.2085956
	speed: 0.0837s/iter; left time: 28.2200s
Epoch: 9 cost time: 27.841179847717285
Epoch: 9, Steps: 318 | Train Loss: 0.2083918 Vali Loss: 0.2212873 Test Loss: 0.1969037
Validation loss decreased (0.221985 --> 0.221287).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1996174
	speed: 0.0686s/iter; left time: 15.0335s
	iters: 200, epoch: 10 | loss: 0.1540607
	speed: 0.0852s/iter; left time: 10.1355s
	iters: 300, epoch: 10 | loss: 0.1965583
	speed: 0.0795s/iter; left time: 1.5103s
Epoch: 10 cost time: 24.504188060760498
Epoch: 10, Steps: 318 | Train Loss: 0.2080088 Vali Loss: 0.2219792 Test Loss: 0.1968734
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.43209752440452576, mae:0.5159167647361755, rmse:0.6573412418365479, mape:0.0184827521443367, mspe:0.0005638455040752888, rse:0.45979925990104675, r2_score:0.7516845316088123, acc:0.9815172478556633
corr: [37.211117 37.192482 37.104256 37.336594 37.085686 37.327213 37.179394
 37.159748 37.03949  37.37169  37.21503  37.444546 37.457767 37.403233
 37.379917 37.429234 37.480972 37.429523 37.675697 37.83212  37.82828
 37.810413 38.003304 38.090088 38.070095]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4338798
	speed: 0.1007s/iter; left time: 309.1607s
	iters: 200, epoch: 1 | loss: 0.2647715
	speed: 0.0962s/iter; left time: 285.7553s
	iters: 300, epoch: 1 | loss: 0.3304320
	speed: 0.0875s/iter; left time: 251.2514s
Epoch: 1 cost time: 30.242581605911255
Epoch: 1, Steps: 317 | Train Loss: 0.4232117 Vali Loss: 0.2573334 Test Loss: 0.2327398
Validation loss decreased (inf --> 0.257333).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2818865
	speed: 0.0938s/iter; left time: 258.4162s
	iters: 200, epoch: 2 | loss: 0.2153235
	speed: 0.0960s/iter; left time: 254.7043s
	iters: 300, epoch: 2 | loss: 0.2677355
	speed: 0.1130s/iter; left time: 288.4868s
Epoch: 2 cost time: 31.874568939208984
Epoch: 2, Steps: 317 | Train Loss: 0.2481801 Vali Loss: 0.2364767 Test Loss: 0.2184681
Validation loss decreased (0.257333 --> 0.236477).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2171944
	speed: 0.1030s/iter; left time: 250.9993s
	iters: 200, epoch: 3 | loss: 0.2597345
	speed: 0.1027s/iter; left time: 239.9853s
	iters: 300, epoch: 3 | loss: 0.2073753
	speed: 0.0917s/iter; left time: 205.2083s
Epoch: 3 cost time: 31.581785678863525
Epoch: 3, Steps: 317 | Train Loss: 0.2323985 Vali Loss: 0.2304050 Test Loss: 0.2109233
Validation loss decreased (0.236477 --> 0.230405).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2042134
	speed: 0.1139s/iter; left time: 241.3695s
	iters: 200, epoch: 4 | loss: 0.1969052
	speed: 0.1038s/iter; left time: 209.7153s
	iters: 300, epoch: 4 | loss: 0.2197411
	speed: 0.1132s/iter; left time: 217.3859s
Epoch: 4 cost time: 34.865034341812134
Epoch: 4, Steps: 317 | Train Loss: 0.2267630 Vali Loss: 0.2313970 Test Loss: 0.2094061
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2221844
	speed: 0.1189s/iter; left time: 214.3546s
	iters: 200, epoch: 5 | loss: 0.2316974
	speed: 0.1012s/iter; left time: 172.3107s
	iters: 300, epoch: 5 | loss: 0.2313105
	speed: 0.1014s/iter; left time: 162.4984s
Epoch: 5 cost time: 34.06085824966431
Epoch: 5, Steps: 317 | Train Loss: 0.2240297 Vali Loss: 0.2301191 Test Loss: 0.2083142
Validation loss decreased (0.230405 --> 0.230119).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1768577
	speed: 0.1143s/iter; left time: 169.8165s
	iters: 200, epoch: 6 | loss: 0.2212273
	speed: 0.1061s/iter; left time: 147.0557s
	iters: 300, epoch: 6 | loss: 0.2795156
	speed: 0.1021s/iter; left time: 131.3191s
Epoch: 6 cost time: 34.163461446762085
Epoch: 6, Steps: 317 | Train Loss: 0.2227143 Vali Loss: 0.2296322 Test Loss: 0.2072876
Validation loss decreased (0.230119 --> 0.229632).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2389323
	speed: 0.1057s/iter; left time: 123.5112s
	iters: 200, epoch: 7 | loss: 0.2667824
	speed: 0.1036s/iter; left time: 110.7620s
	iters: 300, epoch: 7 | loss: 0.2342308
	speed: 0.1047s/iter; left time: 101.4233s
Epoch: 7 cost time: 31.749821186065674
Epoch: 7, Steps: 317 | Train Loss: 0.2222544 Vali Loss: 0.2300742 Test Loss: 0.2073065
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1728026
	speed: 0.1059s/iter; left time: 90.2255s
	iters: 200, epoch: 8 | loss: 0.2575826
	speed: 0.1069s/iter; left time: 80.3642s
	iters: 300, epoch: 8 | loss: 0.2277520
	speed: 0.0965s/iter; left time: 62.9301s
Epoch: 8 cost time: 32.55424404144287
Epoch: 8, Steps: 317 | Train Loss: 0.2218966 Vali Loss: 0.2298318 Test Loss: 0.2073084
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1755841
	speed: 0.1056s/iter; left time: 56.4906s
	iters: 200, epoch: 9 | loss: 0.2685016
	speed: 0.1105s/iter; left time: 48.0458s
	iters: 300, epoch: 9 | loss: 0.2010498
	speed: 0.0997s/iter; left time: 33.4027s
Epoch: 9 cost time: 32.31825494766235
Epoch: 9, Steps: 317 | Train Loss: 0.2216897 Vali Loss: 0.2296740 Test Loss: 0.2072362
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.45488446950912476, mae:0.5294070243835449, rmse:0.6744512319564819, mape:0.01897568069398403, mspe:0.0005939320544712245, rse:0.47142866253852844, r2_score:0.7305353392670526, acc:0.981024319306016
corr: [37.0604   37.004013 37.15782  37.535057 37.147984 37.379692 37.22238
 37.282436 37.152775 37.2516   37.518482 37.37195  37.415756 37.747112
 37.734245 37.698074 37.481297 37.39658  37.630245 37.649822 37.85688
 37.747013 37.812283 37.74438  37.739876 37.568783 37.89601  37.898434
 37.94396  38.03037 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4702291
	speed: 0.1406s/iter; left time: 433.2216s
	iters: 200, epoch: 1 | loss: 0.3508423
	speed: 0.1226s/iter; left time: 365.4528s
	iters: 300, epoch: 1 | loss: 0.2529511
	speed: 0.0946s/iter; left time: 272.5460s
Epoch: 1 cost time: 36.919307231903076
Epoch: 1, Steps: 318 | Train Loss: 0.4624615 Vali Loss: 0.2696896 Test Loss: 0.2425356
Validation loss decreased (inf --> 0.269690).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2176803
	speed: 0.1154s/iter; left time: 318.7937s
	iters: 200, epoch: 2 | loss: 0.2449484
	speed: 0.1139s/iter; left time: 303.2489s
	iters: 300, epoch: 2 | loss: 0.2530093
	speed: 0.1046s/iter; left time: 267.9886s
Epoch: 2 cost time: 34.70527410507202
Epoch: 2, Steps: 318 | Train Loss: 0.2466909 Vali Loss: 0.2282600 Test Loss: 0.2092370
Validation loss decreased (0.269690 --> 0.228260).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2566487
	speed: 0.1116s/iter; left time: 272.7824s
	iters: 200, epoch: 3 | loss: 0.2375060
	speed: 0.1220s/iter; left time: 286.0293s
	iters: 300, epoch: 3 | loss: 0.1973873
	speed: 0.1144s/iter; left time: 256.8338s
Epoch: 3 cost time: 36.68912696838379
Epoch: 3, Steps: 318 | Train Loss: 0.2139860 Vali Loss: 0.2155218 Test Loss: 0.1917393
Validation loss decreased (0.228260 --> 0.215522).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1784334
	speed: 0.1240s/iter; left time: 263.6424s
	iters: 200, epoch: 4 | loss: 0.2068133
	speed: 0.1138s/iter; left time: 230.6509s
	iters: 300, epoch: 4 | loss: 0.2179610
	speed: 0.1092s/iter; left time: 210.4051s
Epoch: 4 cost time: 36.676114082336426
Epoch: 4, Steps: 318 | Train Loss: 0.2028667 Vali Loss: 0.2142741 Test Loss: 0.1875376
Validation loss decreased (0.215522 --> 0.214274).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2727383
	speed: 0.1197s/iter; left time: 216.6022s
	iters: 200, epoch: 5 | loss: 0.2191001
	speed: 0.1072s/iter; left time: 183.2723s
	iters: 300, epoch: 5 | loss: 0.2099785
	speed: 0.1094s/iter; left time: 175.9555s
Epoch: 5 cost time: 35.98203635215759
Epoch: 5, Steps: 318 | Train Loss: 0.1988344 Vali Loss: 0.2097221 Test Loss: 0.1844350
Validation loss decreased (0.214274 --> 0.209722).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1495440
	speed: 0.1251s/iter; left time: 186.5678s
	iters: 200, epoch: 6 | loss: 0.2211205
	speed: 0.1122s/iter; left time: 156.0184s
	iters: 300, epoch: 6 | loss: 0.2443933
	speed: 0.1250s/iter; left time: 161.3757s
Epoch: 6 cost time: 38.244696617126465
Epoch: 6, Steps: 318 | Train Loss: 0.1969742 Vali Loss: 0.2119180 Test Loss: 0.1836584
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2971401
	speed: 0.1331s/iter; left time: 156.0997s
	iters: 200, epoch: 7 | loss: 0.2383133
	speed: 0.1214s/iter; left time: 130.2460s
	iters: 300, epoch: 7 | loss: 0.1340527
	speed: 0.1201s/iter; left time: 116.8443s
Epoch: 7 cost time: 39.61569666862488
Epoch: 7, Steps: 318 | Train Loss: 0.1956900 Vali Loss: 0.2107556 Test Loss: 0.1832796
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2026140
	speed: 0.1292s/iter; left time: 110.5086s
	iters: 200, epoch: 8 | loss: 0.2037109
	speed: 0.0913s/iter; left time: 68.9285s
	iters: 300, epoch: 8 | loss: 0.1834847
	speed: 0.1126s/iter; left time: 73.7345s
Epoch: 8 cost time: 34.99219512939453
Epoch: 8, Steps: 318 | Train Loss: 0.1947729 Vali Loss: 0.2110274 Test Loss: 0.1830358
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.40473541617393494, mae:0.5040837526321411, rmse:0.6361882090568542, mape:0.018027370795607567, mspe:0.0005237641162239015, rse:0.44576844573020935, r2_score:0.7727843492050649, acc:0.9819726292043924
corr: [37.89383  37.834465 37.855232 37.76076  37.87684  37.571735 37.74216
 37.865253 37.775173 37.96943 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3541554
	speed: 0.1120s/iter; left time: 345.1098s
	iters: 200, epoch: 1 | loss: 0.2858332
	speed: 0.0997s/iter; left time: 297.3232s
	iters: 300, epoch: 1 | loss: 0.3381178
	speed: 0.1154s/iter; left time: 332.5725s
Epoch: 1 cost time: 34.873016119003296
Epoch: 1, Steps: 318 | Train Loss: 0.4039568 Vali Loss: 0.2290758 Test Loss: 0.2039057
Validation loss decreased (inf --> 0.229076).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2209623
	speed: 0.1260s/iter; left time: 348.2532s
	iters: 200, epoch: 2 | loss: 0.1479630
	speed: 0.1003s/iter; left time: 267.1709s
	iters: 300, epoch: 2 | loss: 0.1816017
	speed: 0.1172s/iter; left time: 300.2879s
Epoch: 2 cost time: 35.639514446258545
Epoch: 2, Steps: 318 | Train Loss: 0.2202599 Vali Loss: 0.2098958 Test Loss: 0.1820095
Validation loss decreased (0.229076 --> 0.209896).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1874965
	speed: 0.1120s/iter; left time: 273.7252s
	iters: 200, epoch: 3 | loss: 0.1824122
	speed: 0.1174s/iter; left time: 275.3399s
	iters: 300, epoch: 3 | loss: 0.1753799
	speed: 0.1122s/iter; left time: 251.8094s
Epoch: 3 cost time: 35.636929512023926
Epoch: 3, Steps: 318 | Train Loss: 0.2037115 Vali Loss: 0.2036820 Test Loss: 0.1758774
Validation loss decreased (0.209896 --> 0.203682).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2557383
	speed: 0.1218s/iter; left time: 259.0034s
	iters: 200, epoch: 4 | loss: 0.1947608
	speed: 0.1035s/iter; left time: 209.7290s
	iters: 300, epoch: 4 | loss: 0.1932242
	speed: 0.1106s/iter; left time: 213.1571s
Epoch: 4 cost time: 35.17158603668213
Epoch: 4, Steps: 318 | Train Loss: 0.1977628 Vali Loss: 0.2025825 Test Loss: 0.1732723
Validation loss decreased (0.203682 --> 0.202583).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1687092
	speed: 0.1227s/iter; left time: 221.9525s
	iters: 200, epoch: 5 | loss: 0.1919304
	speed: 0.1071s/iter; left time: 182.9981s
	iters: 300, epoch: 5 | loss: 0.2001515
	speed: 0.1134s/iter; left time: 182.4541s
Epoch: 5 cost time: 36.303751707077026
Epoch: 5, Steps: 318 | Train Loss: 0.1951701 Vali Loss: 0.2011536 Test Loss: 0.1716296
Validation loss decreased (0.202583 --> 0.201154).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2187338
	speed: 0.1193s/iter; left time: 177.9447s
	iters: 200, epoch: 6 | loss: 0.1887256
	speed: 0.1074s/iter; left time: 149.3601s
	iters: 300, epoch: 6 | loss: 0.2441724
	speed: 0.1131s/iter; left time: 146.0301s
Epoch: 6 cost time: 35.847986459732056
Epoch: 6, Steps: 318 | Train Loss: 0.1937155 Vali Loss: 0.2018646 Test Loss: 0.1730475
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1542260
	speed: 0.1234s/iter; left time: 144.8062s
	iters: 200, epoch: 7 | loss: 0.1877884
	speed: 0.1100s/iter; left time: 118.0524s
	iters: 300, epoch: 7 | loss: 0.1711199
	speed: 0.1074s/iter; left time: 104.5454s
Epoch: 7 cost time: 36.48966145515442
Epoch: 7, Steps: 318 | Train Loss: 0.1930978 Vali Loss: 0.2008407 Test Loss: 0.1718766
Validation loss decreased (0.201154 --> 0.200841).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2349759
	speed: 0.1193s/iter; left time: 102.0300s
	iters: 200, epoch: 8 | loss: 0.2172918
	speed: 0.1129s/iter; left time: 85.2087s
	iters: 300, epoch: 8 | loss: 0.1639430
	speed: 0.1122s/iter; left time: 73.4759s
Epoch: 8 cost time: 36.72695732116699
Epoch: 8, Steps: 318 | Train Loss: 0.1928728 Vali Loss: 0.2002127 Test Loss: 0.1713511
Validation loss decreased (0.200841 --> 0.200213).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2153135
	speed: 0.1164s/iter; left time: 62.4931s
	iters: 200, epoch: 9 | loss: 0.2069083
	speed: 0.1080s/iter; left time: 47.2166s
	iters: 300, epoch: 9 | loss: 0.1685484
	speed: 0.1073s/iter; left time: 36.1505s
Epoch: 9 cost time: 35.493542432785034
Epoch: 9, Steps: 318 | Train Loss: 0.1925990 Vali Loss: 0.1995591 Test Loss: 0.1712592
Validation loss decreased (0.200213 --> 0.199559).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2061007
	speed: 0.1183s/iter; left time: 25.9148s
	iters: 200, epoch: 10 | loss: 0.1449415
	speed: 0.1092s/iter; left time: 12.9905s
	iters: 300, epoch: 10 | loss: 0.1894231
	speed: 0.1087s/iter; left time: 2.0648s
Epoch: 10 cost time: 35.14597725868225
Epoch: 10, Steps: 318 | Train Loss: 0.1924156 Vali Loss: 0.2012717 Test Loss: 0.1711905
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.37582170963287354, mae:0.4808398187160492, rmse:0.6130430102348328, mape:0.017202995717525482, mspe:0.0004876436141785234, rse:0.4293486773967743, r2_score:0.7845233918320732, acc:0.9827970042824745
corr: [37.54555  37.56414  37.522526 37.58213  37.51814  37.513737 37.693695
 37.6102   37.568935 37.701786 37.46575  37.533653 37.639877 37.780617
 37.721775]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3968971
	speed: 0.1179s/iter; left time: 363.2956s
	iters: 200, epoch: 1 | loss: 0.2931131
	speed: 0.0837s/iter; left time: 249.3983s
	iters: 300, epoch: 1 | loss: 0.2097615
	speed: 0.1028s/iter; left time: 296.1494s
Epoch: 1 cost time: 31.83328104019165
Epoch: 1, Steps: 318 | Train Loss: 0.4702978 Vali Loss: 0.2720087 Test Loss: 0.2523266
Validation loss decreased (inf --> 0.272009).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2588461
	speed: 0.1237s/iter; left time: 341.8898s
	iters: 200, epoch: 2 | loss: 0.2521582
	speed: 0.1052s/iter; left time: 280.2446s
	iters: 300, epoch: 2 | loss: 0.2509782
	speed: 0.1067s/iter; left time: 273.4621s
Epoch: 2 cost time: 35.229318380355835
Epoch: 2, Steps: 318 | Train Loss: 0.2528963 Vali Loss: 0.2250601 Test Loss: 0.2228904
Validation loss decreased (0.272009 --> 0.225060).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2647803
	speed: 0.1106s/iter; left time: 270.2973s
	iters: 200, epoch: 3 | loss: 0.2432479
	speed: 0.1095s/iter; left time: 256.6988s
	iters: 300, epoch: 3 | loss: 0.2486302
	speed: 0.1086s/iter; left time: 243.8263s
Epoch: 3 cost time: 34.65764379501343
Epoch: 3, Steps: 318 | Train Loss: 0.2313168 Vali Loss: 0.2163534 Test Loss: 0.2120223
Validation loss decreased (0.225060 --> 0.216353).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2161462
	speed: 0.1285s/iter; left time: 273.3849s
	iters: 200, epoch: 4 | loss: 0.2694173
	speed: 0.1216s/iter; left time: 246.4324s
	iters: 300, epoch: 4 | loss: 0.2226580
	speed: 0.1135s/iter; left time: 218.6746s
Epoch: 4 cost time: 38.41193222999573
Epoch: 4, Steps: 318 | Train Loss: 0.2228717 Vali Loss: 0.2102766 Test Loss: 0.2128315
Validation loss decreased (0.216353 --> 0.210277).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2513871
	speed: 0.1176s/iter; left time: 212.6932s
	iters: 200, epoch: 5 | loss: 0.2691951
	speed: 0.1103s/iter; left time: 188.4922s
	iters: 300, epoch: 5 | loss: 0.2397585
	speed: 0.1136s/iter; left time: 182.7736s
Epoch: 5 cost time: 36.223270654678345
Epoch: 5, Steps: 318 | Train Loss: 0.2193218 Vali Loss: 0.2085797 Test Loss: 0.2108758
Validation loss decreased (0.210277 --> 0.208580).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2512971
	speed: 0.1161s/iter; left time: 173.0497s
	iters: 200, epoch: 6 | loss: 0.1890454
	speed: 0.1122s/iter; left time: 156.0693s
	iters: 300, epoch: 6 | loss: 0.3427272
	speed: 0.1135s/iter; left time: 146.4703s
Epoch: 6 cost time: 37.017035722732544
Epoch: 6, Steps: 318 | Train Loss: 0.2171693 Vali Loss: 0.2068290 Test Loss: 0.2103448
Validation loss decreased (0.208580 --> 0.206829).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1850003
	speed: 0.1232s/iter; left time: 144.4613s
	iters: 200, epoch: 7 | loss: 0.1777283
	speed: 0.1076s/iter; left time: 115.4800s
	iters: 300, epoch: 7 | loss: 0.1698889
	speed: 0.1062s/iter; left time: 103.3243s
Epoch: 7 cost time: 35.30914759635925
Epoch: 7, Steps: 318 | Train Loss: 0.2164111 Vali Loss: 0.2077605 Test Loss: 0.2101712
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2125836
	speed: 0.1221s/iter; left time: 104.3672s
	iters: 200, epoch: 8 | loss: 0.1750881
	speed: 0.1086s/iter; left time: 81.9833s
	iters: 300, epoch: 8 | loss: 0.2829503
	speed: 0.1110s/iter; left time: 72.6793s
Epoch: 8 cost time: 35.985867977142334
Epoch: 8, Steps: 318 | Train Loss: 0.2158571 Vali Loss: 0.2078447 Test Loss: 0.2098721
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1515411
	speed: 0.1097s/iter; left time: 58.9286s
	iters: 200, epoch: 9 | loss: 0.2201080
	speed: 0.1031s/iter; left time: 45.0464s
	iters: 300, epoch: 9 | loss: 0.1825104
	speed: 0.1087s/iter; left time: 36.6466s
Epoch: 9 cost time: 33.51760244369507
Epoch: 9, Steps: 318 | Train Loss: 0.2153513 Vali Loss: 0.2083348 Test Loss: 0.2097024
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.4615933299064636, mae:0.5401878952980042, rmse:0.6794065833091736, mape:0.01930847391486168, mspe:0.0005954736843705177, rse:0.4755513072013855, r2_score:0.7365377561849084, acc:0.9806915260851383
corr: [38.056854 37.652416 37.70823  37.616455 38.102066 37.513157 37.59535
 37.34979  37.50896  37.614388 37.61396  37.776638 37.700207 37.568527
 37.53257  37.575035 37.895485 37.70937  37.885345 37.942043]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3640302
	speed: 0.1277s/iter; left time: 393.4280s
	iters: 200, epoch: 1 | loss: 0.2055366
	speed: 0.0838s/iter; left time: 249.8078s
	iters: 300, epoch: 1 | loss: 0.3267305
	speed: 0.0893s/iter; left time: 257.1901s
Epoch: 1 cost time: 31.590094566345215
Epoch: 1, Steps: 318 | Train Loss: 0.3957924 Vali Loss: 0.2472056 Test Loss: 0.2280485
Validation loss decreased (inf --> 0.247206).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2415598
	speed: 0.1000s/iter; left time: 276.3232s
	iters: 200, epoch: 2 | loss: 0.2614149
	speed: 0.0925s/iter; left time: 246.2204s
	iters: 300, epoch: 2 | loss: 0.2756529
	speed: 0.0889s/iter; left time: 227.9336s
Epoch: 2 cost time: 29.647456169128418
Epoch: 2, Steps: 318 | Train Loss: 0.2336038 Vali Loss: 0.2271918 Test Loss: 0.2060984
Validation loss decreased (0.247206 --> 0.227192).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1856292
	speed: 0.1033s/iter; left time: 252.5000s
	iters: 200, epoch: 3 | loss: 0.2041870
	speed: 0.0863s/iter; left time: 202.4682s
	iters: 300, epoch: 3 | loss: 0.1720999
	speed: 0.0858s/iter; left time: 192.5995s
Epoch: 3 cost time: 29.314225673675537
Epoch: 3, Steps: 318 | Train Loss: 0.2181886 Vali Loss: 0.2241676 Test Loss: 0.2012906
Validation loss decreased (0.227192 --> 0.224168).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2179995
	speed: 0.1051s/iter; left time: 223.4818s
	iters: 200, epoch: 4 | loss: 0.1998385
	speed: 0.0852s/iter; left time: 172.8000s
	iters: 300, epoch: 4 | loss: 0.2060409
	speed: 0.0867s/iter; left time: 167.1438s
Epoch: 4 cost time: 29.233909845352173
Epoch: 4, Steps: 318 | Train Loss: 0.2124744 Vali Loss: 0.2226604 Test Loss: 0.1994092
Validation loss decreased (0.224168 --> 0.222660).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1513460
	speed: 0.0963s/iter; left time: 174.2018s
	iters: 200, epoch: 5 | loss: 0.2119084
	speed: 0.0859s/iter; left time: 146.7964s
	iters: 300, epoch: 5 | loss: 0.2255782
	speed: 0.0875s/iter; left time: 140.7744s
Epoch: 5 cost time: 28.62401795387268
Epoch: 5, Steps: 318 | Train Loss: 0.2101939 Vali Loss: 0.2221204 Test Loss: 0.1978341
Validation loss decreased (0.222660 --> 0.222120).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2232574
	speed: 0.0941s/iter; left time: 140.2989s
	iters: 200, epoch: 6 | loss: 0.2048561
	speed: 0.0874s/iter; left time: 121.5491s
	iters: 300, epoch: 6 | loss: 0.2095439
	speed: 0.0877s/iter; left time: 113.2147s
Epoch: 6 cost time: 28.30785870552063
Epoch: 6, Steps: 318 | Train Loss: 0.2093096 Vali Loss: 0.2230158 Test Loss: 0.1974587
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2001160
	speed: 0.0856s/iter; left time: 100.3551s
	iters: 200, epoch: 7 | loss: 0.1790381
	speed: 0.0942s/iter; left time: 101.0334s
	iters: 300, epoch: 7 | loss: 0.2331517
	speed: 0.0903s/iter; left time: 87.8370s
Epoch: 7 cost time: 28.941916465759277
Epoch: 7, Steps: 318 | Train Loss: 0.2087478 Vali Loss: 0.2216926 Test Loss: 0.1966520
Validation loss decreased (0.222120 --> 0.221693).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2233463
	speed: 0.0951s/iter; left time: 81.2852s
	iters: 200, epoch: 8 | loss: 0.2308799
	speed: 0.0920s/iter; left time: 69.4378s
	iters: 300, epoch: 8 | loss: 0.1629531
	speed: 0.0865s/iter; left time: 56.6349s
Epoch: 8 cost time: 28.71048879623413
Epoch: 8, Steps: 318 | Train Loss: 0.2086948 Vali Loss: 0.2223571 Test Loss: 0.1966710
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2497817
	speed: 0.1037s/iter; left time: 55.7060s
	iters: 200, epoch: 9 | loss: 0.2335946
	speed: 0.0964s/iter; left time: 42.1197s
	iters: 300, epoch: 9 | loss: 0.2093486
	speed: 0.0863s/iter; left time: 29.0696s
Epoch: 9 cost time: 30.071352243423462
Epoch: 9, Steps: 318 | Train Loss: 0.2083223 Vali Loss: 0.2210042 Test Loss: 0.1967029
Validation loss decreased (0.221693 --> 0.221004).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2000688
	speed: 0.1002s/iter; left time: 21.9351s
	iters: 200, epoch: 10 | loss: 0.1544687
	speed: 0.0858s/iter; left time: 10.2097s
	iters: 300, epoch: 10 | loss: 0.1960577
	speed: 0.0880s/iter; left time: 1.6722s
Epoch: 10 cost time: 29.074715614318848
Epoch: 10, Steps: 318 | Train Loss: 0.2079726 Vali Loss: 0.2216899 Test Loss: 0.1966730
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.43165677785873413, mae:0.5156731605529785, rmse:0.6570059061050415, mape:0.018474256619811058, mspe:0.0005632886313833296, rse:0.4595646858215332, r2_score:0.7516012022733314, acc:0.9815257433801889
corr: [37.170574 37.155575 37.0565   37.30448  37.05844  37.303978 37.153366
 37.148098 37.016354 37.354626 37.193584 37.411694 37.431736 37.373817
 37.35959  37.40774  37.461136 37.414265 37.67139  37.811672 37.813545
 37.804035 37.98892  38.077698 38.062103]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4350779
	speed: 0.1365s/iter; left time: 419.1259s
	iters: 200, epoch: 1 | loss: 0.2640752
	speed: 0.1095s/iter; left time: 325.2146s
	iters: 300, epoch: 1 | loss: 0.3205718
	speed: 0.1090s/iter; left time: 312.8732s
Epoch: 1 cost time: 37.49898099899292
Epoch: 1, Steps: 317 | Train Loss: 0.4183990 Vali Loss: 0.2622959 Test Loss: 0.2345596
Validation loss decreased (inf --> 0.262296).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2846635
	speed: 0.1162s/iter; left time: 319.9668s
	iters: 200, epoch: 2 | loss: 0.2118373
	speed: 0.1192s/iter; left time: 316.2971s
	iters: 300, epoch: 2 | loss: 0.2705044
	speed: 0.1097s/iter; left time: 280.1682s
Epoch: 2 cost time: 36.68825101852417
Epoch: 2, Steps: 317 | Train Loss: 0.2456840 Vali Loss: 0.2429515 Test Loss: 0.2193577
Validation loss decreased (0.262296 --> 0.242951).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2150929
	speed: 0.1094s/iter; left time: 266.5704s
	iters: 200, epoch: 3 | loss: 0.2516193
	speed: 0.1086s/iter; left time: 253.7512s
	iters: 300, epoch: 3 | loss: 0.2208329
	speed: 0.1152s/iter; left time: 257.5988s
Epoch: 3 cost time: 34.918211698532104
Epoch: 3, Steps: 317 | Train Loss: 0.2298704 Vali Loss: 0.2392503 Test Loss: 0.2129879
Validation loss decreased (0.242951 --> 0.239250).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2029980
	speed: 0.1145s/iter; left time: 242.6744s
	iters: 200, epoch: 4 | loss: 0.1938675
	speed: 0.1017s/iter; left time: 205.4521s
	iters: 300, epoch: 4 | loss: 0.2111291
	speed: 0.1083s/iter; left time: 207.9266s
Epoch: 4 cost time: 34.419920682907104
Epoch: 4, Steps: 317 | Train Loss: 0.2241264 Vali Loss: 0.2406547 Test Loss: 0.2130974
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2268894
	speed: 0.1179s/iter; left time: 212.4969s
	iters: 200, epoch: 5 | loss: 0.2374560
	speed: 0.1021s/iter; left time: 173.8068s
	iters: 300, epoch: 5 | loss: 0.2280038
	speed: 0.1075s/iter; left time: 172.2485s
Epoch: 5 cost time: 34.94654178619385
Epoch: 5, Steps: 317 | Train Loss: 0.2213269 Vali Loss: 0.2401633 Test Loss: 0.2128198
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1834395
	speed: 0.1182s/iter; left time: 175.6154s
	iters: 200, epoch: 6 | loss: 0.2168579
	speed: 0.1083s/iter; left time: 150.1507s
	iters: 300, epoch: 6 | loss: 0.2694015
	speed: 0.0992s/iter; left time: 127.5254s
Epoch: 6 cost time: 33.658527135849
Epoch: 6, Steps: 317 | Train Loss: 0.2199818 Vali Loss: 0.2396606 Test Loss: 0.2118139
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.46739351749420166, mae:0.5374512076377869, rmse:0.6836618185043335, mape:0.019266409799456596, mspe:0.0006105696666054428, rse:0.4778667092323303, r2_score:0.7229291328232861, acc:0.9807335902005434
corr: [37.21506  37.0898   37.273735 37.571274 37.254704 37.32902  37.164684
 37.24621  37.173985 37.368267 37.566074 37.424107 37.55219  37.70335
 37.612774 37.72577  37.567757 37.527992 37.69039  37.678524 37.841423
 37.617176 37.744316 37.675915 37.696026 37.598816 37.90585  38.027348
 38.080757 38.043736]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4468780
	speed: 0.1323s/iter; left time: 407.5233s
	iters: 200, epoch: 1 | loss: 0.3536949
	speed: 0.1200s/iter; left time: 357.6772s
	iters: 300, epoch: 1 | loss: 0.2677357
	speed: 0.1149s/iter; left time: 331.1550s
Epoch: 1 cost time: 39.062997817993164
Epoch: 1, Steps: 318 | Train Loss: 0.4611738 Vali Loss: 0.2826900 Test Loss: 0.2507377
Validation loss decreased (inf --> 0.282690).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2602695
	speed: 0.1320s/iter; left time: 364.8462s
	iters: 200, epoch: 2 | loss: 0.2545486
	speed: 0.1115s/iter; left time: 296.8621s
	iters: 300, epoch: 2 | loss: 0.2512940
	speed: 0.1211s/iter; left time: 310.4592s
Epoch: 2 cost time: 38.97038221359253
Epoch: 2, Steps: 318 | Train Loss: 0.2588991 Vali Loss: 0.2467850 Test Loss: 0.2162786
Validation loss decreased (0.282690 --> 0.246785).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2610074
	speed: 0.1374s/iter; left time: 336.0640s
	iters: 200, epoch: 3 | loss: 0.2572960
	speed: 0.1186s/iter; left time: 278.0581s
	iters: 300, epoch: 3 | loss: 0.2055071
	speed: 0.1202s/iter; left time: 269.8541s
Epoch: 3 cost time: 39.94227719306946
Epoch: 3, Steps: 318 | Train Loss: 0.2260280 Vali Loss: 0.2279178 Test Loss: 0.2019841
Validation loss decreased (0.246785 --> 0.227918).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1809205
	speed: 0.1350s/iter; left time: 287.1615s
	iters: 200, epoch: 4 | loss: 0.2179562
	speed: 0.1304s/iter; left time: 264.3878s
	iters: 300, epoch: 4 | loss: 0.2403034
	speed: 0.1113s/iter; left time: 214.4965s
Epoch: 4 cost time: 40.08516550064087
Epoch: 4, Steps: 318 | Train Loss: 0.2126318 Vali Loss: 0.2318746 Test Loss: 0.1966951
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2751169
	speed: 0.1311s/iter; left time: 237.2010s
	iters: 200, epoch: 5 | loss: 0.2227421
	speed: 0.1248s/iter; left time: 213.2730s
	iters: 300, epoch: 5 | loss: 0.2240704
	speed: 0.1208s/iter; left time: 194.4068s
Epoch: 5 cost time: 39.69882917404175
Epoch: 5, Steps: 318 | Train Loss: 0.2064910 Vali Loss: 0.2230973 Test Loss: 0.1942034
Validation loss decreased (0.227918 --> 0.223097).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1468635
	speed: 0.1317s/iter; left time: 196.3107s
	iters: 200, epoch: 6 | loss: 0.2447872
	speed: 0.1249s/iter; left time: 173.7843s
	iters: 300, epoch: 6 | loss: 0.2429444
	speed: 0.1213s/iter; left time: 156.5779s
Epoch: 6 cost time: 39.951740026474
Epoch: 6, Steps: 318 | Train Loss: 0.2043543 Vali Loss: 0.2235277 Test Loss: 0.1928655
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.3082744
	speed: 0.1260s/iter; left time: 147.8073s
	iters: 200, epoch: 7 | loss: 0.2657624
	speed: 0.1268s/iter; left time: 136.0441s
	iters: 300, epoch: 7 | loss: 0.1347595
	speed: 0.1263s/iter; left time: 122.8992s
Epoch: 7 cost time: 40.378257751464844
Epoch: 7, Steps: 318 | Train Loss: 0.2022841 Vali Loss: 0.2229311 Test Loss: 0.1926772
Validation loss decreased (0.223097 --> 0.222931).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2286172
	speed: 0.1273s/iter; left time: 108.8695s
	iters: 200, epoch: 8 | loss: 0.2210903
	speed: 0.1264s/iter; left time: 95.3944s
	iters: 300, epoch: 8 | loss: 0.2043124
	speed: 0.1199s/iter; left time: 78.5181s
Epoch: 8 cost time: 39.60691952705383
Epoch: 8, Steps: 318 | Train Loss: 0.2017026 Vali Loss: 0.2221626 Test Loss: 0.1923799
Validation loss decreased (0.222931 --> 0.222163).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1406984
	speed: 0.1213s/iter; left time: 65.1253s
	iters: 200, epoch: 9 | loss: 0.1493482
	speed: 0.1201s/iter; left time: 52.4879s
	iters: 300, epoch: 9 | loss: 0.2228682
	speed: 0.1244s/iter; left time: 41.9343s
Epoch: 9 cost time: 38.444180488586426
Epoch: 9, Steps: 318 | Train Loss: 0.2005533 Vali Loss: 0.2235749 Test Loss: 0.1921495
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2467587
	speed: 0.1326s/iter; left time: 29.0380s
	iters: 200, epoch: 10 | loss: 0.2128354
	speed: 0.1167s/iter; left time: 13.8843s
	iters: 300, epoch: 10 | loss: 0.1996384
	speed: 0.1189s/iter; left time: 2.2590s
Epoch: 10 cost time: 39.21884799003601
Epoch: 10, Steps: 318 | Train Loss: 0.2007597 Vali Loss: 0.2226889 Test Loss: 0.1920570
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.42217016220092773, mae:0.5135907530784607, rmse:0.6497462391853333, mape:0.018385108560323715, mspe:0.0005477869999594986, rse:0.45526838302612305, r2_score:0.761141064217182, acc:0.9816148914396763
corr: [37.77535  37.74034  37.836246 37.6287   37.79433  37.599606 37.603504
 37.686012 37.456947 37.766056]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3617591
	speed: 0.1336s/iter; left time: 411.6896s
	iters: 200, epoch: 1 | loss: 0.2864440
	speed: 0.1046s/iter; left time: 311.6827s
	iters: 300, epoch: 1 | loss: 0.3401511
	speed: 0.1189s/iter; left time: 342.5273s
Epoch: 1 cost time: 37.913586139678955
Epoch: 1, Steps: 318 | Train Loss: 0.4035788 Vali Loss: 0.2301380 Test Loss: 0.2041219
Validation loss decreased (inf --> 0.230138).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2211282
	speed: 0.1285s/iter; left time: 355.0778s
	iters: 200, epoch: 2 | loss: 0.1482374
	speed: 0.1170s/iter; left time: 311.5058s
	iters: 300, epoch: 2 | loss: 0.1814330
	speed: 0.1209s/iter; left time: 309.9503s
Epoch: 2 cost time: 38.98901176452637
Epoch: 2, Steps: 318 | Train Loss: 0.2203129 Vali Loss: 0.2102955 Test Loss: 0.1820647
Validation loss decreased (0.230138 --> 0.210295).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1876576
	speed: 0.1270s/iter; left time: 310.4584s
	iters: 200, epoch: 3 | loss: 0.1823400
	speed: 0.1180s/iter; left time: 276.6000s
	iters: 300, epoch: 3 | loss: 0.1756451
	speed: 0.1244s/iter; left time: 279.2670s
Epoch: 3 cost time: 39.191537380218506
Epoch: 3, Steps: 318 | Train Loss: 0.2036149 Vali Loss: 0.2040327 Test Loss: 0.1759001
Validation loss decreased (0.210295 --> 0.204033).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2551356
	speed: 0.1209s/iter; left time: 257.2263s
	iters: 200, epoch: 4 | loss: 0.1949660
	speed: 0.1268s/iter; left time: 256.9739s
	iters: 300, epoch: 4 | loss: 0.1920867
	speed: 0.1143s/iter; left time: 220.1797s
Epoch: 4 cost time: 37.995436906814575
Epoch: 4, Steps: 318 | Train Loss: 0.1976341 Vali Loss: 0.2028987 Test Loss: 0.1733272
Validation loss decreased (0.204033 --> 0.202899).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1684913
	speed: 0.1292s/iter; left time: 233.6962s
	iters: 200, epoch: 5 | loss: 0.1922841
	speed: 0.1261s/iter; left time: 215.5590s
	iters: 300, epoch: 5 | loss: 0.2005197
	speed: 0.1151s/iter; left time: 185.1710s
Epoch: 5 cost time: 39.2794976234436
Epoch: 5, Steps: 318 | Train Loss: 0.1951212 Vali Loss: 0.2013349 Test Loss: 0.1716336
Validation loss decreased (0.202899 --> 0.201335).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2195489
	speed: 0.1259s/iter; left time: 187.7816s
	iters: 200, epoch: 6 | loss: 0.1888050
	speed: 0.1222s/iter; left time: 170.0397s
	iters: 300, epoch: 6 | loss: 0.2437262
	speed: 0.1220s/iter; left time: 157.4951s
Epoch: 6 cost time: 39.30109930038452
Epoch: 6, Steps: 318 | Train Loss: 0.1935839 Vali Loss: 0.2020710 Test Loss: 0.1729956
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1541741
	speed: 0.1213s/iter; left time: 142.2397s
	iters: 200, epoch: 7 | loss: 0.1876604
	speed: 0.1213s/iter; left time: 130.1093s
	iters: 300, epoch: 7 | loss: 0.1703633
	speed: 0.1221s/iter; left time: 118.7709s
Epoch: 7 cost time: 38.74971413612366
Epoch: 7, Steps: 318 | Train Loss: 0.1929456 Vali Loss: 0.2010214 Test Loss: 0.1718627
Validation loss decreased (0.201335 --> 0.201021).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2352742
	speed: 0.1359s/iter; left time: 116.1682s
	iters: 200, epoch: 8 | loss: 0.2170791
	speed: 0.1217s/iter; left time: 91.8897s
	iters: 300, epoch: 8 | loss: 0.1642618
	speed: 0.1183s/iter; left time: 77.4835s
Epoch: 8 cost time: 39.679986238479614
Epoch: 8, Steps: 318 | Train Loss: 0.1927219 Vali Loss: 0.2003976 Test Loss: 0.1713434
Validation loss decreased (0.201021 --> 0.200398).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2150998
	speed: 0.1332s/iter; left time: 71.5454s
	iters: 200, epoch: 9 | loss: 0.2057863
	speed: 0.1221s/iter; left time: 53.3750s
	iters: 300, epoch: 9 | loss: 0.1672928
	speed: 0.1168s/iter; left time: 39.3542s
Epoch: 9 cost time: 39.57249212265015
Epoch: 9, Steps: 318 | Train Loss: 0.1924489 Vali Loss: 0.1997305 Test Loss: 0.1712535
Validation loss decreased (0.200398 --> 0.199731).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2060291
	speed: 0.1291s/iter; left time: 28.2673s
	iters: 200, epoch: 10 | loss: 0.1446117
	speed: 0.1293s/iter; left time: 15.3866s
	iters: 300, epoch: 10 | loss: 0.1898538
	speed: 0.1229s/iter; left time: 2.3345s
Epoch: 10 cost time: 40.24194359779358
Epoch: 10, Steps: 318 | Train Loss: 0.1922606 Vali Loss: 0.2014505 Test Loss: 0.1711879
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.37580907344818115, mae:0.4806705117225647, rmse:0.6130326986312866, mape:0.017197301611304283, mspe:0.0004876764724031091, rse:0.4293414354324341, r2_score:0.7844846052804615, acc:0.9828026983886957
corr: [37.52263  37.5462   37.513786 37.59551  37.514793 37.505604 37.692554
 37.594593 37.568447 37.698673 37.460476 37.540096 37.663803 37.777416
 37.715256]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4080604
	speed: 0.1350s/iter; left time: 416.0466s
	iters: 200, epoch: 1 | loss: 0.2880072
	speed: 0.1527s/iter; left time: 455.2443s
	iters: 300, epoch: 1 | loss: 0.1909652
	speed: 0.1348s/iter; left time: 388.4844s
Epoch: 1 cost time: 45.03401327133179
Epoch: 1, Steps: 318 | Train Loss: 0.4622880 Vali Loss: 0.2868941 Test Loss: 0.2573185
Validation loss decreased (inf --> 0.286894).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2667037
	speed: 0.1631s/iter; left time: 450.6868s
	iters: 200, epoch: 2 | loss: 0.2390714
	speed: 0.1329s/iter; left time: 353.9300s
	iters: 300, epoch: 2 | loss: 0.2664653
	speed: 0.1402s/iter; left time: 359.2381s
Epoch: 2 cost time: 45.658759355545044
Epoch: 2, Steps: 318 | Train Loss: 0.2549497 Vali Loss: 0.2433999 Test Loss: 0.2280815
Validation loss decreased (0.286894 --> 0.243400).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2718728
	speed: 0.1633s/iter; left time: 399.2577s
	iters: 200, epoch: 3 | loss: 0.2492590
	speed: 0.1367s/iter; left time: 320.4716s
	iters: 300, epoch: 3 | loss: 0.2311157
	speed: 0.1488s/iter; left time: 333.9649s
Epoch: 3 cost time: 47.46467089653015
Epoch: 3, Steps: 318 | Train Loss: 0.2350044 Vali Loss: 0.2345139 Test Loss: 0.2190225
Validation loss decreased (0.243400 --> 0.234514).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2230990
	speed: 0.1737s/iter; left time: 369.4032s
	iters: 200, epoch: 4 | loss: 0.2930783
	speed: 0.1521s/iter; left time: 308.2956s
	iters: 300, epoch: 4 | loss: 0.2475202
	speed: 0.1566s/iter; left time: 301.8047s
Epoch: 4 cost time: 51.10429048538208
Epoch: 4, Steps: 318 | Train Loss: 0.2272062 Vali Loss: 0.2308345 Test Loss: 0.2229461
Validation loss decreased (0.234514 --> 0.230835).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2433012
	speed: 0.1682s/iter; left time: 304.2832s
	iters: 200, epoch: 5 | loss: 0.2668860
	speed: 0.1557s/iter; left time: 266.0106s
	iters: 300, epoch: 5 | loss: 0.2352493
	speed: 0.1418s/iter; left time: 228.0898s
Epoch: 5 cost time: 49.83555817604065
Epoch: 5, Steps: 318 | Train Loss: 0.2237150 Vali Loss: 0.2302969 Test Loss: 0.2206830
Validation loss decreased (0.230835 --> 0.230297).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2418712
	speed: 0.1444s/iter; left time: 215.2701s
	iters: 200, epoch: 6 | loss: 0.1835114
	speed: 0.1447s/iter; left time: 201.2574s
	iters: 300, epoch: 6 | loss: 0.3471648
	speed: 0.1475s/iter; left time: 190.3925s
Epoch: 6 cost time: 46.03775429725647
Epoch: 6, Steps: 318 | Train Loss: 0.2221196 Vali Loss: 0.2275616 Test Loss: 0.2199200
Validation loss decreased (0.230297 --> 0.227562).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1729912
	speed: 0.1409s/iter; left time: 165.3194s
	iters: 200, epoch: 7 | loss: 0.1782316
	speed: 0.1429s/iter; left time: 153.3253s
	iters: 300, epoch: 7 | loss: 0.1827825
	speed: 0.1518s/iter; left time: 147.7415s
Epoch: 7 cost time: 46.15716338157654
Epoch: 7, Steps: 318 | Train Loss: 0.2214107 Vali Loss: 0.2278563 Test Loss: 0.2200304
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1994959
	speed: 0.1636s/iter; left time: 139.8533s
	iters: 200, epoch: 8 | loss: 0.1744128
	speed: 0.1391s/iter; left time: 105.0005s
	iters: 300, epoch: 8 | loss: 0.2488179
	speed: 0.1485s/iter; left time: 97.2927s
Epoch: 8 cost time: 47.88271737098694
Epoch: 8, Steps: 318 | Train Loss: 0.2208586 Vali Loss: 0.2280993 Test Loss: 0.2201152
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1660978
	speed: 0.1578s/iter; left time: 84.7160s
	iters: 200, epoch: 9 | loss: 0.2507375
	speed: 0.1536s/iter; left time: 67.1291s
	iters: 300, epoch: 9 | loss: 0.1839244
	speed: 0.1405s/iter; left time: 47.3413s
Epoch: 9 cost time: 47.78032445907593
Epoch: 9, Steps: 318 | Train Loss: 0.2207734 Vali Loss: 0.2290416 Test Loss: 0.2198697
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.4826057553291321, mae:0.5551137924194336, rmse:0.6946983337402344, mape:0.0198192335665226, mspe:0.0006194837624207139, rse:0.4862547814846039, r2_score:0.7260172956189402, acc:0.9801807664334774
corr: [38.24879  37.33559  37.798668 37.678757 38.10386  37.566975 37.745
 37.38364  37.625088 37.484062 37.494946 37.915436 37.912582 37.741028
 37.594467 37.539684 38.121975 37.941612 38.01533  38.02556 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3803054
	speed: 0.1028s/iter; left time: 316.6467s
	iters: 200, epoch: 1 | loss: 0.2340351
	speed: 0.0986s/iter; left time: 294.0307s
	iters: 300, epoch: 1 | loss: 0.3562632
	speed: 0.0931s/iter; left time: 268.1103s
Epoch: 1 cost time: 31.084128379821777
Epoch: 1, Steps: 318 | Train Loss: 0.4119199 Vali Loss: 0.2828563 Test Loss: 0.2527570
Validation loss decreased (inf --> 0.282856).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2633338
	speed: 0.1015s/iter; left time: 280.4571s
	iters: 200, epoch: 2 | loss: 0.2693367
	speed: 0.0950s/iter; left time: 253.0925s
	iters: 300, epoch: 2 | loss: 0.2766339
	speed: 0.0968s/iter; left time: 248.1903s
Epoch: 2 cost time: 31.139090538024902
Epoch: 2, Steps: 318 | Train Loss: 0.2469044 Vali Loss: 0.2555149 Test Loss: 0.2221558
Validation loss decreased (0.282856 --> 0.255515).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1934712
	speed: 0.1052s/iter; left time: 257.1638s
	iters: 200, epoch: 3 | loss: 0.2159446
	speed: 0.0942s/iter; left time: 220.8698s
	iters: 300, epoch: 3 | loss: 0.1816251
	speed: 0.0904s/iter; left time: 203.0430s
Epoch: 3 cost time: 30.769426584243774
Epoch: 3, Steps: 318 | Train Loss: 0.2240247 Vali Loss: 0.2473095 Test Loss: 0.2145989
Validation loss decreased (0.255515 --> 0.247310).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2303571
	speed: 0.1025s/iter; left time: 218.0543s
	iters: 200, epoch: 4 | loss: 0.2073753
	speed: 0.0891s/iter; left time: 180.5732s
	iters: 300, epoch: 4 | loss: 0.2018631
	speed: 0.0921s/iter; left time: 177.4943s
Epoch: 4 cost time: 30.211313486099243
Epoch: 4, Steps: 318 | Train Loss: 0.2164058 Vali Loss: 0.2443591 Test Loss: 0.2116896
Validation loss decreased (0.247310 --> 0.244359).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1612876
	speed: 0.0996s/iter; left time: 180.1446s
	iters: 200, epoch: 5 | loss: 0.2044464
	speed: 0.0932s/iter; left time: 159.3409s
	iters: 300, epoch: 5 | loss: 0.2242865
	speed: 0.0952s/iter; left time: 153.1196s
Epoch: 5 cost time: 30.4416401386261
Epoch: 5, Steps: 318 | Train Loss: 0.2130499 Vali Loss: 0.2424274 Test Loss: 0.2097897
Validation loss decreased (0.244359 --> 0.242427).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2316178
	speed: 0.1062s/iter; left time: 158.3351s
	iters: 200, epoch: 6 | loss: 0.2068549
	speed: 0.0976s/iter; left time: 135.7585s
	iters: 300, epoch: 6 | loss: 0.2028380
	speed: 0.0915s/iter; left time: 118.1879s
Epoch: 6 cost time: 31.160505294799805
Epoch: 6, Steps: 318 | Train Loss: 0.2120546 Vali Loss: 0.2437660 Test Loss: 0.2085464
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1938191
	speed: 0.1035s/iter; left time: 121.3959s
	iters: 200, epoch: 7 | loss: 0.1811571
	speed: 0.0960s/iter; left time: 102.9660s
	iters: 300, epoch: 7 | loss: 0.2369248
	speed: 0.0961s/iter; left time: 93.5046s
Epoch: 7 cost time: 31.278239965438843
Epoch: 7, Steps: 318 | Train Loss: 0.2110569 Vali Loss: 0.2412754 Test Loss: 0.2078396
Validation loss decreased (0.242427 --> 0.241275).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2144343
	speed: 0.1072s/iter; left time: 91.6324s
	iters: 200, epoch: 8 | loss: 0.2292920
	speed: 0.1003s/iter; left time: 75.6962s
	iters: 300, epoch: 8 | loss: 0.1667711
	speed: 0.0960s/iter; left time: 62.8965s
Epoch: 8 cost time: 32.16283583641052
Epoch: 8, Steps: 318 | Train Loss: 0.2108163 Vali Loss: 0.2423805 Test Loss: 0.2078119
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2615699
	speed: 0.1046s/iter; left time: 56.1654s
	iters: 200, epoch: 9 | loss: 0.2366056
	speed: 0.0909s/iter; left time: 39.7379s
	iters: 300, epoch: 9 | loss: 0.2181218
	speed: 0.0979s/iter; left time: 32.9773s
Epoch: 9 cost time: 31.346133708953857
Epoch: 9, Steps: 318 | Train Loss: 0.2104541 Vali Loss: 0.2404705 Test Loss: 0.2077660
Validation loss decreased (0.241275 --> 0.240471).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1928058
	speed: 0.1022s/iter; left time: 22.3751s
	iters: 200, epoch: 10 | loss: 0.1610955
	speed: 0.0956s/iter; left time: 11.3750s
	iters: 300, epoch: 10 | loss: 0.1903637
	speed: 0.1044s/iter; left time: 1.9837s
Epoch: 10 cost time: 31.84836196899414
Epoch: 10, Steps: 318 | Train Loss: 0.2101277 Vali Loss: 0.2417741 Test Loss: 0.2077296
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.4559343755245209, mae:0.5333616733551025, rmse:0.6752291321754456, mape:0.019114376977086067, mspe:0.0005957297398708761, rse:0.4723115563392639, r2_score:0.7354941065824473, acc:0.9808856230229139
corr: [37.1635   37.121315 37.12679  37.314857 37.355354 37.557903 37.292946
 37.287106 37.19652  37.395046 37.268925 37.624424 37.65815  37.51921
 37.501633 37.436264 37.62264  37.56647  37.809322 38.115532 37.98699
 38.067333 38.373318 38.311047 38.278763]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4349106
	speed: 0.1530s/iter; left time: 470.0131s
	iters: 200, epoch: 1 | loss: 0.2657806
	speed: 0.1607s/iter; left time: 477.5171s
	iters: 300, epoch: 1 | loss: 0.3269092
	speed: 0.1502s/iter; left time: 431.0816s
Epoch: 1 cost time: 49.38598370552063
Epoch: 1, Steps: 317 | Train Loss: 0.4237163 Vali Loss: 0.2548561 Test Loss: 0.2351181
Validation loss decreased (inf --> 0.254856).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2854723
	speed: 0.1725s/iter; left time: 475.1097s
	iters: 200, epoch: 2 | loss: 0.2151926
	speed: 0.1626s/iter; left time: 431.4168s
	iters: 300, epoch: 2 | loss: 0.2681691
	speed: 0.1817s/iter; left time: 464.0285s
Epoch: 2 cost time: 55.43092083930969
Epoch: 2, Steps: 317 | Train Loss: 0.2484019 Vali Loss: 0.2291500 Test Loss: 0.2200179
Validation loss decreased (0.254856 --> 0.229150).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2168396
	speed: 0.1767s/iter; left time: 430.7269s
	iters: 200, epoch: 3 | loss: 0.2573122
	speed: 0.1595s/iter; left time: 372.8302s
	iters: 300, epoch: 3 | loss: 0.2069004
	speed: 0.1709s/iter; left time: 382.3519s
Epoch: 3 cost time: 53.33757424354553
Epoch: 3, Steps: 317 | Train Loss: 0.2324669 Vali Loss: 0.2253567 Test Loss: 0.2116899
Validation loss decreased (0.229150 --> 0.225357).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2088292
	speed: 0.1548s/iter; left time: 328.1603s
	iters: 200, epoch: 4 | loss: 0.1999460
	speed: 0.1537s/iter; left time: 310.4555s
	iters: 300, epoch: 4 | loss: 0.2192504
	speed: 0.1654s/iter; left time: 317.6542s
Epoch: 4 cost time: 49.70197606086731
Epoch: 4, Steps: 317 | Train Loss: 0.2268936 Vali Loss: 0.2273720 Test Loss: 0.2101618
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2218028
	speed: 0.1703s/iter; left time: 307.1045s
	iters: 200, epoch: 5 | loss: 0.2332729
	speed: 0.1735s/iter; left time: 295.4999s
	iters: 300, epoch: 5 | loss: 0.2273972
	speed: 0.1785s/iter; left time: 286.1099s
Epoch: 5 cost time: 54.20879340171814
Epoch: 5, Steps: 317 | Train Loss: 0.2241198 Vali Loss: 0.2263744 Test Loss: 0.2090603
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1784208
	speed: 0.1793s/iter; left time: 266.4617s
	iters: 200, epoch: 6 | loss: 0.2206970
	speed: 0.1795s/iter; left time: 248.7268s
	iters: 300, epoch: 6 | loss: 0.2781249
	speed: 0.1711s/iter; left time: 220.0313s
Epoch: 6 cost time: 56.03990316390991
Epoch: 6, Steps: 317 | Train Loss: 0.2227386 Vali Loss: 0.2264227 Test Loss: 0.2077860
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.4645451009273529, mae:0.5372238755226135, rmse:0.6815754771232605, mape:0.019256742671132088, mspe:0.0006067572976462543, rse:0.47640836238861084, r2_score:0.7266979646363357, acc:0.9807432573288679
corr: [37.272602 37.212967 37.32284  37.696297 37.339775 37.59798  37.42376
 37.406166 37.262184 37.38158  37.62608  37.404892 37.519325 37.88643
 37.817135 37.816456 37.591957 37.536007 37.76821  37.682213 37.89686
 37.83159  37.90367  37.876625 37.800076 37.618298 37.930252 37.986637
 37.97945  38.0496  ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4670516
	speed: 0.1438s/iter; left time: 443.0342s
	iters: 200, epoch: 1 | loss: 0.3562161
	speed: 0.1281s/iter; left time: 381.9490s
	iters: 300, epoch: 1 | loss: 0.2745267
	speed: 0.1188s/iter; left time: 342.2342s
Epoch: 1 cost time: 41.45884704589844
Epoch: 1, Steps: 318 | Train Loss: 0.4637406 Vali Loss: 0.3211573 Test Loss: 0.2692098
Validation loss decreased (inf --> 0.321157).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2303374
	speed: 0.1229s/iter; left time: 339.4734s
	iters: 200, epoch: 2 | loss: 0.2986849
	speed: 0.1249s/iter; left time: 332.6367s
	iters: 300, epoch: 2 | loss: 0.2731766
	speed: 0.1242s/iter; left time: 318.2491s
Epoch: 2 cost time: 39.71086764335632
Epoch: 2, Steps: 318 | Train Loss: 0.2609012 Vali Loss: 0.2688553 Test Loss: 0.2276331
Validation loss decreased (0.321157 --> 0.268855).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2774492
	speed: 0.1367s/iter; left time: 334.2613s
	iters: 200, epoch: 3 | loss: 0.2609743
	speed: 0.1183s/iter; left time: 277.4736s
	iters: 300, epoch: 3 | loss: 0.2295300
	speed: 0.1233s/iter; left time: 276.7624s
Epoch: 3 cost time: 40.43742251396179
Epoch: 3, Steps: 318 | Train Loss: 0.2309895 Vali Loss: 0.2527357 Test Loss: 0.2143258
Validation loss decreased (0.268855 --> 0.252736).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1864475
	speed: 0.1246s/iter; left time: 265.0189s
	iters: 200, epoch: 4 | loss: 0.2369061
	speed: 0.1279s/iter; left time: 259.3473s
	iters: 300, epoch: 4 | loss: 0.2253679
	speed: 0.1330s/iter; left time: 256.3466s
Epoch: 4 cost time: 40.86420917510986
Epoch: 4, Steps: 318 | Train Loss: 0.2169347 Vali Loss: 0.2531364 Test Loss: 0.2137497
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2776905
	speed: 0.1386s/iter; left time: 250.8142s
	iters: 200, epoch: 5 | loss: 0.2311918
	speed: 0.1252s/iter; left time: 213.9383s
	iters: 300, epoch: 5 | loss: 0.2102785
	speed: 0.1164s/iter; left time: 187.2661s
Epoch: 5 cost time: 40.316150188446045
Epoch: 5, Steps: 318 | Train Loss: 0.2105277 Vali Loss: 0.2452422 Test Loss: 0.2107648
Validation loss decreased (0.252736 --> 0.245242).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1670256
	speed: 0.1353s/iter; left time: 201.7215s
	iters: 200, epoch: 6 | loss: 0.2249619
	speed: 0.1314s/iter; left time: 182.7450s
	iters: 300, epoch: 6 | loss: 0.2530535
	speed: 0.1252s/iter; left time: 161.6703s
Epoch: 6 cost time: 41.46109962463379
Epoch: 6, Steps: 318 | Train Loss: 0.2078696 Vali Loss: 0.2470919 Test Loss: 0.2104832
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.3314785
	speed: 0.1374s/iter; left time: 161.1569s
	iters: 200, epoch: 7 | loss: 0.2121596
	speed: 0.1340s/iter; left time: 143.7815s
	iters: 300, epoch: 7 | loss: 0.1329027
	speed: 0.1379s/iter; left time: 134.1552s
Epoch: 7 cost time: 43.32538676261902
Epoch: 7, Steps: 318 | Train Loss: 0.2058959 Vali Loss: 0.2462521 Test Loss: 0.2098027
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2130535
	speed: 0.1342s/iter; left time: 114.7064s
	iters: 200, epoch: 8 | loss: 0.2162108
	speed: 0.1253s/iter; left time: 94.5698s
	iters: 300, epoch: 8 | loss: 0.2045883
	speed: 0.1247s/iter; left time: 81.6755s
Epoch: 8 cost time: 40.95946764945984
Epoch: 8, Steps: 318 | Train Loss: 0.2053346 Vali Loss: 0.2464461 Test Loss: 0.2100393
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.4625151753425598, mae:0.5395961999893188, rmse:0.6800847053527832, mape:0.019290393218398094, mspe:0.0005976964021101594, rse:0.4765261113643646, r2_score:0.7374041108625665, acc:0.9807096067816019
corr: [37.75477  37.759193 37.585545 37.68072  37.774494 37.884586 37.84424
 37.96584  37.999615 38.152596]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3621493
	speed: 0.1371s/iter; left time: 422.2932s
	iters: 200, epoch: 1 | loss: 0.2870128
	speed: 0.1238s/iter; left time: 368.9299s
	iters: 300, epoch: 1 | loss: 0.3423755
	speed: 0.1212s/iter; left time: 349.2085s
Epoch: 1 cost time: 40.35435390472412
Epoch: 1, Steps: 318 | Train Loss: 0.4041925 Vali Loss: 0.2295775 Test Loss: 0.2051338
Validation loss decreased (inf --> 0.229577).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2224598
	speed: 0.1181s/iter; left time: 326.4423s
	iters: 200, epoch: 2 | loss: 0.1481539
	speed: 0.1125s/iter; left time: 299.5252s
	iters: 300, epoch: 2 | loss: 0.1824853
	speed: 0.0995s/iter; left time: 254.9213s
Epoch: 2 cost time: 34.961944341659546
Epoch: 2, Steps: 318 | Train Loss: 0.2208327 Vali Loss: 0.2098381 Test Loss: 0.1822443
Validation loss decreased (0.229577 --> 0.209838).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1880719
	speed: 0.1133s/iter; left time: 277.0553s
	iters: 200, epoch: 3 | loss: 0.1826310
	speed: 0.1036s/iter; left time: 242.9367s
	iters: 300, epoch: 3 | loss: 0.1756230
	speed: 0.1035s/iter; left time: 232.3221s
Epoch: 3 cost time: 34.107436418533325
Epoch: 3, Steps: 318 | Train Loss: 0.2039231 Vali Loss: 0.2036723 Test Loss: 0.1759106
Validation loss decreased (0.209838 --> 0.203672).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2554928
	speed: 0.1140s/iter; left time: 242.4037s
	iters: 200, epoch: 4 | loss: 0.1947364
	speed: 0.1053s/iter; left time: 213.5157s
	iters: 300, epoch: 4 | loss: 0.1918087
	speed: 0.1016s/iter; left time: 195.8527s
Epoch: 4 cost time: 34.13585186004639
Epoch: 4, Steps: 318 | Train Loss: 0.1978871 Vali Loss: 0.2025725 Test Loss: 0.1732755
Validation loss decreased (0.203672 --> 0.202573).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1688700
	speed: 0.1079s/iter; left time: 195.1562s
	iters: 200, epoch: 5 | loss: 0.1920839
	speed: 0.0934s/iter; left time: 159.5730s
	iters: 300, epoch: 5 | loss: 0.2005979
	speed: 0.1040s/iter; left time: 167.3525s
Epoch: 5 cost time: 32.632214307785034
Epoch: 5, Steps: 318 | Train Loss: 0.1952522 Vali Loss: 0.2010415 Test Loss: 0.1716233
Validation loss decreased (0.202573 --> 0.201041).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2193352
	speed: 0.1057s/iter; left time: 157.6477s
	iters: 200, epoch: 6 | loss: 0.1886930
	speed: 0.1090s/iter; left time: 151.6473s
	iters: 300, epoch: 6 | loss: 0.2444100
	speed: 0.0978s/iter; left time: 126.2021s
Epoch: 6 cost time: 32.520625829696655
Epoch: 6, Steps: 318 | Train Loss: 0.1937898 Vali Loss: 0.2017868 Test Loss: 0.1729819
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1537701
	speed: 0.1076s/iter; left time: 126.2485s
	iters: 200, epoch: 7 | loss: 0.1878602
	speed: 0.1056s/iter; left time: 113.3219s
	iters: 300, epoch: 7 | loss: 0.1706719
	speed: 0.0987s/iter; left time: 96.0273s
Epoch: 7 cost time: 32.79290747642517
Epoch: 7, Steps: 318 | Train Loss: 0.1931483 Vali Loss: 0.2007446 Test Loss: 0.1718338
Validation loss decreased (0.201041 --> 0.200745).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2352584
	speed: 0.1093s/iter; left time: 93.4408s
	iters: 200, epoch: 8 | loss: 0.2171443
	speed: 0.1047s/iter; left time: 79.0134s
	iters: 300, epoch: 8 | loss: 0.1646037
	speed: 0.1008s/iter; left time: 66.0023s
Epoch: 8 cost time: 33.55533289909363
Epoch: 8, Steps: 318 | Train Loss: 0.1929177 Vali Loss: 0.2001489 Test Loss: 0.1713121
Validation loss decreased (0.200745 --> 0.200149).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2154890
	speed: 0.1110s/iter; left time: 59.6236s
	iters: 200, epoch: 9 | loss: 0.2060899
	speed: 0.0934s/iter; left time: 40.7951s
	iters: 300, epoch: 9 | loss: 0.1670446
	speed: 0.1068s/iter; left time: 35.9952s
Epoch: 9 cost time: 33.29474210739136
Epoch: 9, Steps: 318 | Train Loss: 0.1926405 Vali Loss: 0.1994673 Test Loss: 0.1712215
Validation loss decreased (0.200149 --> 0.199467).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2062093
	speed: 0.1096s/iter; left time: 23.9972s
	iters: 200, epoch: 10 | loss: 0.1445919
	speed: 0.1015s/iter; left time: 12.0794s
	iters: 300, epoch: 10 | loss: 0.1902442
	speed: 0.1001s/iter; left time: 1.9017s
Epoch: 10 cost time: 33.22112679481506
Epoch: 10, Steps: 318 | Train Loss: 0.1924556 Vali Loss: 0.2011816 Test Loss: 0.1711531
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.3757389485836029, mae:0.48067331314086914, rmse:0.6129754781723022, mape:0.017196621745824814, mspe:0.0004875010345131159, rse:0.4293014109134674, r2_score:0.7844849970282851, acc:0.9828033782541752
corr: [37.519787 37.538757 37.50652  37.58839  37.510155 37.4988   37.68584
 37.58724  37.565327 37.695965 37.456623 37.54348  37.663544 37.773067
 37.717434]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4041018
	speed: 0.1637s/iter; left time: 504.4143s
	iters: 200, epoch: 1 | loss: 0.2971793
	speed: 0.1592s/iter; left time: 474.6102s
	iters: 300, epoch: 1 | loss: 0.2041025
	speed: 0.1536s/iter; left time: 442.6181s
Epoch: 1 cost time: 50.769081830978394
Epoch: 1, Steps: 318 | Train Loss: 0.4655168 Vali Loss: 0.2692503 Test Loss: 0.2493235
Validation loss decreased (inf --> 0.269250).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2625388
	speed: 0.1693s/iter; left time: 467.7183s
	iters: 200, epoch: 2 | loss: 0.2541154
	speed: 0.1608s/iter; left time: 428.1098s
	iters: 300, epoch: 2 | loss: 0.2497448
	speed: 0.1559s/iter; left time: 399.4611s
Epoch: 2 cost time: 52.25996708869934
Epoch: 2, Steps: 318 | Train Loss: 0.2529267 Vali Loss: 0.2185787 Test Loss: 0.2198713
Validation loss decreased (0.269250 --> 0.218579).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2700936
	speed: 0.1572s/iter; left time: 384.4720s
	iters: 200, epoch: 3 | loss: 0.2468444
	speed: 0.1585s/iter; left time: 371.7542s
	iters: 300, epoch: 3 | loss: 0.2440353
	speed: 0.1617s/iter; left time: 362.9166s
Epoch: 3 cost time: 50.987412452697754
Epoch: 3, Steps: 318 | Train Loss: 0.2315056 Vali Loss: 0.2056651 Test Loss: 0.2092533
Validation loss decreased (0.218579 --> 0.205665).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2072454
	speed: 0.1540s/iter; left time: 327.4981s
	iters: 200, epoch: 4 | loss: 0.2695842
	speed: 0.1550s/iter; left time: 314.2073s
	iters: 300, epoch: 4 | loss: 0.2173937
	speed: 0.1591s/iter; left time: 306.5623s
Epoch: 4 cost time: 49.675297021865845
Epoch: 4, Steps: 318 | Train Loss: 0.2233744 Vali Loss: 0.1987134 Test Loss: 0.2099320
Validation loss decreased (0.205665 --> 0.198713).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2537089
	speed: 0.1688s/iter; left time: 305.3877s
	iters: 200, epoch: 5 | loss: 0.2695865
	speed: 0.1475s/iter; left time: 252.0423s
	iters: 300, epoch: 5 | loss: 0.2541114
	speed: 0.1571s/iter; left time: 252.7405s
Epoch: 5 cost time: 50.161131858825684
Epoch: 5, Steps: 318 | Train Loss: 0.2199027 Vali Loss: 0.1967165 Test Loss: 0.2078536
Validation loss decreased (0.198713 --> 0.196716).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2396907
	speed: 0.1609s/iter; left time: 239.9070s
	iters: 200, epoch: 6 | loss: 0.1802169
	speed: 0.1587s/iter; left time: 220.7116s
	iters: 300, epoch: 6 | loss: 0.3323604
	speed: 0.1534s/iter; left time: 198.0571s
Epoch: 6 cost time: 49.96300935745239
Epoch: 6, Steps: 318 | Train Loss: 0.2179424 Vali Loss: 0.1936200 Test Loss: 0.2072949
Validation loss decreased (0.196716 --> 0.193620).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1916890
	speed: 0.1639s/iter; left time: 192.2238s
	iters: 200, epoch: 7 | loss: 0.1808058
	speed: 0.1589s/iter; left time: 170.5006s
	iters: 300, epoch: 7 | loss: 0.1709010
	speed: 0.1559s/iter; left time: 151.7161s
Epoch: 7 cost time: 51.0488486289978
Epoch: 7, Steps: 318 | Train Loss: 0.2172603 Vali Loss: 0.1940912 Test Loss: 0.2069767
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2137276
	speed: 0.1685s/iter; left time: 144.0504s
	iters: 200, epoch: 8 | loss: 0.1711610
	speed: 0.1466s/iter; left time: 110.6600s
	iters: 300, epoch: 8 | loss: 0.2722619
	speed: 0.1587s/iter; left time: 103.9219s
Epoch: 8 cost time: 50.554068088531494
Epoch: 8, Steps: 318 | Train Loss: 0.2167542 Vali Loss: 0.1939693 Test Loss: 0.2066487
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1556918
	speed: 0.1760s/iter; left time: 94.5121s
	iters: 200, epoch: 9 | loss: 0.2067205
	speed: 0.1618s/iter; left time: 70.7127s
	iters: 300, epoch: 9 | loss: 0.1874578
	speed: 0.1445s/iter; left time: 48.7039s
Epoch: 9 cost time: 50.946988582611084
Epoch: 9, Steps: 318 | Train Loss: 0.2163196 Vali Loss: 0.1945307 Test Loss: 0.2064834
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.45490050315856934, mae:0.5356160998344421, rmse:0.6744630932807922, mape:0.01913609355688095, mspe:0.0005858828080818057, rse:0.47209107875823975, r2_score:0.7403654412881501, acc:0.980863906443119
corr: [38.040028 37.63226  37.6888   37.52558  37.891678 37.440712 37.5176
 37.222282 37.470997 37.51611  37.530663 37.796066 37.674957 37.518158
 37.41058  37.45962  37.86527  37.577656 37.754173 37.7184  ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3586610
	speed: 0.1208s/iter; left time: 372.2121s
	iters: 200, epoch: 1 | loss: 0.2212865
	speed: 0.0984s/iter; left time: 293.4335s
	iters: 300, epoch: 1 | loss: 0.3384430
	speed: 0.1003s/iter; left time: 288.9265s
Epoch: 1 cost time: 33.89537239074707
Epoch: 1, Steps: 318 | Train Loss: 0.4075599 Vali Loss: 0.2918587 Test Loss: 0.2569543
Validation loss decreased (inf --> 0.291859).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2612780
	speed: 0.1104s/iter; left time: 304.9254s
	iters: 200, epoch: 2 | loss: 0.2875506
	speed: 0.1012s/iter; left time: 269.5965s
	iters: 300, epoch: 2 | loss: 0.2736927
	speed: 0.1115s/iter; left time: 285.8547s
Epoch: 2 cost time: 34.442930698394775
Epoch: 2, Steps: 318 | Train Loss: 0.2517332 Vali Loss: 0.2443615 Test Loss: 0.2237281
Validation loss decreased (0.291859 --> 0.244361).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1937941
	speed: 0.1041s/iter; left time: 254.6329s
	iters: 200, epoch: 3 | loss: 0.2187582
	speed: 0.1058s/iter; left time: 248.1157s
	iters: 300, epoch: 3 | loss: 0.1717569
	speed: 0.1104s/iter; left time: 247.8519s
Epoch: 3 cost time: 34.10297775268555
Epoch: 3, Steps: 318 | Train Loss: 0.2289157 Vali Loss: 0.2399024 Test Loss: 0.2163123
Validation loss decreased (0.244361 --> 0.239902).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2322012
	speed: 0.1239s/iter; left time: 263.5231s
	iters: 200, epoch: 4 | loss: 0.2057399
	speed: 0.1094s/iter; left time: 221.8244s
	iters: 300, epoch: 4 | loss: 0.2198061
	speed: 0.1153s/iter; left time: 222.2153s
Epoch: 4 cost time: 36.67087960243225
Epoch: 4, Steps: 318 | Train Loss: 0.2212017 Vali Loss: 0.2375352 Test Loss: 0.2130997
Validation loss decreased (0.239902 --> 0.237535).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1540713
	speed: 0.1115s/iter; left time: 201.6843s
	iters: 200, epoch: 5 | loss: 0.2144584
	speed: 0.1125s/iter; left time: 192.2934s
	iters: 300, epoch: 5 | loss: 0.2407357
	speed: 0.1092s/iter; left time: 175.7679s
Epoch: 5 cost time: 35.484416007995605
Epoch: 5, Steps: 318 | Train Loss: 0.2180415 Vali Loss: 0.2362331 Test Loss: 0.2102681
Validation loss decreased (0.237535 --> 0.236233).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2335464
	speed: 0.1134s/iter; left time: 169.1055s
	iters: 200, epoch: 6 | loss: 0.2151158
	speed: 0.1080s/iter; left time: 150.2105s
	iters: 300, epoch: 6 | loss: 0.2105656
	speed: 0.1128s/iter; left time: 145.6541s
Epoch: 6 cost time: 35.495460987091064
Epoch: 6, Steps: 318 | Train Loss: 0.2165848 Vali Loss: 0.2369793 Test Loss: 0.2097627
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1940445
	speed: 0.1191s/iter; left time: 139.6811s
	iters: 200, epoch: 7 | loss: 0.1953078
	speed: 0.1149s/iter; left time: 123.3404s
	iters: 300, epoch: 7 | loss: 0.2295953
	speed: 0.1123s/iter; left time: 109.2504s
Epoch: 7 cost time: 36.85725402832031
Epoch: 7, Steps: 318 | Train Loss: 0.2158993 Vali Loss: 0.2354102 Test Loss: 0.2087554
Validation loss decreased (0.236233 --> 0.235410).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2209345
	speed: 0.1147s/iter; left time: 98.0743s
	iters: 200, epoch: 8 | loss: 0.2339310
	speed: 0.1024s/iter; left time: 77.2977s
	iters: 300, epoch: 8 | loss: 0.1629492
	speed: 0.1186s/iter; left time: 77.6786s
Epoch: 8 cost time: 35.75209140777588
Epoch: 8, Steps: 318 | Train Loss: 0.2156736 Vali Loss: 0.2364715 Test Loss: 0.2088725
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2475188
	speed: 0.1150s/iter; left time: 61.7800s
	iters: 200, epoch: 9 | loss: 0.2362828
	speed: 0.1079s/iter; left time: 47.1546s
	iters: 300, epoch: 9 | loss: 0.2154066
	speed: 0.1105s/iter; left time: 37.2285s
Epoch: 9 cost time: 35.33331537246704
Epoch: 9, Steps: 318 | Train Loss: 0.2151355 Vali Loss: 0.2348064 Test Loss: 0.2089215
Validation loss decreased (0.235410 --> 0.234806).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1941224
	speed: 0.1199s/iter; left time: 26.2680s
	iters: 200, epoch: 10 | loss: 0.1628106
	speed: 0.1061s/iter; left time: 12.6209s
	iters: 300, epoch: 10 | loss: 0.2027779
	speed: 0.1146s/iter; left time: 2.1777s
Epoch: 10 cost time: 35.99777150154114
Epoch: 10, Steps: 318 | Train Loss: 0.2148794 Vali Loss: 0.2357117 Test Loss: 0.2088660
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.45847004652023315, mae:0.5342320203781128, rmse:0.6771041750907898, mape:0.019124198704957962, mspe:0.0005965051241219044, rse:0.4736230671405792, r2_score:0.7357474036980076, acc:0.980875801295042
corr: [37.151344 37.10785  36.989487 37.243378 37.149265 37.41101  37.224957
 37.18174  37.11595  37.41459  37.189083 37.42799  37.45291  37.473236
 37.4691   37.508423 37.558804 37.51685  37.713867 37.812057 37.79543
 37.759346 38.182552 38.24488  38.161007]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4377648
	speed: 0.1736s/iter; left time: 533.0468s
	iters: 200, epoch: 1 | loss: 0.2592691
	speed: 0.1566s/iter; left time: 465.1175s
	iters: 300, epoch: 1 | loss: 0.3218030
	speed: 0.1735s/iter; left time: 498.1641s
Epoch: 1 cost time: 52.654558420181274
Epoch: 1, Steps: 317 | Train Loss: 0.4202611 Vali Loss: 0.2587304 Test Loss: 0.2341682
Validation loss decreased (inf --> 0.258730).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2862206
	speed: 0.1756s/iter; left time: 483.7039s
	iters: 200, epoch: 2 | loss: 0.2074409
	speed: 0.1633s/iter; left time: 433.4759s
	iters: 300, epoch: 2 | loss: 0.2668220
	speed: 0.1575s/iter; left time: 402.2528s
Epoch: 2 cost time: 53.638258934020996
Epoch: 2, Steps: 317 | Train Loss: 0.2448160 Vali Loss: 0.2398968 Test Loss: 0.2214713
Validation loss decreased (0.258730 --> 0.239897).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2149130
	speed: 0.1565s/iter; left time: 381.4609s
	iters: 200, epoch: 3 | loss: 0.2432975
	speed: 0.1638s/iter; left time: 382.8004s
	iters: 300, epoch: 3 | loss: 0.2194037
	speed: 0.1768s/iter; left time: 395.5361s
Epoch: 3 cost time: 52.80387210845947
Epoch: 3, Steps: 317 | Train Loss: 0.2280877 Vali Loss: 0.2367877 Test Loss: 0.2139181
Validation loss decreased (0.239897 --> 0.236788).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1993409
	speed: 0.1761s/iter; left time: 373.2627s
	iters: 200, epoch: 4 | loss: 0.1964514
	speed: 0.1518s/iter; left time: 306.6535s
	iters: 300, epoch: 4 | loss: 0.2112004
	speed: 0.1668s/iter; left time: 320.1894s
Epoch: 4 cost time: 52.60423040390015
Epoch: 4, Steps: 317 | Train Loss: 0.2223337 Vali Loss: 0.2389916 Test Loss: 0.2141720
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2154588
	speed: 0.1866s/iter; left time: 336.3711s
	iters: 200, epoch: 5 | loss: 0.2310279
	speed: 0.1670s/iter; left time: 284.4148s
	iters: 300, epoch: 5 | loss: 0.2296979
	speed: 0.1709s/iter; left time: 273.9896s
Epoch: 5 cost time: 55.33229184150696
Epoch: 5, Steps: 317 | Train Loss: 0.2195950 Vali Loss: 0.2386312 Test Loss: 0.2133376
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1847574
	speed: 0.1780s/iter; left time: 264.4391s
	iters: 200, epoch: 6 | loss: 0.2142969
	speed: 0.1672s/iter; left time: 231.7522s
	iters: 300, epoch: 6 | loss: 0.2677277
	speed: 0.1676s/iter; left time: 215.5092s
Epoch: 6 cost time: 54.39101052284241
Epoch: 6, Steps: 317 | Train Loss: 0.2181620 Vali Loss: 0.2383385 Test Loss: 0.2122867
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.46943482756614685, mae:0.5394877791404724, rmse:0.6851531267166138, mape:0.01934240572154522, mspe:0.0006136249285191298, rse:0.47890910506248474, r2_score:0.7218004481642023, acc:0.9806575942784548
corr: [37.019962 36.91189  37.100273 37.420006 37.147087 37.27844  37.048306
 37.050476 36.945736 37.132156 37.33121  37.268528 37.42752  37.629013
 37.551556 37.573498 37.43736  37.387093 37.601784 37.650402 37.86432
 37.65341  37.809757 37.679867 37.671734 37.51652  37.824883 38.018684
 38.09151  38.024292]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2974555
	speed: 0.1106s/iter; left time: 340.7869s
	iters: 200, epoch: 1 | loss: 0.2359242
	speed: 0.0702s/iter; left time: 209.3489s
	iters: 300, epoch: 1 | loss: 0.2504236
	speed: 0.0729s/iter; left time: 210.1147s
Epoch: 1 cost time: 26.221164226531982
Epoch: 1, Steps: 318 | Train Loss: 0.3506416 Vali Loss: 0.2097634 Test Loss: 0.2309295
Validation loss decreased (inf --> 0.209763).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1879126
	speed: 0.0861s/iter; left time: 237.8822s
	iters: 200, epoch: 2 | loss: 0.1879271
	speed: 0.0754s/iter; left time: 200.7942s
	iters: 300, epoch: 2 | loss: 0.1335635
	speed: 0.0748s/iter; left time: 191.7281s
Epoch: 2 cost time: 25.060049057006836
Epoch: 2, Steps: 318 | Train Loss: 0.1963858 Vali Loss: 0.1909274 Test Loss: 0.2132242
Validation loss decreased (0.209763 --> 0.190927).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1448152
	speed: 0.0757s/iter; left time: 185.0382s
	iters: 200, epoch: 3 | loss: 0.1672179
	speed: 0.0790s/iter; left time: 185.3434s
	iters: 300, epoch: 3 | loss: 0.1857099
	speed: 0.0807s/iter; left time: 181.0944s
Epoch: 3 cost time: 24.77302122116089
Epoch: 3, Steps: 318 | Train Loss: 0.1797476 Vali Loss: 0.1869416 Test Loss: 0.2139607
Validation loss decreased (0.190927 --> 0.186942).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1799995
	speed: 0.0824s/iter; left time: 175.2009s
	iters: 200, epoch: 4 | loss: 0.1713088
	speed: 0.0761s/iter; left time: 154.2976s
	iters: 300, epoch: 4 | loss: 0.1805115
	speed: 0.0746s/iter; left time: 143.7778s
Epoch: 4 cost time: 24.752197742462158
Epoch: 4, Steps: 318 | Train Loss: 0.1732662 Vali Loss: 0.1851117 Test Loss: 0.2132197
Validation loss decreased (0.186942 --> 0.185112).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1802367
	speed: 0.0659s/iter; left time: 119.1307s
	iters: 200, epoch: 5 | loss: 0.1656771
	speed: 0.0408s/iter; left time: 69.7256s
	iters: 300, epoch: 5 | loss: 0.1722847
	speed: 0.0417s/iter; left time: 67.1225s
Epoch: 5 cost time: 15.943087339401245
Epoch: 5, Steps: 318 | Train Loss: 0.1706272 Vali Loss: 0.1853703 Test Loss: 0.2151399
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1855815
	speed: 0.0547s/iter; left time: 81.5614s
	iters: 200, epoch: 6 | loss: 0.1777902
	speed: 0.0282s/iter; left time: 39.1744s
	iters: 300, epoch: 6 | loss: 0.1269093
	speed: 0.0513s/iter; left time: 66.1943s
Epoch: 6 cost time: 14.278206586837769
Epoch: 6, Steps: 318 | Train Loss: 0.1694546 Vali Loss: 0.1837758 Test Loss: 0.2142215
Validation loss decreased (0.185112 --> 0.183776).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1745772
	speed: 0.0478s/iter; left time: 56.0565s
	iters: 200, epoch: 7 | loss: 0.1996974
	speed: 0.0261s/iter; left time: 28.0255s
	iters: 300, epoch: 7 | loss: 0.1595937
	speed: 0.0476s/iter; left time: 46.2997s
Epoch: 7 cost time: 12.806994438171387
Epoch: 7, Steps: 318 | Train Loss: 0.1684140 Vali Loss: 0.1842870 Test Loss: 0.2141442
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1223700
	speed: 0.0357s/iter; left time: 30.5425s
	iters: 200, epoch: 8 | loss: 0.1491145
	speed: 0.0371s/iter; left time: 27.9827s
	iters: 300, epoch: 8 | loss: 0.1636643
	speed: 0.0387s/iter; left time: 25.3539s
Epoch: 8 cost time: 11.610690116882324
Epoch: 8, Steps: 318 | Train Loss: 0.1678712 Vali Loss: 0.1831825 Test Loss: 0.2139872
Validation loss decreased (0.183776 --> 0.183182).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2032406
	speed: 0.0623s/iter; left time: 33.4413s
	iters: 200, epoch: 9 | loss: 0.1837137
	speed: 0.0533s/iter; left time: 23.2749s
	iters: 300, epoch: 9 | loss: 0.1215986
	speed: 0.0394s/iter; left time: 13.2904s
Epoch: 9 cost time: 16.77443504333496
Epoch: 9, Steps: 318 | Train Loss: 0.1677352 Vali Loss: 0.1823237 Test Loss: 0.2140123
Validation loss decreased (0.183182 --> 0.182324).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1575266
	speed: 0.0488s/iter; left time: 10.6799s
	iters: 200, epoch: 10 | loss: 0.1522250
	speed: 0.0407s/iter; left time: 4.8427s
	iters: 300, epoch: 10 | loss: 0.1756576
	speed: 0.0508s/iter; left time: 0.9647s
Epoch: 10 cost time: 14.538693189620972
Epoch: 10, Steps: 318 | Train Loss: 0.1676339 Vali Loss: 0.1835261 Test Loss: 0.2140315
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.46964162588119507, mae:0.5364593863487244, rmse:0.6853040456771851, mape:0.019142933189868927, mspe:0.0006024160538800061, rse:0.4801832437515259, r2_score:0.7470017144637142, acc:0.9808570668101311
corr: [36.669266 36.88676  37.01545  37.24916  36.934948 37.374317 37.072052
 37.316414 37.278484 36.90804 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3523172
	speed: 0.0897s/iter; left time: 276.4116s
	iters: 200, epoch: 1 | loss: 0.2789400
	speed: 0.0220s/iter; left time: 65.7283s
	iters: 300, epoch: 1 | loss: 0.2657884
	speed: 0.0495s/iter; left time: 142.7519s
Epoch: 1 cost time: 17.777581214904785
Epoch: 1, Steps: 318 | Train Loss: 0.4158762 Vali Loss: 0.2305114 Test Loss: 0.2319847
Validation loss decreased (inf --> 0.230511).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1911876
	speed: 0.0912s/iter; left time: 251.9921s
	iters: 200, epoch: 2 | loss: 0.2188411
	speed: 0.0462s/iter; left time: 123.1016s
	iters: 300, epoch: 2 | loss: 0.1345723
	speed: 0.0672s/iter; left time: 172.3449s
Epoch: 2 cost time: 21.1855571269989
Epoch: 2, Steps: 318 | Train Loss: 0.2228219 Vali Loss: 0.2155085 Test Loss: 0.1993664
Validation loss decreased (0.230511 --> 0.215508).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2210750
	speed: 0.0616s/iter; left time: 150.6306s
	iters: 200, epoch: 3 | loss: 0.2338601
	speed: 0.0434s/iter; left time: 101.7231s
	iters: 300, epoch: 3 | loss: 0.2047722
	speed: 0.0418s/iter; left time: 93.7885s
Epoch: 3 cost time: 15.753191947937012
Epoch: 3, Steps: 318 | Train Loss: 0.2013577 Vali Loss: 0.2097937 Test Loss: 0.1976906
Validation loss decreased (0.215508 --> 0.209794).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1367449
	speed: 0.0540s/iter; left time: 114.8670s
	iters: 200, epoch: 4 | loss: 0.1588047
	speed: 0.0616s/iter; left time: 124.9496s
	iters: 300, epoch: 4 | loss: 0.2715436
	speed: 0.0484s/iter; left time: 93.3023s
Epoch: 4 cost time: 17.180440425872803
Epoch: 4, Steps: 318 | Train Loss: 0.1940584 Vali Loss: 0.2049781 Test Loss: 0.1901848
Validation loss decreased (0.209794 --> 0.204978).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1613927
	speed: 0.0475s/iter; left time: 85.9416s
	iters: 200, epoch: 5 | loss: 0.1777390
	speed: 0.0640s/iter; left time: 109.3192s
	iters: 300, epoch: 5 | loss: 0.1897852
	speed: 0.0598s/iter; left time: 96.1852s
Epoch: 5 cost time: 17.961865425109863
Epoch: 5, Steps: 318 | Train Loss: 0.1903249 Vali Loss: 0.2031049 Test Loss: 0.1876597
Validation loss decreased (0.204978 --> 0.203105).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1887205
	speed: 0.0671s/iter; left time: 100.0884s
	iters: 200, epoch: 6 | loss: 0.1538531
	speed: 0.0506s/iter; left time: 70.4381s
	iters: 300, epoch: 6 | loss: 0.1878942
	speed: 0.0435s/iter; left time: 56.1325s
Epoch: 6 cost time: 16.5318443775177
Epoch: 6, Steps: 318 | Train Loss: 0.1889578 Vali Loss: 0.2029114 Test Loss: 0.1865384
Validation loss decreased (0.203105 --> 0.202911).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1891478
	speed: 0.0930s/iter; left time: 109.0926s
	iters: 200, epoch: 7 | loss: 0.2240147
	speed: 0.0358s/iter; left time: 38.4653s
	iters: 300, epoch: 7 | loss: 0.1336232
	speed: 0.0603s/iter; left time: 58.6402s
Epoch: 7 cost time: 19.867817401885986
Epoch: 7, Steps: 318 | Train Loss: 0.1886060 Vali Loss: 0.2026405 Test Loss: 0.1861786
Validation loss decreased (0.202911 --> 0.202640).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2555055
	speed: 0.0488s/iter; left time: 41.6820s
	iters: 200, epoch: 8 | loss: 0.1926161
	speed: 0.0538s/iter; left time: 40.6531s
	iters: 300, epoch: 8 | loss: 0.2117995
	speed: 0.0494s/iter; left time: 32.3515s
Epoch: 8 cost time: 16.612707138061523
Epoch: 8, Steps: 318 | Train Loss: 0.1877981 Vali Loss: 0.2014198 Test Loss: 0.1859328
Validation loss decreased (0.202640 --> 0.201420).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1574708
	speed: 0.0547s/iter; left time: 29.3959s
	iters: 200, epoch: 9 | loss: 0.2124171
	speed: 0.0624s/iter; left time: 27.2894s
	iters: 300, epoch: 9 | loss: 0.1566369
	speed: 0.0421s/iter; left time: 14.1813s
Epoch: 9 cost time: 16.278728246688843
Epoch: 9, Steps: 318 | Train Loss: 0.1874920 Vali Loss: 0.2026108 Test Loss: 0.1858714
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1883907
	speed: 0.0369s/iter; left time: 8.0760s
	iters: 200, epoch: 10 | loss: 0.2096822
	speed: 0.0316s/iter; left time: 3.7662s
	iters: 300, epoch: 10 | loss: 0.1605266
	speed: 0.0486s/iter; left time: 0.9240s
Epoch: 10 cost time: 12.118287563323975
Epoch: 10, Steps: 318 | Train Loss: 0.1875957 Vali Loss: 0.2017012 Test Loss: 0.1858034
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.40802231431007385, mae:0.49528753757476807, rmse:0.6387662291526794, mape:0.017727885395288467, mspe:0.0005306701059453189, rse:0.44736409187316895, r2_score:0.7740406223306018, acc:0.9822721146047115
corr: [36.708115 36.686092 36.405113 36.60266  36.734882 36.99395  36.944138
 36.927677 37.13799  37.28242  37.347    37.36968  37.519924 37.327744
 36.8742  ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3097164
	speed: 0.0711s/iter; left time: 218.9327s
	iters: 200, epoch: 1 | loss: 0.3532874
	speed: 0.0611s/iter; left time: 182.2478s
	iters: 300, epoch: 1 | loss: 0.2706949
	speed: 0.0269s/iter; left time: 77.4253s
Epoch: 1 cost time: 17.185493230819702
Epoch: 1, Steps: 318 | Train Loss: 0.4075329 Vali Loss: 0.2568000 Test Loss: 0.2340071
Validation loss decreased (inf --> 0.256800).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2396202
	speed: 0.0618s/iter; left time: 170.7952s
	iters: 200, epoch: 2 | loss: 0.1802935
	speed: 0.0433s/iter; left time: 115.3877s
	iters: 300, epoch: 2 | loss: 0.2498426
	speed: 0.0403s/iter; left time: 103.2834s
Epoch: 2 cost time: 15.670374393463135
Epoch: 2, Steps: 318 | Train Loss: 0.2178881 Vali Loss: 0.2391027 Test Loss: 0.2293026
Validation loss decreased (0.256800 --> 0.239103).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1776675
	speed: 0.0368s/iter; left time: 89.9628s
	iters: 200, epoch: 3 | loss: 0.2009370
	speed: 0.0618s/iter; left time: 144.9617s
	iters: 300, epoch: 3 | loss: 0.1957259
	speed: 0.0404s/iter; left time: 90.8010s
Epoch: 3 cost time: 14.390160083770752
Epoch: 3, Steps: 318 | Train Loss: 0.1986417 Vali Loss: 0.2174458 Test Loss: 0.2162030
Validation loss decreased (0.239103 --> 0.217446).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1788798
	speed: 0.0667s/iter; left time: 141.8587s
	iters: 200, epoch: 4 | loss: 0.2055831
	speed: 0.0325s/iter; left time: 65.8745s
	iters: 300, epoch: 4 | loss: 0.2137554
	speed: 0.0503s/iter; left time: 96.8877s
Epoch: 4 cost time: 15.432836055755615
Epoch: 4, Steps: 318 | Train Loss: 0.1922516 Vali Loss: 0.2163486 Test Loss: 0.2117524
Validation loss decreased (0.217446 --> 0.216349).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2879579
	speed: 0.0494s/iter; left time: 89.2943s
	iters: 200, epoch: 5 | loss: 0.1666898
	speed: 0.0367s/iter; left time: 62.7341s
	iters: 300, epoch: 5 | loss: 0.1875853
	speed: 0.0658s/iter; left time: 105.8940s
Epoch: 5 cost time: 16.075161933898926
Epoch: 5, Steps: 318 | Train Loss: 0.1894044 Vali Loss: 0.2132045 Test Loss: 0.2137308
Validation loss decreased (0.216349 --> 0.213204).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1920798
	speed: 0.0344s/iter; left time: 51.3074s
	iters: 200, epoch: 6 | loss: 0.1791871
	speed: 0.0659s/iter; left time: 91.6014s
	iters: 300, epoch: 6 | loss: 0.2629041
	speed: 0.0338s/iter; left time: 43.6098s
Epoch: 6 cost time: 14.598859786987305
Epoch: 6, Steps: 318 | Train Loss: 0.1875500 Vali Loss: 0.2147030 Test Loss: 0.2122498
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1622546
	speed: 0.0524s/iter; left time: 61.4281s
	iters: 200, epoch: 7 | loss: 0.2962954
	speed: 0.0319s/iter; left time: 34.2607s
	iters: 300, epoch: 7 | loss: 0.1476109
	speed: 0.0489s/iter; left time: 47.5952s
Epoch: 7 cost time: 14.15024733543396
Epoch: 7, Steps: 318 | Train Loss: 0.1875400 Vali Loss: 0.2138159 Test Loss: 0.2139212
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1737378
	speed: 0.0493s/iter; left time: 42.1431s
	iters: 200, epoch: 8 | loss: 0.2043468
	speed: 0.0497s/iter; left time: 37.5204s
	iters: 300, epoch: 8 | loss: 0.1837876
	speed: 0.0614s/iter; left time: 40.2410s
Epoch: 8 cost time: 17.362656593322754
Epoch: 8, Steps: 318 | Train Loss: 0.1868586 Vali Loss: 0.2147414 Test Loss: 0.2143427
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.46902382373809814, mae:0.5242862701416016, rmse:0.6848531365394592, mape:0.01884033903479576, mspe:0.0006202564109116793, rse:0.4793635904788971, r2_score:0.7430931123693794, acc:0.9811596609652042
corr: [35.720074 35.55878  35.935455 35.72921  36.083366 35.69145  36.038742
 35.898468 36.100914 36.15762  36.166622 36.36887  36.38943  36.423164
 36.70061  36.609016 36.77048  37.098614 36.82999  37.12054 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4247213
	speed: 0.0782s/iter; left time: 241.0241s
	iters: 200, epoch: 1 | loss: 0.2966815
	speed: 0.0458s/iter; left time: 136.4316s
	iters: 300, epoch: 1 | loss: 0.3322529
	speed: 0.0521s/iter; left time: 150.0248s
Epoch: 1 cost time: 18.42147922515869
Epoch: 1, Steps: 318 | Train Loss: 0.4342582 Vali Loss: 0.3070543 Test Loss: 0.2887309
Validation loss decreased (inf --> 0.307054).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.3118662
	speed: 0.0439s/iter; left time: 121.2112s
	iters: 200, epoch: 2 | loss: 0.2619610
	speed: 0.0582s/iter; left time: 154.9991s
	iters: 300, epoch: 2 | loss: 0.2356708
	speed: 0.0512s/iter; left time: 131.1678s
Epoch: 2 cost time: 15.667552709579468
Epoch: 2, Steps: 318 | Train Loss: 0.2930581 Vali Loss: 0.3035258 Test Loss: 0.2788658
Validation loss decreased (0.307054 --> 0.303526).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2950558
	speed: 0.0734s/iter; left time: 179.5647s
	iters: 200, epoch: 3 | loss: 0.3167970
	speed: 0.0383s/iter; left time: 89.8213s
	iters: 300, epoch: 3 | loss: 0.2963318
	speed: 0.0621s/iter; left time: 139.3312s
Epoch: 3 cost time: 18.396587133407593
Epoch: 3, Steps: 318 | Train Loss: 0.2770631 Vali Loss: 0.3028729 Test Loss: 0.2730102
Validation loss decreased (0.303526 --> 0.302873).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2675269
	speed: 0.0729s/iter; left time: 155.1076s
	iters: 200, epoch: 4 | loss: 0.2683412
	speed: 0.0421s/iter; left time: 85.3739s
	iters: 300, epoch: 4 | loss: 0.2635000
	speed: 0.0577s/iter; left time: 111.2681s
Epoch: 4 cost time: 18.534117937088013
Epoch: 4, Steps: 318 | Train Loss: 0.2707478 Vali Loss: 0.3050283 Test Loss: 0.2732705
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2504043
	speed: 0.0661s/iter; left time: 119.5232s
	iters: 200, epoch: 5 | loss: 0.2344575
	speed: 0.0490s/iter; left time: 83.7137s
	iters: 300, epoch: 5 | loss: 0.3561487
	speed: 0.0618s/iter; left time: 99.4267s
Epoch: 5 cost time: 18.00751781463623
Epoch: 5, Steps: 318 | Train Loss: 0.2682408 Vali Loss: 0.3064488 Test Loss: 0.2708475
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2623884
	speed: 0.0704s/iter; left time: 104.9034s
	iters: 200, epoch: 6 | loss: 0.2620079
	speed: 0.0496s/iter; left time: 69.0224s
	iters: 300, epoch: 6 | loss: 0.2633511
	speed: 0.0569s/iter; left time: 73.3988s
Epoch: 6 cost time: 18.016989946365356
Epoch: 6, Steps: 318 | Train Loss: 0.2668998 Vali Loss: 0.3063723 Test Loss: 0.2699804
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.5991101264953613, mae:0.6085025072097778, rmse:0.7740220427513123, mape:0.0218519177287817, mspe:0.0007877001189626753, rse:0.5414155125617981, r2_score:0.642205225473526, acc:0.9781480822712183
corr: [37.786568 37.71595  37.030262 37.401276 37.669937 37.274673 37.83618
 37.31189  37.23661  37.74109  37.330036 37.322227 37.725513 37.618626
 37.0093   37.790825 37.75509  37.973404 37.429222 38.153233 38.132927
 37.966995 38.258804 37.738667 37.971058]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4596500
	speed: 0.0781s/iter; left time: 239.8180s
	iters: 200, epoch: 1 | loss: 0.3378761
	speed: 0.0553s/iter; left time: 164.2716s
	iters: 300, epoch: 1 | loss: 0.3151632
	speed: 0.0592s/iter; left time: 170.0407s
Epoch: 1 cost time: 19.5991952419281
Epoch: 1, Steps: 317 | Train Loss: 0.4529831 Vali Loss: 0.3009040 Test Loss: 0.2981821
Validation loss decreased (inf --> 0.300904).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2659439
	speed: 0.0724s/iter; left time: 199.3078s
	iters: 200, epoch: 2 | loss: 0.3005712
	speed: 0.0431s/iter; left time: 114.4927s
	iters: 300, epoch: 2 | loss: 0.3627411
	speed: 0.0668s/iter; left time: 170.6375s
Epoch: 2 cost time: 18.56645917892456
Epoch: 2, Steps: 317 | Train Loss: 0.2951965 Vali Loss: 0.3073477 Test Loss: 0.2853168
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2502672
	speed: 0.0657s/iter; left time: 160.1731s
	iters: 200, epoch: 3 | loss: 0.3144135
	speed: 0.0373s/iter; left time: 87.0613s
	iters: 300, epoch: 3 | loss: 0.2744794
	speed: 0.0682s/iter; left time: 152.4583s
Epoch: 3 cost time: 17.62999200820923
Epoch: 3, Steps: 317 | Train Loss: 0.2794470 Vali Loss: 0.3126203 Test Loss: 0.2804934
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2263544
	speed: 0.0756s/iter; left time: 160.3107s
	iters: 200, epoch: 4 | loss: 0.2589352
	speed: 0.0399s/iter; left time: 80.6373s
	iters: 300, epoch: 4 | loss: 0.2395662
	speed: 0.0524s/iter; left time: 100.5725s
Epoch: 4 cost time: 17.93542504310608
Epoch: 4, Steps: 317 | Train Loss: 0.2738098 Vali Loss: 0.3156261 Test Loss: 0.2784852
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000063
Early stopping
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.6543487906455994, mae:0.6417067050933838, rmse:0.808918297290802, mape:0.022990992292761803, mspe:0.0008521219133399427, rse:0.5654186010360718, r2_score:0.6251509229126898, acc:0.9770090077072382
corr: [37.820713 37.611073 37.442307 37.301304 38.085182 37.44985  37.99828
 37.66927  37.58585  38.078213 37.71852  37.694324 37.941525 37.797066
 37.20796  37.940247 37.959923 38.196274 37.568634 38.28245  38.479183
 38.1603   38.44291  38.01813  38.40557  38.8594   38.538757 38.5099
 38.706448 38.206444]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2859068
	speed: 0.0908s/iter; left time: 279.6443s
	iters: 200, epoch: 1 | loss: 0.2448958
	speed: 0.0406s/iter; left time: 120.9277s
	iters: 300, epoch: 1 | loss: 0.2539809
	speed: 0.0645s/iter; left time: 185.9436s
Epoch: 1 cost time: 20.36994743347168
Epoch: 1, Steps: 318 | Train Loss: 0.3229388 Vali Loss: 0.2089817 Test Loss: 0.1898996
Validation loss decreased (inf --> 0.208982).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1944840
	speed: 0.0549s/iter; left time: 151.6824s
	iters: 200, epoch: 2 | loss: 0.1908622
	speed: 0.0539s/iter; left time: 143.5210s
	iters: 300, epoch: 2 | loss: 0.1355956
	speed: 0.0544s/iter; left time: 139.2999s
Epoch: 2 cost time: 17.263886213302612
Epoch: 2, Steps: 318 | Train Loss: 0.1906826 Vali Loss: 0.1854818 Test Loss: 0.1585685
Validation loss decreased (0.208982 --> 0.185482).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1411389
	speed: 0.0623s/iter; left time: 152.3581s
	iters: 200, epoch: 3 | loss: 0.1617153
	speed: 0.0535s/iter; left time: 125.4451s
	iters: 300, epoch: 3 | loss: 0.1891751
	speed: 0.0683s/iter; left time: 153.3864s
Epoch: 3 cost time: 19.306479692459106
Epoch: 3, Steps: 318 | Train Loss: 0.1735468 Vali Loss: 0.1797945 Test Loss: 0.1509847
Validation loss decreased (0.185482 --> 0.179795).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1696993
	speed: 0.0541s/iter; left time: 115.0275s
	iters: 200, epoch: 4 | loss: 0.1868111
	speed: 0.0732s/iter; left time: 148.3235s
	iters: 300, epoch: 4 | loss: 0.1779916
	speed: 0.0475s/iter; left time: 91.4944s
Epoch: 4 cost time: 18.561883687973022
Epoch: 4, Steps: 318 | Train Loss: 0.1675013 Vali Loss: 0.1767142 Test Loss: 0.1474042
Validation loss decreased (0.179795 --> 0.176714).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1858389
	speed: 0.0662s/iter; left time: 119.7527s
	iters: 200, epoch: 5 | loss: 0.1842733
	speed: 0.0597s/iter; left time: 101.9764s
	iters: 300, epoch: 5 | loss: 0.1614537
	speed: 0.0493s/iter; left time: 79.3578s
Epoch: 5 cost time: 18.47019338607788
Epoch: 5, Steps: 318 | Train Loss: 0.1647695 Vali Loss: 0.1765651 Test Loss: 0.1466479
Validation loss decreased (0.176714 --> 0.176565).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1732613
	speed: 0.0859s/iter; left time: 128.0135s
	iters: 200, epoch: 6 | loss: 0.1664429
	speed: 0.0493s/iter; left time: 68.6090s
	iters: 300, epoch: 6 | loss: 0.1387820
	speed: 0.0704s/iter; left time: 90.8354s
Epoch: 6 cost time: 21.580854654312134
Epoch: 6, Steps: 318 | Train Loss: 0.1641388 Vali Loss: 0.1753846 Test Loss: 0.1458162
Validation loss decreased (0.176565 --> 0.175385).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1733987
	speed: 0.0591s/iter; left time: 69.3584s
	iters: 200, epoch: 7 | loss: 0.1981771
	speed: 0.0833s/iter; left time: 89.3491s
	iters: 300, epoch: 7 | loss: 0.1663674
	speed: 0.0459s/iter; left time: 44.6816s
Epoch: 7 cost time: 19.9967622756958
Epoch: 7, Steps: 318 | Train Loss: 0.1632200 Vali Loss: 0.1750624 Test Loss: 0.1451369
Validation loss decreased (0.175385 --> 0.175062).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1364634
	speed: 0.0740s/iter; left time: 63.3034s
	iters: 200, epoch: 8 | loss: 0.1488232
	speed: 0.0581s/iter; left time: 43.8770s
	iters: 300, epoch: 8 | loss: 0.1738047
	speed: 0.0643s/iter; left time: 42.1379s
Epoch: 8 cost time: 20.132559537887573
Epoch: 8, Steps: 318 | Train Loss: 0.1624268 Vali Loss: 0.1739816 Test Loss: 0.1450573
Validation loss decreased (0.175062 --> 0.173982).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1896788
	speed: 0.0604s/iter; left time: 32.4553s
	iters: 200, epoch: 9 | loss: 0.1736793
	speed: 0.0664s/iter; left time: 29.0306s
	iters: 300, epoch: 9 | loss: 0.1164315
	speed: 0.0648s/iter; left time: 21.8521s
Epoch: 9 cost time: 20.894145011901855
Epoch: 9, Steps: 318 | Train Loss: 0.1623889 Vali Loss: 0.1737195 Test Loss: 0.1450083
Validation loss decreased (0.173982 --> 0.173719).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1538139
	speed: 0.0547s/iter; left time: 11.9743s
	iters: 200, epoch: 10 | loss: 0.1359456
	speed: 0.0562s/iter; left time: 6.6912s
	iters: 300, epoch: 10 | loss: 0.1828063
	speed: 0.0593s/iter; left time: 1.1263s
Epoch: 10 cost time: 18.485625982284546
Epoch: 10, Steps: 318 | Train Loss: 0.1620760 Vali Loss: 0.1746030 Test Loss: 0.1449768
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.31821495294570923, mae:0.4409513771533966, rmse:0.5641054511070251, mape:0.015785381197929382, mspe:0.0004140236123930663, rse:0.3952610492706299, r2_score:0.8239707419595881, acc:0.9842146188020706
corr: [37.15266  36.98419  37.347668 37.248264 37.141582 37.38848  37.10854
 37.315334 37.36529  37.217167]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3085148
	speed: 0.0947s/iter; left time: 291.6402s
	iters: 200, epoch: 1 | loss: 0.2301516
	speed: 0.0575s/iter; left time: 171.4787s
	iters: 300, epoch: 1 | loss: 0.2097847
	speed: 0.0641s/iter; left time: 184.6679s
Epoch: 1 cost time: 22.08846402168274
Epoch: 1, Steps: 318 | Train Loss: 0.3641726 Vali Loss: 0.2111847 Test Loss: 0.1926524
Validation loss decreased (inf --> 0.211185).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1693834
	speed: 0.0553s/iter; left time: 152.8152s
	iters: 200, epoch: 2 | loss: 0.1998111
	speed: 0.0740s/iter; left time: 196.9320s
	iters: 300, epoch: 2 | loss: 0.1268533
	speed: 0.0631s/iter; left time: 161.7561s
Epoch: 2 cost time: 20.43573760986328
Epoch: 2, Steps: 318 | Train Loss: 0.1990998 Vali Loss: 0.1986446 Test Loss: 0.1673175
Validation loss decreased (0.211185 --> 0.198645).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1711420
	speed: 0.0618s/iter; left time: 151.1726s
	iters: 200, epoch: 3 | loss: 0.2193988
	speed: 0.0679s/iter; left time: 159.2946s
	iters: 300, epoch: 3 | loss: 0.1749942
	speed: 0.0578s/iter; left time: 129.7238s
Epoch: 3 cost time: 19.937033891677856
Epoch: 3, Steps: 318 | Train Loss: 0.1844099 Vali Loss: 0.1975969 Test Loss: 0.1720451
Validation loss decreased (0.198645 --> 0.197597).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1123068
	speed: 0.0827s/iter; left time: 175.8757s
	iters: 200, epoch: 4 | loss: 0.1397230
	speed: 0.0630s/iter; left time: 127.6902s
	iters: 300, epoch: 4 | loss: 0.2513354
	speed: 0.0613s/iter; left time: 118.1842s
Epoch: 4 cost time: 22.192277908325195
Epoch: 4, Steps: 318 | Train Loss: 0.1805222 Vali Loss: 0.1941118 Test Loss: 0.1670752
Validation loss decreased (0.197597 --> 0.194112).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1499220
	speed: 0.0336s/iter; left time: 60.8670s
	iters: 200, epoch: 5 | loss: 0.1834862
	speed: 0.0343s/iter; left time: 58.6262s
	iters: 300, epoch: 5 | loss: 0.1859479
	speed: 0.0234s/iter; left time: 37.5922s
Epoch: 5 cost time: 9.52022933959961
Epoch: 5, Steps: 318 | Train Loss: 0.1773097 Vali Loss: 0.1922141 Test Loss: 0.1641063
Validation loss decreased (0.194112 --> 0.192214).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1738224
	speed: 0.0442s/iter; left time: 65.9398s
	iters: 200, epoch: 6 | loss: 0.1363481
	speed: 0.0309s/iter; left time: 42.9468s
	iters: 300, epoch: 6 | loss: 0.1794179
	speed: 0.0280s/iter; left time: 36.1853s
Epoch: 6 cost time: 10.64493441581726
Epoch: 6, Steps: 318 | Train Loss: 0.1767276 Vali Loss: 0.1931056 Test Loss: 0.1645407
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1900697
	speed: 0.0492s/iter; left time: 57.7570s
	iters: 200, epoch: 7 | loss: 0.2068021
	speed: 0.0245s/iter; left time: 26.2423s
	iters: 300, epoch: 7 | loss: 0.1281857
	speed: 0.0367s/iter; left time: 35.6926s
Epoch: 7 cost time: 11.451460123062134
Epoch: 7, Steps: 318 | Train Loss: 0.1761990 Vali Loss: 0.1915836 Test Loss: 0.1639931
Validation loss decreased (0.192214 --> 0.191584).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2155947
	speed: 0.0383s/iter; left time: 32.7375s
	iters: 200, epoch: 8 | loss: 0.1718770
	speed: 0.0284s/iter; left time: 21.4128s
	iters: 300, epoch: 8 | loss: 0.1888726
	speed: 0.0339s/iter; left time: 22.1985s
Epoch: 8 cost time: 10.53501009941101
Epoch: 8, Steps: 318 | Train Loss: 0.1760273 Vali Loss: 0.1909828 Test Loss: 0.1638811
Validation loss decreased (0.191584 --> 0.190983).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1490612
	speed: 0.0410s/iter; left time: 22.0358s
	iters: 200, epoch: 9 | loss: 0.2008687
	speed: 0.0270s/iter; left time: 11.7895s
	iters: 300, epoch: 9 | loss: 0.1505335
	speed: 0.0397s/iter; left time: 13.3764s
Epoch: 9 cost time: 11.204257011413574
Epoch: 9, Steps: 318 | Train Loss: 0.1753799 Vali Loss: 0.1927074 Test Loss: 0.1638025
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1675214
	speed: 0.0364s/iter; left time: 7.9799s
	iters: 200, epoch: 10 | loss: 0.1933811
	speed: 0.0296s/iter; left time: 3.5243s
	iters: 300, epoch: 10 | loss: 0.1471723
	speed: 0.0467s/iter; left time: 0.8880s
Epoch: 10 cost time: 11.82729434967041
Epoch: 10, Steps: 318 | Train Loss: 0.1754752 Vali Loss: 0.1920349 Test Loss: 0.1637097
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.3596305549144745, mae:0.4663597643375397, rmse:0.5996920466423035, mape:0.016679611057043076, mspe:0.0004664763400796801, rse:0.4199982285499573, r2_score:0.8055666702942148, acc:0.9833203889429569
corr: [37.173023 37.117897 37.06382  37.01193  37.08091  37.117435 37.09564
 37.163662 37.280636 37.17759  37.124214 37.107353 37.1743   37.163673
 37.26211 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2748382
	speed: 0.0434s/iter; left time: 133.6048s
	iters: 200, epoch: 1 | loss: 0.3168425
	speed: 0.0343s/iter; left time: 102.2738s
	iters: 300, epoch: 1 | loss: 0.2603635
	speed: 0.0271s/iter; left time: 78.2114s
Epoch: 1 cost time: 10.775634765625
Epoch: 1, Steps: 318 | Train Loss: 0.3628016 Vali Loss: 0.2186156 Test Loss: 0.2042331
Validation loss decreased (inf --> 0.218616).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2091718
	speed: 0.0368s/iter; left time: 101.7576s
	iters: 200, epoch: 2 | loss: 0.1612641
	speed: 0.0190s/iter; left time: 50.7019s
	iters: 300, epoch: 2 | loss: 0.2311954
	speed: 0.0312s/iter; left time: 80.0553s
Epoch: 2 cost time: 9.030051469802856
Epoch: 2, Steps: 318 | Train Loss: 0.2059387 Vali Loss: 0.2164057 Test Loss: 0.1945186
Validation loss decreased (0.218616 --> 0.216406).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1779952
	speed: 0.0452s/iter; left time: 110.4904s
	iters: 200, epoch: 3 | loss: 0.1924954
	speed: 0.0211s/iter; left time: 49.4805s
	iters: 300, epoch: 3 | loss: 0.1912526
	speed: 0.0310s/iter; left time: 69.6747s
Epoch: 3 cost time: 10.141232013702393
Epoch: 3, Steps: 318 | Train Loss: 0.1928689 Vali Loss: 0.2028453 Test Loss: 0.1808216
Validation loss decreased (0.216406 --> 0.202845).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1763044
	speed: 0.0483s/iter; left time: 102.8242s
	iters: 200, epoch: 4 | loss: 0.1996240
	speed: 0.0201s/iter; left time: 40.8090s
	iters: 300, epoch: 4 | loss: 0.2048010
	speed: 0.0209s/iter; left time: 40.2017s
Epoch: 4 cost time: 9.238815546035767
Epoch: 4, Steps: 318 | Train Loss: 0.1886064 Vali Loss: 0.2034173 Test Loss: 0.1744117
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2756008
	speed: 0.0358s/iter; left time: 64.7419s
	iters: 200, epoch: 5 | loss: 0.1681950
	speed: 0.0409s/iter; left time: 69.9633s
	iters: 300, epoch: 5 | loss: 0.1867093
	speed: 0.0219s/iter; left time: 35.3020s
Epoch: 5 cost time: 10.176001787185669
Epoch: 5, Steps: 318 | Train Loss: 0.1869873 Vali Loss: 0.2013756 Test Loss: 0.1747891
Validation loss decreased (0.202845 --> 0.201376).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1930520
	speed: 0.0489s/iter; left time: 72.9028s
	iters: 200, epoch: 6 | loss: 0.1806464
	speed: 0.0287s/iter; left time: 39.9748s
	iters: 300, epoch: 6 | loss: 0.2546769
	speed: 0.0197s/iter; left time: 25.4929s
Epoch: 6 cost time: 10.045121908187866
Epoch: 6, Steps: 318 | Train Loss: 0.1850900 Vali Loss: 0.2032606 Test Loss: 0.1742640
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1640699
	speed: 0.0347s/iter; left time: 40.6902s
	iters: 200, epoch: 7 | loss: 0.2850173
	speed: 0.0322s/iter; left time: 34.5919s
	iters: 300, epoch: 7 | loss: 0.1445368
	speed: 0.0205s/iter; left time: 19.9742s
Epoch: 7 cost time: 9.11377739906311
Epoch: 7, Steps: 318 | Train Loss: 0.1851450 Vali Loss: 0.2021502 Test Loss: 0.1746586
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1667120
	speed: 0.0413s/iter; left time: 35.3107s
	iters: 200, epoch: 8 | loss: 0.2050037
	speed: 0.0383s/iter; left time: 28.8993s
	iters: 300, epoch: 8 | loss: 0.1766604
	speed: 0.0188s/iter; left time: 12.3180s
Epoch: 8 cost time: 10.173821687698364
Epoch: 8, Steps: 318 | Train Loss: 0.1845060 Vali Loss: 0.2031602 Test Loss: 0.1747384
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.3835679292678833, mae:0.4841783940792084, rmse:0.6193286180496216, mape:0.017352085560560226, mspe:0.0005014232592657208, rse:0.43349963426589966, r2_score:0.7863795474647802, acc:0.9826479144394398
corr: [37.494804 37.31277  37.566517 37.38776  37.5041   37.396255 37.31832
 37.30278  37.399094 37.161255 37.226116 37.171124 37.340244 37.320732
 37.49333  37.320232 37.282253 37.295994 37.307205 37.25695 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3568285
	speed: 0.0389s/iter; left time: 119.7913s
	iters: 200, epoch: 1 | loss: 0.2874908
	speed: 0.0298s/iter; left time: 88.9401s
	iters: 300, epoch: 1 | loss: 0.2527483
	speed: 0.0386s/iter; left time: 111.1497s
Epoch: 1 cost time: 11.433230876922607
Epoch: 1, Steps: 318 | Train Loss: 0.3745443 Vali Loss: 0.2426655 Test Loss: 0.2375751
Validation loss decreased (inf --> 0.242666).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2349674
	speed: 0.0271s/iter; left time: 74.8559s
	iters: 200, epoch: 2 | loss: 0.2198919
	speed: 0.0368s/iter; left time: 98.0756s
	iters: 300, epoch: 2 | loss: 0.1837005
	speed: 0.0298s/iter; left time: 76.5029s
Epoch: 2 cost time: 9.697672843933105
Epoch: 2, Steps: 318 | Train Loss: 0.2275377 Vali Loss: 0.2257461 Test Loss: 0.2077603
Validation loss decreased (0.242666 --> 0.225746).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1994944
	speed: 0.0380s/iter; left time: 92.9227s
	iters: 200, epoch: 3 | loss: 0.2363003
	speed: 0.0189s/iter; left time: 44.2674s
	iters: 300, epoch: 3 | loss: 0.2226665
	speed: 0.0239s/iter; left time: 53.5442s
Epoch: 3 cost time: 8.834943294525146
Epoch: 3, Steps: 318 | Train Loss: 0.2112384 Vali Loss: 0.2206937 Test Loss: 0.1977233
Validation loss decreased (0.225746 --> 0.220694).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2246805
	speed: 0.0401s/iter; left time: 85.3311s
	iters: 200, epoch: 4 | loss: 0.2073120
	speed: 0.0199s/iter; left time: 40.3239s
	iters: 300, epoch: 4 | loss: 0.2083200
	speed: 0.0322s/iter; left time: 62.0633s
Epoch: 4 cost time: 9.60293984413147
Epoch: 4, Steps: 318 | Train Loss: 0.2055219 Vali Loss: 0.2217050 Test Loss: 0.1975416
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2134598
	speed: 0.0324s/iter; left time: 58.5680s
	iters: 200, epoch: 5 | loss: 0.1822994
	speed: 0.0287s/iter; left time: 48.9972s
	iters: 300, epoch: 5 | loss: 0.2840798
	speed: 0.0246s/iter; left time: 39.5419s
Epoch: 5 cost time: 8.893624305725098
Epoch: 5, Steps: 318 | Train Loss: 0.2036612 Vali Loss: 0.2188565 Test Loss: 0.1941282
Validation loss decreased (0.220694 --> 0.218856).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2217912
	speed: 0.0349s/iter; left time: 52.0273s
	iters: 200, epoch: 6 | loss: 0.2055112
	speed: 0.0340s/iter; left time: 47.2776s
	iters: 300, epoch: 6 | loss: 0.2020749
	speed: 0.0247s/iter; left time: 31.8296s
Epoch: 6 cost time: 9.967977523803711
Epoch: 6, Steps: 318 | Train Loss: 0.2024513 Vali Loss: 0.2189487 Test Loss: 0.1933821
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1455463
	speed: 0.0325s/iter; left time: 38.1172s
	iters: 200, epoch: 7 | loss: 0.1741138
	speed: 0.0220s/iter; left time: 23.5838s
	iters: 300, epoch: 7 | loss: 0.2064613
	speed: 0.0342s/iter; left time: 33.2321s
Epoch: 7 cost time: 9.184431791305542
Epoch: 7, Steps: 318 | Train Loss: 0.2017191 Vali Loss: 0.2191476 Test Loss: 0.1931055
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1805323
	speed: 0.0470s/iter; left time: 40.2188s
	iters: 200, epoch: 8 | loss: 0.2033637
	speed: 0.0315s/iter; left time: 23.7588s
	iters: 300, epoch: 8 | loss: 0.1754744
	speed: 0.0179s/iter; left time: 11.7131s
Epoch: 8 cost time: 9.97667407989502
Epoch: 8, Steps: 318 | Train Loss: 0.2020163 Vali Loss: 0.2178744 Test Loss: 0.1931174
Validation loss decreased (0.218856 --> 0.217874).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2143889
	speed: 0.0378s/iter; left time: 20.3159s
	iters: 200, epoch: 9 | loss: 0.1701743
	speed: 0.0384s/iter; left time: 16.7865s
	iters: 300, epoch: 9 | loss: 0.2116079
	speed: 0.0201s/iter; left time: 6.7625s
Epoch: 9 cost time: 10.243390560150146
Epoch: 9, Steps: 318 | Train Loss: 0.2016625 Vali Loss: 0.2194136 Test Loss: 0.1931179
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1978326
	speed: 0.0485s/iter; left time: 10.6161s
	iters: 200, epoch: 10 | loss: 0.1810398
	speed: 0.0256s/iter; left time: 3.0453s
	iters: 300, epoch: 10 | loss: 0.2357985
	speed: 0.0223s/iter; left time: 0.4232s
Epoch: 10 cost time: 9.943382978439331
Epoch: 10, Steps: 318 | Train Loss: 0.2010022 Vali Loss: 0.2189674 Test Loss: 0.1930521
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.42378857731819153, mae:0.508349597454071, rmse:0.6509904861450195, mape:0.018201028928160667, mspe:0.0005529648624360561, rse:0.45535698533058167, r2_score:0.7675338336322295, acc:0.9817989710718393
corr: [37.44563  37.38263  37.25732  37.09442  37.129982 37.24809  37.25534
 37.123    37.167065 37.02678  37.069656 37.192364 37.244274 37.26901
 37.04231  37.29395  37.152576 37.543667 37.417065 37.535515 37.332043
 37.222347 37.449203 37.42674  37.408493]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4016867
	speed: 0.0465s/iter; left time: 142.6915s
	iters: 200, epoch: 1 | loss: 0.2719854
	speed: 0.0329s/iter; left time: 97.8417s
	iters: 300, epoch: 1 | loss: 0.2752186
	speed: 0.0334s/iter; left time: 95.7506s
Epoch: 1 cost time: 11.617656946182251
Epoch: 1, Steps: 317 | Train Loss: 0.3758697 Vali Loss: 0.2698940 Test Loss: 0.2697109
Validation loss decreased (inf --> 0.269894).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2088269
	speed: 0.0449s/iter; left time: 123.6568s
	iters: 200, epoch: 2 | loss: 0.2343777
	speed: 0.0323s/iter; left time: 85.7999s
	iters: 300, epoch: 2 | loss: 0.3155884
	speed: 0.0294s/iter; left time: 75.1690s
Epoch: 2 cost time: 10.982773780822754
Epoch: 2, Steps: 317 | Train Loss: 0.2392564 Vali Loss: 0.2586796 Test Loss: 0.2287896
Validation loss decreased (0.269894 --> 0.258680).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1878634
	speed: 0.0307s/iter; left time: 74.8766s
	iters: 200, epoch: 3 | loss: 0.2749429
	speed: 0.0318s/iter; left time: 74.2785s
	iters: 300, epoch: 3 | loss: 0.2214465
	speed: 0.0295s/iter; left time: 65.9781s
Epoch: 3 cost time: 9.797662734985352
Epoch: 3, Steps: 317 | Train Loss: 0.2215491 Vali Loss: 0.2492286 Test Loss: 0.2146118
Validation loss decreased (0.258680 --> 0.249229).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1718137
	speed: 0.0271s/iter; left time: 57.5381s
	iters: 200, epoch: 4 | loss: 0.2134976
	speed: 0.0244s/iter; left time: 49.3411s
	iters: 300, epoch: 4 | loss: 0.2010702
	speed: 0.0258s/iter; left time: 49.5847s
Epoch: 4 cost time: 8.099123477935791
Epoch: 4, Steps: 317 | Train Loss: 0.2156407 Vali Loss: 0.2488658 Test Loss: 0.2095834
Validation loss decreased (0.249229 --> 0.248866).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1993816
	speed: 0.0375s/iter; left time: 67.6847s
	iters: 200, epoch: 5 | loss: 0.2091151
	speed: 0.0205s/iter; left time: 34.9076s
	iters: 300, epoch: 5 | loss: 0.2364207
	speed: 0.0278s/iter; left time: 44.5822s
Epoch: 5 cost time: 9.167098045349121
Epoch: 5, Steps: 317 | Train Loss: 0.2125985 Vali Loss: 0.2478106 Test Loss: 0.2119878
Validation loss decreased (0.248866 --> 0.247811).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2159864
	speed: 0.0418s/iter; left time: 62.1778s
	iters: 200, epoch: 6 | loss: 0.1886859
	speed: 0.0254s/iter; left time: 35.1488s
	iters: 300, epoch: 6 | loss: 0.1843309
	speed: 0.0185s/iter; left time: 23.8511s
Epoch: 6 cost time: 8.898164510726929
Epoch: 6, Steps: 317 | Train Loss: 0.2112514 Vali Loss: 0.2456208 Test Loss: 0.2098153
Validation loss decreased (0.247811 --> 0.245621).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1705200
	speed: 0.0421s/iter; left time: 49.1785s
	iters: 200, epoch: 7 | loss: 0.2555323
	speed: 0.0310s/iter; left time: 33.1607s
	iters: 300, epoch: 7 | loss: 0.1958316
	speed: 0.0236s/iter; left time: 22.8394s
Epoch: 7 cost time: 9.978257417678833
Epoch: 7, Steps: 317 | Train Loss: 0.2103733 Vali Loss: 0.2458097 Test Loss: 0.2091681
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1774204
	speed: 0.0393s/iter; left time: 33.4648s
	iters: 200, epoch: 8 | loss: 0.2018994
	speed: 0.0312s/iter; left time: 23.4848s
	iters: 300, epoch: 8 | loss: 0.1989408
	speed: 0.0323s/iter; left time: 21.0637s
Epoch: 8 cost time: 10.59913682937622
Epoch: 8, Steps: 317 | Train Loss: 0.2101062 Vali Loss: 0.2458880 Test Loss: 0.2090899
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1931068
	speed: 0.0239s/iter; left time: 12.8011s
	iters: 200, epoch: 9 | loss: 0.1832190
	speed: 0.0273s/iter; left time: 11.8559s
	iters: 300, epoch: 9 | loss: 0.2024879
	speed: 0.0342s/iter; left time: 11.4500s
Epoch: 9 cost time: 9.406810283660889
Epoch: 9, Steps: 317 | Train Loss: 0.2099728 Vali Loss: 0.2446656 Test Loss: 0.2088649
Validation loss decreased (0.245621 --> 0.244666).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2151959
	speed: 0.0284s/iter; left time: 6.1991s
	iters: 200, epoch: 10 | loss: 0.1908331
	speed: 0.0189s/iter; left time: 2.2297s
	iters: 300, epoch: 10 | loss: 0.2219703
	speed: 0.0275s/iter; left time: 0.4959s
Epoch: 10 cost time: 7.820146560668945
Epoch: 10, Steps: 317 | Train Loss: 0.2096600 Vali Loss: 0.2447308 Test Loss: 0.2088079
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.4583457112312317, mae:0.5277438163757324, rmse:0.6770123243331909, mape:0.018905963748693466, mspe:0.0005993762752041221, rse:0.4732188284397125, r2_score:0.7531680593654507, acc:0.9810940362513065
corr: [37.59587  37.45248  37.40623  37.229885 37.340084 37.396847 37.26162
 37.26414  37.35777  37.129433 37.237297 37.326603 37.312542 37.330006
 37.341343 37.13184  37.181797 37.48492  37.497124 37.44022  37.303
 37.344143 37.42447  37.470554 37.48672  37.36186  37.231464 37.453033
 37.619083 37.61376 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2835249
	speed: 0.0664s/iter; left time: 204.7171s
	iters: 200, epoch: 1 | loss: 0.2431908
	speed: 0.0313s/iter; left time: 93.4109s
	iters: 300, epoch: 1 | loss: 0.2512630
	speed: 0.0418s/iter; left time: 120.4826s
Epoch: 1 cost time: 14.662233829498291
Epoch: 1, Steps: 318 | Train Loss: 0.3232808 Vali Loss: 0.2084745 Test Loss: 0.1884035
Validation loss decreased (inf --> 0.208475).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1942263
	speed: 0.0618s/iter; left time: 170.6547s
	iters: 200, epoch: 2 | loss: 0.1907301
	speed: 0.0296s/iter; left time: 78.7580s
	iters: 300, epoch: 2 | loss: 0.1354001
	speed: 0.0447s/iter; left time: 114.6736s
Epoch: 2 cost time: 14.553830623626709
Epoch: 2, Steps: 318 | Train Loss: 0.1897625 Vali Loss: 0.1847950 Test Loss: 0.1576119
Validation loss decreased (0.208475 --> 0.184795).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1417085
	speed: 0.0583s/iter; left time: 142.5309s
	iters: 200, epoch: 3 | loss: 0.1612188
	speed: 0.0370s/iter; left time: 86.6863s
	iters: 300, epoch: 3 | loss: 0.1896623
	speed: 0.0459s/iter; left time: 102.9551s
Epoch: 3 cost time: 14.703225374221802
Epoch: 3, Steps: 318 | Train Loss: 0.1729988 Vali Loss: 0.1792070 Test Loss: 0.1501333
Validation loss decreased (0.184795 --> 0.179207).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1680807
	speed: 0.0473s/iter; left time: 100.6175s
	iters: 200, epoch: 4 | loss: 0.1849568
	speed: 0.0368s/iter; left time: 74.6243s
	iters: 300, epoch: 4 | loss: 0.1775969
	speed: 0.0412s/iter; left time: 79.3215s
Epoch: 4 cost time: 13.102680683135986
Epoch: 4, Steps: 318 | Train Loss: 0.1670822 Vali Loss: 0.1761963 Test Loss: 0.1466988
Validation loss decreased (0.179207 --> 0.176196).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1842185
	speed: 0.0517s/iter; left time: 93.5342s
	iters: 200, epoch: 5 | loss: 0.1844928
	speed: 0.0323s/iter; left time: 55.2377s
	iters: 300, epoch: 5 | loss: 0.1607276
	speed: 0.0480s/iter; left time: 77.1760s
Epoch: 5 cost time: 13.730708837509155
Epoch: 5, Steps: 318 | Train Loss: 0.1643911 Vali Loss: 0.1761371 Test Loss: 0.1460095
Validation loss decreased (0.176196 --> 0.176137).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1720553
	speed: 0.0621s/iter; left time: 92.6197s
	iters: 200, epoch: 6 | loss: 0.1669901
	speed: 0.0262s/iter; left time: 36.4103s
	iters: 300, epoch: 6 | loss: 0.1387515
	speed: 0.0414s/iter; left time: 53.4280s
Epoch: 6 cost time: 14.021085977554321
Epoch: 6, Steps: 318 | Train Loss: 0.1638198 Vali Loss: 0.1749253 Test Loss: 0.1451990
Validation loss decreased (0.176137 --> 0.174925).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1726131
	speed: 0.0637s/iter; left time: 74.6887s
	iters: 200, epoch: 7 | loss: 0.1971764
	speed: 0.0391s/iter; left time: 41.9915s
	iters: 300, epoch: 7 | loss: 0.1655958
	speed: 0.0515s/iter; left time: 50.1190s
Epoch: 7 cost time: 15.895223617553711
Epoch: 7, Steps: 318 | Train Loss: 0.1628730 Vali Loss: 0.1745656 Test Loss: 0.1445368
Validation loss decreased (0.174925 --> 0.174566).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1352143
	speed: 0.0465s/iter; left time: 39.7210s
	iters: 200, epoch: 8 | loss: 0.1483834
	speed: 0.0506s/iter; left time: 38.1985s
	iters: 300, epoch: 8 | loss: 0.1723589
	speed: 0.0368s/iter; left time: 24.1105s
Epoch: 8 cost time: 13.80736780166626
Epoch: 8, Steps: 318 | Train Loss: 0.1620646 Vali Loss: 0.1735433 Test Loss: 0.1444743
Validation loss decreased (0.174566 --> 0.173543).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1892286
	speed: 0.0487s/iter; left time: 26.1405s
	iters: 200, epoch: 9 | loss: 0.1728130
	speed: 0.0362s/iter; left time: 15.8304s
	iters: 300, epoch: 9 | loss: 0.1168464
	speed: 0.0455s/iter; left time: 15.3402s
Epoch: 9 cost time: 13.455443620681763
Epoch: 9, Steps: 318 | Train Loss: 0.1620533 Vali Loss: 0.1732956 Test Loss: 0.1444298
Validation loss decreased (0.173543 --> 0.173296).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1537601
	speed: 0.0525s/iter; left time: 11.4889s
	iters: 200, epoch: 10 | loss: 0.1350830
	speed: 0.0278s/iter; left time: 3.3065s
	iters: 300, epoch: 10 | loss: 0.1831799
	speed: 0.0367s/iter; left time: 0.6976s
Epoch: 10 cost time: 12.63273024559021
Epoch: 10, Steps: 318 | Train Loss: 0.1617306 Vali Loss: 0.1741486 Test Loss: 0.1443973
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.3169456124305725, mae:0.44007644057273865, rmse:0.5629792213439941, mape:0.015754126012325287, mspe:0.00041237688856199384, rse:0.3944718837738037, r2_score:0.824238744637376, acc:0.9842458739876747
corr: [37.15997  36.97449  37.32904  37.241196 37.12029  37.40111  37.12035
 37.304016 37.355755 37.230957]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2892908
	speed: 0.0652s/iter; left time: 200.9377s
	iters: 200, epoch: 1 | loss: 0.2580948
	speed: 0.0432s/iter; left time: 128.7700s
	iters: 300, epoch: 1 | loss: 0.2317749
	speed: 0.0523s/iter; left time: 150.6807s
Epoch: 1 cost time: 16.64794635772705
Epoch: 1, Steps: 318 | Train Loss: 0.3765407 Vali Loss: 0.2170776 Test Loss: 0.1935461
Validation loss decreased (inf --> 0.217078).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1895292
	speed: 0.0498s/iter; left time: 137.6585s
	iters: 200, epoch: 2 | loss: 0.2197695
	speed: 0.0392s/iter; left time: 104.2719s
	iters: 300, epoch: 2 | loss: 0.1381570
	speed: 0.0507s/iter; left time: 129.9464s
Epoch: 2 cost time: 14.605335474014282
Epoch: 2, Steps: 318 | Train Loss: 0.2050910 Vali Loss: 0.2039941 Test Loss: 0.1689275
Validation loss decreased (0.217078 --> 0.203994).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1776720
	speed: 0.0613s/iter; left time: 149.8787s
	iters: 200, epoch: 3 | loss: 0.2278338
	speed: 0.0418s/iter; left time: 98.0494s
	iters: 300, epoch: 3 | loss: 0.1888576
	speed: 0.0479s/iter; left time: 107.5957s
Epoch: 3 cost time: 16.109540224075317
Epoch: 3, Steps: 318 | Train Loss: 0.1896433 Vali Loss: 0.2003607 Test Loss: 0.1720756
Validation loss decreased (0.203994 --> 0.200361).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1222484
	speed: 0.0515s/iter; left time: 109.4468s
	iters: 200, epoch: 4 | loss: 0.1462069
	speed: 0.0419s/iter; left time: 84.9361s
	iters: 300, epoch: 4 | loss: 0.2424830
	speed: 0.0459s/iter; left time: 88.4419s
Epoch: 4 cost time: 14.570934772491455
Epoch: 4, Steps: 318 | Train Loss: 0.1854924 Vali Loss: 0.1972098 Test Loss: 0.1669852
Validation loss decreased (0.200361 --> 0.197210).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1455212
	speed: 0.0511s/iter; left time: 92.4868s
	iters: 200, epoch: 5 | loss: 0.1948578
	speed: 0.0402s/iter; left time: 68.6515s
	iters: 300, epoch: 5 | loss: 0.1908764
	speed: 0.0434s/iter; left time: 69.8992s
Epoch: 5 cost time: 14.510039567947388
Epoch: 5, Steps: 318 | Train Loss: 0.1823859 Vali Loss: 0.1954886 Test Loss: 0.1646200
Validation loss decreased (0.197210 --> 0.195489).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1761959
	speed: 0.0510s/iter; left time: 76.0535s
	iters: 200, epoch: 6 | loss: 0.1372990
	speed: 0.0521s/iter; left time: 72.4403s
	iters: 300, epoch: 6 | loss: 0.1798347
	speed: 0.0402s/iter; left time: 51.9306s
Epoch: 6 cost time: 14.989196538925171
Epoch: 6, Steps: 318 | Train Loss: 0.1816000 Vali Loss: 0.1952097 Test Loss: 0.1644398
Validation loss decreased (0.195489 --> 0.195210).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1895851
	speed: 0.0438s/iter; left time: 51.4218s
	iters: 200, epoch: 7 | loss: 0.2064205
	speed: 0.0515s/iter; left time: 55.2459s
	iters: 300, epoch: 7 | loss: 0.1261850
	speed: 0.0426s/iter; left time: 41.4662s
Epoch: 7 cost time: 14.742960453033447
Epoch: 7, Steps: 318 | Train Loss: 0.1816553 Vali Loss: 0.1945533 Test Loss: 0.1640429
Validation loss decreased (0.195210 --> 0.194553).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2268685
	speed: 0.0444s/iter; left time: 37.9695s
	iters: 200, epoch: 8 | loss: 0.1778588
	speed: 0.0458s/iter; left time: 34.5455s
	iters: 300, epoch: 8 | loss: 0.2002896
	speed: 0.0452s/iter; left time: 29.6330s
Epoch: 8 cost time: 14.208369493484497
Epoch: 8, Steps: 318 | Train Loss: 0.1811964 Vali Loss: 0.1936222 Test Loss: 0.1639732
Validation loss decreased (0.194553 --> 0.193622).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1481587
	speed: 0.0566s/iter; left time: 30.4143s
	iters: 200, epoch: 9 | loss: 0.2106443
	speed: 0.0329s/iter; left time: 14.3790s
	iters: 300, epoch: 9 | loss: 0.1581508
	speed: 0.0442s/iter; left time: 14.9115s
Epoch: 9 cost time: 14.43237566947937
Epoch: 9, Steps: 318 | Train Loss: 0.1806886 Vali Loss: 0.1949282 Test Loss: 0.1638605
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1778219
	speed: 0.0560s/iter; left time: 12.2581s
	iters: 200, epoch: 10 | loss: 0.1927822
	speed: 0.0422s/iter; left time: 5.0231s
	iters: 300, epoch: 10 | loss: 0.1473573
	speed: 0.0460s/iter; left time: 0.8733s
Epoch: 10 cost time: 15.365076780319214
Epoch: 10, Steps: 318 | Train Loss: 0.1808225 Vali Loss: 0.1942926 Test Loss: 0.1637897
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.3598327338695526, mae:0.4658200442790985, rmse:0.5998606085777283, mape:0.01666240766644478, mspe:0.0004673119692597538, rse:0.42011627554893494, r2_score:0.8032144319750562, acc:0.9833375923335552
corr: [37.04303  37.11178  37.072304 37.049076 37.15425  37.03535  37.265053
 37.22538  37.343105 37.248726 37.170258 37.350388 37.479816 37.40826
 37.50317 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2729525
	speed: 0.0737s/iter; left time: 227.1607s
	iters: 200, epoch: 1 | loss: 0.3171974
	speed: 0.0296s/iter; left time: 88.2640s
	iters: 300, epoch: 1 | loss: 0.2640969
	speed: 0.0446s/iter; left time: 128.5910s
Epoch: 1 cost time: 15.80036473274231
Epoch: 1, Steps: 318 | Train Loss: 0.3744923 Vali Loss: 0.2228249 Test Loss: 0.2057622
Validation loss decreased (inf --> 0.222825).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2140333
	speed: 0.0594s/iter; left time: 164.0758s
	iters: 200, epoch: 2 | loss: 0.1721591
	speed: 0.0438s/iter; left time: 116.5997s
	iters: 300, epoch: 2 | loss: 0.2343237
	speed: 0.0562s/iter; left time: 144.1234s
Epoch: 2 cost time: 16.834105014801025
Epoch: 2, Steps: 318 | Train Loss: 0.2108271 Vali Loss: 0.2187083 Test Loss: 0.1956147
Validation loss decreased (0.222825 --> 0.218708).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1718502
	speed: 0.0516s/iter; left time: 126.2084s
	iters: 200, epoch: 3 | loss: 0.1941325
	speed: 0.0416s/iter; left time: 97.6241s
	iters: 300, epoch: 3 | loss: 0.1914302
	speed: 0.0541s/iter; left time: 121.3972s
Epoch: 3 cost time: 15.37312388420105
Epoch: 3, Steps: 318 | Train Loss: 0.1970944 Vali Loss: 0.2041287 Test Loss: 0.1827907
Validation loss decreased (0.218708 --> 0.204129).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1780082
	speed: 0.0530s/iter; left time: 112.6708s
	iters: 200, epoch: 4 | loss: 0.2065455
	speed: 0.0327s/iter; left time: 66.2996s
	iters: 300, epoch: 4 | loss: 0.2150579
	speed: 0.0552s/iter; left time: 106.3811s
Epoch: 4 cost time: 14.519497871398926
Epoch: 4, Steps: 318 | Train Loss: 0.1928030 Vali Loss: 0.2042301 Test Loss: 0.1766914
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2849172
	speed: 0.0566s/iter; left time: 102.4413s
	iters: 200, epoch: 5 | loss: 0.1778854
	speed: 0.0498s/iter; left time: 85.0919s
	iters: 300, epoch: 5 | loss: 0.1826481
	speed: 0.0414s/iter; left time: 66.6195s
Epoch: 5 cost time: 15.192384958267212
Epoch: 5, Steps: 318 | Train Loss: 0.1908443 Vali Loss: 0.2021948 Test Loss: 0.1774983
Validation loss decreased (0.204129 --> 0.202195).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2000034
	speed: 0.0577s/iter; left time: 86.0588s
	iters: 200, epoch: 6 | loss: 0.1852033
	speed: 0.0462s/iter; left time: 64.3136s
	iters: 300, epoch: 6 | loss: 0.2653767
	speed: 0.0455s/iter; left time: 58.7337s
Epoch: 6 cost time: 15.35412073135376
Epoch: 6, Steps: 318 | Train Loss: 0.1891081 Vali Loss: 0.2039058 Test Loss: 0.1766373
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1668979
	speed: 0.0489s/iter; left time: 57.3910s
	iters: 200, epoch: 7 | loss: 0.2819826
	speed: 0.0482s/iter; left time: 51.7714s
	iters: 300, epoch: 7 | loss: 0.1441859
	speed: 0.0432s/iter; left time: 41.9919s
Epoch: 7 cost time: 14.480206489562988
Epoch: 7, Steps: 318 | Train Loss: 0.1890971 Vali Loss: 0.2028662 Test Loss: 0.1768887
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1705616
	speed: 0.0539s/iter; left time: 46.0444s
	iters: 200, epoch: 8 | loss: 0.2146578
	speed: 0.0539s/iter; left time: 40.6749s
	iters: 300, epoch: 8 | loss: 0.1737241
	speed: 0.0411s/iter; left time: 26.9187s
Epoch: 8 cost time: 15.493658304214478
Epoch: 8, Steps: 318 | Train Loss: 0.1886124 Vali Loss: 0.2038634 Test Loss: 0.1769912
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.3895130753517151, mae:0.4863564670085907, rmse:0.6241098046302795, mape:0.01742900162935257, mspe:0.0005090138292871416, rse:0.4368462562561035, r2_score:0.7842089330197323, acc:0.9825709983706474
corr: [37.409622 37.344967 37.56051  37.53196  37.487133 37.298737 37.330097
 37.395226 37.533405 37.396873 37.354813 37.22588  37.29623  37.411026
 37.66301  37.167755 37.256454 37.415688 37.396343 37.50453 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3368313
	speed: 0.0580s/iter; left time: 178.6567s
	iters: 200, epoch: 1 | loss: 0.2711523
	speed: 0.0518s/iter; left time: 154.4597s
	iters: 300, epoch: 1 | loss: 0.2392675
	speed: 0.0517s/iter; left time: 148.8683s
Epoch: 1 cost time: 17.165762901306152
Epoch: 1, Steps: 318 | Train Loss: 0.3618501 Vali Loss: 0.2299169 Test Loss: 0.2130493
Validation loss decreased (inf --> 0.229917).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2121645
	speed: 0.0438s/iter; left time: 121.1356s
	iters: 200, epoch: 2 | loss: 0.2069381
	speed: 0.0529s/iter; left time: 140.9023s
	iters: 300, epoch: 2 | loss: 0.1611580
	speed: 0.0400s/iter; left time: 102.5115s
Epoch: 2 cost time: 14.581657409667969
Epoch: 2, Steps: 318 | Train Loss: 0.2143857 Vali Loss: 0.2160859 Test Loss: 0.1965708
Validation loss decreased (0.229917 --> 0.216086).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1948691
	speed: 0.0420s/iter; left time: 102.6037s
	iters: 200, epoch: 3 | loss: 0.2212424
	speed: 0.0533s/iter; left time: 124.8824s
	iters: 300, epoch: 3 | loss: 0.2344020
	speed: 0.0449s/iter; left time: 100.8088s
Epoch: 3 cost time: 14.90022587776184
Epoch: 3, Steps: 318 | Train Loss: 0.2016290 Vali Loss: 0.2123147 Test Loss: 0.1907667
Validation loss decreased (0.216086 --> 0.212315).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2246715
	speed: 0.0493s/iter; left time: 104.8771s
	iters: 200, epoch: 4 | loss: 0.1961135
	speed: 0.0553s/iter; left time: 112.1890s
	iters: 300, epoch: 4 | loss: 0.2007708
	speed: 0.0384s/iter; left time: 74.0040s
Epoch: 4 cost time: 15.057888746261597
Epoch: 4, Steps: 318 | Train Loss: 0.1973423 Vali Loss: 0.2133176 Test Loss: 0.1948334
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1971091
	speed: 0.0494s/iter; left time: 89.3137s
	iters: 200, epoch: 5 | loss: 0.1868945
	speed: 0.0547s/iter; left time: 93.4856s
	iters: 300, epoch: 5 | loss: 0.2487241
	speed: 0.0367s/iter; left time: 58.9771s
Epoch: 5 cost time: 14.812899589538574
Epoch: 5, Steps: 318 | Train Loss: 0.1953823 Vali Loss: 0.2108578 Test Loss: 0.1896341
Validation loss decreased (0.212315 --> 0.210858).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2164115
	speed: 0.0452s/iter; left time: 67.4647s
	iters: 200, epoch: 6 | loss: 0.1975375
	speed: 0.0528s/iter; left time: 73.4625s
	iters: 300, epoch: 6 | loss: 0.1971733
	speed: 0.0450s/iter; left time: 58.0996s
Epoch: 6 cost time: 15.267422914505005
Epoch: 6, Steps: 318 | Train Loss: 0.1945602 Vali Loss: 0.2115246 Test Loss: 0.1894117
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1517669
	speed: 0.0477s/iter; left time: 55.9676s
	iters: 200, epoch: 7 | loss: 0.1617078
	speed: 0.0483s/iter; left time: 51.8425s
	iters: 300, epoch: 7 | loss: 0.1997313
	speed: 0.0495s/iter; left time: 48.1199s
Epoch: 7 cost time: 15.288549900054932
Epoch: 7, Steps: 318 | Train Loss: 0.1937288 Vali Loss: 0.2118725 Test Loss: 0.1890842
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1833136
	speed: 0.0501s/iter; left time: 42.8463s
	iters: 200, epoch: 8 | loss: 0.1895797
	speed: 0.0545s/iter; left time: 41.1307s
	iters: 300, epoch: 8 | loss: 0.1656338
	speed: 0.0401s/iter; left time: 26.2503s
Epoch: 8 cost time: 15.218680143356323
Epoch: 8, Steps: 318 | Train Loss: 0.1938175 Vali Loss: 0.2103350 Test Loss: 0.1891899
Validation loss decreased (0.210858 --> 0.210335).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2063733
	speed: 0.0418s/iter; left time: 22.4328s
	iters: 200, epoch: 9 | loss: 0.1606869
	speed: 0.0527s/iter; left time: 23.0233s
	iters: 300, epoch: 9 | loss: 0.2082421
	speed: 0.0551s/iter; left time: 18.5575s
Epoch: 9 cost time: 15.921481370925903
Epoch: 9, Steps: 318 | Train Loss: 0.1934841 Vali Loss: 0.2119969 Test Loss: 0.1892647
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1731808
	speed: 0.0458s/iter; left time: 10.0269s
	iters: 200, epoch: 10 | loss: 0.1794419
	speed: 0.0516s/iter; left time: 6.1350s
	iters: 300, epoch: 10 | loss: 0.2308869
	speed: 0.0406s/iter; left time: 0.7717s
Epoch: 10 cost time: 14.455753564834595
Epoch: 10, Steps: 318 | Train Loss: 0.1932062 Vali Loss: 0.2115367 Test Loss: 0.1892062
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.41516992449760437, mae:0.500177800655365, rmse:0.6443368196487427, mape:0.017897510901093483, mspe:0.0005399850779213011, rse:0.45070287585258484, r2_score:0.7742201659363634, acc:0.9821024890989065
corr: [37.13593  37.000744 36.926292 36.806015 36.886375 36.796036 36.867847
 36.885773 36.87192  37.07704  37.049477 36.95833  37.027943 37.018787
 37.00758  37.137573 37.13328  37.509624 37.35671  37.688225 37.510612
 37.373707 37.410923 37.381695 37.34481 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4042636
	speed: 0.0616s/iter; left time: 189.3208s
	iters: 200, epoch: 1 | loss: 0.2739675
	speed: 0.0532s/iter; left time: 157.9979s
	iters: 300, epoch: 1 | loss: 0.2788780
	speed: 0.0398s/iter; left time: 114.2244s
Epoch: 1 cost time: 16.02874994277954
Epoch: 1, Steps: 317 | Train Loss: 0.3775072 Vali Loss: 0.2718412 Test Loss: 0.2721192
Validation loss decreased (inf --> 0.271841).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2096228
	speed: 0.0423s/iter; left time: 116.5647s
	iters: 200, epoch: 2 | loss: 0.2382725
	speed: 0.0529s/iter; left time: 140.3711s
	iters: 300, epoch: 2 | loss: 0.3208257
	speed: 0.0366s/iter; left time: 93.5721s
Epoch: 2 cost time: 14.004185438156128
Epoch: 2, Steps: 317 | Train Loss: 0.2415681 Vali Loss: 0.2623679 Test Loss: 0.2308923
Validation loss decreased (0.271841 --> 0.262368).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1902160
	speed: 0.0422s/iter; left time: 102.8893s
	iters: 200, epoch: 3 | loss: 0.2740721
	speed: 0.0465s/iter; left time: 108.6628s
	iters: 300, epoch: 3 | loss: 0.2219156
	speed: 0.0349s/iter; left time: 77.9651s
Epoch: 3 cost time: 12.983629941940308
Epoch: 3, Steps: 317 | Train Loss: 0.2233765 Vali Loss: 0.2528361 Test Loss: 0.2162108
Validation loss decreased (0.262368 --> 0.252836).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1744422
	speed: 0.0644s/iter; left time: 136.6313s
	iters: 200, epoch: 4 | loss: 0.2157915
	speed: 0.0334s/iter; left time: 67.5228s
	iters: 300, epoch: 4 | loss: 0.2019348
	speed: 0.0433s/iter; left time: 83.2158s
Epoch: 4 cost time: 15.016895532608032
Epoch: 4, Steps: 317 | Train Loss: 0.2171501 Vali Loss: 0.2521794 Test Loss: 0.2107952
Validation loss decreased (0.252836 --> 0.252179).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2008113
	speed: 0.0630s/iter; left time: 113.5966s
	iters: 200, epoch: 5 | loss: 0.2112369
	speed: 0.0346s/iter; left time: 58.9038s
	iters: 300, epoch: 5 | loss: 0.2384624
	speed: 0.0343s/iter; left time: 54.9191s
Epoch: 5 cost time: 13.669174671173096
Epoch: 5, Steps: 317 | Train Loss: 0.2139478 Vali Loss: 0.2507431 Test Loss: 0.2130612
Validation loss decreased (0.252179 --> 0.250743).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2166196
	speed: 0.0451s/iter; left time: 67.0116s
	iters: 200, epoch: 6 | loss: 0.1894239
	speed: 0.0488s/iter; left time: 67.6254s
	iters: 300, epoch: 6 | loss: 0.1844866
	speed: 0.0369s/iter; left time: 47.4571s
Epoch: 6 cost time: 13.494160652160645
Epoch: 6, Steps: 317 | Train Loss: 0.2125865 Vali Loss: 0.2485516 Test Loss: 0.2109154
Validation loss decreased (0.250743 --> 0.248552).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1705608
	speed: 0.0447s/iter; left time: 52.2508s
	iters: 200, epoch: 7 | loss: 0.2555577
	speed: 0.0359s/iter; left time: 38.3390s
	iters: 300, epoch: 7 | loss: 0.1981798
	speed: 0.0440s/iter; left time: 42.6090s
Epoch: 7 cost time: 13.001602411270142
Epoch: 7, Steps: 317 | Train Loss: 0.2116463 Vali Loss: 0.2487213 Test Loss: 0.2102729
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1794948
	speed: 0.0415s/iter; left time: 35.3222s
	iters: 200, epoch: 8 | loss: 0.2025797
	speed: 0.0473s/iter; left time: 35.5829s
	iters: 300, epoch: 8 | loss: 0.1988354
	speed: 0.0413s/iter; left time: 26.8974s
Epoch: 8 cost time: 13.463099241256714
Epoch: 8, Steps: 317 | Train Loss: 0.2113863 Vali Loss: 0.2487815 Test Loss: 0.2101599
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1942267
	speed: 0.0491s/iter; left time: 26.2935s
	iters: 200, epoch: 9 | loss: 0.1850244
	speed: 0.0287s/iter; left time: 12.4965s
	iters: 300, epoch: 9 | loss: 0.2047134
	speed: 0.0605s/iter; left time: 20.2658s
Epoch: 9 cost time: 14.641798257827759
Epoch: 9, Steps: 317 | Train Loss: 0.2112621 Vali Loss: 0.2475347 Test Loss: 0.2099378
Validation loss decreased (0.248552 --> 0.247535).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2156743
	speed: 0.0602s/iter; left time: 13.1316s
	iters: 200, epoch: 10 | loss: 0.1898687
	speed: 0.0341s/iter; left time: 4.0185s
	iters: 300, epoch: 10 | loss: 0.2256913
	speed: 0.0377s/iter; left time: 0.6791s
Epoch: 10 cost time: 13.739124536514282
Epoch: 10, Steps: 317 | Train Loss: 0.2109101 Vali Loss: 0.2475890 Test Loss: 0.2098768
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.4607001543045044, mae:0.529242217540741, rmse:0.6787489652633667, mape:0.018960341811180115, mspe:0.0006025514448992908, rse:0.47443270683288574, r2_score:0.7518626756367421, acc:0.9810396581888199
corr: [37.619686 37.45634  37.40218  37.229954 37.322456 37.393875 37.26624
 37.26872  37.36582  37.125835 37.220253 37.311672 37.296906 37.320374
 37.31921  37.105907 37.148384 37.478767 37.48491  37.42499  37.29197
 37.328205 37.43045  37.45776  37.476936 37.3408   37.22878  37.470036
 37.629246 37.62397 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2822393
	speed: 0.0725s/iter; left time: 223.4247s
	iters: 200, epoch: 1 | loss: 0.2430217
	speed: 0.0664s/iter; left time: 198.0739s
	iters: 300, epoch: 1 | loss: 0.2482783
	speed: 0.0547s/iter; left time: 157.6673s
Epoch: 1 cost time: 20.452373504638672
Epoch: 1, Steps: 318 | Train Loss: 0.3238714 Vali Loss: 0.2071597 Test Loss: 0.1869833
Validation loss decreased (inf --> 0.207160).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1940717
	speed: 0.0582s/iter; left time: 160.8933s
	iters: 200, epoch: 2 | loss: 0.1902261
	speed: 0.0662s/iter; left time: 176.1709s
	iters: 300, epoch: 2 | loss: 0.1353412
	speed: 0.0627s/iter; left time: 160.8280s
Epoch: 2 cost time: 20.185213565826416
Epoch: 2, Steps: 318 | Train Loss: 0.1889132 Vali Loss: 0.1832867 Test Loss: 0.1564925
Validation loss decreased (0.207160 --> 0.183287).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1412451
	speed: 0.0549s/iter; left time: 134.3453s
	iters: 200, epoch: 3 | loss: 0.1606515
	speed: 0.0568s/iter; left time: 133.2466s
	iters: 300, epoch: 3 | loss: 0.1901847
	speed: 0.0657s/iter; left time: 147.4504s
Epoch: 3 cost time: 18.95758891105652
Epoch: 3, Steps: 318 | Train Loss: 0.1723969 Vali Loss: 0.1777723 Test Loss: 0.1490880
Validation loss decreased (0.183287 --> 0.177772).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1667749
	speed: 0.0544s/iter; left time: 115.7309s
	iters: 200, epoch: 4 | loss: 0.1831500
	speed: 0.0628s/iter; left time: 127.3740s
	iters: 300, epoch: 4 | loss: 0.1773126
	speed: 0.0682s/iter; left time: 131.4329s
Epoch: 4 cost time: 19.349117279052734
Epoch: 4, Steps: 318 | Train Loss: 0.1665812 Vali Loss: 0.1748737 Test Loss: 0.1457442
Validation loss decreased (0.177772 --> 0.174874).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1821532
	speed: 0.0451s/iter; left time: 81.4992s
	iters: 200, epoch: 5 | loss: 0.1842713
	speed: 0.0643s/iter; left time: 109.8234s
	iters: 300, epoch: 5 | loss: 0.1608120
	speed: 0.0670s/iter; left time: 107.8621s
Epoch: 5 cost time: 18.453503847122192
Epoch: 5, Steps: 318 | Train Loss: 0.1639462 Vali Loss: 0.1748363 Test Loss: 0.1450935
Validation loss decreased (0.174874 --> 0.174836).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1704176
	speed: 0.0541s/iter; left time: 80.7040s
	iters: 200, epoch: 6 | loss: 0.1665768
	speed: 0.0640s/iter; left time: 89.0770s
	iters: 300, epoch: 6 | loss: 0.1382901
	speed: 0.0728s/iter; left time: 93.9520s
Epoch: 6 cost time: 20.05660319328308
Epoch: 6, Steps: 318 | Train Loss: 0.1634182 Vali Loss: 0.1736395 Test Loss: 0.1443050
Validation loss decreased (0.174836 --> 0.173639).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1713980
	speed: 0.0642s/iter; left time: 75.3363s
	iters: 200, epoch: 7 | loss: 0.1959528
	speed: 0.0629s/iter; left time: 67.4492s
	iters: 300, epoch: 7 | loss: 0.1652985
	speed: 0.0632s/iter; left time: 61.5230s
Epoch: 7 cost time: 20.298139095306396
Epoch: 7, Steps: 318 | Train Loss: 0.1624337 Vali Loss: 0.1732935 Test Loss: 0.1436592
Validation loss decreased (0.173639 --> 0.173293).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1341784
	speed: 0.0708s/iter; left time: 60.5245s
	iters: 200, epoch: 8 | loss: 0.1474558
	speed: 0.0587s/iter; left time: 44.2901s
	iters: 300, epoch: 8 | loss: 0.1722221
	speed: 0.0672s/iter; left time: 44.0130s
Epoch: 8 cost time: 20.7054500579834
Epoch: 8, Steps: 318 | Train Loss: 0.1616306 Vali Loss: 0.1723105 Test Loss: 0.1436069
Validation loss decreased (0.173293 --> 0.172310).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1890696
	speed: 0.0714s/iter; left time: 38.3277s
	iters: 200, epoch: 9 | loss: 0.1719547
	speed: 0.0605s/iter; left time: 26.4489s
	iters: 300, epoch: 9 | loss: 0.1169337
	speed: 0.0600s/iter; left time: 20.2289s
Epoch: 9 cost time: 19.758514881134033
Epoch: 9, Steps: 318 | Train Loss: 0.1616355 Vali Loss: 0.1720437 Test Loss: 0.1435663
Validation loss decreased (0.172310 --> 0.172044).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1535986
	speed: 0.0728s/iter; left time: 15.9350s
	iters: 200, epoch: 10 | loss: 0.1346028
	speed: 0.0640s/iter; left time: 7.6132s
	iters: 300, epoch: 10 | loss: 0.1827494
	speed: 0.0597s/iter; left time: 1.1350s
Epoch: 10 cost time: 20.82033681869507
Epoch: 10, Steps: 318 | Train Loss: 0.1613189 Vali Loss: 0.1728922 Test Loss: 0.1435331
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.31505075097084045, mae:0.4389052093029022, rmse:0.5612938404083252, mape:0.015712179243564606, mspe:0.00040991147398017347, rse:0.3932909667491913, r2_score:0.8249628917553435, acc:0.9842878207564354
corr: [37.160957 36.968605 37.30879  37.22958  37.103092 37.39357  37.122272
 37.294712 37.350487 37.23818 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2993220
	speed: 0.0641s/iter; left time: 197.4399s
	iters: 200, epoch: 1 | loss: 0.2413978
	speed: 0.0556s/iter; left time: 165.6148s
	iters: 300, epoch: 1 | loss: 0.2154270
	speed: 0.0414s/iter; left time: 119.2242s
Epoch: 1 cost time: 16.798139572143555
Epoch: 1, Steps: 318 | Train Loss: 0.3579446 Vali Loss: 0.2021682 Test Loss: 0.1870353
Validation loss decreased (inf --> 0.202168).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1683848
	speed: 0.0564s/iter; left time: 155.9659s
	iters: 200, epoch: 2 | loss: 0.1893564
	speed: 0.0452s/iter; left time: 120.2376s
	iters: 300, epoch: 2 | loss: 0.1229604
	speed: 0.0511s/iter; left time: 130.9791s
Epoch: 2 cost time: 15.862820625305176
Epoch: 2, Steps: 318 | Train Loss: 0.1971595 Vali Loss: 0.1943784 Test Loss: 0.1669719
Validation loss decreased (0.202168 --> 0.194378).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1827024
	speed: 0.0584s/iter; left time: 142.8443s
	iters: 200, epoch: 3 | loss: 0.2158059
	speed: 0.0370s/iter; left time: 86.8693s
	iters: 300, epoch: 3 | loss: 0.1600774
	speed: 0.0437s/iter; left time: 98.0636s
Epoch: 3 cost time: 14.490808725357056
Epoch: 3, Steps: 318 | Train Loss: 0.1821678 Vali Loss: 0.1895353 Test Loss: 0.1696578
Validation loss decreased (0.194378 --> 0.189535).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1196404
	speed: 0.0532s/iter; left time: 113.2118s
	iters: 200, epoch: 4 | loss: 0.1438836
	speed: 0.0723s/iter; left time: 146.5555s
	iters: 300, epoch: 4 | loss: 0.2576993
	speed: 0.0357s/iter; left time: 68.8125s
Epoch: 4 cost time: 16.78603506088257
Epoch: 4, Steps: 318 | Train Loss: 0.1773554 Vali Loss: 0.1880682 Test Loss: 0.1654522
Validation loss decreased (0.189535 --> 0.188068).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1481883
	speed: 0.0780s/iter; left time: 141.0167s
	iters: 200, epoch: 5 | loss: 0.1660185
	speed: 0.0481s/iter; left time: 82.1974s
	iters: 300, epoch: 5 | loss: 0.1662109
	speed: 0.0528s/iter; left time: 85.0239s
Epoch: 5 cost time: 18.67215085029602
Epoch: 5, Steps: 318 | Train Loss: 0.1745495 Vali Loss: 0.1873839 Test Loss: 0.1627758
Validation loss decreased (0.188068 --> 0.187384).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1663377
	speed: 0.0498s/iter; left time: 74.2924s
	iters: 200, epoch: 6 | loss: 0.1359593
	speed: 0.0642s/iter; left time: 89.3018s
	iters: 300, epoch: 6 | loss: 0.1627301
	speed: 0.0471s/iter; left time: 60.7509s
Epoch: 6 cost time: 17.352953672409058
Epoch: 6, Steps: 318 | Train Loss: 0.1733964 Vali Loss: 0.1874313 Test Loss: 0.1630718
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1834524
	speed: 0.0609s/iter; left time: 71.4843s
	iters: 200, epoch: 7 | loss: 0.1845039
	speed: 0.0588s/iter; left time: 63.0631s
	iters: 300, epoch: 7 | loss: 0.1278655
	speed: 0.0474s/iter; left time: 46.1272s
Epoch: 7 cost time: 17.47376322746277
Epoch: 7, Steps: 318 | Train Loss: 0.1734046 Vali Loss: 0.1872787 Test Loss: 0.1626827
Validation loss decreased (0.187384 --> 0.187279).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2245114
	speed: 0.0574s/iter; left time: 49.1010s
	iters: 200, epoch: 8 | loss: 0.1821709
	speed: 0.0348s/iter; left time: 26.2740s
	iters: 300, epoch: 8 | loss: 0.1849786
	speed: 0.0585s/iter; left time: 38.3353s
Epoch: 8 cost time: 15.789186716079712
Epoch: 8, Steps: 318 | Train Loss: 0.1728625 Vali Loss: 0.1860652 Test Loss: 0.1626435
Validation loss decreased (0.187279 --> 0.186065).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1436481
	speed: 0.0558s/iter; left time: 29.9557s
	iters: 200, epoch: 9 | loss: 0.1950671
	speed: 0.0506s/iter; left time: 22.1029s
	iters: 300, epoch: 9 | loss: 0.1362934
	speed: 0.0587s/iter; left time: 19.7917s
Epoch: 9 cost time: 17.45366668701172
Epoch: 9, Steps: 318 | Train Loss: 0.1724137 Vali Loss: 0.1874398 Test Loss: 0.1625278
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1787266
	speed: 0.0569s/iter; left time: 12.4575s
	iters: 200, epoch: 10 | loss: 0.1937634
	speed: 0.0562s/iter; left time: 6.6858s
	iters: 300, epoch: 10 | loss: 0.1538039
	speed: 0.0487s/iter; left time: 0.9261s
Epoch: 10 cost time: 17.154300212860107
Epoch: 10, Steps: 318 | Train Loss: 0.1724665 Vali Loss: 0.1869115 Test Loss: 0.1624377
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.3569147288799286, mae:0.4632463753223419, rmse:0.5974234342575073, mape:0.01658003218472004, mspe:0.0004638857499230653, rse:0.4184093773365021, r2_score:0.8076049361388944, acc:0.98341996781528
corr: [37.18073  37.11596  37.062344 36.892242 36.834038 37.03634  36.97563
 37.044624 37.187454 37.220238 37.160378 37.228104 37.27117  37.200542
 37.161697]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2854616
	speed: 0.0449s/iter; left time: 138.4771s
	iters: 200, epoch: 1 | loss: 0.3240021
	speed: 0.0251s/iter; left time: 74.8958s
	iters: 300, epoch: 1 | loss: 0.2714605
	speed: 0.0254s/iter; left time: 73.2064s
Epoch: 1 cost time: 10.103821754455566
Epoch: 1, Steps: 318 | Train Loss: 0.3694464 Vali Loss: 0.2270073 Test Loss: 0.2149667
Validation loss decreased (inf --> 0.227007).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2323490
	speed: 0.0310s/iter; left time: 85.5762s
	iters: 200, epoch: 2 | loss: 0.1652427
	speed: 0.0279s/iter; left time: 74.3600s
	iters: 300, epoch: 2 | loss: 0.2753167
	speed: 0.0278s/iter; left time: 71.2087s
Epoch: 2 cost time: 9.222355365753174
Epoch: 2, Steps: 318 | Train Loss: 0.2125411 Vali Loss: 0.2190923 Test Loss: 0.2057703
Validation loss decreased (0.227007 --> 0.219092).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1788686
	speed: 0.0323s/iter; left time: 79.0358s
	iters: 200, epoch: 3 | loss: 0.1828732
	speed: 0.0274s/iter; left time: 64.3266s
	iters: 300, epoch: 3 | loss: 0.1863081
	speed: 0.0277s/iter; left time: 62.2565s
Epoch: 3 cost time: 9.284220457077026
Epoch: 3, Steps: 318 | Train Loss: 0.1964493 Vali Loss: 0.2097555 Test Loss: 0.1911469
Validation loss decreased (0.219092 --> 0.209755).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1801007
	speed: 0.0332s/iter; left time: 70.5169s
	iters: 200, epoch: 4 | loss: 0.2091182
	speed: 0.0277s/iter; left time: 56.1269s
	iters: 300, epoch: 4 | loss: 0.2104471
	speed: 0.0281s/iter; left time: 54.1426s
Epoch: 4 cost time: 9.425270318984985
Epoch: 4, Steps: 318 | Train Loss: 0.1907015 Vali Loss: 0.2128711 Test Loss: 0.1866691
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2778084
	speed: 0.0323s/iter; left time: 58.3572s
	iters: 200, epoch: 5 | loss: 0.1784947
	speed: 0.0289s/iter; left time: 49.3941s
	iters: 300, epoch: 5 | loss: 0.1772371
	speed: 0.0275s/iter; left time: 44.2736s
Epoch: 5 cost time: 9.398235082626343
Epoch: 5, Steps: 318 | Train Loss: 0.1880486 Vali Loss: 0.2094581 Test Loss: 0.1866598
Validation loss decreased (0.209755 --> 0.209458).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2097852
	speed: 0.0323s/iter; left time: 48.0908s
	iters: 200, epoch: 6 | loss: 0.1888914
	speed: 0.0290s/iter; left time: 40.3445s
	iters: 300, epoch: 6 | loss: 0.2607653
	speed: 0.0281s/iter; left time: 36.3335s
Epoch: 6 cost time: 9.490581274032593
Epoch: 6, Steps: 318 | Train Loss: 0.1859979 Vali Loss: 0.2112275 Test Loss: 0.1858818
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1869121
	speed: 0.0320s/iter; left time: 37.5035s
	iters: 200, epoch: 7 | loss: 0.2759340
	speed: 0.0288s/iter; left time: 30.9211s
	iters: 300, epoch: 7 | loss: 0.1467509
	speed: 0.0281s/iter; left time: 27.3280s
Epoch: 7 cost time: 9.425602197647095
Epoch: 7, Steps: 318 | Train Loss: 0.1857539 Vali Loss: 0.2084513 Test Loss: 0.1858884
Validation loss decreased (0.209458 --> 0.208451).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1526812
	speed: 0.0322s/iter; left time: 27.5557s
	iters: 200, epoch: 8 | loss: 0.1928364
	speed: 0.0281s/iter; left time: 21.1872s
	iters: 300, epoch: 8 | loss: 0.1691409
	speed: 0.0273s/iter; left time: 17.9048s
Epoch: 8 cost time: 9.31187891960144
Epoch: 8, Steps: 318 | Train Loss: 0.1849384 Vali Loss: 0.2100948 Test Loss: 0.1860041
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1945617
	speed: 0.0344s/iter; left time: 18.4975s
	iters: 200, epoch: 9 | loss: 0.1525817
	speed: 0.0284s/iter; left time: 12.4084s
	iters: 300, epoch: 9 | loss: 0.2040821
	speed: 0.0284s/iter; left time: 9.5750s
Epoch: 9 cost time: 9.704761028289795
Epoch: 9, Steps: 318 | Train Loss: 0.1850331 Vali Loss: 0.2102965 Test Loss: 0.1859136
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1066644
	speed: 0.0316s/iter; left time: 6.9253s
	iters: 200, epoch: 10 | loss: 0.1426917
	speed: 0.0278s/iter; left time: 3.3076s
	iters: 300, epoch: 10 | loss: 0.1730832
	speed: 0.0275s/iter; left time: 0.5231s
Epoch: 10 cost time: 9.233295679092407
Epoch: 10, Steps: 318 | Train Loss: 0.1851445 Vali Loss: 0.2096910 Test Loss: 0.1858915
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.4079248309135437, mae:0.49722373485565186, rmse:0.6386899352073669, mape:0.017809990793466568, mspe:0.0005317018367350101, rse:0.44705161452293396, r2_score:0.7742092847829256, acc:0.9821900092065334
corr: [37.41944  37.3293   37.517036 37.48854  37.585865 37.291172 37.333805
 37.332333 37.391663 37.29983  37.322563 37.36759  37.407757 37.399456
 37.46758  37.35634  37.36674  37.34118  37.449192 37.211388]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3340330
	speed: 0.0462s/iter; left time: 142.2863s
	iters: 200, epoch: 1 | loss: 0.2713953
	speed: 0.0283s/iter; left time: 84.4180s
	iters: 300, epoch: 1 | loss: 0.2361038
	speed: 0.0272s/iter; left time: 78.3528s
Epoch: 1 cost time: 10.719702959060669
Epoch: 1, Steps: 318 | Train Loss: 0.3599075 Vali Loss: 0.2279627 Test Loss: 0.2113721
Validation loss decreased (inf --> 0.227963).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2117294
	speed: 0.0312s/iter; left time: 86.3053s
	iters: 200, epoch: 2 | loss: 0.2073403
	speed: 0.0259s/iter; left time: 69.0285s
	iters: 300, epoch: 2 | loss: 0.1605754
	speed: 0.0279s/iter; left time: 71.5002s
Epoch: 2 cost time: 9.023593425750732
Epoch: 2, Steps: 318 | Train Loss: 0.2141429 Vali Loss: 0.2159716 Test Loss: 0.1958051
Validation loss decreased (0.227963 --> 0.215972).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1937868
	speed: 0.0307s/iter; left time: 75.1732s
	iters: 200, epoch: 3 | loss: 0.2215625
	speed: 0.0260s/iter; left time: 61.0646s
	iters: 300, epoch: 3 | loss: 0.2360089
	speed: 0.0276s/iter; left time: 61.8699s
Epoch: 3 cost time: 8.952327013015747
Epoch: 3, Steps: 318 | Train Loss: 0.2016019 Vali Loss: 0.2125099 Test Loss: 0.1902636
Validation loss decreased (0.215972 --> 0.212510).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2312052
	speed: 0.0312s/iter; left time: 66.3557s
	iters: 200, epoch: 4 | loss: 0.1960808
	speed: 0.0272s/iter; left time: 55.0568s
	iters: 300, epoch: 4 | loss: 0.2000615
	speed: 0.0260s/iter; left time: 50.0117s
Epoch: 4 cost time: 8.976160764694214
Epoch: 4, Steps: 318 | Train Loss: 0.1974285 Vali Loss: 0.2137650 Test Loss: 0.1944454
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1971548
	speed: 0.0319s/iter; left time: 57.7021s
	iters: 200, epoch: 5 | loss: 0.1870886
	speed: 0.0282s/iter; left time: 48.1845s
	iters: 300, epoch: 5 | loss: 0.2503466
	speed: 0.0265s/iter; left time: 42.5703s
Epoch: 5 cost time: 9.171480655670166
Epoch: 5, Steps: 318 | Train Loss: 0.1953055 Vali Loss: 0.2112808 Test Loss: 0.1893131
Validation loss decreased (0.212510 --> 0.211281).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2179230
	speed: 0.0314s/iter; left time: 46.7762s
	iters: 200, epoch: 6 | loss: 0.1972380
	speed: 0.0263s/iter; left time: 36.5758s
	iters: 300, epoch: 6 | loss: 0.1963829
	speed: 0.0267s/iter; left time: 34.4346s
Epoch: 6 cost time: 8.950386047363281
Epoch: 6, Steps: 318 | Train Loss: 0.1945746 Vali Loss: 0.2118856 Test Loss: 0.1891699
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1529718
	speed: 0.0304s/iter; left time: 35.6018s
	iters: 200, epoch: 7 | loss: 0.1612427
	speed: 0.0262s/iter; left time: 28.0974s
	iters: 300, epoch: 7 | loss: 0.2002362
	speed: 0.0282s/iter; left time: 27.4617s
Epoch: 7 cost time: 9.018670558929443
Epoch: 7, Steps: 318 | Train Loss: 0.1937801 Vali Loss: 0.2122143 Test Loss: 0.1889914
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1833731
	speed: 0.0299s/iter; left time: 25.5561s
	iters: 200, epoch: 8 | loss: 0.1885066
	speed: 0.0259s/iter; left time: 19.5874s
	iters: 300, epoch: 8 | loss: 0.1657744
	speed: 0.0263s/iter; left time: 17.2201s
Epoch: 8 cost time: 8.771768569946289
Epoch: 8, Steps: 318 | Train Loss: 0.1937874 Vali Loss: 0.2106202 Test Loss: 0.1890353
Validation loss decreased (0.211281 --> 0.210620).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2056747
	speed: 0.0320s/iter; left time: 17.1845s
	iters: 200, epoch: 9 | loss: 0.1613091
	speed: 0.0270s/iter; left time: 11.7891s
	iters: 300, epoch: 9 | loss: 0.2092160
	speed: 0.0278s/iter; left time: 9.3822s
Epoch: 9 cost time: 9.20824122428894
Epoch: 9, Steps: 318 | Train Loss: 0.1934418 Vali Loss: 0.2123119 Test Loss: 0.1891100
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1725000
	speed: 0.0302s/iter; left time: 6.6123s
	iters: 200, epoch: 10 | loss: 0.1799038
	speed: 0.0273s/iter; left time: 3.2516s
	iters: 300, epoch: 10 | loss: 0.2315705
	speed: 0.0270s/iter; left time: 0.5125s
Epoch: 10 cost time: 8.966447353363037
Epoch: 10, Steps: 318 | Train Loss: 0.1933010 Vali Loss: 0.2118179 Test Loss: 0.1890343
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.41483059525489807, mae:0.5001432299613953, rmse:0.6440734267234802, mape:0.017894724383950233, mspe:0.0005393560859374702, rse:0.4505186676979065, r2_score:0.7746837579789114, acc:0.9821052756160498
corr: [37.109287 36.97996  36.911716 36.782448 36.87864  36.78707  36.869217
 36.888634 36.850853 37.07929  37.037277 36.976913 37.037495 37.022118
 37.00528  37.14103  37.128258 37.512325 37.335716 37.674885 37.53915
 37.389076 37.431137 37.370777 37.31349 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4017296
	speed: 0.0471s/iter; left time: 144.5035s
	iters: 200, epoch: 1 | loss: 0.2721242
	speed: 0.0310s/iter; left time: 92.0369s
	iters: 300, epoch: 1 | loss: 0.2773769
	speed: 0.0301s/iter; left time: 86.3490s
Epoch: 1 cost time: 11.319000959396362
Epoch: 1, Steps: 317 | Train Loss: 0.3770081 Vali Loss: 0.2712025 Test Loss: 0.2717745
Validation loss decreased (inf --> 0.271203).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2098994
	speed: 0.0340s/iter; left time: 93.5122s
	iters: 200, epoch: 2 | loss: 0.2350465
	speed: 0.0299s/iter; left time: 79.4440s
	iters: 300, epoch: 2 | loss: 0.3189679
	speed: 0.0288s/iter; left time: 73.6282s
Epoch: 2 cost time: 9.805032730102539
Epoch: 2, Steps: 317 | Train Loss: 0.2405880 Vali Loss: 0.2610696 Test Loss: 0.2302168
Validation loss decreased (0.271203 --> 0.261070).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1895730
	speed: 0.0335s/iter; left time: 81.5462s
	iters: 200, epoch: 3 | loss: 0.2757333
	speed: 0.0291s/iter; left time: 67.9887s
	iters: 300, epoch: 3 | loss: 0.2217286
	speed: 0.0300s/iter; left time: 67.0499s
Epoch: 3 cost time: 9.792078256607056
Epoch: 3, Steps: 317 | Train Loss: 0.2225169 Vali Loss: 0.2518267 Test Loss: 0.2157684
Validation loss decreased (0.261070 --> 0.251827).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1722371
	speed: 0.0339s/iter; left time: 71.7701s
	iters: 200, epoch: 4 | loss: 0.2148773
	speed: 0.0279s/iter; left time: 56.4543s
	iters: 300, epoch: 4 | loss: 0.2017271
	speed: 0.0308s/iter; left time: 59.1364s
Epoch: 4 cost time: 9.802447319030762
Epoch: 4, Steps: 317 | Train Loss: 0.2163981 Vali Loss: 0.2512613 Test Loss: 0.2104794
Validation loss decreased (0.251827 --> 0.251261).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1993799
	speed: 0.0332s/iter; left time: 59.8680s
	iters: 200, epoch: 5 | loss: 0.2098566
	speed: 0.0287s/iter; left time: 48.8404s
	iters: 300, epoch: 5 | loss: 0.2373243
	speed: 0.0291s/iter; left time: 46.6363s
Epoch: 5 cost time: 9.627798318862915
Epoch: 5, Steps: 317 | Train Loss: 0.2132715 Vali Loss: 0.2500345 Test Loss: 0.2127379
Validation loss decreased (0.251261 --> 0.250034).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2157964
	speed: 0.0330s/iter; left time: 48.9928s
	iters: 200, epoch: 6 | loss: 0.1889802
	speed: 0.0286s/iter; left time: 39.6497s
	iters: 300, epoch: 6 | loss: 0.1850485
	speed: 0.0292s/iter; left time: 37.5249s
Epoch: 6 cost time: 9.596451997756958
Epoch: 6, Steps: 317 | Train Loss: 0.2118792 Vali Loss: 0.2477698 Test Loss: 0.2105471
Validation loss decreased (0.250034 --> 0.247770).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1708259
	speed: 0.0327s/iter; left time: 38.2542s
	iters: 200, epoch: 7 | loss: 0.2557643
	speed: 0.0289s/iter; left time: 30.8961s
	iters: 300, epoch: 7 | loss: 0.1960932
	speed: 0.0296s/iter; left time: 28.6851s
Epoch: 7 cost time: 9.644034624099731
Epoch: 7, Steps: 317 | Train Loss: 0.2109516 Vali Loss: 0.2479831 Test Loss: 0.2098880
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1792481
	speed: 0.0340s/iter; left time: 28.9644s
	iters: 200, epoch: 8 | loss: 0.2029641
	speed: 0.0297s/iter; left time: 22.3685s
	iters: 300, epoch: 8 | loss: 0.1996889
	speed: 0.0289s/iter; left time: 18.8591s
Epoch: 8 cost time: 9.798963785171509
Epoch: 8, Steps: 317 | Train Loss: 0.2106809 Vali Loss: 0.2480402 Test Loss: 0.2098010
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1934968
	speed: 0.0338s/iter; left time: 18.0675s
	iters: 200, epoch: 9 | loss: 0.1835772
	speed: 0.0284s/iter; left time: 12.3534s
	iters: 300, epoch: 9 | loss: 0.2029128
	speed: 0.0295s/iter; left time: 9.8723s
Epoch: 9 cost time: 9.692556381225586
Epoch: 9, Steps: 317 | Train Loss: 0.2105808 Vali Loss: 0.2467969 Test Loss: 0.2095786
Validation loss decreased (0.247770 --> 0.246797).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2145411
	speed: 0.0337s/iter; left time: 7.3542s
	iters: 200, epoch: 10 | loss: 0.1920264
	speed: 0.0296s/iter; left time: 3.4935s
	iters: 300, epoch: 10 | loss: 0.2228192
	speed: 0.0291s/iter; left time: 0.5232s
Epoch: 10 cost time: 9.757959842681885
Epoch: 10, Steps: 317 | Train Loss: 0.2102602 Vali Loss: 0.2468536 Test Loss: 0.2095152
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.45991209149360657, mae:0.5285710096359253, rmse:0.6781681776046753, mape:0.018936997279524803, mspe:0.0006016269326210022, rse:0.47402673959732056, r2_score:0.7517634913721652, acc:0.9810630027204752
corr: [37.624245 37.465687 37.415215 37.217094 37.345825 37.407513 37.281166
 37.279186 37.370113 37.13582  37.258675 37.357353 37.34154  37.351223
 37.351215 37.14133  37.20121  37.526863 37.528156 37.45906  37.298298
 37.354694 37.457176 37.48923  37.507793 37.353714 37.24628  37.47652
 37.628914 37.622307]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2823486
	speed: 0.0539s/iter; left time: 166.0443s
	iters: 200, epoch: 1 | loss: 0.2424661
	speed: 0.0349s/iter; left time: 103.8909s
	iters: 300, epoch: 1 | loss: 0.2486904
	speed: 0.0320s/iter; left time: 92.2109s
Epoch: 1 cost time: 12.642141342163086
Epoch: 1, Steps: 318 | Train Loss: 0.3249544 Vali Loss: 0.2082060 Test Loss: 0.1873972
Validation loss decreased (inf --> 0.208206).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1942012
	speed: 0.0386s/iter; left time: 106.7652s
	iters: 200, epoch: 2 | loss: 0.1903803
	speed: 0.0323s/iter; left time: 86.1403s
	iters: 300, epoch: 2 | loss: 0.1350447
	speed: 0.0341s/iter; left time: 87.3097s
Epoch: 2 cost time: 11.159784317016602
Epoch: 2, Steps: 318 | Train Loss: 0.1889855 Vali Loss: 0.1843300 Test Loss: 0.1568656
Validation loss decreased (0.208206 --> 0.184330).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1419137
	speed: 0.0379s/iter; left time: 92.5545s
	iters: 200, epoch: 3 | loss: 0.1606704
	speed: 0.0331s/iter; left time: 77.5657s
	iters: 300, epoch: 3 | loss: 0.1905848
	speed: 0.0350s/iter; left time: 78.6269s
Epoch: 3 cost time: 11.279960632324219
Epoch: 3, Steps: 318 | Train Loss: 0.1724166 Vali Loss: 0.1789120 Test Loss: 0.1494517
Validation loss decreased (0.184330 --> 0.178912).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1663650
	speed: 0.0384s/iter; left time: 81.6336s
	iters: 200, epoch: 4 | loss: 0.1834577
	speed: 0.0326s/iter; left time: 66.0337s
	iters: 300, epoch: 4 | loss: 0.1762438
	speed: 0.0349s/iter; left time: 67.2591s
Epoch: 4 cost time: 11.323349714279175
Epoch: 4, Steps: 318 | Train Loss: 0.1665901 Vali Loss: 0.1760132 Test Loss: 0.1461447
Validation loss decreased (0.178912 --> 0.176013).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1824569
	speed: 0.0384s/iter; left time: 69.4436s
	iters: 200, epoch: 5 | loss: 0.1843415
	speed: 0.0325s/iter; left time: 55.4645s
	iters: 300, epoch: 5 | loss: 0.1603830
	speed: 0.0337s/iter; left time: 54.2885s
Epoch: 5 cost time: 11.123972654342651
Epoch: 5, Steps: 318 | Train Loss: 0.1639203 Vali Loss: 0.1759966 Test Loss: 0.1454935
Validation loss decreased (0.176013 --> 0.175997).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1709099
	speed: 0.0386s/iter; left time: 57.5130s
	iters: 200, epoch: 6 | loss: 0.1665661
	speed: 0.0344s/iter; left time: 47.8275s
	iters: 300, epoch: 6 | loss: 0.1389058
	speed: 0.0336s/iter; left time: 43.4348s
Epoch: 6 cost time: 11.332146406173706
Epoch: 6, Steps: 318 | Train Loss: 0.1633979 Vali Loss: 0.1748274 Test Loss: 0.1447036
Validation loss decreased (0.175997 --> 0.174827).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1717131
	speed: 0.0383s/iter; left time: 44.9388s
	iters: 200, epoch: 7 | loss: 0.1956727
	speed: 0.0336s/iter; left time: 36.0208s
	iters: 300, epoch: 7 | loss: 0.1652923
	speed: 0.0342s/iter; left time: 33.2929s
Epoch: 7 cost time: 11.310308933258057
Epoch: 7, Steps: 318 | Train Loss: 0.1624258 Vali Loss: 0.1744605 Test Loss: 0.1440624
Validation loss decreased (0.174827 --> 0.174461).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1338928
	speed: 0.0375s/iter; left time: 32.0795s
	iters: 200, epoch: 8 | loss: 0.1477729
	speed: 0.0342s/iter; left time: 25.8475s
	iters: 300, epoch: 8 | loss: 0.1714331
	speed: 0.0322s/iter; left time: 21.0895s
Epoch: 8 cost time: 11.027743101119995
Epoch: 8, Steps: 318 | Train Loss: 0.1615991 Vali Loss: 0.1734857 Test Loss: 0.1440132
Validation loss decreased (0.174461 --> 0.173486).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1885095
	speed: 0.0384s/iter; left time: 20.5950s
	iters: 200, epoch: 9 | loss: 0.1714901
	speed: 0.0360s/iter; left time: 15.7173s
	iters: 300, epoch: 9 | loss: 0.1172986
	speed: 0.0357s/iter; left time: 12.0314s
Epoch: 9 cost time: 11.696266889572144
Epoch: 9, Steps: 318 | Train Loss: 0.1615976 Vali Loss: 0.1732313 Test Loss: 0.1439725
Validation loss decreased (0.173486 --> 0.173231).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1529698
	speed: 0.0635s/iter; left time: 13.9162s
	iters: 200, epoch: 10 | loss: 0.1348375
	speed: 0.0673s/iter; left time: 8.0070s
	iters: 300, epoch: 10 | loss: 0.1830220
	speed: 0.0594s/iter; left time: 1.1284s
Epoch: 10 cost time: 20.415910720825195
Epoch: 10, Steps: 318 | Train Loss: 0.1612722 Vali Loss: 0.1740734 Test Loss: 0.1439391
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.3159419000148773, mae:0.43921756744384766, rmse:0.5620871186256409, mape:0.015723910182714462, mspe:0.0004111389280296862, rse:0.3938468098640442, r2_score:0.8244121467570226, acc:0.9842760898172855
corr: [37.18221  36.965687 37.310318 37.236366 37.083065 37.410492 37.134956
 37.303093 37.366074 37.247894]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2985470
	speed: 0.1386s/iter; left time: 427.0985s
	iters: 200, epoch: 1 | loss: 0.2415289
	speed: 0.1271s/iter; left time: 378.8404s
	iters: 300, epoch: 1 | loss: 0.2100559
	speed: 0.1513s/iter; left time: 435.9247s
Epoch: 1 cost time: 44.55476450920105
Epoch: 1, Steps: 318 | Train Loss: 0.3601685 Vali Loss: 0.2054711 Test Loss: 0.1864473
Validation loss decreased (inf --> 0.205471).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1685448
	speed: 0.1456s/iter; left time: 402.2089s
	iters: 200, epoch: 2 | loss: 0.1891710
	speed: 0.1481s/iter; left time: 394.4149s
	iters: 300, epoch: 2 | loss: 0.1232971
	speed: 0.1451s/iter; left time: 371.8386s
Epoch: 2 cost time: 46.846742153167725
Epoch: 2, Steps: 318 | Train Loss: 0.1960594 Vali Loss: 0.1942962 Test Loss: 0.1656259
Validation loss decreased (0.205471 --> 0.194296).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1792055
	speed: 0.1372s/iter; left time: 335.3462s
	iters: 200, epoch: 3 | loss: 0.2159237
	speed: 0.1406s/iter; left time: 329.7237s
	iters: 300, epoch: 3 | loss: 0.1637370
	speed: 0.1516s/iter; left time: 340.3032s
Epoch: 3 cost time: 45.79523420333862
Epoch: 3, Steps: 318 | Train Loss: 0.1816702 Vali Loss: 0.1882696 Test Loss: 0.1677745
Validation loss decreased (0.194296 --> 0.188270).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1165950
	speed: 0.1559s/iter; left time: 331.6392s
	iters: 200, epoch: 4 | loss: 0.1408309
	speed: 0.1583s/iter; left time: 320.9696s
	iters: 300, epoch: 4 | loss: 0.2581946
	speed: 0.1510s/iter; left time: 290.9723s
Epoch: 4 cost time: 49.260459423065186
Epoch: 4, Steps: 318 | Train Loss: 0.1771400 Vali Loss: 0.1870577 Test Loss: 0.1643252
Validation loss decreased (0.188270 --> 0.187058).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1453236
	speed: 0.1560s/iter; left time: 282.1949s
	iters: 200, epoch: 5 | loss: 0.1675824
	speed: 0.1505s/iter; left time: 257.1396s
	iters: 300, epoch: 5 | loss: 0.1708207
	speed: 0.1419s/iter; left time: 228.3642s
Epoch: 5 cost time: 47.5134961605072
Epoch: 5, Steps: 318 | Train Loss: 0.1744098 Vali Loss: 0.1861326 Test Loss: 0.1617139
Validation loss decreased (0.187058 --> 0.186133).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1666211
	speed: 0.1460s/iter; left time: 217.6637s
	iters: 200, epoch: 6 | loss: 0.1346789
	speed: 0.1535s/iter; left time: 213.4585s
	iters: 300, epoch: 6 | loss: 0.1635704
	speed: 0.1389s/iter; left time: 179.2758s
Epoch: 6 cost time: 46.543790102005005
Epoch: 6, Steps: 318 | Train Loss: 0.1735422 Vali Loss: 0.1863527 Test Loss: 0.1619757
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1873665
	speed: 0.1519s/iter; left time: 178.1220s
	iters: 200, epoch: 7 | loss: 0.1891164
	speed: 0.1540s/iter; left time: 165.2871s
	iters: 300, epoch: 7 | loss: 0.1267733
	speed: 0.1354s/iter; left time: 131.7849s
Epoch: 7 cost time: 46.67810893058777
Epoch: 7, Steps: 318 | Train Loss: 0.1734620 Vali Loss: 0.1859547 Test Loss: 0.1615630
Validation loss decreased (0.186133 --> 0.185955).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2197397
	speed: 0.1388s/iter; left time: 118.6326s
	iters: 200, epoch: 8 | loss: 0.1807409
	speed: 0.1494s/iter; left time: 112.8171s
	iters: 300, epoch: 8 | loss: 0.1840600
	speed: 0.1544s/iter; left time: 101.1391s
Epoch: 8 cost time: 46.878275871276855
Epoch: 8, Steps: 318 | Train Loss: 0.1729218 Vali Loss: 0.1848565 Test Loss: 0.1615330
Validation loss decreased (0.185955 --> 0.184857).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1426245
	speed: 0.1470s/iter; left time: 78.9487s
	iters: 200, epoch: 9 | loss: 0.1920887
	speed: 0.1582s/iter; left time: 69.1280s
	iters: 300, epoch: 9 | loss: 0.1371738
	speed: 0.1534s/iter; left time: 51.6842s
Epoch: 9 cost time: 48.831859827041626
Epoch: 9, Steps: 318 | Train Loss: 0.1724624 Vali Loss: 0.1864032 Test Loss: 0.1614358
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1746879
	speed: 0.1393s/iter; left time: 30.5136s
	iters: 200, epoch: 10 | loss: 0.1960125
	speed: 0.1523s/iter; left time: 18.1286s
	iters: 300, epoch: 10 | loss: 0.1524892
	speed: 0.1585s/iter; left time: 3.0113s
Epoch: 10 cost time: 47.203678131103516
Epoch: 10, Steps: 318 | Train Loss: 0.1724966 Vali Loss: 0.1857720 Test Loss: 0.1613427
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.3544779419898987, mae:0.46246373653411865, rmse:0.5953804850578308, mape:0.016550328582525253, mspe:0.00046041602035984397, rse:0.4169786274433136, r2_score:0.8079304933057703, acc:0.9834496714174747
corr: [37.16606  37.111446 37.052    36.906902 36.89536  37.069664 37.047703
 37.094395 37.280914 37.33144  37.24491  37.311913 37.34685  37.28058
 37.216503]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2976177
	speed: 0.1547s/iter; left time: 476.5077s
	iters: 200, epoch: 1 | loss: 0.3408383
	speed: 0.1110s/iter; left time: 330.8698s
	iters: 300, epoch: 1 | loss: 0.2791823
	speed: 0.1412s/iter; left time: 406.6886s
Epoch: 1 cost time: 43.51464343070984
Epoch: 1, Steps: 318 | Train Loss: 0.3726393 Vali Loss: 0.2235072 Test Loss: 0.2088684
Validation loss decreased (inf --> 0.223507).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2236041
	speed: 0.1303s/iter; left time: 360.1394s
	iters: 200, epoch: 2 | loss: 0.1641357
	speed: 0.1495s/iter; left time: 398.1799s
	iters: 300, epoch: 2 | loss: 0.2565667
	speed: 0.1529s/iter; left time: 392.0078s
Epoch: 2 cost time: 46.538386821746826
Epoch: 2, Steps: 318 | Train Loss: 0.2108394 Vali Loss: 0.2177396 Test Loss: 0.1994218
Validation loss decreased (0.223507 --> 0.217740).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1807170
	speed: 0.1663s/iter; left time: 406.6836s
	iters: 200, epoch: 3 | loss: 0.1857325
	speed: 0.1843s/iter; left time: 432.2550s
	iters: 300, epoch: 3 | loss: 0.1919376
	speed: 0.1719s/iter; left time: 385.9570s
Epoch: 3 cost time: 55.53993225097656
Epoch: 3, Steps: 318 | Train Loss: 0.1949334 Vali Loss: 0.2058154 Test Loss: 0.1870568
Validation loss decreased (0.217740 --> 0.205815).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1753849
	speed: 0.1660s/iter; left time: 353.1883s
	iters: 200, epoch: 4 | loss: 0.2013130
	speed: 0.1630s/iter; left time: 330.4256s
	iters: 300, epoch: 4 | loss: 0.2078500
	speed: 0.1556s/iter; left time: 299.8153s
Epoch: 4 cost time: 51.69099259376526
Epoch: 4, Steps: 318 | Train Loss: 0.1896738 Vali Loss: 0.2050184 Test Loss: 0.1798217
Validation loss decreased (0.205815 --> 0.205018).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2905799
	speed: 0.1744s/iter; left time: 315.4137s
	iters: 200, epoch: 5 | loss: 0.1718919
	speed: 0.1652s/iter; left time: 282.3286s
	iters: 300, epoch: 5 | loss: 0.1803516
	speed: 0.1594s/iter; left time: 256.4636s
Epoch: 5 cost time: 53.200974464416504
Epoch: 5, Steps: 318 | Train Loss: 0.1870055 Vali Loss: 0.2019308 Test Loss: 0.1804843
Validation loss decreased (0.205018 --> 0.201931).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1983026
	speed: 0.1671s/iter; left time: 249.1172s
	iters: 200, epoch: 6 | loss: 0.1780224
	speed: 0.1735s/iter; left time: 241.3296s
	iters: 300, epoch: 6 | loss: 0.2576244
	speed: 0.1681s/iter; left time: 216.9721s
Epoch: 6 cost time: 53.938239097595215
Epoch: 6, Steps: 318 | Train Loss: 0.1855349 Vali Loss: 0.2039693 Test Loss: 0.1800519
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1707717
	speed: 0.1573s/iter; left time: 184.4552s
	iters: 200, epoch: 7 | loss: 0.2847511
	speed: 0.1692s/iter; left time: 181.5356s
	iters: 300, epoch: 7 | loss: 0.1450305
	speed: 0.1577s/iter; left time: 153.4687s
Epoch: 7 cost time: 51.742796659469604
Epoch: 7, Steps: 318 | Train Loss: 0.1855476 Vali Loss: 0.2028136 Test Loss: 0.1803354
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1631136
	speed: 0.1621s/iter; left time: 138.5675s
	iters: 200, epoch: 8 | loss: 0.2009956
	speed: 0.1888s/iter; left time: 142.5500s
	iters: 300, epoch: 8 | loss: 0.1746814
	speed: 0.1613s/iter; left time: 105.6374s
Epoch: 8 cost time: 54.29654574394226
Epoch: 8, Steps: 318 | Train Loss: 0.1847706 Vali Loss: 0.2035939 Test Loss: 0.1803889
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.3960658013820648, mae:0.49335911870002747, rmse:0.6293376088142395, mape:0.017668263986706734, mspe:0.0005162442103028297, rse:0.4405054450035095, r2_score:0.7815107068054675, acc:0.9823317360132933
corr: [37.170727 37.104965 37.342144 37.27036  37.33819  37.127537 37.166203
 37.17812  37.237114 37.129215 37.25726  37.184723 37.2768   37.212692
 37.376106 37.345337 37.26392  37.330967 37.425137 37.195026]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3439879
	speed: 0.1299s/iter; left time: 400.2675s
	iters: 200, epoch: 1 | loss: 0.2723974
	speed: 0.1087s/iter; left time: 324.0906s
	iters: 300, epoch: 1 | loss: 0.2404169
	speed: 0.1154s/iter; left time: 332.4228s
Epoch: 1 cost time: 37.36072492599487
Epoch: 1, Steps: 318 | Train Loss: 0.3641776 Vali Loss: 0.2297348 Test Loss: 0.2134493
Validation loss decreased (inf --> 0.229735).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2137635
	speed: 0.0977s/iter; left time: 270.0395s
	iters: 200, epoch: 2 | loss: 0.2081042
	speed: 0.1035s/iter; left time: 275.5347s
	iters: 300, epoch: 2 | loss: 0.1624692
	speed: 0.1068s/iter; left time: 273.7750s
Epoch: 2 cost time: 32.635634422302246
Epoch: 2, Steps: 318 | Train Loss: 0.2155079 Vali Loss: 0.2156891 Test Loss: 0.1969519
Validation loss decreased (0.229735 --> 0.215689).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1964817
	speed: 0.0993s/iter; left time: 242.7416s
	iters: 200, epoch: 3 | loss: 0.2223621
	speed: 0.0988s/iter; left time: 231.7912s
	iters: 300, epoch: 3 | loss: 0.2357195
	speed: 0.1046s/iter; left time: 234.7704s
Epoch: 3 cost time: 32.217828035354614
Epoch: 3, Steps: 318 | Train Loss: 0.2025632 Vali Loss: 0.2118929 Test Loss: 0.1910127
Validation loss decreased (0.215689 --> 0.211893).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2330966
	speed: 0.0996s/iter; left time: 211.8952s
	iters: 200, epoch: 4 | loss: 0.1985804
	speed: 0.0909s/iter; left time: 184.3458s
	iters: 300, epoch: 4 | loss: 0.2010183
	speed: 0.1038s/iter; left time: 200.0564s
Epoch: 4 cost time: 31.284435987472534
Epoch: 4, Steps: 318 | Train Loss: 0.1984193 Vali Loss: 0.2126835 Test Loss: 0.1954584
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1981960
	speed: 0.0948s/iter; left time: 171.4577s
	iters: 200, epoch: 5 | loss: 0.1882545
	speed: 0.0983s/iter; left time: 167.9132s
	iters: 300, epoch: 5 | loss: 0.2534155
	speed: 0.0961s/iter; left time: 154.6422s
Epoch: 5 cost time: 30.848302364349365
Epoch: 5, Steps: 318 | Train Loss: 0.1963449 Vali Loss: 0.2100724 Test Loss: 0.1900233
Validation loss decreased (0.211893 --> 0.210072).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2174972
	speed: 0.1075s/iter; left time: 160.2170s
	iters: 200, epoch: 6 | loss: 0.1987861
	speed: 0.0888s/iter; left time: 123.5564s
	iters: 300, epoch: 6 | loss: 0.1977509
	speed: 0.0933s/iter; left time: 120.4169s
Epoch: 6 cost time: 30.626978874206543
Epoch: 6, Steps: 318 | Train Loss: 0.1956169 Vali Loss: 0.2106202 Test Loss: 0.1898421
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1525463
	speed: 0.1045s/iter; left time: 122.6231s
	iters: 200, epoch: 7 | loss: 0.1622257
	speed: 0.1074s/iter; left time: 115.2271s
	iters: 300, epoch: 7 | loss: 0.2011034
	speed: 0.0974s/iter; left time: 94.7858s
Epoch: 7 cost time: 32.720120429992676
Epoch: 7, Steps: 318 | Train Loss: 0.1948364 Vali Loss: 0.2109060 Test Loss: 0.1894974
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1844283
	speed: 0.1060s/iter; left time: 90.6184s
	iters: 200, epoch: 8 | loss: 0.1899248
	speed: 0.1075s/iter; left time: 81.1567s
	iters: 300, epoch: 8 | loss: 0.1656791
	speed: 0.1008s/iter; left time: 66.0337s
Epoch: 8 cost time: 33.602617502212524
Epoch: 8, Steps: 318 | Train Loss: 0.1948750 Vali Loss: 0.2094104 Test Loss: 0.1895990
Validation loss decreased (0.210072 --> 0.209410).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2070806
	speed: 0.1053s/iter; left time: 56.5547s
	iters: 200, epoch: 9 | loss: 0.1626568
	speed: 0.0997s/iter; left time: 43.5695s
	iters: 300, epoch: 9 | loss: 0.2088542
	speed: 0.0974s/iter; left time: 32.8260s
Epoch: 9 cost time: 32.103453397750854
Epoch: 9, Steps: 318 | Train Loss: 0.1945446 Vali Loss: 0.2110089 Test Loss: 0.1896956
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1730374
	speed: 0.1085s/iter; left time: 23.7670s
	iters: 200, epoch: 10 | loss: 0.1798163
	speed: 0.1065s/iter; left time: 12.6783s
	iters: 300, epoch: 10 | loss: 0.2313746
	speed: 0.0994s/iter; left time: 1.8885s
Epoch: 10 cost time: 33.67725849151611
Epoch: 10, Steps: 318 | Train Loss: 0.1943236 Vali Loss: 0.2105414 Test Loss: 0.1896286
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.41606760025024414, mae:0.5011006593704224, rmse:0.6450330018997192, mape:0.01792931742966175, mspe:0.0005409335717558861, rse:0.4511898458003998, r2_score:0.773657950337443, acc:0.9820706825703382
corr: [37.152443 37.03211  36.944683 36.84252  36.914818 36.805687 36.882187
 36.88376  36.872124 37.104763 37.05985  36.987473 37.060024 37.056423
 37.048023 37.166046 37.15446  37.516586 37.36841  37.68249  37.553146
 37.404232 37.44361  37.408176 37.365242]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4006467
	speed: 0.1287s/iter; left time: 395.1949s
	iters: 200, epoch: 1 | loss: 0.2726340
	speed: 0.1138s/iter; left time: 338.0814s
	iters: 300, epoch: 1 | loss: 0.2781356
	speed: 0.1639s/iter; left time: 470.6514s
Epoch: 1 cost time: 44.065566539764404
Epoch: 1, Steps: 317 | Train Loss: 0.3769362 Vali Loss: 0.2723113 Test Loss: 0.2723048
Validation loss decreased (inf --> 0.272311).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2104982
	speed: 0.1597s/iter; left time: 439.9386s
	iters: 200, epoch: 2 | loss: 0.2361993
	speed: 0.1722s/iter; left time: 457.0501s
	iters: 300, epoch: 2 | loss: 0.3190981
	speed: 0.1736s/iter; left time: 443.4639s
Epoch: 2 cost time: 53.579092502593994
Epoch: 2, Steps: 317 | Train Loss: 0.2410448 Vali Loss: 0.2628232 Test Loss: 0.2309246
Validation loss decreased (0.272311 --> 0.262823).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1898811
	speed: 0.1837s/iter; left time: 447.7648s
	iters: 200, epoch: 3 | loss: 0.2762031
	speed: 0.1830s/iter; left time: 427.5749s
	iters: 300, epoch: 3 | loss: 0.2218906
	speed: 0.1763s/iter; left time: 394.4331s
Epoch: 3 cost time: 57.27402710914612
Epoch: 3, Steps: 317 | Train Loss: 0.2228613 Vali Loss: 0.2534741 Test Loss: 0.2161600
Validation loss decreased (0.262823 --> 0.253474).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1724584
	speed: 0.1773s/iter; left time: 375.8661s
	iters: 200, epoch: 4 | loss: 0.2148813
	speed: 0.1870s/iter; left time: 377.6448s
	iters: 300, epoch: 4 | loss: 0.2023990
	speed: 0.1727s/iter; left time: 331.6467s
Epoch: 4 cost time: 56.64894342422485
Epoch: 4, Steps: 317 | Train Loss: 0.2166349 Vali Loss: 0.2527600 Test Loss: 0.2108415
Validation loss decreased (0.253474 --> 0.252760).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1998265
	speed: 0.1875s/iter; left time: 337.9930s
	iters: 200, epoch: 5 | loss: 0.2094476
	speed: 0.1703s/iter; left time: 289.9856s
	iters: 300, epoch: 5 | loss: 0.2378899
	speed: 0.1702s/iter; left time: 272.8603s
Epoch: 5 cost time: 55.84905433654785
Epoch: 5, Steps: 317 | Train Loss: 0.2134750 Vali Loss: 0.2514243 Test Loss: 0.2130228
Validation loss decreased (0.252760 --> 0.251424).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2163519
	speed: 0.1740s/iter; left time: 258.5164s
	iters: 200, epoch: 6 | loss: 0.1891500
	speed: 0.1827s/iter; left time: 253.2314s
	iters: 300, epoch: 6 | loss: 0.1850122
	speed: 0.1693s/iter; left time: 217.7665s
Epoch: 6 cost time: 55.63382840156555
Epoch: 6, Steps: 317 | Train Loss: 0.2120641 Vali Loss: 0.2491651 Test Loss: 0.2108774
Validation loss decreased (0.251424 --> 0.249165).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1710796
	speed: 0.1708s/iter; left time: 199.6689s
	iters: 200, epoch: 7 | loss: 0.2558651
	speed: 0.1772s/iter; left time: 189.4627s
	iters: 300, epoch: 7 | loss: 0.1963544
	speed: 0.1732s/iter; left time: 167.8131s
Epoch: 7 cost time: 55.107929706573486
Epoch: 7, Steps: 317 | Train Loss: 0.2111290 Vali Loss: 0.2493541 Test Loss: 0.2102090
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1798354
	speed: 0.1766s/iter; left time: 150.4717s
	iters: 200, epoch: 8 | loss: 0.2027175
	speed: 0.1552s/iter; left time: 116.7371s
	iters: 300, epoch: 8 | loss: 0.1995064
	speed: 0.1698s/iter; left time: 110.6834s
Epoch: 8 cost time: 53.046985387802124
Epoch: 8, Steps: 317 | Train Loss: 0.2108585 Vali Loss: 0.2494061 Test Loss: 0.2101147
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1936843
	speed: 0.1761s/iter; left time: 94.2233s
	iters: 200, epoch: 9 | loss: 0.1836585
	speed: 0.1639s/iter; left time: 71.2807s
	iters: 300, epoch: 9 | loss: 0.2029693
	speed: 0.1592s/iter; left time: 53.3293s
Epoch: 9 cost time: 52.49246120452881
Epoch: 9, Steps: 317 | Train Loss: 0.2107526 Vali Loss: 0.2481558 Test Loss: 0.2098990
Validation loss decreased (0.249165 --> 0.248156).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2137864
	speed: 0.1635s/iter; left time: 35.6450s
	iters: 200, epoch: 10 | loss: 0.1924201
	speed: 0.1833s/iter; left time: 21.6349s
	iters: 300, epoch: 10 | loss: 0.2229064
	speed: 0.1713s/iter; left time: 3.0830s
Epoch: 10 cost time: 54.70836901664734
Epoch: 10, Steps: 317 | Train Loss: 0.2104312 Vali Loss: 0.2482188 Test Loss: 0.2098385
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.4606151580810547, mae:0.5289231538772583, rmse:0.6786863207817078, mape:0.01895015500485897, mspe:0.0006026967312209308, rse:0.4743889570236206, r2_score:0.7510129858185587, acc:0.981049844995141
corr: [37.611702 37.455147 37.38681  37.19179  37.343002 37.405632 37.277054
 37.266247 37.348076 37.12976  37.25109  37.351254 37.323997 37.33657
 37.340904 37.130165 37.188858 37.505283 37.51941  37.44145  37.285923
 37.343407 37.44336  37.48178  37.49482  37.33556  37.211243 37.451736
 37.613808 37.606445]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.5538077
	speed: 0.1166s/iter; left time: 359.3203s
	iters: 200, epoch: 1 | loss: 0.2891427
	speed: 0.0998s/iter; left time: 297.5103s
	iters: 300, epoch: 1 | loss: 0.2133205
	speed: 0.0929s/iter; left time: 267.6963s
Epoch: 1 cost time: 32.67800688743591
Epoch: 1, Steps: 318 | Train Loss: 0.4025472 Vali Loss: 0.1887282 Test Loss: 0.2595239
Validation loss decreased (inf --> 0.188728).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1973188
	speed: 0.1089s/iter; left time: 300.9150s
	iters: 200, epoch: 2 | loss: 0.2234540
	speed: 0.0995s/iter; left time: 264.9648s
	iters: 300, epoch: 2 | loss: 0.2195677
	speed: 0.1065s/iter; left time: 272.8577s
Epoch: 2 cost time: 32.350192070007324
Epoch: 2, Steps: 318 | Train Loss: 0.1977649 Vali Loss: 0.1649988 Test Loss: 0.2357323
Validation loss decreased (0.188728 --> 0.164999).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2003008
	speed: 0.1113s/iter; left time: 272.1392s
	iters: 200, epoch: 3 | loss: 0.1434882
	speed: 0.0978s/iter; left time: 229.3396s
	iters: 300, epoch: 3 | loss: 0.1891868
	speed: 0.0996s/iter; left time: 223.5079s
Epoch: 3 cost time: 32.675372838974
Epoch: 3, Steps: 318 | Train Loss: 0.1785672 Vali Loss: 0.1564560 Test Loss: 0.2247911
Validation loss decreased (0.164999 --> 0.156456).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1893932
	speed: 0.0928s/iter; left time: 197.4155s
	iters: 200, epoch: 4 | loss: 0.1556105
	speed: 0.1052s/iter; left time: 213.2542s
	iters: 300, epoch: 4 | loss: 0.1884568
	speed: 0.1074s/iter; left time: 206.9584s
Epoch: 4 cost time: 32.26624131202698
Epoch: 4, Steps: 318 | Train Loss: 0.1710416 Vali Loss: 0.1550246 Test Loss: 0.2334275
Validation loss decreased (0.156456 --> 0.155025).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1718735
	speed: 0.1137s/iter; left time: 205.6922s
	iters: 200, epoch: 5 | loss: 0.1870353
	speed: 0.1076s/iter; left time: 183.8178s
	iters: 300, epoch: 5 | loss: 0.1315580
	speed: 0.1046s/iter; left time: 168.2772s
Epoch: 5 cost time: 34.02224588394165
Epoch: 5, Steps: 318 | Train Loss: 0.1684304 Vali Loss: 0.1539176 Test Loss: 0.2310700
Validation loss decreased (0.155025 --> 0.153918).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1352346
	speed: 0.1039s/iter; left time: 154.8892s
	iters: 200, epoch: 6 | loss: 0.1308412
	speed: 0.1025s/iter; left time: 142.5434s
	iters: 300, epoch: 6 | loss: 0.1602820
	speed: 0.1004s/iter; left time: 129.5637s
Epoch: 6 cost time: 32.59816813468933
Epoch: 6, Steps: 318 | Train Loss: 0.1661456 Vali Loss: 0.1538777 Test Loss: 0.2291965
Validation loss decreased (0.153918 --> 0.153878).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1512881
	speed: 0.0990s/iter; left time: 116.1500s
	iters: 200, epoch: 7 | loss: 0.1544729
	speed: 0.1070s/iter; left time: 114.8229s
	iters: 300, epoch: 7 | loss: 0.1615974
	speed: 0.0922s/iter; left time: 89.7134s
Epoch: 7 cost time: 31.62325882911682
Epoch: 7, Steps: 318 | Train Loss: 0.1654738 Vali Loss: 0.1525084 Test Loss: 0.2293907
Validation loss decreased (0.153878 --> 0.152508).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1640743
	speed: 0.0910s/iter; left time: 77.8142s
	iters: 200, epoch: 8 | loss: 0.1472477
	speed: 0.0962s/iter; left time: 72.6652s
	iters: 300, epoch: 8 | loss: 0.1332201
	speed: 0.0954s/iter; left time: 62.5130s
Epoch: 8 cost time: 29.06519341468811
Epoch: 8, Steps: 318 | Train Loss: 0.1657563 Vali Loss: 0.1529915 Test Loss: 0.2292888
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1583894
	speed: 0.0972s/iter; left time: 52.1779s
	iters: 200, epoch: 9 | loss: 0.1625469
	speed: 0.1082s/iter; left time: 47.2816s
	iters: 300, epoch: 9 | loss: 0.1137879
	speed: 0.1015s/iter; left time: 34.2118s
Epoch: 9 cost time: 32.58230423927307
Epoch: 9, Steps: 318 | Train Loss: 0.1654001 Vali Loss: 0.1521749 Test Loss: 0.2299337
Validation loss decreased (0.152508 --> 0.152175).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1966085
	speed: 0.1196s/iter; left time: 26.1970s
	iters: 200, epoch: 10 | loss: 0.1629454
	speed: 0.1037s/iter; left time: 12.3381s
	iters: 300, epoch: 10 | loss: 0.1658547
	speed: 0.0975s/iter; left time: 1.8522s
Epoch: 10 cost time: 33.91987442970276
Epoch: 10, Steps: 318 | Train Loss: 0.1653427 Vali Loss: 0.1519596 Test Loss: 0.2298544
Validation loss decreased (0.152175 --> 0.151960).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.5044063329696655, mae:0.539758563041687, rmse:0.71021568775177, mape:0.019214889034628868, mspe:0.0006382277351804078, rse:0.4976385235786438, r2_score:0.7221565147492488, acc:0.9807851109653711
corr: [35.762684 35.907265 35.99524  36.30028  36.5386   36.43423  36.410767
 36.686584 36.836426 37.00859 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2739184
	speed: 0.1065s/iter; left time: 327.9767s
	iters: 200, epoch: 1 | loss: 0.3305602
	speed: 0.0710s/iter; left time: 211.7317s
	iters: 300, epoch: 1 | loss: 0.2481900
	speed: 0.0699s/iter; left time: 201.3285s
Epoch: 1 cost time: 25.884622812271118
Epoch: 1, Steps: 318 | Train Loss: 0.3608136 Vali Loss: 0.2359829 Test Loss: 0.2318355
Validation loss decreased (inf --> 0.235983).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1670287
	speed: 0.0839s/iter; left time: 231.7657s
	iters: 200, epoch: 2 | loss: 0.2656791
	speed: 0.0741s/iter; left time: 197.3037s
	iters: 300, epoch: 2 | loss: 0.2572584
	speed: 0.0713s/iter; left time: 182.8402s
Epoch: 2 cost time: 24.007052421569824
Epoch: 2, Steps: 318 | Train Loss: 0.2102922 Vali Loss: 0.2028153 Test Loss: 0.2053969
Validation loss decreased (0.235983 --> 0.202815).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1859454
	speed: 0.0825s/iter; left time: 201.6492s
	iters: 200, epoch: 3 | loss: 0.1727809
	speed: 0.0618s/iter; left time: 144.9090s
	iters: 300, epoch: 3 | loss: 0.1833705
	speed: 0.0727s/iter; left time: 163.2777s
Epoch: 3 cost time: 23.31393337249756
Epoch: 3, Steps: 318 | Train Loss: 0.1919211 Vali Loss: 0.2009908 Test Loss: 0.2045161
Validation loss decreased (0.202815 --> 0.200991).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1477637
	speed: 0.0824s/iter; left time: 175.3258s
	iters: 200, epoch: 4 | loss: 0.2030185
	speed: 0.0605s/iter; left time: 122.5456s
	iters: 300, epoch: 4 | loss: 0.1485350
	speed: 0.0632s/iter; left time: 121.7548s
Epoch: 4 cost time: 22.390666246414185
Epoch: 4, Steps: 318 | Train Loss: 0.1855396 Vali Loss: 0.1963680 Test Loss: 0.2009160
Validation loss decreased (0.200991 --> 0.196368).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2255123
	speed: 0.0846s/iter; left time: 153.0636s
	iters: 200, epoch: 5 | loss: 0.1801372
	speed: 0.0662s/iter; left time: 113.1868s
	iters: 300, epoch: 5 | loss: 0.1511842
	speed: 0.0634s/iter; left time: 102.0885s
Epoch: 5 cost time: 23.013490915298462
Epoch: 5, Steps: 318 | Train Loss: 0.1826753 Vali Loss: 0.1954382 Test Loss: 0.1994718
Validation loss decreased (0.196368 --> 0.195438).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1851238
	speed: 0.0813s/iter; left time: 121.2585s
	iters: 200, epoch: 6 | loss: 0.2118818
	speed: 0.0733s/iter; left time: 101.9939s
	iters: 300, epoch: 6 | loss: 0.1651850
	speed: 0.0655s/iter; left time: 84.5106s
Epoch: 6 cost time: 23.480942726135254
Epoch: 6, Steps: 318 | Train Loss: 0.1814999 Vali Loss: 0.1959562 Test Loss: 0.1991142
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1698033
	speed: 0.0713s/iter; left time: 83.6700s
	iters: 200, epoch: 7 | loss: 0.1475407
	speed: 0.0770s/iter; left time: 82.6194s
	iters: 300, epoch: 7 | loss: 0.1763415
	speed: 0.0481s/iter; left time: 46.8382s
Epoch: 7 cost time: 21.043307065963745
Epoch: 7, Steps: 318 | Train Loss: 0.1812345 Vali Loss: 0.1950860 Test Loss: 0.2000416
Validation loss decreased (0.195438 --> 0.195086).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1234598
	speed: 0.0794s/iter; left time: 67.9019s
	iters: 200, epoch: 8 | loss: 0.1293450
	speed: 0.0664s/iter; left time: 50.1072s
	iters: 300, epoch: 8 | loss: 0.1876203
	speed: 0.0828s/iter; left time: 54.2293s
Epoch: 8 cost time: 24.174590349197388
Epoch: 8, Steps: 318 | Train Loss: 0.1802111 Vali Loss: 0.1965284 Test Loss: 0.1993596
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1533957
	speed: 0.0653s/iter; left time: 35.0875s
	iters: 200, epoch: 9 | loss: 0.1523002
	speed: 0.0851s/iter; left time: 37.1707s
	iters: 300, epoch: 9 | loss: 0.1315893
	speed: 0.0641s/iter; left time: 21.6184s
Epoch: 9 cost time: 23.325512886047363
Epoch: 9, Steps: 318 | Train Loss: 0.1804342 Vali Loss: 0.1958244 Test Loss: 0.1994085
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2174651
	speed: 0.0811s/iter; left time: 17.7640s
	iters: 200, epoch: 10 | loss: 0.1758896
	speed: 0.0653s/iter; left time: 7.7691s
	iters: 300, epoch: 10 | loss: 0.1544418
	speed: 0.0739s/iter; left time: 1.4044s
Epoch: 10 cost time: 23.66980218887329
Epoch: 10, Steps: 318 | Train Loss: 0.1804878 Vali Loss: 0.1941723 Test Loss: 0.1992213
Validation loss decreased (0.195086 --> 0.194172).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.43718329071998596, mae:0.5169455409049988, rmse:0.6611983776092529, mape:0.01845777966082096, mspe:0.0005627520731650293, rse:0.46307459473609924, r2_score:0.7592451229490786, acc:0.981542220339179
corr: [36.621613 36.871605 37.079384 36.93828  36.310165 36.748814 36.852196
 36.803284 37.15709  36.650978 36.69808  36.961502 37.002193 37.07326
 36.986588]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4144524
	speed: 0.0982s/iter; left time: 302.7069s
	iters: 200, epoch: 1 | loss: 0.2176591
	speed: 0.1201s/iter; left time: 357.9565s
	iters: 300, epoch: 1 | loss: 0.1867225
	speed: 0.1047s/iter; left time: 301.5813s
Epoch: 1 cost time: 34.05549216270447
Epoch: 1, Steps: 318 | Train Loss: 0.3550155 Vali Loss: 0.2170787 Test Loss: 0.2398463
Validation loss decreased (inf --> 0.217079).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2121570
	speed: 0.1127s/iter; left time: 311.4296s
	iters: 200, epoch: 2 | loss: 0.2199702
	speed: 0.1032s/iter; left time: 274.7035s
	iters: 300, epoch: 2 | loss: 0.2453909
	speed: 0.1040s/iter; left time: 266.6713s
Epoch: 2 cost time: 33.89346170425415
Epoch: 2, Steps: 318 | Train Loss: 0.2142097 Vali Loss: 0.2124205 Test Loss: 0.2228245
Validation loss decreased (0.217079 --> 0.212421).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1651137
	speed: 0.1097s/iter; left time: 268.1668s
	iters: 200, epoch: 3 | loss: 0.2535366
	speed: 0.0996s/iter; left time: 233.5668s
	iters: 300, epoch: 3 | loss: 0.1680896
	speed: 0.1025s/iter; left time: 230.0643s
Epoch: 3 cost time: 32.96076202392578
Epoch: 3, Steps: 318 | Train Loss: 0.1983269 Vali Loss: 0.2017749 Test Loss: 0.2176872
Validation loss decreased (0.212421 --> 0.201775).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1581797
	speed: 0.1235s/iter; left time: 262.5945s
	iters: 200, epoch: 4 | loss: 0.2121099
	speed: 0.1002s/iter; left time: 203.0370s
	iters: 300, epoch: 4 | loss: 0.1434905
	speed: 0.1088s/iter; left time: 209.7108s
Epoch: 4 cost time: 35.13511633872986
Epoch: 4, Steps: 318 | Train Loss: 0.1927088 Vali Loss: 0.2052363 Test Loss: 0.2179640
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2136444
	speed: 0.1179s/iter; left time: 213.2143s
	iters: 200, epoch: 5 | loss: 0.1841663
	speed: 0.0826s/iter; left time: 141.0837s
	iters: 300, epoch: 5 | loss: 0.1803551
	speed: 0.1063s/iter; left time: 171.0086s
Epoch: 5 cost time: 32.22624087333679
Epoch: 5, Steps: 318 | Train Loss: 0.1903657 Vali Loss: 0.2004452 Test Loss: 0.2116938
Validation loss decreased (0.201775 --> 0.200445).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1745328
	speed: 0.1063s/iter; left time: 158.4392s
	iters: 200, epoch: 6 | loss: 0.1844519
	speed: 0.0892s/iter; left time: 124.1323s
	iters: 300, epoch: 6 | loss: 0.2314208
	speed: 0.1138s/iter; left time: 146.9335s
Epoch: 6 cost time: 32.828468561172485
Epoch: 6, Steps: 318 | Train Loss: 0.1892687 Vali Loss: 0.2005839 Test Loss: 0.2132954
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1858291
	speed: 0.1115s/iter; left time: 130.8335s
	iters: 200, epoch: 7 | loss: 0.1874452
	speed: 0.0873s/iter; left time: 93.6828s
	iters: 300, epoch: 7 | loss: 0.1964313
	speed: 0.1022s/iter; left time: 99.4743s
Epoch: 7 cost time: 32.2207190990448
Epoch: 7, Steps: 318 | Train Loss: 0.1888049 Vali Loss: 0.2003524 Test Loss: 0.2115397
Validation loss decreased (0.200445 --> 0.200352).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2192520
	speed: 0.1113s/iter; left time: 95.1529s
	iters: 200, epoch: 8 | loss: 0.2079878
	speed: 0.0707s/iter; left time: 53.3564s
	iters: 300, epoch: 8 | loss: 0.1874783
	speed: 0.1043s/iter; left time: 68.2871s
Epoch: 8 cost time: 30.56080937385559
Epoch: 8, Steps: 318 | Train Loss: 0.1884473 Vali Loss: 0.2004137 Test Loss: 0.2121917
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2105541
	speed: 0.1163s/iter; left time: 62.4511s
	iters: 200, epoch: 9 | loss: 0.2507183
	speed: 0.1032s/iter; left time: 45.1155s
	iters: 300, epoch: 9 | loss: 0.2306418
	speed: 0.1060s/iter; left time: 35.7074s
Epoch: 9 cost time: 34.12644147872925
Epoch: 9, Steps: 318 | Train Loss: 0.1878974 Vali Loss: 0.1999781 Test Loss: 0.2122869
Validation loss decreased (0.200352 --> 0.199978).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1946442
	speed: 0.1069s/iter; left time: 23.4216s
	iters: 200, epoch: 10 | loss: 0.2191140
	speed: 0.0869s/iter; left time: 10.3380s
	iters: 300, epoch: 10 | loss: 0.1875307
	speed: 0.1026s/iter; left time: 1.9501s
Epoch: 10 cost time: 31.026748657226562
Epoch: 10, Steps: 318 | Train Loss: 0.1881746 Vali Loss: 0.2005329 Test Loss: 0.2123392
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.4658552408218384, mae:0.5270373821258545, rmse:0.6825358867645264, mape:0.01884044148027897, mspe:0.0006018716376274824, rse:0.47774162888526917, r2_score:0.7553917742278393, acc:0.981159558519721
corr: [36.484795 36.53432  36.83051  36.61744  36.587677 36.99017  37.084106
 37.073837 37.369556 37.174282 37.320244 37.475643 37.381435 37.434483
 37.33491  37.418682 37.369987 37.612827 37.885197 37.346836]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3339846
	speed: 0.0976s/iter; left time: 300.5724s
	iters: 200, epoch: 1 | loss: 0.2909932
	speed: 0.0796s/iter; left time: 237.3256s
	iters: 300, epoch: 1 | loss: 0.2710447
	speed: 0.0693s/iter; left time: 199.5552s
Epoch: 1 cost time: 26.110432147979736
Epoch: 1, Steps: 318 | Train Loss: 0.4088308 Vali Loss: 0.2683815 Test Loss: 0.2602300
Validation loss decreased (inf --> 0.268381).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2684193
	speed: 0.0781s/iter; left time: 215.7439s
	iters: 200, epoch: 2 | loss: 0.2119768
	speed: 0.0820s/iter; left time: 218.2395s
	iters: 300, epoch: 2 | loss: 0.2879780
	speed: 0.0786s/iter; left time: 201.4638s
Epoch: 2 cost time: 25.302276134490967
Epoch: 2, Steps: 318 | Train Loss: 0.2670750 Vali Loss: 0.2492242 Test Loss: 0.2411926
Validation loss decreased (0.268381 --> 0.249224).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2872335
	speed: 0.0859s/iter; left time: 210.0155s
	iters: 200, epoch: 3 | loss: 0.1715101
	speed: 0.0756s/iter; left time: 177.1974s
	iters: 300, epoch: 3 | loss: 0.2900049
	speed: 0.0812s/iter; left time: 182.2302s
Epoch: 3 cost time: 25.748807907104492
Epoch: 3, Steps: 318 | Train Loss: 0.2452285 Vali Loss: 0.2390623 Test Loss: 0.2270367
Validation loss decreased (0.249224 --> 0.239062).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2121212
	speed: 0.0877s/iter; left time: 186.5372s
	iters: 200, epoch: 4 | loss: 0.1909970
	speed: 0.0799s/iter; left time: 161.9926s
	iters: 300, epoch: 4 | loss: 0.2295652
	speed: 0.0843s/iter; left time: 162.4627s
Epoch: 4 cost time: 26.81200361251831
Epoch: 4, Steps: 318 | Train Loss: 0.2355562 Vali Loss: 0.2363326 Test Loss: 0.2252492
Validation loss decreased (0.239062 --> 0.236333).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2293870
	speed: 0.0832s/iter; left time: 150.5944s
	iters: 200, epoch: 5 | loss: 0.2421732
	speed: 0.0843s/iter; left time: 144.1073s
	iters: 300, epoch: 5 | loss: 0.2529432
	speed: 0.0758s/iter; left time: 121.8863s
Epoch: 5 cost time: 25.459141731262207
Epoch: 5, Steps: 318 | Train Loss: 0.2316664 Vali Loss: 0.2340135 Test Loss: 0.2212747
Validation loss decreased (0.236333 --> 0.234013).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2260747
	speed: 0.0928s/iter; left time: 138.3459s
	iters: 200, epoch: 6 | loss: 0.1941747
	speed: 0.0628s/iter; left time: 87.2994s
	iters: 300, epoch: 6 | loss: 0.2250217
	speed: 0.0852s/iter; left time: 110.0339s
Epoch: 6 cost time: 25.351792335510254
Epoch: 6, Steps: 318 | Train Loss: 0.2299035 Vali Loss: 0.2348135 Test Loss: 0.2210616
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1842924
	speed: 0.0909s/iter; left time: 106.6675s
	iters: 200, epoch: 7 | loss: 0.2015663
	speed: 0.0842s/iter; left time: 90.3492s
	iters: 300, epoch: 7 | loss: 0.2200653
	speed: 0.0774s/iter; left time: 75.2740s
Epoch: 7 cost time: 26.451854467391968
Epoch: 7, Steps: 318 | Train Loss: 0.2291953 Vali Loss: 0.2345105 Test Loss: 0.2209578
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2559212
	speed: 0.0922s/iter; left time: 78.8156s
	iters: 200, epoch: 8 | loss: 0.2589029
	speed: 0.0766s/iter; left time: 57.8690s
	iters: 300, epoch: 8 | loss: 0.2386306
	speed: 0.0766s/iter; left time: 50.1411s
Epoch: 8 cost time: 25.621979236602783
Epoch: 8, Steps: 318 | Train Loss: 0.2284780 Vali Loss: 0.2337771 Test Loss: 0.2202638
Validation loss decreased (0.234013 --> 0.233777).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1879373
	speed: 0.0935s/iter; left time: 50.2178s
	iters: 200, epoch: 9 | loss: 0.2615721
	speed: 0.0806s/iter; left time: 35.2407s
	iters: 300, epoch: 9 | loss: 0.2279945
	speed: 0.0819s/iter; left time: 27.5938s
Epoch: 9 cost time: 26.95916771888733
Epoch: 9, Steps: 318 | Train Loss: 0.2280004 Vali Loss: 0.2329729 Test Loss: 0.2201428
Validation loss decreased (0.233777 --> 0.232973).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2088466
	speed: 0.0711s/iter; left time: 15.5659s
	iters: 200, epoch: 10 | loss: 0.1911737
	speed: 0.0776s/iter; left time: 9.2365s
	iters: 300, epoch: 10 | loss: 0.2094440
	speed: 0.0727s/iter; left time: 1.3810s
Epoch: 10 cost time: 23.68550992012024
Epoch: 10, Steps: 318 | Train Loss: 0.2281460 Vali Loss: 0.2343879 Test Loss: 0.2200241
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.48309463262557983, mae:0.5512394309043884, rmse:0.6950501203536987, mape:0.01971849426627159, mspe:0.0006269921432249248, rse:0.4861759543418884, r2_score:0.7291767227041719, acc:0.9802815057337284
corr: [36.962677 37.23646  36.985973 37.132294 37.39091  37.295338 37.402702
 37.617867 37.54382  37.94872  38.035877 38.000248 37.989025 37.90812
 38.013058 37.98453  38.04118  38.00799  38.068527 38.03017  38.406525
 38.470703 38.38218  38.58185  38.520523]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4118816
	speed: 0.1203s/iter; left time: 369.5902s
	iters: 200, epoch: 1 | loss: 0.2413858
	speed: 0.1057s/iter; left time: 314.1284s
	iters: 300, epoch: 1 | loss: 0.2454303
	speed: 0.1018s/iter; left time: 292.1544s
Epoch: 1 cost time: 33.969886779785156
Epoch: 1, Steps: 317 | Train Loss: 0.3551594 Vali Loss: 0.2506277 Test Loss: 0.2475248
Validation loss decreased (inf --> 0.250628).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2089033
	speed: 0.1174s/iter; left time: 323.2821s
	iters: 200, epoch: 2 | loss: 0.2234653
	speed: 0.0982s/iter; left time: 260.7235s
	iters: 300, epoch: 2 | loss: 0.1823054
	speed: 0.0968s/iter; left time: 247.3188s
Epoch: 2 cost time: 32.891437292099
Epoch: 2, Steps: 317 | Train Loss: 0.2215794 Vali Loss: 0.2334977 Test Loss: 0.2327840
Validation loss decreased (0.250628 --> 0.233498).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2197358
	speed: 0.1151s/iter; left time: 280.3874s
	iters: 200, epoch: 3 | loss: 0.1565951
	speed: 0.0933s/iter; left time: 218.0874s
	iters: 300, epoch: 3 | loss: 0.1890223
	speed: 0.1108s/iter; left time: 247.8484s
Epoch: 3 cost time: 33.73658323287964
Epoch: 3, Steps: 317 | Train Loss: 0.2069939 Vali Loss: 0.2345176 Test Loss: 0.2354211
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2431951
	speed: 0.0973s/iter; left time: 206.3513s
	iters: 200, epoch: 4 | loss: 0.1984144
	speed: 0.0916s/iter; left time: 184.9978s
	iters: 300, epoch: 4 | loss: 0.2151837
	speed: 0.1012s/iter; left time: 194.3676s
Epoch: 4 cost time: 30.565489053726196
Epoch: 4, Steps: 317 | Train Loss: 0.2020389 Vali Loss: 0.2317598 Test Loss: 0.2329848
Validation loss decreased (0.233498 --> 0.231760).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2419799
	speed: 0.1103s/iter; left time: 198.8400s
	iters: 200, epoch: 5 | loss: 0.2018259
	speed: 0.0980s/iter; left time: 166.9051s
	iters: 300, epoch: 5 | loss: 0.2073973
	speed: 0.1062s/iter; left time: 170.2920s
Epoch: 5 cost time: 33.29740381240845
Epoch: 5, Steps: 317 | Train Loss: 0.1994281 Vali Loss: 0.2328630 Test Loss: 0.2312648
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2310396
	speed: 0.1230s/iter; left time: 182.7478s
	iters: 200, epoch: 6 | loss: 0.1863388
	speed: 0.0622s/iter; left time: 86.2635s
	iters: 300, epoch: 6 | loss: 0.2074002
	speed: 0.1135s/iter; left time: 145.9937s
Epoch: 6 cost time: 31.372659921646118
Epoch: 6, Steps: 317 | Train Loss: 0.1986339 Vali Loss: 0.2316111 Test Loss: 0.2304142
Validation loss decreased (0.231760 --> 0.231611).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1529517
	speed: 0.1050s/iter; left time: 122.7259s
	iters: 200, epoch: 7 | loss: 0.2456099
	speed: 0.1075s/iter; left time: 114.8919s
	iters: 300, epoch: 7 | loss: 0.2137069
	speed: 0.0999s/iter; left time: 96.7823s
Epoch: 7 cost time: 33.16185784339905
Epoch: 7, Steps: 317 | Train Loss: 0.1979326 Vali Loss: 0.2323773 Test Loss: 0.2303658
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1854151
	speed: 0.1139s/iter; left time: 97.0509s
	iters: 200, epoch: 8 | loss: 0.1802590
	speed: 0.0996s/iter; left time: 74.8882s
	iters: 300, epoch: 8 | loss: 0.1665023
	speed: 0.1084s/iter; left time: 70.7010s
Epoch: 8 cost time: 34.07703256607056
Epoch: 8, Steps: 317 | Train Loss: 0.1973478 Vali Loss: 0.2328543 Test Loss: 0.2302199
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2008902
	speed: 0.1094s/iter; left time: 58.5344s
	iters: 200, epoch: 9 | loss: 0.1842009
	speed: 0.1087s/iter; left time: 47.2777s
	iters: 300, epoch: 9 | loss: 0.2200683
	speed: 0.1019s/iter; left time: 34.1531s
Epoch: 9 cost time: 33.355868101119995
Epoch: 9, Steps: 317 | Train Loss: 0.1975127 Vali Loss: 0.2326996 Test Loss: 0.2303193
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.5056350231170654, mae:0.550028920173645, rmse:0.7110801935195923, mape:0.019682995975017548, mspe:0.0006576816085726023, rse:0.4970315992832184, r2_score:0.7179669092970193, acc:0.9803170040249825
corr: [36.365856 36.279644 36.498627 36.25675  36.003918 36.477264 36.44682
 36.333473 36.633167 36.31429  36.39146  36.308704 36.480865 36.77306
 36.611942 36.729927 36.480843 36.916298 36.9227   36.883507 36.553875
 36.908596 36.60632  36.68182  36.77216  36.879906 36.865967 36.621597
 36.730587 36.82665 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3920234
	speed: 0.1147s/iter; left time: 353.3955s
	iters: 200, epoch: 1 | loss: 0.1923025
	speed: 0.0852s/iter; left time: 254.0068s
	iters: 300, epoch: 1 | loss: 0.1801567
	speed: 0.0769s/iter; left time: 221.5257s
Epoch: 1 cost time: 29.12151861190796
Epoch: 1, Steps: 318 | Train Loss: 0.2876153 Vali Loss: 0.1916621 Test Loss: 0.1840800
Validation loss decreased (inf --> 0.191662).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1818892
	speed: 0.0906s/iter; left time: 250.3298s
	iters: 200, epoch: 2 | loss: 0.1741220
	speed: 0.0759s/iter; left time: 202.1679s
	iters: 300, epoch: 2 | loss: 0.1724973
	speed: 0.0773s/iter; left time: 198.2280s
Epoch: 2 cost time: 25.833775281906128
Epoch: 2, Steps: 318 | Train Loss: 0.1669533 Vali Loss: 0.1513947 Test Loss: 0.1409024
Validation loss decreased (0.191662 --> 0.151395).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1886371
	speed: 0.0935s/iter; left time: 228.6137s
	iters: 200, epoch: 3 | loss: 0.1274158
	speed: 0.0765s/iter; left time: 179.3511s
	iters: 300, epoch: 3 | loss: 0.1721492
	speed: 0.0794s/iter; left time: 178.1516s
Epoch: 3 cost time: 26.379798889160156
Epoch: 3, Steps: 318 | Train Loss: 0.1526839 Vali Loss: 0.1435313 Test Loss: 0.1345412
Validation loss decreased (0.151395 --> 0.143531).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1553363
	speed: 0.0906s/iter; left time: 192.8068s
	iters: 200, epoch: 4 | loss: 0.1304596
	speed: 0.0755s/iter; left time: 153.0619s
	iters: 300, epoch: 4 | loss: 0.1469398
	speed: 0.0762s/iter; left time: 146.8299s
Epoch: 4 cost time: 25.608076572418213
Epoch: 4, Steps: 318 | Train Loss: 0.1465877 Vali Loss: 0.1450085 Test Loss: 0.1364460
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1452831
	speed: 0.0901s/iter; left time: 162.9788s
	iters: 200, epoch: 5 | loss: 0.1864011
	speed: 0.0792s/iter; left time: 135.4017s
	iters: 300, epoch: 5 | loss: 0.1358174
	speed: 0.0793s/iter; left time: 127.5828s
Epoch: 5 cost time: 26.39453125
Epoch: 5, Steps: 318 | Train Loss: 0.1441186 Vali Loss: 0.1439278 Test Loss: 0.1343890
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1214293
	speed: 0.0880s/iter; left time: 131.1993s
	iters: 200, epoch: 6 | loss: 0.1152343
	speed: 0.0764s/iter; left time: 106.3386s
	iters: 300, epoch: 6 | loss: 0.1386133
	speed: 0.0755s/iter; left time: 97.4820s
Epoch: 6 cost time: 25.497934341430664
Epoch: 6, Steps: 318 | Train Loss: 0.1419315 Vali Loss: 0.1435267 Test Loss: 0.1328772
Validation loss decreased (0.143531 --> 0.143527).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1437703
	speed: 0.0881s/iter; left time: 103.3046s
	iters: 200, epoch: 7 | loss: 0.1319775
	speed: 0.0780s/iter; left time: 83.7125s
	iters: 300, epoch: 7 | loss: 0.1461588
	speed: 0.0811s/iter; left time: 78.9450s
Epoch: 7 cost time: 26.060285568237305
Epoch: 7, Steps: 318 | Train Loss: 0.1410479 Vali Loss: 0.1428426 Test Loss: 0.1333316
Validation loss decreased (0.143527 --> 0.142843).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1387004
	speed: 0.0939s/iter; left time: 80.3008s
	iters: 200, epoch: 8 | loss: 0.1361373
	speed: 0.0813s/iter; left time: 61.4141s
	iters: 300, epoch: 8 | loss: 0.1165975
	speed: 0.0753s/iter; left time: 49.2948s
Epoch: 8 cost time: 26.461408376693726
Epoch: 8, Steps: 318 | Train Loss: 0.1411619 Vali Loss: 0.1432337 Test Loss: 0.1330707
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1377559
	speed: 0.0866s/iter; left time: 46.4820s
	iters: 200, epoch: 9 | loss: 0.1494405
	speed: 0.0763s/iter; left time: 33.3607s
	iters: 300, epoch: 9 | loss: 0.0958916
	speed: 0.0784s/iter; left time: 26.4272s
Epoch: 9 cost time: 25.546408891677856
Epoch: 9, Steps: 318 | Train Loss: 0.1409114 Vali Loss: 0.1425200 Test Loss: 0.1332115
Validation loss decreased (0.142843 --> 0.142520).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1594674
	speed: 0.0933s/iter; left time: 20.4384s
	iters: 200, epoch: 10 | loss: 0.1226452
	speed: 0.0853s/iter; left time: 10.1476s
	iters: 300, epoch: 10 | loss: 0.1562839
	speed: 0.0734s/iter; left time: 1.3947s
Epoch: 10 cost time: 26.468685626983643
Epoch: 10, Steps: 318 | Train Loss: 0.1411143 Vali Loss: 0.1421781 Test Loss: 0.1331517
Validation loss decreased (0.142520 --> 0.142178).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.2921960949897766, mae:0.4169664978981018, rmse:0.5405516624450684, mape:0.014912630431354046, mspe:0.00037856202106922865, rse:0.3787572383880615, r2_score:0.8437026618854457, acc:0.985087369568646
corr: [36.706745 36.934223 36.593327 36.736427 36.95448  36.805786 36.750546
 36.885162 37.18207  36.945274]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2775798
	speed: 0.1109s/iter; left time: 341.6315s
	iters: 200, epoch: 1 | loss: 0.3340455
	speed: 0.0896s/iter; left time: 266.9697s
	iters: 300, epoch: 1 | loss: 0.2606485
	speed: 0.0786s/iter; left time: 226.3406s
Epoch: 1 cost time: 29.71993350982666
Epoch: 1, Steps: 318 | Train Loss: 0.3612174 Vali Loss: 0.2550821 Test Loss: 0.2378145
Validation loss decreased (inf --> 0.255082).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1876476
	speed: 0.0965s/iter; left time: 266.5165s
	iters: 200, epoch: 2 | loss: 0.2558771
	speed: 0.0979s/iter; left time: 260.5816s
	iters: 300, epoch: 2 | loss: 0.2550189
	speed: 0.0999s/iter; left time: 256.1112s
Epoch: 2 cost time: 31.099230527877808
Epoch: 2, Steps: 318 | Train Loss: 0.2302907 Vali Loss: 0.2106749 Test Loss: 0.1966444
Validation loss decreased (0.255082 --> 0.210675).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1956633
	speed: 0.1056s/iter; left time: 258.1155s
	iters: 200, epoch: 3 | loss: 0.1881997
	speed: 0.0912s/iter; left time: 213.9498s
	iters: 300, epoch: 3 | loss: 0.2108894
	speed: 0.0896s/iter; left time: 201.1467s
Epoch: 3 cost time: 30.2771635055542
Epoch: 3, Steps: 318 | Train Loss: 0.2076471 Vali Loss: 0.2007567 Test Loss: 0.1885117
Validation loss decreased (0.210675 --> 0.200757).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1717161
	speed: 0.0984s/iter; left time: 209.2990s
	iters: 200, epoch: 4 | loss: 0.2395796
	speed: 0.0984s/iter; left time: 199.4767s
	iters: 300, epoch: 4 | loss: 0.1748195
	speed: 0.0981s/iter; left time: 188.9838s
Epoch: 4 cost time: 31.174664735794067
Epoch: 4, Steps: 318 | Train Loss: 0.1986152 Vali Loss: 0.1963003 Test Loss: 0.1822947
Validation loss decreased (0.200757 --> 0.196300).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2098919
	speed: 0.1131s/iter; left time: 204.5839s
	iters: 200, epoch: 5 | loss: 0.2041272
	speed: 0.0975s/iter; left time: 166.5769s
	iters: 300, epoch: 5 | loss: 0.1699990
	speed: 0.0917s/iter; left time: 147.5271s
Epoch: 5 cost time: 32.198846101760864
Epoch: 5, Steps: 318 | Train Loss: 0.1943420 Vali Loss: 0.1936890 Test Loss: 0.1800399
Validation loss decreased (0.196300 --> 0.193689).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1912208
	speed: 0.1107s/iter; left time: 165.0787s
	iters: 200, epoch: 6 | loss: 0.2162434
	speed: 0.0925s/iter; left time: 128.7211s
	iters: 300, epoch: 6 | loss: 0.1564524
	speed: 0.0982s/iter; left time: 126.7209s
Epoch: 6 cost time: 31.948673963546753
Epoch: 6, Steps: 318 | Train Loss: 0.1923554 Vali Loss: 0.1937616 Test Loss: 0.1793012
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2045619
	speed: 0.1019s/iter; left time: 119.5864s
	iters: 200, epoch: 7 | loss: 0.1565703
	speed: 0.0832s/iter; left time: 89.2752s
	iters: 300, epoch: 7 | loss: 0.1971929
	speed: 0.0949s/iter; left time: 92.3568s
Epoch: 7 cost time: 29.423158645629883
Epoch: 7, Steps: 318 | Train Loss: 0.1914008 Vali Loss: 0.1927643 Test Loss: 0.1794025
Validation loss decreased (0.193689 --> 0.192764).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1426509
	speed: 0.1111s/iter; left time: 94.9525s
	iters: 200, epoch: 8 | loss: 0.1322268
	speed: 0.0973s/iter; left time: 73.4553s
	iters: 300, epoch: 8 | loss: 0.2177528
	speed: 0.0982s/iter; left time: 64.3111s
Epoch: 8 cost time: 32.21650576591492
Epoch: 8, Steps: 318 | Train Loss: 0.1904969 Vali Loss: 0.1940115 Test Loss: 0.1789956
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1653064
	speed: 0.1138s/iter; left time: 61.1357s
	iters: 200, epoch: 9 | loss: 0.1507450
	speed: 0.0915s/iter; left time: 39.9947s
	iters: 300, epoch: 9 | loss: 0.1481415
	speed: 0.0934s/iter; left time: 31.4748s
Epoch: 9 cost time: 31.5165913105011
Epoch: 9, Steps: 318 | Train Loss: 0.1906632 Vali Loss: 0.1938908 Test Loss: 0.1788223
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2344522
	speed: 0.1006s/iter; left time: 22.0230s
	iters: 200, epoch: 10 | loss: 0.1831218
	speed: 0.0801s/iter; left time: 9.5376s
	iters: 300, epoch: 10 | loss: 0.1654030
	speed: 0.1000s/iter; left time: 1.9005s
Epoch: 10 cost time: 29.78903317451477
Epoch: 10, Steps: 318 | Train Loss: 0.1905853 Vali Loss: 0.1913853 Test Loss: 0.1786380
Validation loss decreased (0.192764 --> 0.191385).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.3920139968395233, mae:0.49209824204444885, rmse:0.6261101961135864, mape:0.01758495159447193, mspe:0.0005067475140094757, rse:0.43850037455558777, r2_score:0.7858451409228506, acc:0.9824150484055281
corr: [37.078644 37.176052 37.43197  37.347946 37.296135 37.488426 37.3185
 37.38812  37.64388  37.380817 37.325863 37.46231  37.604324 37.543617
 37.65609 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3099002
	speed: 0.1205s/iter; left time: 371.1135s
	iters: 200, epoch: 1 | loss: 0.1872826
	speed: 0.0886s/iter; left time: 264.2470s
	iters: 300, epoch: 1 | loss: 0.1686273
	speed: 0.0886s/iter; left time: 255.2760s
Epoch: 1 cost time: 31.28291368484497
Epoch: 1, Steps: 318 | Train Loss: 0.2981027 Vali Loss: 0.1876536 Test Loss: 0.1938759
Validation loss decreased (inf --> 0.187654).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1935234
	speed: 0.0873s/iter; left time: 241.0723s
	iters: 200, epoch: 2 | loss: 0.1785920
	speed: 0.0825s/iter; left time: 219.6461s
	iters: 300, epoch: 2 | loss: 0.2463522
	speed: 0.0889s/iter; left time: 227.7600s
Epoch: 2 cost time: 27.377272844314575
Epoch: 2, Steps: 318 | Train Loss: 0.1938452 Vali Loss: 0.1878734 Test Loss: 0.1732496
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1586599
	speed: 0.0889s/iter; left time: 217.4742s
	iters: 200, epoch: 3 | loss: 0.2349851
	speed: 0.0758s/iter; left time: 177.7366s
	iters: 300, epoch: 3 | loss: 0.1451025
	speed: 0.0785s/iter; left time: 176.1375s
Epoch: 3 cost time: 25.84681534767151
Epoch: 3, Steps: 318 | Train Loss: 0.1826290 Vali Loss: 0.1832755 Test Loss: 0.1703691
Validation loss decreased (0.187654 --> 0.183276).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1415367
	speed: 0.0884s/iter; left time: 188.0277s
	iters: 200, epoch: 4 | loss: 0.2043886
	speed: 0.0742s/iter; left time: 150.4037s
	iters: 300, epoch: 4 | loss: 0.1282442
	speed: 0.0789s/iter; left time: 152.0030s
Epoch: 4 cost time: 25.46943473815918
Epoch: 4, Steps: 318 | Train Loss: 0.1777828 Vali Loss: 0.1888428 Test Loss: 0.1716513
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1724218
	speed: 0.1036s/iter; left time: 187.4231s
	iters: 200, epoch: 5 | loss: 0.1653531
	speed: 0.0759s/iter; left time: 129.6694s
	iters: 300, epoch: 5 | loss: 0.1644060
	speed: 0.0772s/iter; left time: 124.2143s
Epoch: 5 cost time: 27.049400329589844
Epoch: 5, Steps: 318 | Train Loss: 0.1749548 Vali Loss: 0.1851424 Test Loss: 0.1684209
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1639412
	speed: 0.0929s/iter; left time: 138.4892s
	iters: 200, epoch: 6 | loss: 0.1848135
	speed: 0.0731s/iter; left time: 101.6742s
	iters: 300, epoch: 6 | loss: 0.2029684
	speed: 0.0780s/iter; left time: 100.6493s
Epoch: 6 cost time: 25.792017936706543
Epoch: 6, Steps: 318 | Train Loss: 0.1739874 Vali Loss: 0.1859119 Test Loss: 0.1696764
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.37386834621429443, mae:0.47056132555007935, rmse:0.6114477515220642, mape:0.016852812841534615, mspe:0.0004877051687799394, rse:0.42798343300819397, r2_score:0.7937937059796225, acc:0.9831471871584654
corr: [36.522774 36.47966  36.583233 36.449005 36.540916 36.63919  36.721386
 36.731552 37.157387 36.69069  36.710724 36.715008 36.68657  36.909748
 36.88948  36.955124 36.90821  37.22111  37.49394  37.408485]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2774278
	speed: 0.1064s/iter; left time: 327.7341s
	iters: 200, epoch: 1 | loss: 0.2420125
	speed: 0.0809s/iter; left time: 241.1151s
	iters: 300, epoch: 1 | loss: 0.2468058
	speed: 0.0833s/iter; left time: 239.8958s
Epoch: 1 cost time: 28.31360387802124
Epoch: 1, Steps: 318 | Train Loss: 0.3317532 Vali Loss: 0.2013163 Test Loss: 0.1912717
Validation loss decreased (inf --> 0.201316).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1754795
	speed: 0.0882s/iter; left time: 243.6516s
	iters: 200, epoch: 2 | loss: 0.1577347
	speed: 0.0849s/iter; left time: 226.2065s
	iters: 300, epoch: 2 | loss: 0.2008958
	speed: 0.0801s/iter; left time: 205.4038s
Epoch: 2 cost time: 26.809595108032227
Epoch: 2, Steps: 318 | Train Loss: 0.2015486 Vali Loss: 0.1935906 Test Loss: 0.1829691
Validation loss decreased (0.201316 --> 0.193591).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2167271
	speed: 0.0904s/iter; left time: 221.1346s
	iters: 200, epoch: 3 | loss: 0.1490788
	speed: 0.0834s/iter; left time: 195.4828s
	iters: 300, epoch: 3 | loss: 0.2209259
	speed: 0.0809s/iter; left time: 181.5941s
Epoch: 3 cost time: 27.147001266479492
Epoch: 3, Steps: 318 | Train Loss: 0.1897209 Vali Loss: 0.1967263 Test Loss: 0.1803316
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1669694
	speed: 0.0909s/iter; left time: 193.2501s
	iters: 200, epoch: 4 | loss: 0.1462461
	speed: 0.0881s/iter; left time: 178.6406s
	iters: 300, epoch: 4 | loss: 0.1607634
	speed: 0.0890s/iter; left time: 171.5714s
Epoch: 4 cost time: 28.441858768463135
Epoch: 4, Steps: 318 | Train Loss: 0.1843571 Vali Loss: 0.1978904 Test Loss: 0.1821518
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1773246
	speed: 0.0903s/iter; left time: 163.3412s
	iters: 200, epoch: 5 | loss: 0.1649646
	speed: 0.0862s/iter; left time: 147.3739s
	iters: 300, epoch: 5 | loss: 0.2187113
	speed: 0.0827s/iter; left time: 133.0661s
Epoch: 5 cost time: 27.638797521591187
Epoch: 5, Steps: 318 | Train Loss: 0.1817013 Vali Loss: 0.1992632 Test Loss: 0.1797385
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000031
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.40151864290237427, mae:0.4882640838623047, rmse:0.6336550116539001, mape:0.017520857974886894, mspe:0.0005287075764499605, rse:0.44323113560676575, r2_score:0.7694948140697775, acc:0.9824791420251131
corr: [36.710022 36.614994 36.379887 36.310925 36.4477   36.35257  36.268234
 36.47102  36.61807  36.67902  36.795277 37.048103 37.043533 36.853226
 36.927574 36.872925 36.792416 36.72281  36.704292 36.517097 37.0358
 36.79393  36.747074 37.290005 37.191372]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4012804
	speed: 0.1095s/iter; left time: 336.2501s
	iters: 200, epoch: 1 | loss: 0.2689988
	speed: 0.0961s/iter; left time: 285.4556s
	iters: 300, epoch: 1 | loss: 0.2930444
	speed: 0.1032s/iter; left time: 296.1490s
Epoch: 1 cost time: 32.481385231018066
Epoch: 1, Steps: 317 | Train Loss: 0.3647237 Vali Loss: 0.2607331 Test Loss: 0.2488384
Validation loss decreased (inf --> 0.260733).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2675309
	speed: 0.1201s/iter; left time: 330.8621s
	iters: 200, epoch: 2 | loss: 0.2470520
	speed: 0.1013s/iter; left time: 268.9076s
	iters: 300, epoch: 2 | loss: 0.1906852
	speed: 0.1048s/iter; left time: 267.7554s
Epoch: 2 cost time: 34.434894323349
Epoch: 2, Steps: 317 | Train Loss: 0.2465564 Vali Loss: 0.2332344 Test Loss: 0.2203061
Validation loss decreased (0.260733 --> 0.233234).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2282548
	speed: 0.1239s/iter; left time: 301.8399s
	iters: 200, epoch: 3 | loss: 0.1579279
	speed: 0.1039s/iter; left time: 242.9021s
	iters: 300, epoch: 3 | loss: 0.2004651
	speed: 0.0979s/iter; left time: 219.0871s
Epoch: 3 cost time: 34.34062123298645
Epoch: 3, Steps: 317 | Train Loss: 0.2232694 Vali Loss: 0.2292803 Test Loss: 0.2162555
Validation loss decreased (0.233234 --> 0.229280).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2505794
	speed: 0.1168s/iter; left time: 247.7039s
	iters: 200, epoch: 4 | loss: 0.2136113
	speed: 0.1013s/iter; left time: 204.5449s
	iters: 300, epoch: 4 | loss: 0.2297602
	speed: 0.1048s/iter; left time: 201.2355s
Epoch: 4 cost time: 34.010032415390015
Epoch: 4, Steps: 317 | Train Loss: 0.2152714 Vali Loss: 0.2250776 Test Loss: 0.2120177
Validation loss decreased (0.229280 --> 0.225078).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2523055
	speed: 0.1116s/iter; left time: 201.2364s
	iters: 200, epoch: 5 | loss: 0.2246896
	speed: 0.0996s/iter; left time: 169.6411s
	iters: 300, epoch: 5 | loss: 0.2185306
	speed: 0.1060s/iter; left time: 169.9799s
Epoch: 5 cost time: 33.49832773208618
Epoch: 5, Steps: 317 | Train Loss: 0.2115799 Vali Loss: 0.2238815 Test Loss: 0.2094443
Validation loss decreased (0.225078 --> 0.223882).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2413326
	speed: 0.1056s/iter; left time: 156.8861s
	iters: 200, epoch: 6 | loss: 0.1899029
	speed: 0.1085s/iter; left time: 150.4053s
	iters: 300, epoch: 6 | loss: 0.2382702
	speed: 0.1047s/iter; left time: 134.6300s
Epoch: 6 cost time: 33.633400201797485
Epoch: 6, Steps: 317 | Train Loss: 0.2102753 Vali Loss: 0.2217117 Test Loss: 0.2081383
Validation loss decreased (0.223882 --> 0.221712).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1603308
	speed: 0.1240s/iter; left time: 144.9302s
	iters: 200, epoch: 7 | loss: 0.2607746
	speed: 0.1007s/iter; left time: 107.6901s
	iters: 300, epoch: 7 | loss: 0.2386228
	speed: 0.1023s/iter; left time: 99.1676s
Epoch: 7 cost time: 33.903958320617676
Epoch: 7, Steps: 317 | Train Loss: 0.2092559 Vali Loss: 0.2226461 Test Loss: 0.2085773
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2150871
	speed: 0.1083s/iter; left time: 92.3078s
	iters: 200, epoch: 8 | loss: 0.2217047
	speed: 0.1037s/iter; left time: 77.9553s
	iters: 300, epoch: 8 | loss: 0.1713029
	speed: 0.1082s/iter; left time: 70.5314s
Epoch: 8 cost time: 33.78689098358154
Epoch: 8, Steps: 317 | Train Loss: 0.2088095 Vali Loss: 0.2226373 Test Loss: 0.2081434
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1988302
	speed: 0.1165s/iter; left time: 62.3055s
	iters: 200, epoch: 9 | loss: 0.1992787
	speed: 0.1040s/iter; left time: 45.2502s
	iters: 300, epoch: 9 | loss: 0.2538878
	speed: 0.1055s/iter; left time: 35.3275s
Epoch: 9 cost time: 34.451066732406616
Epoch: 9, Steps: 317 | Train Loss: 0.2088549 Vali Loss: 0.2221010 Test Loss: 0.2082368
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.45675128698349, mae:0.529895544052124, rmse:0.6758337616920471, mape:0.018955504521727562, mspe:0.0005941726849414408, rse:0.47239503264427185, r2_score:0.7415081532657024, acc:0.9810444954782724
corr: [36.58296  36.342285 36.68985  36.532047 36.53752  36.77629  36.86902
 36.706863 37.073338 36.81939  36.91442  36.82144  36.980522 37.020386
 37.152893 37.034363 37.252666 37.357754 37.44678  37.573154 37.39375
 37.683434 37.67629  37.638885 37.624054 37.966663 37.847042 37.61873
 37.814426 38.08375 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3976599
	speed: 0.1246s/iter; left time: 383.7721s
	iters: 200, epoch: 1 | loss: 0.1928670
	speed: 0.0873s/iter; left time: 260.3873s
	iters: 300, epoch: 1 | loss: 0.1790336
	speed: 0.0970s/iter; left time: 279.4563s
Epoch: 1 cost time: 32.367393016815186
Epoch: 1, Steps: 318 | Train Loss: 0.2925564 Vali Loss: 0.1920365 Test Loss: 0.1843453
Validation loss decreased (inf --> 0.192036).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1830339
	speed: 0.0929s/iter; left time: 256.6732s
	iters: 200, epoch: 2 | loss: 0.1743653
	speed: 0.0901s/iter; left time: 239.8455s
	iters: 300, epoch: 2 | loss: 0.1731145
	speed: 0.0824s/iter; left time: 211.1490s
Epoch: 2 cost time: 27.97322916984558
Epoch: 2, Steps: 318 | Train Loss: 0.1670328 Vali Loss: 0.1513477 Test Loss: 0.1408777
Validation loss decreased (0.192036 --> 0.151348).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1884440
	speed: 0.0991s/iter; left time: 242.2626s
	iters: 200, epoch: 3 | loss: 0.1272835
	speed: 0.0962s/iter; left time: 225.5846s
	iters: 300, epoch: 3 | loss: 0.1712715
	speed: 0.0890s/iter; left time: 199.8420s
Epoch: 3 cost time: 30.230921506881714
Epoch: 3, Steps: 318 | Train Loss: 0.1526895 Vali Loss: 0.1434930 Test Loss: 0.1344583
Validation loss decreased (0.151348 --> 0.143493).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1540097
	speed: 0.0940s/iter; left time: 199.8502s
	iters: 200, epoch: 4 | loss: 0.1314116
	speed: 0.0923s/iter; left time: 187.1832s
	iters: 300, epoch: 4 | loss: 0.1474777
	speed: 0.1005s/iter; left time: 193.7394s
Epoch: 4 cost time: 31.09394407272339
Epoch: 4, Steps: 318 | Train Loss: 0.1465971 Vali Loss: 0.1450298 Test Loss: 0.1363817
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1452034
	speed: 0.0923s/iter; left time: 167.0200s
	iters: 200, epoch: 5 | loss: 0.1864033
	speed: 0.0858s/iter; left time: 146.6251s
	iters: 300, epoch: 5 | loss: 0.1358096
	speed: 0.0931s/iter; left time: 149.7874s
Epoch: 5 cost time: 29.147690534591675
Epoch: 5, Steps: 318 | Train Loss: 0.1441207 Vali Loss: 0.1439497 Test Loss: 0.1342774
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1204080
	speed: 0.0923s/iter; left time: 137.6064s
	iters: 200, epoch: 6 | loss: 0.1157987
	speed: 0.0945s/iter; left time: 131.4303s
	iters: 300, epoch: 6 | loss: 0.1384409
	speed: 0.0897s/iter; left time: 115.8532s
Epoch: 6 cost time: 29.27245283126831
Epoch: 6, Steps: 318 | Train Loss: 0.1419408 Vali Loss: 0.1435225 Test Loss: 0.1328046
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.29506349563598633, mae:0.41902533173561096, rmse:0.5431974530220032, mape:0.015002431347966194, mspe:0.00038406686508096755, rse:0.3806111216545105, r2_score:0.834480293637269, acc:0.9849975686520338
corr: [36.734642 36.972984 36.640743 36.730984 37.018696 36.803783 36.755367
 36.861877 37.1862   36.872112]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2616906
	speed: 0.1158s/iter; left time: 356.7560s
	iters: 200, epoch: 1 | loss: 0.2491474
	speed: 0.0972s/iter; left time: 289.8700s
	iters: 300, epoch: 1 | loss: 0.1817188
	speed: 0.1003s/iter; left time: 288.9946s
Epoch: 1 cost time: 33.03327178955078
Epoch: 1, Steps: 318 | Train Loss: 0.3008961 Vali Loss: 0.1975515 Test Loss: 0.1822416
Validation loss decreased (inf --> 0.197552).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1551706
	speed: 0.1128s/iter; left time: 311.7971s
	iters: 200, epoch: 2 | loss: 0.2090581
	speed: 0.1033s/iter; left time: 275.1852s
	iters: 300, epoch: 2 | loss: 0.2288173
	speed: 0.1036s/iter; left time: 265.6266s
Epoch: 2 cost time: 34.488107442855835
Epoch: 2, Steps: 318 | Train Loss: 0.1806067 Vali Loss: 0.1690771 Test Loss: 0.1562347
Validation loss decreased (0.197552 --> 0.169077).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1593632
	speed: 0.1167s/iter; left time: 285.2496s
	iters: 200, epoch: 3 | loss: 0.1626634
	speed: 0.1003s/iter; left time: 235.2475s
	iters: 300, epoch: 3 | loss: 0.1581025
	speed: 0.1024s/iter; left time: 229.9691s
Epoch: 3 cost time: 33.79657053947449
Epoch: 3, Steps: 318 | Train Loss: 0.1690018 Vali Loss: 0.1654584 Test Loss: 0.1539910
Validation loss decreased (0.169077 --> 0.165458).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1483261
	speed: 0.1054s/iter; left time: 224.1759s
	iters: 200, epoch: 4 | loss: 0.1772097
	speed: 0.1041s/iter; left time: 211.0525s
	iters: 300, epoch: 4 | loss: 0.1522404
	speed: 0.1129s/iter; left time: 217.5156s
Epoch: 4 cost time: 34.440351486206055
Epoch: 4, Steps: 318 | Train Loss: 0.1645555 Vali Loss: 0.1637945 Test Loss: 0.1526457
Validation loss decreased (0.165458 --> 0.163794).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1906858
	speed: 0.1082s/iter; left time: 195.7158s
	iters: 200, epoch: 5 | loss: 0.1615701
	speed: 0.0959s/iter; left time: 163.9267s
	iters: 300, epoch: 5 | loss: 0.1352915
	speed: 0.1077s/iter; left time: 173.2618s
Epoch: 5 cost time: 33.17430830001831
Epoch: 5, Steps: 318 | Train Loss: 0.1617454 Vali Loss: 0.1641230 Test Loss: 0.1520724
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1664592
	speed: 0.1163s/iter; left time: 173.4046s
	iters: 200, epoch: 6 | loss: 0.1829524
	speed: 0.1013s/iter; left time: 140.8413s
	iters: 300, epoch: 6 | loss: 0.1273543
	speed: 0.1076s/iter; left time: 138.9666s
Epoch: 6 cost time: 34.221551179885864
Epoch: 6, Steps: 318 | Train Loss: 0.1607241 Vali Loss: 0.1652244 Test Loss: 0.1513744
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1546927
	speed: 0.1042s/iter; left time: 122.2411s
	iters: 200, epoch: 7 | loss: 0.1137226
	speed: 0.0958s/iter; left time: 102.7950s
	iters: 300, epoch: 7 | loss: 0.1679877
	speed: 0.1049s/iter; left time: 102.0414s
Epoch: 7 cost time: 32.32869839668274
Epoch: 7, Steps: 318 | Train Loss: 0.1599013 Vali Loss: 0.1649286 Test Loss: 0.1522715
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.3349749743938446, mae:0.4464559853076935, rmse:0.5787702202796936, mape:0.015983521938323975, mspe:0.0004361940373200923, rse:0.40534549951553345, r2_score:0.8142165624779588, acc:0.984016478061676
corr: [36.656937 36.737747 36.763515 36.69752  36.728058 36.79564  36.725735
 36.787514 37.097347 36.828186 36.988495 37.117176 37.11117  37.220497
 37.23197 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3078512
	speed: 0.1494s/iter; left time: 460.1571s
	iters: 200, epoch: 1 | loss: 0.1861845
	speed: 0.1240s/iter; left time: 369.5369s
	iters: 300, epoch: 1 | loss: 0.1679454
	speed: 0.0920s/iter; left time: 265.1436s
Epoch: 1 cost time: 38.06284523010254
Epoch: 1, Steps: 318 | Train Loss: 0.2972150 Vali Loss: 0.1874290 Test Loss: 0.1935339
Validation loss decreased (inf --> 0.187429).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1929103
	speed: 0.0978s/iter; left time: 270.1793s
	iters: 200, epoch: 2 | loss: 0.1781572
	speed: 0.0830s/iter; left time: 221.0695s
	iters: 300, epoch: 2 | loss: 0.2465309
	speed: 0.0925s/iter; left time: 237.0861s
Epoch: 2 cost time: 29.00911521911621
Epoch: 2, Steps: 318 | Train Loss: 0.1936247 Vali Loss: 0.1877012 Test Loss: 0.1731291
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1581534
	speed: 0.0953s/iter; left time: 233.0294s
	iters: 200, epoch: 3 | loss: 0.2345342
	speed: 0.0912s/iter; left time: 213.9449s
	iters: 300, epoch: 3 | loss: 0.1452251
	speed: 0.1005s/iter; left time: 225.7040s
Epoch: 3 cost time: 30.96245265007019
Epoch: 3, Steps: 318 | Train Loss: 0.1824903 Vali Loss: 0.1831362 Test Loss: 0.1703061
Validation loss decreased (0.187429 --> 0.183136).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1416836
	speed: 0.1127s/iter; left time: 239.7952s
	iters: 200, epoch: 4 | loss: 0.2044679
	speed: 0.1073s/iter; left time: 217.5250s
	iters: 300, epoch: 4 | loss: 0.1284067
	speed: 0.0897s/iter; left time: 172.7854s
Epoch: 4 cost time: 32.60051941871643
Epoch: 4, Steps: 318 | Train Loss: 0.1776281 Vali Loss: 0.1886841 Test Loss: 0.1716598
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1722786
	speed: 0.1021s/iter; left time: 184.6294s
	iters: 200, epoch: 5 | loss: 0.1654590
	speed: 0.0867s/iter; left time: 148.2114s
	iters: 300, epoch: 5 | loss: 0.1645786
	speed: 0.0957s/iter; left time: 154.0150s
Epoch: 5 cost time: 30.34695863723755
Epoch: 5, Steps: 318 | Train Loss: 0.1747376 Vali Loss: 0.1849397 Test Loss: 0.1684476
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1640848
	speed: 0.1009s/iter; left time: 150.4515s
	iters: 200, epoch: 6 | loss: 0.1849479
	speed: 0.1025s/iter; left time: 142.5545s
	iters: 300, epoch: 6 | loss: 0.2032057
	speed: 0.1023s/iter; left time: 132.0423s
Epoch: 6 cost time: 32.583799600601196
Epoch: 6, Steps: 318 | Train Loss: 0.1737549 Vali Loss: 0.1856843 Test Loss: 0.1697393
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.37373003363609314, mae:0.470462828874588, rmse:0.6113346219062805, mape:0.016849709674715996, mspe:0.0004875677695963532, rse:0.42790424823760986, r2_score:0.7937334624177468, acc:0.983150290325284
corr: [36.53167  36.490833 36.59713  36.454067 36.55323  36.642056 36.72922
 36.726696 37.15236  36.685043 36.706722 36.710304 36.67947  36.900406
 36.884132 36.95638  36.906864 37.21916  37.48953  37.4012  ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2835398
	speed: 0.1113s/iter; left time: 342.7825s
	iters: 200, epoch: 1 | loss: 0.2544409
	speed: 0.0926s/iter; left time: 275.9983s
	iters: 300, epoch: 1 | loss: 0.2387798
	speed: 0.0974s/iter; left time: 280.5367s
Epoch: 1 cost time: 32.21481943130493
Epoch: 1, Steps: 318 | Train Loss: 0.3317089 Vali Loss: 0.2072470 Test Loss: 0.2001531
Validation loss decreased (inf --> 0.207247).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1851113
	speed: 0.1008s/iter; left time: 278.6026s
	iters: 200, epoch: 2 | loss: 0.1520189
	speed: 0.0934s/iter; left time: 248.6780s
	iters: 300, epoch: 2 | loss: 0.2039024
	speed: 0.0882s/iter; left time: 226.0505s
Epoch: 2 cost time: 29.924728393554688
Epoch: 2, Steps: 318 | Train Loss: 0.2026586 Vali Loss: 0.1967813 Test Loss: 0.1900175
Validation loss decreased (0.207247 --> 0.196781).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2194888
	speed: 0.1043s/iter; left time: 255.1276s
	iters: 200, epoch: 3 | loss: 0.1530534
	speed: 0.0933s/iter; left time: 218.8471s
	iters: 300, epoch: 3 | loss: 0.2157940
	speed: 0.0889s/iter; left time: 199.5873s
Epoch: 3 cost time: 30.62353491783142
Epoch: 3, Steps: 318 | Train Loss: 0.1898608 Vali Loss: 0.1979199 Test Loss: 0.1875716
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1705519
	speed: 0.1002s/iter; left time: 213.0198s
	iters: 200, epoch: 4 | loss: 0.1445747
	speed: 0.0895s/iter; left time: 181.3557s
	iters: 300, epoch: 4 | loss: 0.1707711
	speed: 0.0966s/iter; left time: 186.2124s
Epoch: 4 cost time: 30.40175724029541
Epoch: 4, Steps: 318 | Train Loss: 0.1840034 Vali Loss: 0.1994600 Test Loss: 0.1905871
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1806719
	speed: 0.1014s/iter; left time: 183.3986s
	iters: 200, epoch: 5 | loss: 0.1717869
	speed: 0.0929s/iter; left time: 158.7182s
	iters: 300, epoch: 5 | loss: 0.2132992
	speed: 0.0870s/iter; left time: 139.9464s
Epoch: 5 cost time: 29.80872344970703
Epoch: 5, Steps: 318 | Train Loss: 0.1813657 Vali Loss: 0.1999974 Test Loss: 0.1875883
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000031
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.4169860780239105, mae:0.4977751076221466, rmse:0.6457445621490479, mape:0.0178600512444973, mspe:0.0005489191971719265, rse:0.4516875743865967, r2_score:0.7678410878979942, acc:0.9821399487555027
corr: [36.797646 36.969822 36.822895 36.87715  37.168694 37.057487 36.897015
 37.1523   37.12655  37.142    37.097992 36.98243  36.85804  36.690933
 36.844715 36.762875 36.835068 36.655422 36.456738 36.486343 37.00049
 36.932964 36.967705 37.23509  37.263443]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3873481
	speed: 0.1179s/iter; left time: 361.9415s
	iters: 200, epoch: 1 | loss: 0.2659244
	speed: 0.0901s/iter; left time: 267.5963s
	iters: 300, epoch: 1 | loss: 0.2657646
	speed: 0.0981s/iter; left time: 281.6583s
Epoch: 1 cost time: 32.41055679321289
Epoch: 1, Steps: 317 | Train Loss: 0.3573033 Vali Loss: 0.2670331 Test Loss: 0.2477104
Validation loss decreased (inf --> 0.267033).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2474470
	speed: 0.1002s/iter; left time: 275.9700s
	iters: 200, epoch: 2 | loss: 0.2414869
	speed: 0.0947s/iter; left time: 251.2271s
	iters: 300, epoch: 2 | loss: 0.1750490
	speed: 0.1016s/iter; left time: 259.5449s
Epoch: 2 cost time: 31.640687704086304
Epoch: 2, Steps: 317 | Train Loss: 0.2373556 Vali Loss: 0.2345791 Test Loss: 0.2167075
Validation loss decreased (0.267033 --> 0.234579).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2312522
	speed: 0.1131s/iter; left time: 275.6324s
	iters: 200, epoch: 3 | loss: 0.1750030
	speed: 0.0954s/iter; left time: 222.9871s
	iters: 300, epoch: 3 | loss: 0.1954572
	speed: 0.1114s/iter; left time: 249.2607s
Epoch: 3 cost time: 33.66875123977661
Epoch: 3, Steps: 317 | Train Loss: 0.2150389 Vali Loss: 0.2375786 Test Loss: 0.2163897
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2384580
	speed: 0.1031s/iter; left time: 218.6347s
	iters: 200, epoch: 4 | loss: 0.1951770
	speed: 0.0892s/iter; left time: 180.1035s
	iters: 300, epoch: 4 | loss: 0.2249314
	speed: 0.0932s/iter; left time: 178.9932s
Epoch: 4 cost time: 30.043169021606445
Epoch: 4, Steps: 317 | Train Loss: 0.2072330 Vali Loss: 0.2355077 Test Loss: 0.2134762
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2405257
	speed: 0.1020s/iter; left time: 183.8489s
	iters: 200, epoch: 5 | loss: 0.2229239
	speed: 0.0933s/iter; left time: 158.8210s
	iters: 300, epoch: 5 | loss: 0.2139979
	speed: 0.0976s/iter; left time: 156.3778s
Epoch: 5 cost time: 30.891162157058716
Epoch: 5, Steps: 317 | Train Loss: 0.2038928 Vali Loss: 0.2325097 Test Loss: 0.2119872
Validation loss decreased (0.234579 --> 0.232510).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2467530
	speed: 0.1027s/iter; left time: 152.5455s
	iters: 200, epoch: 6 | loss: 0.1829627
	speed: 0.0905s/iter; left time: 125.4776s
	iters: 300, epoch: 6 | loss: 0.2144783
	speed: 0.0885s/iter; left time: 113.8272s
Epoch: 6 cost time: 29.874704837799072
Epoch: 6, Steps: 317 | Train Loss: 0.2023041 Vali Loss: 0.2304069 Test Loss: 0.2104179
Validation loss decreased (0.232510 --> 0.230407).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1595074
	speed: 0.1000s/iter; left time: 116.9200s
	iters: 200, epoch: 7 | loss: 0.2457677
	speed: 0.0955s/iter; left time: 102.1172s
	iters: 300, epoch: 7 | loss: 0.2357568
	speed: 0.0935s/iter; left time: 90.6405s
Epoch: 7 cost time: 30.639400959014893
Epoch: 7, Steps: 317 | Train Loss: 0.2018783 Vali Loss: 0.2322461 Test Loss: 0.2112873
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2073144
	speed: 0.0990s/iter; left time: 84.3852s
	iters: 200, epoch: 8 | loss: 0.2029967
	speed: 0.0948s/iter; left time: 71.3095s
	iters: 300, epoch: 8 | loss: 0.1676730
	speed: 0.1053s/iter; left time: 68.6871s
Epoch: 8 cost time: 31.817288637161255
Epoch: 8, Steps: 317 | Train Loss: 0.2011128 Vali Loss: 0.2320390 Test Loss: 0.2107831
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2082035
	speed: 0.1040s/iter; left time: 55.6665s
	iters: 200, epoch: 9 | loss: 0.1840065
	speed: 0.0924s/iter; left time: 40.1742s
	iters: 300, epoch: 9 | loss: 0.2165686
	speed: 0.0920s/iter; left time: 30.8355s
Epoch: 9 cost time: 30.48421311378479
Epoch: 9, Steps: 317 | Train Loss: 0.2012030 Vali Loss: 0.2313909 Test Loss: 0.2112355
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.4617539048194885, mae:0.5347719192504883, rmse:0.6795247793197632, mape:0.019122425466775894, mspe:0.0006000048597343266, rse:0.47497496008872986, r2_score:0.7465263547880324, acc:0.9808775745332241
corr: [36.7305   36.57362  36.713142 36.705444 36.63319  36.74721  36.807472
 36.80521  36.92637  36.87025  36.86789  36.907127 36.934708 37.162823
 37.254242 37.23141  37.22847  37.560192 37.53544  37.602566 37.257736
 37.689404 37.68076  37.55128  37.485184 37.85293  37.819492 37.318653
 37.88003  37.78403 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4051100
	speed: 0.1199s/iter; left time: 369.3197s
	iters: 200, epoch: 1 | loss: 0.1917398
	speed: 0.1009s/iter; left time: 300.7849s
	iters: 300, epoch: 1 | loss: 0.1752588
	speed: 0.1000s/iter; left time: 288.0766s
Epoch: 1 cost time: 33.63937950134277
Epoch: 1, Steps: 318 | Train Loss: 0.2992061 Vali Loss: 0.1914669 Test Loss: 0.1845600
Validation loss decreased (inf --> 0.191467).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1831360
	speed: 0.1022s/iter; left time: 282.4775s
	iters: 200, epoch: 2 | loss: 0.1748982
	speed: 0.0966s/iter; left time: 257.3004s
	iters: 300, epoch: 2 | loss: 0.1728027
	speed: 0.1071s/iter; left time: 274.5034s
Epoch: 2 cost time: 32.674500703811646
Epoch: 2, Steps: 318 | Train Loss: 0.1659649 Vali Loss: 0.1508751 Test Loss: 0.1401989
Validation loss decreased (0.191467 --> 0.150875).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1893820
	speed: 0.1048s/iter; left time: 256.1304s
	iters: 200, epoch: 3 | loss: 0.1266900
	speed: 0.0970s/iter; left time: 227.5143s
	iters: 300, epoch: 3 | loss: 0.1699879
	speed: 0.0962s/iter; left time: 215.9440s
Epoch: 3 cost time: 31.83021593093872
Epoch: 3, Steps: 318 | Train Loss: 0.1520517 Vali Loss: 0.1424339 Test Loss: 0.1341614
Validation loss decreased (0.150875 --> 0.142434).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1520715
	speed: 0.1034s/iter; left time: 219.9788s
	iters: 200, epoch: 4 | loss: 0.1330786
	speed: 0.0904s/iter; left time: 183.2233s
	iters: 300, epoch: 4 | loss: 0.1480019
	speed: 0.0911s/iter; left time: 175.5722s
Epoch: 4 cost time: 30.060242891311646
Epoch: 4, Steps: 318 | Train Loss: 0.1461507 Vali Loss: 0.1441541 Test Loss: 0.1362057
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1466842
	speed: 0.1061s/iter; left time: 191.9181s
	iters: 200, epoch: 5 | loss: 0.1867178
	speed: 0.0967s/iter; left time: 165.2498s
	iters: 300, epoch: 5 | loss: 0.1352272
	speed: 0.1028s/iter; left time: 165.3398s
Epoch: 5 cost time: 32.669503927230835
Epoch: 5, Steps: 318 | Train Loss: 0.1437615 Vali Loss: 0.1429345 Test Loss: 0.1339512
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1188683
	speed: 0.1061s/iter; left time: 158.1531s
	iters: 200, epoch: 6 | loss: 0.1156361
	speed: 0.0994s/iter; left time: 138.2360s
	iters: 300, epoch: 6 | loss: 0.1378660
	speed: 0.0965s/iter; left time: 124.6275s
Epoch: 6 cost time: 31.945012092590332
Epoch: 6, Steps: 318 | Train Loss: 0.1416304 Vali Loss: 0.1426081 Test Loss: 0.1325245
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.2944119870662689, mae:0.4185953736305237, rmse:0.5425974726676941, mape:0.01498897559940815, mspe:0.0003834528033621609, rse:0.3801906704902649, r2_score:0.8348569082490471, acc:0.9850110244005919
corr: [36.716236 36.948734 36.637756 36.73067  37.006615 36.774582 36.739117
 36.884968 37.19476  36.867588]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2609560
	speed: 0.1401s/iter; left time: 431.7101s
	iters: 200, epoch: 1 | loss: 0.2503639
	speed: 0.1550s/iter; left time: 462.1731s
	iters: 300, epoch: 1 | loss: 0.1807332
	speed: 0.1240s/iter; left time: 357.1224s
Epoch: 1 cost time: 43.92351818084717
Epoch: 1, Steps: 318 | Train Loss: 0.3032347 Vali Loss: 0.1973876 Test Loss: 0.1828984
Validation loss decreased (inf --> 0.197388).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1559456
	speed: 0.1231s/iter; left time: 340.2554s
	iters: 200, epoch: 2 | loss: 0.2087981
	speed: 0.1137s/iter; left time: 302.7633s
	iters: 300, epoch: 2 | loss: 0.2280935
	speed: 0.1258s/iter; left time: 322.4160s
Epoch: 2 cost time: 38.005035638809204
Epoch: 2, Steps: 318 | Train Loss: 0.1808298 Vali Loss: 0.1689806 Test Loss: 0.1559430
Validation loss decreased (0.197388 --> 0.168981).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1587632
	speed: 0.1176s/iter; left time: 287.5062s
	iters: 200, epoch: 3 | loss: 0.1637809
	speed: 0.1110s/iter; left time: 260.4009s
	iters: 300, epoch: 3 | loss: 0.1576596
	speed: 0.1205s/iter; left time: 270.5484s
Epoch: 3 cost time: 36.96293640136719
Epoch: 3, Steps: 318 | Train Loss: 0.1693208 Vali Loss: 0.1654350 Test Loss: 0.1537784
Validation loss decreased (0.168981 --> 0.165435).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1493770
	speed: 0.1179s/iter; left time: 250.7477s
	iters: 200, epoch: 4 | loss: 0.1780897
	speed: 0.1169s/iter; left time: 236.8792s
	iters: 300, epoch: 4 | loss: 0.1528167
	speed: 0.1186s/iter; left time: 228.5044s
Epoch: 4 cost time: 37.33649563789368
Epoch: 4, Steps: 318 | Train Loss: 0.1649286 Vali Loss: 0.1638289 Test Loss: 0.1522512
Validation loss decreased (0.165435 --> 0.163829).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1916964
	speed: 0.1257s/iter; left time: 227.4359s
	iters: 200, epoch: 5 | loss: 0.1609695
	speed: 0.1203s/iter; left time: 205.6031s
	iters: 300, epoch: 5 | loss: 0.1340662
	speed: 0.1205s/iter; left time: 193.8227s
Epoch: 5 cost time: 39.03918981552124
Epoch: 5, Steps: 318 | Train Loss: 0.1621400 Vali Loss: 0.1642410 Test Loss: 0.1518109
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1671227
	speed: 0.1198s/iter; left time: 178.5972s
	iters: 200, epoch: 6 | loss: 0.1829624
	speed: 0.1096s/iter; left time: 152.4034s
	iters: 300, epoch: 6 | loss: 0.1280167
	speed: 0.1080s/iter; left time: 139.4501s
Epoch: 6 cost time: 36.04772400856018
Epoch: 6, Steps: 318 | Train Loss: 0.1611472 Vali Loss: 0.1652658 Test Loss: 0.1510641
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1554237
	speed: 0.1268s/iter; left time: 148.7585s
	iters: 200, epoch: 7 | loss: 0.1148235
	speed: 0.1147s/iter; left time: 123.0315s
	iters: 300, epoch: 7 | loss: 0.1700839
	speed: 0.1172s/iter; left time: 114.0491s
Epoch: 7 cost time: 37.82023620605469
Epoch: 7, Steps: 318 | Train Loss: 0.1602745 Vali Loss: 0.1649732 Test Loss: 0.1519582
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.3341093063354492, mae:0.44587424397468567, rmse:0.5780218839645386, mape:0.015961872413754463, mspe:0.0004348810180090368, rse:0.40482139587402344, r2_score:0.8147289934654675, acc:0.9840381275862455
corr: [36.644302 36.717743 36.75941  36.705803 36.737335 36.812317 36.73944
 36.81069  37.102722 36.840637 36.99557  37.139824 37.133183 37.23635
 37.262875]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3090770
	speed: 0.1623s/iter; left time: 499.9311s
	iters: 200, epoch: 1 | loss: 0.1880168
	speed: 0.1221s/iter; left time: 364.1235s
	iters: 300, epoch: 1 | loss: 0.1682329
	speed: 0.1123s/iter; left time: 323.6105s
Epoch: 1 cost time: 41.85195755958557
Epoch: 1, Steps: 318 | Train Loss: 0.2991293 Vali Loss: 0.1878815 Test Loss: 0.1939179
Validation loss decreased (inf --> 0.187882).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2015234
	speed: 0.1377s/iter; left time: 380.5020s
	iters: 200, epoch: 2 | loss: 0.1778511
	speed: 0.1364s/iter; left time: 363.1181s
	iters: 300, epoch: 2 | loss: 0.2469989
	speed: 0.1344s/iter; left time: 344.3453s
Epoch: 2 cost time: 43.14223289489746
Epoch: 2, Steps: 318 | Train Loss: 0.1938438 Vali Loss: 0.1874252 Test Loss: 0.1732277
Validation loss decreased (0.187882 --> 0.187425).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1584749
	speed: 0.1291s/iter; left time: 315.5507s
	iters: 200, epoch: 3 | loss: 0.2348918
	speed: 0.1226s/iter; left time: 287.5102s
	iters: 300, epoch: 3 | loss: 0.1454472
	speed: 0.1276s/iter; left time: 286.4665s
Epoch: 3 cost time: 40.10565662384033
Epoch: 3, Steps: 318 | Train Loss: 0.1827087 Vali Loss: 0.1830229 Test Loss: 0.1704299
Validation loss decreased (0.187425 --> 0.183023).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1418850
	speed: 0.1293s/iter; left time: 274.9357s
	iters: 200, epoch: 4 | loss: 0.2042758
	speed: 0.1294s/iter; left time: 262.2885s
	iters: 300, epoch: 4 | loss: 0.1285700
	speed: 0.1264s/iter; left time: 243.5791s
Epoch: 4 cost time: 40.785181283950806
Epoch: 4, Steps: 318 | Train Loss: 0.1778612 Vali Loss: 0.1886397 Test Loss: 0.1716020
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1728726
	speed: 0.1323s/iter; left time: 239.3372s
	iters: 200, epoch: 5 | loss: 0.1652940
	speed: 0.1386s/iter; left time: 236.8102s
	iters: 300, epoch: 5 | loss: 0.1634654
	speed: 0.1326s/iter; left time: 213.4106s
Epoch: 5 cost time: 42.63050961494446
Epoch: 5, Steps: 318 | Train Loss: 0.1750400 Vali Loss: 0.1849333 Test Loss: 0.1684015
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1643317
	speed: 0.1312s/iter; left time: 195.6595s
	iters: 200, epoch: 6 | loss: 0.1848157
	speed: 0.1193s/iter; left time: 165.8870s
	iters: 300, epoch: 6 | loss: 0.2021478
	speed: 0.1391s/iter; left time: 179.5469s
Epoch: 6 cost time: 41.19643807411194
Epoch: 6, Steps: 318 | Train Loss: 0.1740864 Vali Loss: 0.1856840 Test Loss: 0.1696441
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.374001681804657, mae:0.47073954343795776, rmse:0.6115567684173584, mape:0.016859620809555054, mspe:0.0004879844782408327, rse:0.42805972695350647, r2_score:0.794033196033974, acc:0.983140379190445
corr: [36.559525 36.502647 36.599625 36.4439   36.52602  36.616444 36.70598
 36.739616 37.156708 36.725903 36.737087 36.73989  36.71415  36.92783
 36.922375 36.989094 36.93713  37.239746 37.506428 37.456375]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2748495
	speed: 0.1251s/iter; left time: 385.3180s
	iters: 200, epoch: 1 | loss: 0.2535534
	speed: 0.1094s/iter; left time: 326.1477s
	iters: 300, epoch: 1 | loss: 0.2391073
	speed: 0.1037s/iter; left time: 298.8332s
Epoch: 1 cost time: 35.815497159957886
Epoch: 1, Steps: 318 | Train Loss: 0.3295437 Vali Loss: 0.2058859 Test Loss: 0.2002940
Validation loss decreased (inf --> 0.205886).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1853859
	speed: 0.1119s/iter; left time: 309.2449s
	iters: 200, epoch: 2 | loss: 0.1509714
	speed: 0.1069s/iter; left time: 284.5839s
	iters: 300, epoch: 2 | loss: 0.2033380
	speed: 0.1090s/iter; left time: 279.2791s
Epoch: 2 cost time: 34.90974187850952
Epoch: 2, Steps: 318 | Train Loss: 0.2023321 Vali Loss: 0.1969289 Test Loss: 0.1905821
Validation loss decreased (0.205886 --> 0.196929).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2183852
	speed: 0.1067s/iter; left time: 260.8997s
	iters: 200, epoch: 3 | loss: 0.1543212
	speed: 0.0995s/iter; left time: 233.3209s
	iters: 300, epoch: 3 | loss: 0.2149431
	speed: 0.1026s/iter; left time: 230.3541s
Epoch: 3 cost time: 32.7839241027832
Epoch: 3, Steps: 318 | Train Loss: 0.1892076 Vali Loss: 0.1974518 Test Loss: 0.1880380
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1693424
	speed: 0.1112s/iter; left time: 236.4921s
	iters: 200, epoch: 4 | loss: 0.1442239
	speed: 0.0972s/iter; left time: 196.9900s
	iters: 300, epoch: 4 | loss: 0.1705718
	speed: 0.0939s/iter; left time: 180.9191s
Epoch: 4 cost time: 31.953540802001953
Epoch: 4, Steps: 318 | Train Loss: 0.1832961 Vali Loss: 0.1990883 Test Loss: 0.1912008
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1788758
	speed: 0.1117s/iter; left time: 201.9855s
	iters: 200, epoch: 5 | loss: 0.1702622
	speed: 0.1136s/iter; left time: 194.1388s
	iters: 300, epoch: 5 | loss: 0.2116891
	speed: 0.1061s/iter; left time: 170.7132s
Epoch: 5 cost time: 35.26253342628479
Epoch: 5, Steps: 318 | Train Loss: 0.1805759 Vali Loss: 0.1997047 Test Loss: 0.1882208
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000031
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.4182250499725342, mae:0.4984836280345917, rmse:0.6467032432556152, mape:0.017887255176901817, mspe:0.0005507142632268369, rse:0.4523581266403198, r2_score:0.7672338928848054, acc:0.9821127448230982
corr: [36.7464   36.90373  36.781742 36.84065  37.166813 37.02853  36.85274
 37.13803  37.102226 37.12544  37.02206  36.94959  36.820107 36.660496
 36.80503  36.726555 36.80664  36.6231   36.476925 36.469555 36.99163
 36.878006 36.945442 37.22694  37.243565]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3814866
	speed: 0.1523s/iter; left time: 467.7794s
	iters: 200, epoch: 1 | loss: 0.2602303
	speed: 0.1283s/iter; left time: 381.2402s
	iters: 300, epoch: 1 | loss: 0.2613702
	speed: 0.1400s/iter; left time: 402.0492s
Epoch: 1 cost time: 44.405383586883545
Epoch: 1, Steps: 317 | Train Loss: 0.3503909 Vali Loss: 0.2408239 Test Loss: 0.2375619
Validation loss decreased (inf --> 0.240824).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2298220
	speed: 0.1526s/iter; left time: 420.2387s
	iters: 200, epoch: 2 | loss: 0.2377827
	speed: 0.1454s/iter; left time: 386.0144s
	iters: 300, epoch: 2 | loss: 0.1787789
	speed: 0.1472s/iter; left time: 376.0526s
Epoch: 2 cost time: 47.07280206680298
Epoch: 2, Steps: 317 | Train Loss: 0.2301964 Vali Loss: 0.2216576 Test Loss: 0.2090480
Validation loss decreased (0.240824 --> 0.221658).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2221500
	speed: 0.1465s/iter; left time: 356.9833s
	iters: 200, epoch: 3 | loss: 0.1682800
	speed: 0.1447s/iter; left time: 338.2742s
	iters: 300, epoch: 3 | loss: 0.1952570
	speed: 0.1598s/iter; left time: 357.4910s
Epoch: 3 cost time: 47.81211757659912
Epoch: 3, Steps: 317 | Train Loss: 0.2117327 Vali Loss: 0.2212903 Test Loss: 0.2089371
Validation loss decreased (0.221658 --> 0.221290).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2437602
	speed: 0.1554s/iter; left time: 329.4466s
	iters: 200, epoch: 4 | loss: 0.2096869
	speed: 0.1349s/iter; left time: 272.4066s
	iters: 300, epoch: 4 | loss: 0.2159100
	speed: 0.1601s/iter; left time: 307.3725s
Epoch: 4 cost time: 47.706244707107544
Epoch: 4, Steps: 317 | Train Loss: 0.2055232 Vali Loss: 0.2197595 Test Loss: 0.2067921
Validation loss decreased (0.221290 --> 0.219760).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2413156
	speed: 0.1579s/iter; left time: 284.6898s
	iters: 200, epoch: 5 | loss: 0.2098128
	speed: 0.1473s/iter; left time: 250.8068s
	iters: 300, epoch: 5 | loss: 0.2080228
	speed: 0.1429s/iter; left time: 229.0752s
Epoch: 5 cost time: 47.529196977615356
Epoch: 5, Steps: 317 | Train Loss: 0.2024138 Vali Loss: 0.2202334 Test Loss: 0.2048746
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2416895
	speed: 0.1634s/iter; left time: 242.7575s
	iters: 200, epoch: 6 | loss: 0.1795293
	speed: 0.1480s/iter; left time: 205.1768s
	iters: 300, epoch: 6 | loss: 0.2182748
	speed: 0.1465s/iter; left time: 188.3864s
Epoch: 6 cost time: 48.20607018470764
Epoch: 6, Steps: 317 | Train Loss: 0.2012146 Vali Loss: 0.2182389 Test Loss: 0.2032560
Validation loss decreased (0.219760 --> 0.218239).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1549522
	speed: 0.1640s/iter; left time: 191.7507s
	iters: 200, epoch: 7 | loss: 0.2354675
	speed: 0.1470s/iter; left time: 157.1891s
	iters: 300, epoch: 7 | loss: 0.2351519
	speed: 0.1551s/iter; left time: 150.2831s
Epoch: 7 cost time: 49.29385590553284
Epoch: 7, Steps: 317 | Train Loss: 0.2002899 Vali Loss: 0.2190851 Test Loss: 0.2038966
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1863925
	speed: 0.1526s/iter; left time: 129.9764s
	iters: 200, epoch: 8 | loss: 0.1899443
	speed: 0.1531s/iter; left time: 115.1508s
	iters: 300, epoch: 8 | loss: 0.1691715
	speed: 0.1606s/iter; left time: 104.6957s
Epoch: 8 cost time: 49.335001707077026
Epoch: 8, Steps: 317 | Train Loss: 0.1997384 Vali Loss: 0.2196977 Test Loss: 0.2039124
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2070170
	speed: 0.1613s/iter; left time: 86.2826s
	iters: 200, epoch: 9 | loss: 0.1766993
	speed: 0.1471s/iter; left time: 64.0018s
	iters: 300, epoch: 9 | loss: 0.2416330
	speed: 0.1493s/iter; left time: 50.0200s
Epoch: 9 cost time: 48.07158422470093
Epoch: 9, Steps: 317 | Train Loss: 0.1998412 Vali Loss: 0.2193521 Test Loss: 0.2043552
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.4460374116897583, mae:0.5202383995056152, rmse:0.6678603291511536, mape:0.01861947774887085, mspe:0.0005811252631247044, rse:0.46682173013687134, r2_score:0.7542510080666924, acc:0.9813805222511292
corr: [36.82103  36.70888  36.75563  36.60938  36.737087 36.75249  36.841
 36.770573 36.964775 36.7594   36.713703 36.81419  36.92424  36.92949
 37.06697  37.12719  37.038624 37.31491  37.42214  37.30352  37.187023
 37.5282   37.430676 37.417374 37.45189  37.660545 37.771572 37.25093
 37.750767 37.8746  ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4014584
	speed: 0.1424s/iter; left time: 438.6460s
	iters: 200, epoch: 1 | loss: 0.1925436
	speed: 0.0989s/iter; left time: 294.8302s
	iters: 300, epoch: 1 | loss: 0.1776119
	speed: 0.0962s/iter; left time: 277.2267s
Epoch: 1 cost time: 35.908687353134155
Epoch: 1, Steps: 318 | Train Loss: 0.2948871 Vali Loss: 0.1913784 Test Loss: 0.1842323
Validation loss decreased (inf --> 0.191378).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1829275
	speed: 0.1182s/iter; left time: 326.6634s
	iters: 200, epoch: 2 | loss: 0.1744025
	speed: 0.0986s/iter; left time: 262.5728s
	iters: 300, epoch: 2 | loss: 0.1733328
	speed: 0.0953s/iter; left time: 244.2042s
Epoch: 2 cost time: 33.31745886802673
Epoch: 2, Steps: 318 | Train Loss: 0.1663868 Vali Loss: 0.1511949 Test Loss: 0.1407542
Validation loss decreased (0.191378 --> 0.151195).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1886400
	speed: 0.1131s/iter; left time: 276.4147s
	iters: 200, epoch: 3 | loss: 0.1265695
	speed: 0.1107s/iter; left time: 259.6576s
	iters: 300, epoch: 3 | loss: 0.1703170
	speed: 0.1018s/iter; left time: 228.5640s
Epoch: 3 cost time: 34.533801317214966
Epoch: 3, Steps: 318 | Train Loss: 0.1521939 Vali Loss: 0.1431021 Test Loss: 0.1343727
Validation loss decreased (0.151195 --> 0.143102).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1530664
	speed: 0.1211s/iter; left time: 257.4863s
	iters: 200, epoch: 4 | loss: 0.1320129
	speed: 0.1031s/iter; left time: 208.9886s
	iters: 300, epoch: 4 | loss: 0.1473065
	speed: 0.1059s/iter; left time: 204.1419s
Epoch: 4 cost time: 34.7741482257843
Epoch: 4, Steps: 318 | Train Loss: 0.1461756 Vali Loss: 0.1447850 Test Loss: 0.1364196
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1452635
	speed: 0.1212s/iter; left time: 219.2371s
	iters: 200, epoch: 5 | loss: 0.1861652
	speed: 0.1103s/iter; left time: 188.5217s
	iters: 300, epoch: 5 | loss: 0.1359579
	speed: 0.0981s/iter; left time: 157.7677s
Epoch: 5 cost time: 34.837984561920166
Epoch: 5, Steps: 318 | Train Loss: 0.1437279 Vali Loss: 0.1436984 Test Loss: 0.1342927
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1195265
	speed: 0.1132s/iter; left time: 168.7263s
	iters: 200, epoch: 6 | loss: 0.1161444
	speed: 0.1106s/iter; left time: 153.7941s
	iters: 300, epoch: 6 | loss: 0.1377725
	speed: 0.1033s/iter; left time: 133.3745s
Epoch: 6 cost time: 34.383349895477295
Epoch: 6, Steps: 318 | Train Loss: 0.1415798 Vali Loss: 0.1432718 Test Loss: 0.1327769
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.2948756814002991, mae:0.4189583659172058, rmse:0.5430245399475098, mape:0.015000429935753345, mspe:0.0003838643606286496, rse:0.38048994541168213, r2_score:0.8349756797043082, acc:0.9849995700642467
corr: [36.730366 36.96989  36.647446 36.742542 37.02079  36.79791  36.757866
 36.872425 37.181335 36.88509 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2595365
	speed: 0.1627s/iter; left time: 501.3431s
	iters: 200, epoch: 1 | loss: 0.2489297
	speed: 0.1592s/iter; left time: 474.4451s
	iters: 300, epoch: 1 | loss: 0.1816393
	speed: 0.1333s/iter; left time: 383.9191s
Epoch: 1 cost time: 47.85026025772095
Epoch: 1, Steps: 318 | Train Loss: 0.2985747 Vali Loss: 0.1971984 Test Loss: 0.1823947
Validation loss decreased (inf --> 0.197198).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1559423
	speed: 0.1380s/iter; left time: 381.3883s
	iters: 200, epoch: 2 | loss: 0.2091905
	speed: 0.1272s/iter; left time: 338.8437s
	iters: 300, epoch: 2 | loss: 0.2275150
	speed: 0.1495s/iter; left time: 383.1556s
Epoch: 2 cost time: 44.72036838531494
Epoch: 2, Steps: 318 | Train Loss: 0.1806157 Vali Loss: 0.1684280 Test Loss: 0.1559520
Validation loss decreased (0.197198 --> 0.168428).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1593425
	speed: 0.1483s/iter; left time: 362.6533s
	iters: 200, epoch: 3 | loss: 0.1619290
	speed: 0.1516s/iter; left time: 355.4303s
	iters: 300, epoch: 3 | loss: 0.1575850
	speed: 0.1606s/iter; left time: 360.5176s
Epoch: 3 cost time: 48.59798312187195
Epoch: 3, Steps: 318 | Train Loss: 0.1690400 Vali Loss: 0.1650778 Test Loss: 0.1535898
Validation loss decreased (0.168428 --> 0.165078).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1486025
	speed: 0.1475s/iter; left time: 313.6911s
	iters: 200, epoch: 4 | loss: 0.1770536
	speed: 0.1549s/iter; left time: 313.9053s
	iters: 300, epoch: 4 | loss: 0.1536977
	speed: 0.1507s/iter; left time: 290.4577s
Epoch: 4 cost time: 48.145105838775635
Epoch: 4, Steps: 318 | Train Loss: 0.1645666 Vali Loss: 0.1637613 Test Loss: 0.1523489
Validation loss decreased (0.165078 --> 0.163761).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1908396
	speed: 0.1450s/iter; left time: 262.2527s
	iters: 200, epoch: 5 | loss: 0.1615647
	speed: 0.1510s/iter; left time: 258.0821s
	iters: 300, epoch: 5 | loss: 0.1363654
	speed: 0.1509s/iter; left time: 242.8089s
Epoch: 5 cost time: 47.25547003746033
Epoch: 5, Steps: 318 | Train Loss: 0.1617410 Vali Loss: 0.1641289 Test Loss: 0.1518196
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1652527
	speed: 0.1463s/iter; left time: 218.2014s
	iters: 200, epoch: 6 | loss: 0.1829480
	speed: 0.1516s/iter; left time: 210.8827s
	iters: 300, epoch: 6 | loss: 0.1273802
	speed: 0.1510s/iter; left time: 194.9786s
Epoch: 6 cost time: 47.558138370513916
Epoch: 6, Steps: 318 | Train Loss: 0.1607307 Vali Loss: 0.1652068 Test Loss: 0.1511464
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1551546
	speed: 0.1565s/iter; left time: 183.6171s
	iters: 200, epoch: 7 | loss: 0.1138195
	speed: 0.1428s/iter; left time: 153.2195s
	iters: 300, epoch: 7 | loss: 0.1685838
	speed: 0.1528s/iter; left time: 148.6721s
Epoch: 7 cost time: 48.08441209793091
Epoch: 7, Steps: 318 | Train Loss: 0.1598726 Vali Loss: 0.1648720 Test Loss: 0.1520067
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.3343237638473511, mae:0.44600722193717957, rmse:0.5782073736190796, mape:0.015967175364494324, mspe:0.0004352579708211124, rse:0.4049513041973114, r2_score:0.8141260166220529, acc:0.9840328246355057
corr: [36.65043  36.721878 36.767612 36.69338  36.727863 36.787724 36.712624
 36.7794   37.083042 36.82585  37.008602 37.13404  37.11974  37.22375
 37.23146 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3046960
	speed: 0.1703s/iter; left time: 524.5896s
	iters: 200, epoch: 1 | loss: 0.1877967
	speed: 0.1423s/iter; left time: 424.2818s
	iters: 300, epoch: 1 | loss: 0.1671070
	speed: 0.1511s/iter; left time: 435.2756s
Epoch: 1 cost time: 49.15202450752258
Epoch: 1, Steps: 318 | Train Loss: 0.3005760 Vali Loss: 0.1879394 Test Loss: 0.1931248
Validation loss decreased (inf --> 0.187939).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1931031
	speed: 0.1570s/iter; left time: 433.8455s
	iters: 200, epoch: 2 | loss: 0.1778728
	speed: 0.1447s/iter; left time: 385.3002s
	iters: 300, epoch: 2 | loss: 0.2485018
	speed: 0.1524s/iter; left time: 390.5678s
Epoch: 2 cost time: 48.6756694316864
Epoch: 2, Steps: 318 | Train Loss: 0.1933588 Vali Loss: 0.1876758 Test Loss: 0.1729845
Validation loss decreased (0.187939 --> 0.187676).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1573690
	speed: 0.1590s/iter; left time: 388.7145s
	iters: 200, epoch: 3 | loss: 0.2342271
	speed: 0.1575s/iter; left time: 369.3094s
	iters: 300, epoch: 3 | loss: 0.1455519
	speed: 0.1450s/iter; left time: 325.5242s
Epoch: 3 cost time: 48.805026054382324
Epoch: 3, Steps: 318 | Train Loss: 0.1821936 Vali Loss: 0.1828890 Test Loss: 0.1699802
Validation loss decreased (0.187676 --> 0.182889).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1413671
	speed: 0.1649s/iter; left time: 350.7146s
	iters: 200, epoch: 4 | loss: 0.2028168
	speed: 0.1528s/iter; left time: 309.7334s
	iters: 300, epoch: 4 | loss: 0.1288958
	speed: 0.1586s/iter; left time: 305.5556s
Epoch: 4 cost time: 50.705775022506714
Epoch: 4, Steps: 318 | Train Loss: 0.1773739 Vali Loss: 0.1886412 Test Loss: 0.1718213
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1719191
	speed: 0.1579s/iter; left time: 285.5645s
	iters: 200, epoch: 5 | loss: 0.1652818
	speed: 0.1612s/iter; left time: 275.5165s
	iters: 300, epoch: 5 | loss: 0.1625487
	speed: 0.1507s/iter; left time: 242.5449s
Epoch: 5 cost time: 49.82816123962402
Epoch: 5, Steps: 318 | Train Loss: 0.1744705 Vali Loss: 0.1848068 Test Loss: 0.1684190
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1645046
	speed: 0.1683s/iter; left time: 250.9711s
	iters: 200, epoch: 6 | loss: 0.1847913
	speed: 0.1542s/iter; left time: 214.5005s
	iters: 300, epoch: 6 | loss: 0.2017181
	speed: 0.1625s/iter; left time: 209.8369s
Epoch: 6 cost time: 51.351436614990234
Epoch: 6, Steps: 318 | Train Loss: 0.1735623 Vali Loss: 0.1855889 Test Loss: 0.1700078
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.37301498651504517, mae:0.4698365330696106, rmse:0.6107495427131653, mape:0.016828209161758423, mspe:0.0004868096439167857, rse:0.42749467492103577, r2_score:0.7943693693417606, acc:0.9831717908382416
corr: [36.571884 36.51012  36.605125 36.436405 36.498272 36.609905 36.710117
 36.757626 37.151073 36.720665 36.738297 36.729244 36.70973  36.91359
 36.90878  36.984592 36.927307 37.198154 37.484715 37.44187 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2793277
	speed: 0.1216s/iter; left time: 374.6306s
	iters: 200, epoch: 1 | loss: 0.2468447
	speed: 0.1024s/iter; left time: 305.1425s
	iters: 300, epoch: 1 | loss: 0.2393491
	speed: 0.1226s/iter; left time: 353.3414s
Epoch: 1 cost time: 36.93375205993652
Epoch: 1, Steps: 318 | Train Loss: 0.3348610 Vali Loss: 0.2084761 Test Loss: 0.2037507
Validation loss decreased (inf --> 0.208476).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1939653
	speed: 0.1305s/iter; left time: 360.6548s
	iters: 200, epoch: 2 | loss: 0.1478964
	speed: 0.1096s/iter; left time: 291.9125s
	iters: 300, epoch: 2 | loss: 0.2000129
	speed: 0.1159s/iter; left time: 297.0003s
Epoch: 2 cost time: 37.67023515701294
Epoch: 2, Steps: 318 | Train Loss: 0.2033189 Vali Loss: 0.1942623 Test Loss: 0.1930256
Validation loss decreased (0.208476 --> 0.194262).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2150034
	speed: 0.1212s/iter; left time: 296.2805s
	iters: 200, epoch: 3 | loss: 0.1558394
	speed: 0.1240s/iter; left time: 290.7183s
	iters: 300, epoch: 3 | loss: 0.2171716
	speed: 0.1290s/iter; left time: 289.6031s
Epoch: 3 cost time: 39.714433431625366
Epoch: 3, Steps: 318 | Train Loss: 0.1883901 Vali Loss: 0.1938277 Test Loss: 0.1926344
Validation loss decreased (0.194262 --> 0.193828).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1727670
	speed: 0.1312s/iter; left time: 279.0030s
	iters: 200, epoch: 4 | loss: 0.1390736
	speed: 0.1192s/iter; left time: 241.5706s
	iters: 300, epoch: 4 | loss: 0.1582497
	speed: 0.1328s/iter; left time: 255.8125s
Epoch: 4 cost time: 40.791173219680786
Epoch: 4, Steps: 318 | Train Loss: 0.1820403 Vali Loss: 0.1929744 Test Loss: 0.1956561
Validation loss decreased (0.193828 --> 0.192974).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1829252
	speed: 0.1272s/iter; left time: 230.1740s
	iters: 200, epoch: 5 | loss: 0.1819208
	speed: 0.1271s/iter; left time: 217.1826s
	iters: 300, epoch: 5 | loss: 0.2131880
	speed: 0.1212s/iter; left time: 195.0063s
Epoch: 5 cost time: 40.003746032714844
Epoch: 5, Steps: 318 | Train Loss: 0.1791134 Vali Loss: 0.1947049 Test Loss: 0.1943546
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1698480
	speed: 0.1243s/iter; left time: 185.3115s
	iters: 200, epoch: 6 | loss: 0.1563377
	speed: 0.1342s/iter; left time: 186.6290s
	iters: 300, epoch: 6 | loss: 0.1962719
	speed: 0.1317s/iter; left time: 170.0363s
Epoch: 6 cost time: 41.315502405166626
Epoch: 6, Steps: 318 | Train Loss: 0.1771051 Vali Loss: 0.1954425 Test Loss: 0.1926945
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1302273
	speed: 0.1257s/iter; left time: 147.4221s
	iters: 200, epoch: 7 | loss: 0.1526038
	speed: 0.1290s/iter; left time: 138.4457s
	iters: 300, epoch: 7 | loss: 0.1646512
	speed: 0.1232s/iter; left time: 119.8669s
Epoch: 7 cost time: 40.15093779563904
Epoch: 7, Steps: 318 | Train Loss: 0.1768563 Vali Loss: 0.1952466 Test Loss: 0.1943380
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.4293597638607025, mae:0.5072591304779053, rmse:0.6552554965019226, mape:0.018179569393396378, mspe:0.0005617833230644464, rse:0.45834028720855713, r2_score:0.7767833697198645, acc:0.9818204306066036
corr: [36.83813  36.832302 36.586452 36.79684  37.011703 36.732826 36.753872
 36.851425 36.771072 37.018917 37.02299  37.07206  36.993748 36.956425
 37.05909  37.095715 37.159805 37.004204 36.755318 36.657887 37.31357
 37.107586 37.187935 37.67954  37.46664 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3950776
	speed: 0.1558s/iter; left time: 478.4664s
	iters: 200, epoch: 1 | loss: 0.2600973
	speed: 0.1398s/iter; left time: 415.4746s
	iters: 300, epoch: 1 | loss: 0.2585918
	speed: 0.1534s/iter; left time: 440.4958s
Epoch: 1 cost time: 47.24704098701477
Epoch: 1, Steps: 317 | Train Loss: 0.3475126 Vali Loss: 0.2322214 Test Loss: 0.2370168
Validation loss decreased (inf --> 0.232221).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2423571
	speed: 0.1574s/iter; left time: 433.4992s
	iters: 200, epoch: 2 | loss: 0.2260541
	speed: 0.1630s/iter; left time: 432.7200s
	iters: 300, epoch: 2 | loss: 0.1710206
	speed: 0.1481s/iter; left time: 378.2547s
Epoch: 2 cost time: 49.56310510635376
Epoch: 2, Steps: 317 | Train Loss: 0.2308497 Vali Loss: 0.2208519 Test Loss: 0.2103069
Validation loss decreased (0.232221 --> 0.220852).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2284556
	speed: 0.1649s/iter; left time: 401.7590s
	iters: 200, epoch: 3 | loss: 0.1673270
	speed: 0.1467s/iter; left time: 342.7878s
	iters: 300, epoch: 3 | loss: 0.1959939
	speed: 0.1449s/iter; left time: 324.1509s
Epoch: 3 cost time: 48.12766194343567
Epoch: 3, Steps: 317 | Train Loss: 0.2119487 Vali Loss: 0.2230356 Test Loss: 0.2097742
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2672309
	speed: 0.1675s/iter; left time: 355.1082s
	iters: 200, epoch: 4 | loss: 0.2042937
	speed: 0.1608s/iter; left time: 324.8219s
	iters: 300, epoch: 4 | loss: 0.2181771
	speed: 0.1457s/iter; left time: 279.8185s
Epoch: 4 cost time: 49.910478591918945
Epoch: 4, Steps: 317 | Train Loss: 0.2048828 Vali Loss: 0.2209393 Test Loss: 0.2078887
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2325674
	speed: 0.1646s/iter; left time: 296.7813s
	iters: 200, epoch: 5 | loss: 0.2249957
	speed: 0.1535s/iter; left time: 261.3991s
	iters: 300, epoch: 5 | loss: 0.2135451
	speed: 0.1522s/iter; left time: 244.0158s
Epoch: 5 cost time: 49.87524962425232
Epoch: 5, Steps: 317 | Train Loss: 0.2013734 Vali Loss: 0.2233105 Test Loss: 0.2051687
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000031
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.46151021122932434, mae:0.5282021760940552, rmse:0.679345428943634, mape:0.018889032304286957, mspe:0.0006007043994031847, rse:0.4748496115207672, r2_score:0.7252681714263027, acc:0.981110967695713
corr: [36.769775 36.75514  36.74651  36.84413  36.61589  36.782833 36.60641
 36.76389  37.02855  36.72669  36.543846 36.6729   36.693516 36.925533
 36.676716 36.790302 36.775784 37.088715 36.971767 37.252655 36.87358
 37.18581  37.08121  37.26227  37.18735  37.45578  37.44136  37.04774
 37.475475 37.52105 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4624446
	speed: 0.1060s/iter; left time: 326.7302s
	iters: 200, epoch: 1 | loss: 0.2242600
	speed: 0.0745s/iter; left time: 222.0509s
	iters: 300, epoch: 1 | loss: 0.3902444
	speed: 0.0722s/iter; left time: 207.8928s
Epoch: 1 cost time: 26.69431233406067
Epoch: 1, Steps: 318 | Train Loss: 0.4127320 Vali Loss: 0.2079654 Test Loss: 0.2776296
Validation loss decreased (inf --> 0.207965).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1829051
	speed: 0.0793s/iter; left time: 219.1875s
	iters: 200, epoch: 2 | loss: 0.2147512
	speed: 0.0719s/iter; left time: 191.5729s
	iters: 300, epoch: 2 | loss: 0.1595434
	speed: 0.0655s/iter; left time: 167.8815s
Epoch: 2 cost time: 23.24993324279785
Epoch: 2, Steps: 318 | Train Loss: 0.2217509 Vali Loss: 0.1929624 Test Loss: 0.2433673
Validation loss decreased (0.207965 --> 0.192962).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1990953
	speed: 0.0720s/iter; left time: 175.9320s
	iters: 200, epoch: 3 | loss: 0.1820060
	speed: 0.0769s/iter; left time: 180.3474s
	iters: 300, epoch: 3 | loss: 0.2056090
	speed: 0.0698s/iter; left time: 156.6486s
Epoch: 3 cost time: 23.23493528366089
Epoch: 3, Steps: 318 | Train Loss: 0.1996990 Vali Loss: 0.1825539 Test Loss: 0.2281533
Validation loss decreased (0.192962 --> 0.182554).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2054643
	speed: 0.0929s/iter; left time: 197.5176s
	iters: 200, epoch: 4 | loss: 0.1464713
	speed: 0.0781s/iter; left time: 158.3852s
	iters: 300, epoch: 4 | loss: 0.1557005
	speed: 0.0719s/iter; left time: 138.5836s
Epoch: 4 cost time: 25.64780068397522
Epoch: 4, Steps: 318 | Train Loss: 0.1890984 Vali Loss: 0.1803093 Test Loss: 0.2227112
Validation loss decreased (0.182554 --> 0.180309).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1905587
	speed: 0.0846s/iter; left time: 153.0615s
	iters: 200, epoch: 5 | loss: 0.1652706
	speed: 0.0730s/iter; left time: 124.6878s
	iters: 300, epoch: 5 | loss: 0.1906009
	speed: 0.0737s/iter; left time: 118.6352s
Epoch: 5 cost time: 24.46728825569153
Epoch: 5, Steps: 318 | Train Loss: 0.1861477 Vali Loss: 0.1804752 Test Loss: 0.2200229
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1631716
	speed: 0.0905s/iter; left time: 134.8917s
	iters: 200, epoch: 6 | loss: 0.1464110
	speed: 0.0778s/iter; left time: 108.1943s
	iters: 300, epoch: 6 | loss: 0.2070644
	speed: 0.0710s/iter; left time: 91.6153s
Epoch: 6 cost time: 24.977351427078247
Epoch: 6, Steps: 318 | Train Loss: 0.1842787 Vali Loss: 0.1812081 Test Loss: 0.2182944
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2210193
	speed: 0.0930s/iter; left time: 109.0322s
	iters: 200, epoch: 7 | loss: 0.1654260
	speed: 0.0783s/iter; left time: 83.9969s
	iters: 300, epoch: 7 | loss: 0.2039832
	speed: 0.0707s/iter; left time: 68.8079s
Epoch: 7 cost time: 25.651015758514404
Epoch: 7, Steps: 318 | Train Loss: 0.1838494 Vali Loss: 0.1798266 Test Loss: 0.2177644
Validation loss decreased (0.180309 --> 0.179827).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1870178
	speed: 0.0849s/iter; left time: 72.6091s
	iters: 200, epoch: 8 | loss: 0.1915393
	speed: 0.0733s/iter; left time: 55.3669s
	iters: 300, epoch: 8 | loss: 0.1606935
	speed: 0.0712s/iter; left time: 46.6434s
Epoch: 8 cost time: 24.50557279586792
Epoch: 8, Steps: 318 | Train Loss: 0.1826179 Vali Loss: 0.1798171 Test Loss: 0.2173932
Validation loss decreased (0.179827 --> 0.179817).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1865902
	speed: 0.0806s/iter; left time: 43.2590s
	iters: 200, epoch: 9 | loss: 0.1505420
	speed: 0.0749s/iter; left time: 32.7506s
	iters: 300, epoch: 9 | loss: 0.1772487
	speed: 0.0688s/iter; left time: 23.2010s
Epoch: 9 cost time: 23.596933364868164
Epoch: 9, Steps: 318 | Train Loss: 0.1830383 Vali Loss: 0.1787431 Test Loss: 0.2172582
Validation loss decreased (0.179817 --> 0.178743).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2143735
	speed: 0.0783s/iter; left time: 17.1510s
	iters: 200, epoch: 10 | loss: 0.2277038
	speed: 0.0758s/iter; left time: 9.0211s
	iters: 300, epoch: 10 | loss: 0.1627155
	speed: 0.0751s/iter; left time: 1.4263s
Epoch: 10 cost time: 24.224857568740845
Epoch: 10, Steps: 318 | Train Loss: 0.1822848 Vali Loss: 0.1818045 Test Loss: 0.2168038
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.4767647087574005, mae:0.5416671633720398, rmse:0.6904814839363098, mape:0.01940477266907692, mspe:0.0006195303285494447, rse:0.48381105065345764, r2_score:0.6816930431816177, acc:0.9805952273309231
corr: [37.47322  37.072025 37.192036 37.61347  37.255104 37.59455  37.341877
 37.41627  37.731422 37.559616]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4428348
	speed: 0.1138s/iter; left time: 350.7233s
	iters: 200, epoch: 1 | loss: 0.2052713
	speed: 0.0818s/iter; left time: 243.8080s
	iters: 300, epoch: 1 | loss: 0.2669886
	speed: 0.0847s/iter; left time: 244.0099s
Epoch: 1 cost time: 29.53981351852417
Epoch: 1, Steps: 318 | Train Loss: 0.4417219 Vali Loss: 0.2902301 Test Loss: 0.2818673
Validation loss decreased (inf --> 0.290230).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2939131
	speed: 0.0900s/iter; left time: 248.6557s
	iters: 200, epoch: 2 | loss: 0.2971058
	speed: 0.0677s/iter; left time: 180.2604s
	iters: 300, epoch: 2 | loss: 0.2715925
	speed: 0.0817s/iter; left time: 209.4294s
Epoch: 2 cost time: 25.513304471969604
Epoch: 2, Steps: 318 | Train Loss: 0.2807748 Vali Loss: 0.2801760 Test Loss: 0.2662877
Validation loss decreased (0.290230 --> 0.280176).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2772408
	speed: 0.0866s/iter; left time: 211.7422s
	iters: 200, epoch: 3 | loss: 0.2271783
	speed: 0.0779s/iter; left time: 182.6855s
	iters: 300, epoch: 3 | loss: 0.1917826
	speed: 0.0766s/iter; left time: 172.0561s
Epoch: 3 cost time: 25.765526294708252
Epoch: 3, Steps: 318 | Train Loss: 0.2642803 Vali Loss: 0.2704305 Test Loss: 0.2600879
Validation loss decreased (0.280176 --> 0.270431).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.3295643
	speed: 0.0799s/iter; left time: 170.0315s
	iters: 200, epoch: 4 | loss: 0.2799547
	speed: 0.0758s/iter; left time: 153.5673s
	iters: 300, epoch: 4 | loss: 0.2743770
	speed: 0.0761s/iter; left time: 146.5484s
Epoch: 4 cost time: 24.659992456436157
Epoch: 4, Steps: 318 | Train Loss: 0.2563786 Vali Loss: 0.2670255 Test Loss: 0.2594974
Validation loss decreased (0.270431 --> 0.267025).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2253667
	speed: 0.0741s/iter; left time: 134.1046s
	iters: 200, epoch: 5 | loss: 0.2080032
	speed: 0.0739s/iter; left time: 126.2961s
	iters: 300, epoch: 5 | loss: 0.2316116
	speed: 0.0728s/iter; left time: 117.0670s
Epoch: 5 cost time: 23.33450484275818
Epoch: 5, Steps: 318 | Train Loss: 0.2529951 Vali Loss: 0.2655371 Test Loss: 0.2574853
Validation loss decreased (0.267025 --> 0.265537).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2584679
	speed: 0.0888s/iter; left time: 132.3402s
	iters: 200, epoch: 6 | loss: 0.2505218
	speed: 0.0709s/iter; left time: 98.6738s
	iters: 300, epoch: 6 | loss: 0.2115741
	speed: 0.0741s/iter; left time: 95.6735s
Epoch: 6 cost time: 24.81742286682129
Epoch: 6, Steps: 318 | Train Loss: 0.2511428 Vali Loss: 0.2630958 Test Loss: 0.2564264
Validation loss decreased (0.265537 --> 0.263096).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2363385
	speed: 0.0860s/iter; left time: 100.9221s
	iters: 200, epoch: 7 | loss: 0.2131111
	speed: 0.0767s/iter; left time: 82.3390s
	iters: 300, epoch: 7 | loss: 0.2363935
	speed: 0.0835s/iter; left time: 81.2915s
Epoch: 7 cost time: 26.393534660339355
Epoch: 7, Steps: 318 | Train Loss: 0.2499526 Vali Loss: 0.2652249 Test Loss: 0.2560537
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2379041
	speed: 0.0853s/iter; left time: 72.9443s
	iters: 200, epoch: 8 | loss: 0.2904619
	speed: 0.0779s/iter; left time: 58.7849s
	iters: 300, epoch: 8 | loss: 0.3009658
	speed: 0.0757s/iter; left time: 49.5932s
Epoch: 8 cost time: 25.080846309661865
Epoch: 8, Steps: 318 | Train Loss: 0.2496186 Vali Loss: 0.2658055 Test Loss: 0.2558180
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2072014
	speed: 0.0876s/iter; left time: 47.0344s
	iters: 200, epoch: 9 | loss: 0.2216322
	speed: 0.0777s/iter; left time: 33.9509s
	iters: 300, epoch: 9 | loss: 0.2233162
	speed: 0.0733s/iter; left time: 24.6933s
Epoch: 9 cost time: 25.2239511013031
Epoch: 9, Steps: 318 | Train Loss: 0.2490012 Vali Loss: 0.2655871 Test Loss: 0.2557844
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.5627177357673645, mae:0.5904438495635986, rmse:0.7501451373100281, mape:0.02117825858294964, mspe:0.0007391850813291967, rse:0.5253690481185913, r2_score:0.6570716608158432, acc:0.9788217414170504
corr: [37.135838 37.441013 37.681408 37.194824 37.685036 37.588913 37.544918
 37.564526 37.431313 37.75221  37.672504 37.88674  37.948498 38.142895
 38.40076 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4940571
	speed: 0.1418s/iter; left time: 436.8086s
	iters: 200, epoch: 1 | loss: 0.2455930
	speed: 0.0709s/iter; left time: 211.4193s
	iters: 300, epoch: 1 | loss: 0.3182809
	speed: 0.0826s/iter; left time: 237.8598s
Epoch: 1 cost time: 30.453690767288208
Epoch: 1, Steps: 318 | Train Loss: 0.4456500 Vali Loss: 0.2628941 Test Loss: 0.2819231
Validation loss decreased (inf --> 0.262894).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2696628
	speed: 0.1374s/iter; left time: 379.5859s
	iters: 200, epoch: 2 | loss: 0.2495946
	speed: 0.1086s/iter; left time: 289.0825s
	iters: 300, epoch: 2 | loss: 0.3451355
	speed: 0.1343s/iter; left time: 344.2328s
Epoch: 2 cost time: 40.403255462646484
Epoch: 2, Steps: 318 | Train Loss: 0.2661495 Vali Loss: 0.2202943 Test Loss: 0.2495234
Validation loss decreased (0.262894 --> 0.220294).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2754034
	speed: 0.1257s/iter; left time: 307.3497s
	iters: 200, epoch: 3 | loss: 0.2321374
	speed: 0.1191s/iter; left time: 279.1884s
	iters: 300, epoch: 3 | loss: 0.2324052
	speed: 0.1068s/iter; left time: 239.8508s
Epoch: 3 cost time: 36.680514335632324
Epoch: 3, Steps: 318 | Train Loss: 0.2455464 Vali Loss: 0.2062189 Test Loss: 0.2399388
Validation loss decreased (0.220294 --> 0.206219).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2030701
	speed: 0.1317s/iter; left time: 280.1517s
	iters: 200, epoch: 4 | loss: 0.3219281
	speed: 0.1109s/iter; left time: 224.7524s
	iters: 300, epoch: 4 | loss: 0.2118902
	speed: 0.1249s/iter; left time: 240.6100s
Epoch: 4 cost time: 38.85031700134277
Epoch: 4, Steps: 318 | Train Loss: 0.2357544 Vali Loss: 0.2028551 Test Loss: 0.2358840
Validation loss decreased (0.206219 --> 0.202855).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1750090
	speed: 0.1323s/iter; left time: 239.3437s
	iters: 200, epoch: 5 | loss: 0.2147540
	speed: 0.1084s/iter; left time: 185.2289s
	iters: 300, epoch: 5 | loss: 0.2053252
	speed: 0.1088s/iter; left time: 175.0650s
Epoch: 5 cost time: 35.84995985031128
Epoch: 5, Steps: 318 | Train Loss: 0.2316239 Vali Loss: 0.1995454 Test Loss: 0.2344037
Validation loss decreased (0.202855 --> 0.199545).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1899755
	speed: 0.1316s/iter; left time: 196.2224s
	iters: 200, epoch: 6 | loss: 0.2422064
	speed: 0.0940s/iter; left time: 130.7528s
	iters: 300, epoch: 6 | loss: 0.2028404
	speed: 0.0704s/iter; left time: 90.9024s
Epoch: 6 cost time: 30.640859127044678
Epoch: 6, Steps: 318 | Train Loss: 0.2298525 Vali Loss: 0.1983568 Test Loss: 0.2330506
Validation loss decreased (0.199545 --> 0.198357).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1886657
	speed: 0.1177s/iter; left time: 138.0547s
	iters: 200, epoch: 7 | loss: 0.2556505
	speed: 0.0963s/iter; left time: 103.3342s
	iters: 300, epoch: 7 | loss: 0.2541113
	speed: 0.1060s/iter; left time: 103.1490s
Epoch: 7 cost time: 34.011521100997925
Epoch: 7, Steps: 318 | Train Loss: 0.2287365 Vali Loss: 0.1986124 Test Loss: 0.2321282
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2160040
	speed: 0.1263s/iter; left time: 107.9751s
	iters: 200, epoch: 8 | loss: 0.2836890
	speed: 0.0774s/iter; left time: 58.4359s
	iters: 300, epoch: 8 | loss: 0.2653865
	speed: 0.1109s/iter; left time: 72.6208s
Epoch: 8 cost time: 33.36500000953674
Epoch: 8, Steps: 318 | Train Loss: 0.2279004 Vali Loss: 0.1967718 Test Loss: 0.2318979
Validation loss decreased (0.198357 --> 0.196772).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.3201009
	speed: 0.1262s/iter; left time: 67.7544s
	iters: 200, epoch: 9 | loss: 0.2991725
	speed: 0.0907s/iter; left time: 39.6216s
	iters: 300, epoch: 9 | loss: 0.2048809
	speed: 0.1028s/iter; left time: 34.6561s
Epoch: 9 cost time: 32.69496536254883
Epoch: 9, Steps: 318 | Train Loss: 0.2275398 Vali Loss: 0.1969560 Test Loss: 0.2317102
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1907789
	speed: 0.1223s/iter; left time: 26.7811s
	iters: 200, epoch: 10 | loss: 0.2963503
	speed: 0.0834s/iter; left time: 9.9276s
	iters: 300, epoch: 10 | loss: 0.2491432
	speed: 0.1209s/iter; left time: 2.2975s
Epoch: 10 cost time: 35.17049765586853
Epoch: 10, Steps: 318 | Train Loss: 0.2274259 Vali Loss: 0.1970429 Test Loss: 0.2316571
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.5088908672332764, mae:0.5599343180656433, rmse:0.7133658528327942, mape:0.02003704011440277, mspe:0.0006632545264437795, rse:0.4993211030960083, r2_score:0.7055762668737176, acc:0.9799629598855972
corr: [37.039337 37.150444 37.14529  37.090717 37.312008 36.983475 37.11406
 37.22247  37.097366 37.123894 37.166107 37.13228  37.111385 37.387955
 37.284126 37.205387 37.393047 37.76082  37.749992 37.797104]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.6151116
	speed: 0.0983s/iter; left time: 302.8609s
	iters: 200, epoch: 1 | loss: 0.3003410
	speed: 0.0709s/iter; left time: 211.2267s
	iters: 300, epoch: 1 | loss: 0.4387392
	speed: 0.0710s/iter; left time: 204.4107s
Epoch: 1 cost time: 25.590729475021362
Epoch: 1, Steps: 318 | Train Loss: 0.5301546 Vali Loss: 0.3389025 Test Loss: 0.2908275
Validation loss decreased (inf --> 0.338903).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.3271572
	speed: 0.0761s/iter; left time: 210.3141s
	iters: 200, epoch: 2 | loss: 0.2669605
	speed: 0.0906s/iter; left time: 241.3680s
	iters: 300, epoch: 2 | loss: 0.2721871
	speed: 0.0772s/iter; left time: 197.7638s
Epoch: 2 cost time: 25.772308349609375
Epoch: 2, Steps: 318 | Train Loss: 0.3075620 Vali Loss: 0.2980926 Test Loss: 0.2724783
Validation loss decreased (0.338903 --> 0.298093).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2402979
	speed: 0.0869s/iter; left time: 212.4169s
	iters: 200, epoch: 3 | loss: 0.2907048
	speed: 0.0731s/iter; left time: 171.3631s
	iters: 300, epoch: 3 | loss: 0.2325373
	speed: 0.0734s/iter; left time: 164.7340s
Epoch: 3 cost time: 24.75682830810547
Epoch: 3, Steps: 318 | Train Loss: 0.2883953 Vali Loss: 0.2900698 Test Loss: 0.2658134
Validation loss decreased (0.298093 --> 0.290070).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2750619
	speed: 0.0832s/iter; left time: 176.9493s
	iters: 200, epoch: 4 | loss: 0.2845943
	speed: 0.0706s/iter; left time: 143.0993s
	iters: 300, epoch: 4 | loss: 0.2031976
	speed: 0.0735s/iter; left time: 141.7264s
Epoch: 4 cost time: 24.229705333709717
Epoch: 4, Steps: 318 | Train Loss: 0.2803725 Vali Loss: 0.2835282 Test Loss: 0.2622102
Validation loss decreased (0.290070 --> 0.283528).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.3250842
	speed: 0.0909s/iter; left time: 164.4846s
	iters: 200, epoch: 5 | loss: 0.2193967
	speed: 0.0725s/iter; left time: 123.9481s
	iters: 300, epoch: 5 | loss: 0.2922878
	speed: 0.0773s/iter; left time: 124.3056s
Epoch: 5 cost time: 25.453752994537354
Epoch: 5, Steps: 318 | Train Loss: 0.2766440 Vali Loss: 0.2832793 Test Loss: 0.2608717
Validation loss decreased (0.283528 --> 0.283279).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2538588
	speed: 0.0938s/iter; left time: 139.8790s
	iters: 200, epoch: 6 | loss: 0.2306440
	speed: 0.0737s/iter; left time: 102.5606s
	iters: 300, epoch: 6 | loss: 0.2526689
	speed: 0.0752s/iter; left time: 97.0363s
Epoch: 6 cost time: 25.485119581222534
Epoch: 6, Steps: 318 | Train Loss: 0.2747547 Vali Loss: 0.2808614 Test Loss: 0.2601919
Validation loss decreased (0.283279 --> 0.280861).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2807597
	speed: 0.0804s/iter; left time: 94.2751s
	iters: 200, epoch: 7 | loss: 0.2883041
	speed: 0.0725s/iter; left time: 77.7662s
	iters: 300, epoch: 7 | loss: 0.2791458
	speed: 0.0679s/iter; left time: 66.1109s
Epoch: 7 cost time: 23.309627056121826
Epoch: 7, Steps: 318 | Train Loss: 0.2736181 Vali Loss: 0.2811369 Test Loss: 0.2601709
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2627710
	speed: 0.0938s/iter; left time: 80.2086s
	iters: 200, epoch: 8 | loss: 0.3000474
	speed: 0.0756s/iter; left time: 57.1001s
	iters: 300, epoch: 8 | loss: 0.2602986
	speed: 0.0727s/iter; left time: 47.6074s
Epoch: 8 cost time: 25.070531606674194
Epoch: 8, Steps: 318 | Train Loss: 0.2728571 Vali Loss: 0.2811994 Test Loss: 0.2601197
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2871068
	speed: 0.0873s/iter; left time: 46.8666s
	iters: 200, epoch: 9 | loss: 0.2946033
	speed: 0.0726s/iter; left time: 31.7142s
	iters: 300, epoch: 9 | loss: 0.2638644
	speed: 0.0741s/iter; left time: 24.9795s
Epoch: 9 cost time: 24.44603180885315
Epoch: 9, Steps: 318 | Train Loss: 0.2730616 Vali Loss: 0.2799393 Test Loss: 0.2600392
Validation loss decreased (0.280861 --> 0.279939).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2220309
	speed: 0.0885s/iter; left time: 19.3888s
	iters: 200, epoch: 10 | loss: 0.2701386
	speed: 0.0716s/iter; left time: 8.5240s
	iters: 300, epoch: 10 | loss: 0.2333257
	speed: 0.0769s/iter; left time: 1.4617s
Epoch: 10 cost time: 25.094341039657593
Epoch: 10, Steps: 318 | Train Loss: 0.2724522 Vali Loss: 0.2814864 Test Loss: 0.2599725
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.5706457495689392, mae:0.5943667888641357, rmse:0.7554109692573547, mape:0.02135440893471241, mspe:0.000752404797822237, rse:0.5283974409103394, r2_score:0.6791800029492433, acc:0.9786455910652876
corr: [37.884335 37.846302 37.82586  38.036568 37.789173 38.040245 38.090767
 37.977913 38.099415 37.977978 38.02964  37.963818 38.201496 38.16643
 38.037502 38.140846 38.126514 38.126545 38.184994 38.070305 38.060368
 37.971058 37.91293  37.659206 37.48105 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4457798
	speed: 0.1204s/iter; left time: 369.7600s
	iters: 200, epoch: 1 | loss: 0.3398535
	speed: 0.0817s/iter; left time: 242.7247s
	iters: 300, epoch: 1 | loss: 0.2658471
	speed: 0.0804s/iter; left time: 230.7618s
Epoch: 1 cost time: 29.30293607711792
Epoch: 1, Steps: 317 | Train Loss: 0.4745854 Vali Loss: 0.3344432 Test Loss: 0.3136510
Validation loss decreased (inf --> 0.334443).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2976786
	speed: 0.0851s/iter; left time: 234.3088s
	iters: 200, epoch: 2 | loss: 0.3736292
	speed: 0.0838s/iter; left time: 222.5261s
	iters: 300, epoch: 2 | loss: 0.2944429
	speed: 0.0839s/iter; left time: 214.3120s
Epoch: 2 cost time: 26.498758554458618
Epoch: 2, Steps: 317 | Train Loss: 0.3111445 Vali Loss: 0.3150878 Test Loss: 0.2914745
Validation loss decreased (0.334443 --> 0.315088).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.3392736
	speed: 0.0875s/iter; left time: 213.1844s
	iters: 200, epoch: 3 | loss: 0.3272127
	speed: 0.0786s/iter; left time: 183.5885s
	iters: 300, epoch: 3 | loss: 0.2693751
	speed: 0.0718s/iter; left time: 160.6690s
Epoch: 3 cost time: 25.14075493812561
Epoch: 3, Steps: 317 | Train Loss: 0.2999122 Vali Loss: 0.3021367 Test Loss: 0.2823404
Validation loss decreased (0.315088 --> 0.302137).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.3258256
	speed: 0.0850s/iter; left time: 180.2592s
	iters: 200, epoch: 4 | loss: 0.2725264
	speed: 0.0708s/iter; left time: 143.0789s
	iters: 300, epoch: 4 | loss: 0.2857250
	speed: 0.0777s/iter; left time: 149.1874s
Epoch: 4 cost time: 24.7446768283844
Epoch: 4, Steps: 317 | Train Loss: 0.2945272 Vali Loss: 0.2995911 Test Loss: 0.2797899
Validation loss decreased (0.302137 --> 0.299591).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2457887
	speed: 0.0806s/iter; left time: 145.2816s
	iters: 200, epoch: 5 | loss: 0.3381470
	speed: 0.0726s/iter; left time: 123.5949s
	iters: 300, epoch: 5 | loss: 0.2875922
	speed: 0.0771s/iter; left time: 123.5882s
Epoch: 5 cost time: 24.35614037513733
Epoch: 5, Steps: 317 | Train Loss: 0.2918413 Vali Loss: 0.2963453 Test Loss: 0.2765391
Validation loss decreased (0.299591 --> 0.296345).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2743213
	speed: 0.0863s/iter; left time: 128.2004s
	iters: 200, epoch: 6 | loss: 0.3501205
	speed: 0.0774s/iter; left time: 107.2388s
	iters: 300, epoch: 6 | loss: 0.2862699
	speed: 0.0763s/iter; left time: 98.1500s
Epoch: 6 cost time: 25.318726778030396
Epoch: 6, Steps: 317 | Train Loss: 0.2900715 Vali Loss: 0.2959016 Test Loss: 0.2762530
Validation loss decreased (0.296345 --> 0.295902).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2647620
	speed: 0.0836s/iter; left time: 97.7768s
	iters: 200, epoch: 7 | loss: 0.3267208
	speed: 0.0778s/iter; left time: 83.1398s
	iters: 300, epoch: 7 | loss: 0.3277237
	speed: 0.0790s/iter; left time: 76.5096s
Epoch: 7 cost time: 25.038819551467896
Epoch: 7, Steps: 317 | Train Loss: 0.2897594 Vali Loss: 0.2956075 Test Loss: 0.2759824
Validation loss decreased (0.295902 --> 0.295607).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.3471347
	speed: 0.0872s/iter; left time: 74.2666s
	iters: 200, epoch: 8 | loss: 0.3027775
	speed: 0.0808s/iter; left time: 60.7949s
	iters: 300, epoch: 8 | loss: 0.2374443
	speed: 0.0745s/iter; left time: 48.5897s
Epoch: 8 cost time: 25.82646679878235
Epoch: 8, Steps: 317 | Train Loss: 0.2892834 Vali Loss: 0.2943422 Test Loss: 0.2756693
Validation loss decreased (0.295607 --> 0.294342).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.3536690
	speed: 0.0857s/iter; left time: 45.8519s
	iters: 200, epoch: 9 | loss: 0.2518437
	speed: 0.0762s/iter; left time: 33.1605s
	iters: 300, epoch: 9 | loss: 0.2572747
	speed: 0.0785s/iter; left time: 26.2861s
Epoch: 9 cost time: 25.434934854507446
Epoch: 9, Steps: 317 | Train Loss: 0.2890569 Vali Loss: 0.2949734 Test Loss: 0.2755889
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.4121128
	speed: 0.0861s/iter; left time: 18.7661s
	iters: 200, epoch: 10 | loss: 0.2796417
	speed: 0.0784s/iter; left time: 9.2559s
	iters: 300, epoch: 10 | loss: 0.2704360
	speed: 0.0807s/iter; left time: 1.4529s
Epoch: 10 cost time: 25.661421060562134
Epoch: 10, Steps: 317 | Train Loss: 0.2885727 Vali Loss: 0.2945460 Test Loss: 0.2755320
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.6049455404281616, mae:0.6233471632003784, rmse:0.7777824401855469, mape:0.022314254194498062, mspe:0.0007864394574426115, rse:0.543655276298523, r2_score:0.6265379297480764, acc:0.9776857458055019
corr: [37.895683 37.357452 37.885704 38.405334 37.7172   38.10185  38.496456
 38.204914 38.518677 38.499855 38.394176 38.36154  38.48802  38.31361
 37.874172 38.210453 38.175663 38.576782 38.581673 38.3092   38.035713
 38.14946  38.51417  38.525017 38.2136   38.179176 38.413433 38.760086
 38.910885 38.98537 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4408253
	speed: 0.1198s/iter; left time: 369.1950s
	iters: 200, epoch: 1 | loss: 0.2068758
	speed: 0.0917s/iter; left time: 273.4298s
	iters: 300, epoch: 1 | loss: 0.2695160
	speed: 0.1071s/iter; left time: 308.6260s
Epoch: 1 cost time: 34.265249252319336
Epoch: 1, Steps: 318 | Train Loss: 0.3678194 Vali Loss: 0.1898770 Test Loss: 0.1996013
Validation loss decreased (inf --> 0.189877).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1660710
	speed: 0.1050s/iter; left time: 290.1854s
	iters: 200, epoch: 2 | loss: 0.2173455
	speed: 0.0958s/iter; left time: 255.1869s
	iters: 300, epoch: 2 | loss: 0.1524017
	speed: 0.0910s/iter; left time: 233.1587s
Epoch: 2 cost time: 30.929173231124878
Epoch: 2, Steps: 318 | Train Loss: 0.1966803 Vali Loss: 0.1756894 Test Loss: 0.1689875
Validation loss decreased (0.189877 --> 0.175689).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1659679
	speed: 0.1063s/iter; left time: 259.8626s
	iters: 200, epoch: 3 | loss: 0.1805956
	speed: 0.0995s/iter; left time: 233.2615s
	iters: 300, epoch: 3 | loss: 0.1760663
	speed: 0.0863s/iter; left time: 193.7103s
Epoch: 3 cost time: 30.77955651283264
Epoch: 3, Steps: 318 | Train Loss: 0.1786735 Vali Loss: 0.1704632 Test Loss: 0.1571604
Validation loss decreased (0.175689 --> 0.170463).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1802682
	speed: 0.1004s/iter; left time: 213.5552s
	iters: 200, epoch: 4 | loss: 0.1362765
	speed: 0.0942s/iter; left time: 190.9389s
	iters: 300, epoch: 4 | loss: 0.1602179
	speed: 0.0979s/iter; left time: 188.6989s
Epoch: 4 cost time: 31.308446407318115
Epoch: 4, Steps: 318 | Train Loss: 0.1711024 Vali Loss: 0.1684946 Test Loss: 0.1546372
Validation loss decreased (0.170463 --> 0.168495).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1681474
	speed: 0.0992s/iter; left time: 179.4342s
	iters: 200, epoch: 5 | loss: 0.1577582
	speed: 0.0976s/iter; left time: 166.8680s
	iters: 300, epoch: 5 | loss: 0.1751619
	speed: 0.0991s/iter; left time: 159.4080s
Epoch: 5 cost time: 31.390668392181396
Epoch: 5, Steps: 318 | Train Loss: 0.1686535 Vali Loss: 0.1678202 Test Loss: 0.1521625
Validation loss decreased (0.168495 --> 0.167820).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1441431
	speed: 0.1047s/iter; left time: 156.0848s
	iters: 200, epoch: 6 | loss: 0.1461452
	speed: 0.0979s/iter; left time: 136.2213s
	iters: 300, epoch: 6 | loss: 0.1610521
	speed: 0.1057s/iter; left time: 136.4734s
Epoch: 6 cost time: 32.6826868057251
Epoch: 6, Steps: 318 | Train Loss: 0.1670855 Vali Loss: 0.1674608 Test Loss: 0.1518077
Validation loss decreased (0.167820 --> 0.167461).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1974913
	speed: 0.1077s/iter; left time: 126.3877s
	iters: 200, epoch: 7 | loss: 0.1564885
	speed: 0.0910s/iter; left time: 97.6689s
	iters: 300, epoch: 7 | loss: 0.2026265
	speed: 0.1015s/iter; left time: 98.7809s
Epoch: 7 cost time: 32.0035400390625
Epoch: 7, Steps: 318 | Train Loss: 0.1668644 Vali Loss: 0.1677376 Test Loss: 0.1517468
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1640456
	speed: 0.0995s/iter; left time: 85.1004s
	iters: 200, epoch: 8 | loss: 0.1666818
	speed: 0.0967s/iter; left time: 73.0166s
	iters: 300, epoch: 8 | loss: 0.1554154
	speed: 0.1007s/iter; left time: 65.9841s
Epoch: 8 cost time: 31.8590304851532
Epoch: 8, Steps: 318 | Train Loss: 0.1658862 Vali Loss: 0.1660066 Test Loss: 0.1519079
Validation loss decreased (0.167461 --> 0.166007).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1807868
	speed: 0.1107s/iter; left time: 59.4302s
	iters: 200, epoch: 9 | loss: 0.1340110
	speed: 0.0961s/iter; left time: 41.9900s
	iters: 300, epoch: 9 | loss: 0.1577584
	speed: 0.1051s/iter; left time: 35.4227s
Epoch: 9 cost time: 32.76864767074585
Epoch: 9, Steps: 318 | Train Loss: 0.1663401 Vali Loss: 0.1667973 Test Loss: 0.1517543
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1767182
	speed: 0.0959s/iter; left time: 21.0122s
	iters: 200, epoch: 10 | loss: 0.2029595
	speed: 0.0881s/iter; left time: 10.4894s
	iters: 300, epoch: 10 | loss: 0.1594309
	speed: 0.0950s/iter; left time: 1.8058s
Epoch: 10 cost time: 30.139182567596436
Epoch: 10, Steps: 318 | Train Loss: 0.1656238 Vali Loss: 0.1676547 Test Loss: 0.1516570
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.33335599303245544, mae:0.44897404313087463, rmse:0.5773698687553406, mape:0.01605970412492752, mspe:0.0004314567777328193, rse:0.4045552611351013, r2_score:0.8149590339219278, acc:0.9839402958750725
corr: [37.348824 37.340763 37.34133  37.29561  37.37502  37.64504  37.536526
 37.655872 37.93736  38.076134]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3765461
	speed: 0.1495s/iter; left time: 460.5228s
	iters: 200, epoch: 1 | loss: 0.1936045
	speed: 0.0891s/iter; left time: 265.7395s
	iters: 300, epoch: 1 | loss: 0.2113191
	speed: 0.1064s/iter; left time: 306.3976s
Epoch: 1 cost time: 36.32767081260681
Epoch: 1, Steps: 318 | Train Loss: 0.3634188 Vali Loss: 0.2149207 Test Loss: 0.2277347
Validation loss decreased (inf --> 0.214921).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2312870
	speed: 0.1005s/iter; left time: 277.7941s
	iters: 200, epoch: 2 | loss: 0.2280891
	speed: 0.0942s/iter; left time: 250.8491s
	iters: 300, epoch: 2 | loss: 0.1984949
	speed: 0.1039s/iter; left time: 266.1828s
Epoch: 2 cost time: 31.273388147354126
Epoch: 2, Steps: 318 | Train Loss: 0.2165770 Vali Loss: 0.2048769 Test Loss: 0.1912132
Validation loss decreased (0.214921 --> 0.204877).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1927883
	speed: 0.1021s/iter; left time: 249.5906s
	iters: 200, epoch: 3 | loss: 0.1966508
	speed: 0.0925s/iter; left time: 216.9442s
	iters: 300, epoch: 3 | loss: 0.1538177
	speed: 0.0881s/iter; left time: 197.7213s
Epoch: 3 cost time: 29.928228616714478
Epoch: 3, Steps: 318 | Train Loss: 0.1966994 Vali Loss: 0.2060333 Test Loss: 0.1893847
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2433819
	speed: 0.0972s/iter; left time: 206.8280s
	iters: 200, epoch: 4 | loss: 0.2245270
	speed: 0.0860s/iter; left time: 174.3521s
	iters: 300, epoch: 4 | loss: 0.1872258
	speed: 0.0994s/iter; left time: 191.4564s
Epoch: 4 cost time: 29.911919593811035
Epoch: 4, Steps: 318 | Train Loss: 0.1891530 Vali Loss: 0.2033961 Test Loss: 0.1947538
Validation loss decreased (0.204877 --> 0.203396).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1796015
	speed: 0.0926s/iter; left time: 167.5182s
	iters: 200, epoch: 5 | loss: 0.1704196
	speed: 0.0885s/iter; left time: 151.2013s
	iters: 300, epoch: 5 | loss: 0.1610793
	speed: 0.0873s/iter; left time: 140.4664s
Epoch: 5 cost time: 28.606053352355957
Epoch: 5, Steps: 318 | Train Loss: 0.1866185 Vali Loss: 0.2039943 Test Loss: 0.1939814
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1827419
	speed: 0.1010s/iter; left time: 150.5681s
	iters: 200, epoch: 6 | loss: 0.1993421
	speed: 0.0935s/iter; left time: 130.0288s
	iters: 300, epoch: 6 | loss: 0.1931315
	speed: 0.1024s/iter; left time: 132.1543s
Epoch: 6 cost time: 31.548073768615723
Epoch: 6, Steps: 318 | Train Loss: 0.1843935 Vali Loss: 0.2027037 Test Loss: 0.1913506
Validation loss decreased (0.203396 --> 0.202704).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1575263
	speed: 0.1006s/iter; left time: 117.9976s
	iters: 200, epoch: 7 | loss: 0.1767289
	speed: 0.0896s/iter; left time: 96.0948s
	iters: 300, epoch: 7 | loss: 0.1649755
	speed: 0.0940s/iter; left time: 91.4352s
Epoch: 7 cost time: 30.271207809448242
Epoch: 7, Steps: 318 | Train Loss: 0.1835860 Vali Loss: 0.2042374 Test Loss: 0.1915616
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1721570
	speed: 0.1008s/iter; left time: 86.1703s
	iters: 200, epoch: 8 | loss: 0.1936370
	speed: 0.0987s/iter; left time: 74.5341s
	iters: 300, epoch: 8 | loss: 0.2555176
	speed: 0.0949s/iter; left time: 62.1830s
Epoch: 8 cost time: 31.063374042510986
Epoch: 8, Steps: 318 | Train Loss: 0.1838671 Vali Loss: 0.2046350 Test Loss: 0.1913762
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1720927
	speed: 0.1015s/iter; left time: 54.5235s
	iters: 200, epoch: 9 | loss: 0.2221863
	speed: 0.0879s/iter; left time: 38.4129s
	iters: 300, epoch: 9 | loss: 0.1833019
	speed: 0.0984s/iter; left time: 33.1480s
Epoch: 9 cost time: 30.47574234008789
Epoch: 9, Steps: 318 | Train Loss: 0.1835742 Vali Loss: 0.2044935 Test Loss: 0.1916266
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.4199114739894867, mae:0.4983220100402832, rmse:0.6480057835578918, mape:0.017804034054279327, mspe:0.0005403929972089827, rse:0.45383507013320923, r2_score:0.7658705861495203, acc:0.9821959659457207
corr: [36.911156 37.064205 37.194912 37.087185 37.021484 37.14216  37.010624
 37.070663 37.001865 37.195347 37.110294 37.438164 37.409527 37.271225
 37.433197]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4356222
	speed: 0.1145s/iter; left time: 352.8937s
	iters: 200, epoch: 1 | loss: 0.2132196
	speed: 0.0957s/iter; left time: 285.1797s
	iters: 300, epoch: 1 | loss: 0.2710493
	speed: 0.0996s/iter; left time: 287.0504s
Epoch: 1 cost time: 32.652406454086304
Epoch: 1, Steps: 318 | Train Loss: 0.3856593 Vali Loss: 0.2143614 Test Loss: 0.2202888
Validation loss decreased (inf --> 0.214361).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2192321
	speed: 0.1140s/iter; left time: 314.8635s
	iters: 200, epoch: 2 | loss: 0.1963161
	speed: 0.1121s/iter; left time: 298.4585s
	iters: 300, epoch: 2 | loss: 0.2346089
	speed: 0.0981s/iter; left time: 251.4592s
Epoch: 2 cost time: 34.26866841316223
Epoch: 2, Steps: 318 | Train Loss: 0.2185739 Vali Loss: 0.2006993 Test Loss: 0.1936686
Validation loss decreased (0.214361 --> 0.200699).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2082927
	speed: 0.1104s/iter; left time: 269.8252s
	iters: 200, epoch: 3 | loss: 0.1942936
	speed: 0.1002s/iter; left time: 234.9081s
	iters: 300, epoch: 3 | loss: 0.1827763
	speed: 0.1022s/iter; left time: 229.4145s
Epoch: 3 cost time: 33.86054015159607
Epoch: 3, Steps: 318 | Train Loss: 0.2041795 Vali Loss: 0.2000243 Test Loss: 0.1878911
Validation loss decreased (0.200699 --> 0.200024).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2028829
	speed: 0.1165s/iter; left time: 247.7459s
	iters: 200, epoch: 4 | loss: 0.2828811
	speed: 0.1083s/iter; left time: 219.5778s
	iters: 300, epoch: 4 | loss: 0.1719251
	speed: 0.1121s/iter; left time: 215.9628s
Epoch: 4 cost time: 35.514716148376465
Epoch: 4, Steps: 318 | Train Loss: 0.1990149 Vali Loss: 0.1992943 Test Loss: 0.1830566
Validation loss decreased (0.200024 --> 0.199294).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1682862
	speed: 0.1139s/iter; left time: 206.0149s
	iters: 200, epoch: 5 | loss: 0.1893923
	speed: 0.1200s/iter; left time: 205.0249s
	iters: 300, epoch: 5 | loss: 0.1847516
	speed: 0.1192s/iter; left time: 191.7200s
Epoch: 5 cost time: 37.28204011917114
Epoch: 5, Steps: 318 | Train Loss: 0.1966339 Vali Loss: 0.1975692 Test Loss: 0.1837623
Validation loss decreased (0.199294 --> 0.197569).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1667928
	speed: 0.1235s/iter; left time: 184.1270s
	iters: 200, epoch: 6 | loss: 0.2072933
	speed: 0.1134s/iter; left time: 157.7715s
	iters: 300, epoch: 6 | loss: 0.1697724
	speed: 0.1130s/iter; left time: 145.8371s
Epoch: 6 cost time: 37.349868297576904
Epoch: 6, Steps: 318 | Train Loss: 0.1956848 Vali Loss: 0.1994647 Test Loss: 0.1822938
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1796362
	speed: 0.1197s/iter; left time: 140.4343s
	iters: 200, epoch: 7 | loss: 0.2175272
	speed: 0.1096s/iter; left time: 117.6154s
	iters: 300, epoch: 7 | loss: 0.2378278
	speed: 0.1069s/iter; left time: 103.9810s
Epoch: 7 cost time: 35.72777795791626
Epoch: 7, Steps: 318 | Train Loss: 0.1952039 Vali Loss: 0.2001042 Test Loss: 0.1826469
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2008170
	speed: 0.1134s/iter; left time: 96.9239s
	iters: 200, epoch: 8 | loss: 0.2505159
	speed: 0.1108s/iter; left time: 83.6704s
	iters: 300, epoch: 8 | loss: 0.2035994
	speed: 0.1151s/iter; left time: 75.3966s
Epoch: 8 cost time: 36.01564025878906
Epoch: 8, Steps: 318 | Train Loss: 0.1948978 Vali Loss: 0.1988215 Test Loss: 0.1822275
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.40325912833213806, mae:0.4958652853965759, rmse:0.6350268721580505, mape:0.017734477296471596, mspe:0.0005233210395090282, rse:0.44448766112327576, r2_score:0.7700098026358612, acc:0.9822655227035284
corr: [37.15563  37.286858 37.375782 37.199375 37.449547 37.20936  37.17068
 37.290665 37.17714  37.18984  37.189083 37.143795 37.264576 37.14907
 37.149216 37.280273 37.457153 37.785545 37.61489  37.546368]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3037699
	speed: 0.1232s/iter; left time: 379.4720s
	iters: 200, epoch: 1 | loss: 0.2716384
	speed: 0.0835s/iter; left time: 248.9479s
	iters: 300, epoch: 1 | loss: 0.2906568
	speed: 0.0864s/iter; left time: 248.9031s
Epoch: 1 cost time: 30.865817070007324
Epoch: 1, Steps: 318 | Train Loss: 0.3739183 Vali Loss: 0.2257330 Test Loss: 0.2345053
Validation loss decreased (inf --> 0.225733).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2846411
	speed: 0.0929s/iter; left time: 256.6696s
	iters: 200, epoch: 2 | loss: 0.1712395
	speed: 0.0836s/iter; left time: 222.5091s
	iters: 300, epoch: 2 | loss: 0.1968285
	speed: 0.0959s/iter; left time: 245.7047s
Epoch: 2 cost time: 29.238660097122192
Epoch: 2, Steps: 318 | Train Loss: 0.2322305 Vali Loss: 0.2100598 Test Loss: 0.2086131
Validation loss decreased (0.225733 --> 0.210060).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2011979
	speed: 0.0945s/iter; left time: 231.0948s
	iters: 200, epoch: 3 | loss: 0.2353857
	speed: 0.0942s/iter; left time: 221.0158s
	iters: 300, epoch: 3 | loss: 0.1786862
	speed: 0.0905s/iter; left time: 203.0667s
Epoch: 3 cost time: 29.74193263053894
Epoch: 3, Steps: 318 | Train Loss: 0.2156144 Vali Loss: 0.2091652 Test Loss: 0.2053256
Validation loss decreased (0.210060 --> 0.209165).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2319421
	speed: 0.1070s/iter; left time: 227.5994s
	iters: 200, epoch: 4 | loss: 0.2540793
	speed: 0.0882s/iter; left time: 178.7558s
	iters: 300, epoch: 4 | loss: 0.1515934
	speed: 0.0864s/iter; left time: 166.5164s
Epoch: 4 cost time: 29.794596910476685
Epoch: 4, Steps: 318 | Train Loss: 0.2100720 Vali Loss: 0.2086372 Test Loss: 0.1966580
Validation loss decreased (0.209165 --> 0.208637).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2089170
	speed: 0.0982s/iter; left time: 177.6702s
	iters: 200, epoch: 5 | loss: 0.1704623
	speed: 0.0882s/iter; left time: 150.6949s
	iters: 300, epoch: 5 | loss: 0.2174768
	speed: 0.0939s/iter; left time: 151.0271s
Epoch: 5 cost time: 29.445099115371704
Epoch: 5, Steps: 318 | Train Loss: 0.2077663 Vali Loss: 0.2084888 Test Loss: 0.1969065
Validation loss decreased (0.208637 --> 0.208489).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1769286
	speed: 0.1015s/iter; left time: 151.3609s
	iters: 200, epoch: 6 | loss: 0.1733570
	speed: 0.0943s/iter; left time: 131.1638s
	iters: 300, epoch: 6 | loss: 0.1588081
	speed: 0.0965s/iter; left time: 124.6237s
Epoch: 6 cost time: 31.119807481765747
Epoch: 6, Steps: 318 | Train Loss: 0.2063447 Vali Loss: 0.2081477 Test Loss: 0.1956247
Validation loss decreased (0.208489 --> 0.208148).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2182699
	speed: 0.0980s/iter; left time: 115.0122s
	iters: 200, epoch: 7 | loss: 0.2379646
	speed: 0.0966s/iter; left time: 103.6030s
	iters: 300, epoch: 7 | loss: 0.1769532
	speed: 0.0900s/iter; left time: 87.5268s
Epoch: 7 cost time: 30.120139122009277
Epoch: 7, Steps: 318 | Train Loss: 0.2062102 Vali Loss: 0.2076213 Test Loss: 0.1958476
Validation loss decreased (0.208148 --> 0.207621).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1837897
	speed: 0.0999s/iter; left time: 85.4342s
	iters: 200, epoch: 8 | loss: 0.2105056
	speed: 0.0874s/iter; left time: 65.9865s
	iters: 300, epoch: 8 | loss: 0.1855759
	speed: 0.0938s/iter; left time: 61.4699s
Epoch: 8 cost time: 29.929107666015625
Epoch: 8, Steps: 318 | Train Loss: 0.2054230 Vali Loss: 0.2083096 Test Loss: 0.1956353
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1918889
	speed: 0.0943s/iter; left time: 50.6159s
	iters: 200, epoch: 9 | loss: 0.2575043
	speed: 0.0888s/iter; left time: 38.8226s
	iters: 300, epoch: 9 | loss: 0.2056886
	speed: 0.0880s/iter; left time: 29.6452s
Epoch: 9 cost time: 28.77244782447815
Epoch: 9, Steps: 318 | Train Loss: 0.2048736 Vali Loss: 0.2065811 Test Loss: 0.1953719
Validation loss decreased (0.207621 --> 0.206581).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1699360
	speed: 0.1047s/iter; left time: 22.9215s
	iters: 200, epoch: 10 | loss: 0.1942857
	speed: 0.0907s/iter; left time: 10.7981s
	iters: 300, epoch: 10 | loss: 0.2123143
	speed: 0.0893s/iter; left time: 1.6963s
Epoch: 10 cost time: 30.359588861465454
Epoch: 10, Steps: 318 | Train Loss: 0.2050406 Vali Loss: 0.2083902 Test Loss: 0.1952698
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.42873597145080566, mae:0.5106483697891235, rmse:0.654779314994812, mape:0.0182938352227211, mspe:0.00055903714383021, rse:0.45800721645355225, r2_score:0.7542373544109984, acc:0.9817061647772789
corr: [37.69528  37.814632 37.809895 37.84228  37.701042 37.656353 37.749947
 37.65144  37.774788 37.640644 37.600185 37.78082  37.755436 37.65815
 37.686512 37.550327 37.75728  37.770554 37.954357 37.880905 37.806644
 37.916912 37.873093 38.02918  37.80214 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3964310
	speed: 0.1252s/iter; left time: 384.5766s
	iters: 200, epoch: 1 | loss: 0.3005999
	speed: 0.1284s/iter; left time: 381.4710s
	iters: 300, epoch: 1 | loss: 0.2050422
	speed: 0.1472s/iter; left time: 422.5821s
Epoch: 1 cost time: 42.303839683532715
Epoch: 1, Steps: 317 | Train Loss: 0.4103386 Vali Loss: 0.2451076 Test Loss: 0.2565337
Validation loss decreased (inf --> 0.245108).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2266340
	speed: 0.1529s/iter; left time: 421.1026s
	iters: 200, epoch: 2 | loss: 0.3029525
	speed: 0.1390s/iter; left time: 368.9080s
	iters: 300, epoch: 2 | loss: 0.2443209
	speed: 0.1382s/iter; left time: 353.0547s
Epoch: 2 cost time: 45.493595600128174
Epoch: 2, Steps: 317 | Train Loss: 0.2479713 Vali Loss: 0.2252103 Test Loss: 0.2273959
Validation loss decreased (0.245108 --> 0.225210).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2324644
	speed: 0.1395s/iter; left time: 340.0119s
	iters: 200, epoch: 3 | loss: 0.2410112
	speed: 0.1458s/iter; left time: 340.6755s
	iters: 300, epoch: 3 | loss: 0.2221789
	speed: 0.1432s/iter; left time: 320.2813s
Epoch: 3 cost time: 45.590290546417236
Epoch: 3, Steps: 317 | Train Loss: 0.2262371 Vali Loss: 0.2081495 Test Loss: 0.2214856
Validation loss decreased (0.225210 --> 0.208150).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2470064
	speed: 0.1490s/iter; left time: 315.9651s
	iters: 200, epoch: 4 | loss: 0.1879978
	speed: 0.1324s/iter; left time: 267.4584s
	iters: 300, epoch: 4 | loss: 0.1870952
	speed: 0.1368s/iter; left time: 262.7481s
Epoch: 4 cost time: 44.2575740814209
Epoch: 4, Steps: 317 | Train Loss: 0.2177957 Vali Loss: 0.2106505 Test Loss: 0.2231362
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1857775
	speed: 0.1403s/iter; left time: 253.0119s
	iters: 200, epoch: 5 | loss: 0.2384220
	speed: 0.1432s/iter; left time: 243.9183s
	iters: 300, epoch: 5 | loss: 0.2220369
	speed: 0.1405s/iter; left time: 225.2042s
Epoch: 5 cost time: 44.440434217453
Epoch: 5, Steps: 317 | Train Loss: 0.2150175 Vali Loss: 0.2102463 Test Loss: 0.2204318
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2046370
	speed: 0.1465s/iter; left time: 217.6528s
	iters: 200, epoch: 6 | loss: 0.2153643
	speed: 0.1313s/iter; left time: 181.9478s
	iters: 300, epoch: 6 | loss: 0.2361971
	speed: 0.1398s/iter; left time: 179.8374s
Epoch: 6 cost time: 44.33159923553467
Epoch: 6, Steps: 317 | Train Loss: 0.2127674 Vali Loss: 0.2095071 Test Loss: 0.2193869
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.48604148626327515, mae:0.542846143245697, rmse:0.6971667408943176, mape:0.019416769966483116, mspe:0.0006301261601038277, rse:0.4873063862323761, r2_score:0.7186599176141224, acc:0.9805832300335169
corr: [36.714794 36.711914 36.803608 36.86531  36.696213 36.879    36.834938
 37.081333 37.125404 37.28088  37.289593 37.26879  36.99361  36.94392
 37.348442 37.32592  37.0838   37.479904 37.439472 37.265137 37.135372
 37.286133 37.456394 37.518044 37.125645 37.10653  37.56071  37.57496
 37.680267 37.519653]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4295231
	speed: 0.1321s/iter; left time: 407.0037s
	iters: 200, epoch: 1 | loss: 0.2225584
	speed: 0.1284s/iter; left time: 382.6825s
	iters: 300, epoch: 1 | loss: 0.2779125
	speed: 0.1187s/iter; left time: 341.8925s
Epoch: 1 cost time: 39.72337245941162
Epoch: 1, Steps: 318 | Train Loss: 0.3789351 Vali Loss: 0.1937195 Test Loss: 0.2063894
Validation loss decreased (inf --> 0.193719).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1695869
	speed: 0.1115s/iter; left time: 308.1025s
	iters: 200, epoch: 2 | loss: 0.2222935
	speed: 0.1048s/iter; left time: 279.0019s
	iters: 300, epoch: 2 | loss: 0.1587896
	speed: 0.1004s/iter; left time: 257.2117s
Epoch: 2 cost time: 33.509469747543335
Epoch: 2, Steps: 318 | Train Loss: 0.2022673 Vali Loss: 0.1797976 Test Loss: 0.1714200
Validation loss decreased (0.193719 --> 0.179798).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1635001
	speed: 0.1231s/iter; left time: 301.0618s
	iters: 200, epoch: 3 | loss: 0.1843897
	speed: 0.1015s/iter; left time: 237.9991s
	iters: 300, epoch: 3 | loss: 0.1733158
	speed: 0.0998s/iter; left time: 224.0870s
Epoch: 3 cost time: 34.35895347595215
Epoch: 3, Steps: 318 | Train Loss: 0.1818158 Vali Loss: 0.1736285 Test Loss: 0.1594085
Validation loss decreased (0.179798 --> 0.173628).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1828772
	speed: 0.1144s/iter; left time: 243.4253s
	iters: 200, epoch: 4 | loss: 0.1412964
	speed: 0.1097s/iter; left time: 222.3709s
	iters: 300, epoch: 4 | loss: 0.1658302
	speed: 0.0970s/iter; left time: 186.9291s
Epoch: 4 cost time: 33.86067318916321
Epoch: 4, Steps: 318 | Train Loss: 0.1735119 Vali Loss: 0.1724489 Test Loss: 0.1560426
Validation loss decreased (0.173628 --> 0.172449).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1716712
	speed: 0.1117s/iter; left time: 202.1012s
	iters: 200, epoch: 5 | loss: 0.1694239
	speed: 0.1062s/iter; left time: 181.4615s
	iters: 300, epoch: 5 | loss: 0.1755548
	speed: 0.0990s/iter; left time: 159.2130s
Epoch: 5 cost time: 33.68906855583191
Epoch: 5, Steps: 318 | Train Loss: 0.1706470 Vali Loss: 0.1715130 Test Loss: 0.1535285
Validation loss decreased (0.172449 --> 0.171513).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1495656
	speed: 0.1110s/iter; left time: 165.4699s
	iters: 200, epoch: 6 | loss: 0.1527003
	speed: 0.1085s/iter; left time: 150.8711s
	iters: 300, epoch: 6 | loss: 0.1682437
	speed: 0.0964s/iter; left time: 124.5079s
Epoch: 6 cost time: 33.48840355873108
Epoch: 6, Steps: 318 | Train Loss: 0.1689428 Vali Loss: 0.1715414 Test Loss: 0.1533053
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2002135
	speed: 0.1156s/iter; left time: 135.6292s
	iters: 200, epoch: 7 | loss: 0.1624136
	speed: 0.1186s/iter; left time: 127.2884s
	iters: 300, epoch: 7 | loss: 0.2051834
	speed: 0.0999s/iter; left time: 97.1616s
Epoch: 7 cost time: 35.10513138771057
Epoch: 7, Steps: 318 | Train Loss: 0.1687372 Vali Loss: 0.1718477 Test Loss: 0.1532717
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1655008
	speed: 0.1168s/iter; left time: 99.8852s
	iters: 200, epoch: 8 | loss: 0.1758099
	speed: 0.1157s/iter; left time: 87.3661s
	iters: 300, epoch: 8 | loss: 0.1579520
	speed: 0.1013s/iter; left time: 66.3680s
Epoch: 8 cost time: 35.49002408981323
Epoch: 8, Steps: 318 | Train Loss: 0.1676211 Vali Loss: 0.1704850 Test Loss: 0.1533311
Validation loss decreased (0.171513 --> 0.170485).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1796642
	speed: 0.1179s/iter; left time: 63.2872s
	iters: 200, epoch: 9 | loss: 0.1340432
	speed: 0.1058s/iter; left time: 46.2416s
	iters: 300, epoch: 9 | loss: 0.1605987
	speed: 0.1054s/iter; left time: 35.5045s
Epoch: 9 cost time: 34.9089093208313
Epoch: 9, Steps: 318 | Train Loss: 0.1681214 Vali Loss: 0.1708566 Test Loss: 0.1532329
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1796888
	speed: 0.1204s/iter; left time: 26.3643s
	iters: 200, epoch: 10 | loss: 0.2038386
	speed: 0.1025s/iter; left time: 12.1968s
	iters: 300, epoch: 10 | loss: 0.1623564
	speed: 0.1056s/iter; left time: 2.0068s
Epoch: 10 cost time: 34.74056935310364
Epoch: 10, Steps: 318 | Train Loss: 0.1673289 Vali Loss: 0.1717998 Test Loss: 0.1531692
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.3364790678024292, mae:0.4516230523586273, rmse:0.5800681710243225, mape:0.01615522801876068, mspe:0.00043571877176873386, rse:0.4064458906650543, r2_score:0.8151238698715229, acc:0.9838447719812393
corr: [37.42955  37.38235  37.41239  37.333267 37.326847 37.6422   37.52305
 37.65932  37.992783 38.07616 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3678282
	speed: 0.1446s/iter; left time: 445.3977s
	iters: 200, epoch: 1 | loss: 0.1969849
	speed: 0.1260s/iter; left time: 375.5248s
	iters: 300, epoch: 1 | loss: 0.1990568
	speed: 0.1065s/iter; left time: 306.7421s
Epoch: 1 cost time: 39.503901720047
Epoch: 1, Steps: 318 | Train Loss: 0.3645751 Vali Loss: 0.2004437 Test Loss: 0.2046190
Validation loss decreased (inf --> 0.200444).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2157892
	speed: 0.1218s/iter; left time: 336.5352s
	iters: 200, epoch: 2 | loss: 0.2035725
	speed: 0.1131s/iter; left time: 301.1055s
	iters: 300, epoch: 2 | loss: 0.1769933
	speed: 0.1245s/iter; left time: 319.1727s
Epoch: 2 cost time: 38.4693763256073
Epoch: 2, Steps: 318 | Train Loss: 0.2037596 Vali Loss: 0.1891018 Test Loss: 0.1727473
Validation loss decreased (0.200444 --> 0.189102).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1962378
	speed: 0.1323s/iter; left time: 323.4123s
	iters: 200, epoch: 3 | loss: 0.1792845
	speed: 0.1294s/iter; left time: 303.3700s
	iters: 300, epoch: 3 | loss: 0.1528243
	speed: 0.1164s/iter; left time: 261.3142s
Epoch: 3 cost time: 40.073742389678955
Epoch: 3, Steps: 318 | Train Loss: 0.1868204 Vali Loss: 0.1877423 Test Loss: 0.1639246
Validation loss decreased (0.189102 --> 0.187742).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2178726
	speed: 0.1356s/iter; left time: 288.4073s
	iters: 200, epoch: 4 | loss: 0.2337469
	speed: 0.1324s/iter; left time: 268.4274s
	iters: 300, epoch: 4 | loss: 0.1879891
	speed: 0.1242s/iter; left time: 239.3205s
Epoch: 4 cost time: 41.32741165161133
Epoch: 4, Steps: 318 | Train Loss: 0.1810324 Vali Loss: 0.1840921 Test Loss: 0.1654126
Validation loss decreased (0.187742 --> 0.184092).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1622184
	speed: 0.1386s/iter; left time: 250.7901s
	iters: 200, epoch: 5 | loss: 0.1524650
	speed: 0.1260s/iter; left time: 215.3566s
	iters: 300, epoch: 5 | loss: 0.1536684
	speed: 0.1258s/iter; left time: 202.3833s
Epoch: 5 cost time: 41.00273323059082
Epoch: 5, Steps: 318 | Train Loss: 0.1789624 Vali Loss: 0.1849965 Test Loss: 0.1649423
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1645954
	speed: 0.1323s/iter; left time: 197.2341s
	iters: 200, epoch: 6 | loss: 0.1819341
	speed: 0.1252s/iter; left time: 174.2033s
	iters: 300, epoch: 6 | loss: 0.1652215
	speed: 0.1228s/iter; left time: 158.5423s
Epoch: 6 cost time: 40.153239488601685
Epoch: 6, Steps: 318 | Train Loss: 0.1772543 Vali Loss: 0.1835440 Test Loss: 0.1630452
Validation loss decreased (0.184092 --> 0.183544).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1635281
	speed: 0.1343s/iter; left time: 157.4894s
	iters: 200, epoch: 7 | loss: 0.1760606
	speed: 0.1358s/iter; left time: 145.6671s
	iters: 300, epoch: 7 | loss: 0.1738098
	speed: 0.1195s/iter; left time: 116.3001s
Epoch: 7 cost time: 41.278040409088135
Epoch: 7, Steps: 318 | Train Loss: 0.1766521 Vali Loss: 0.1845841 Test Loss: 0.1635335
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1592982
	speed: 0.1349s/iter; left time: 115.2976s
	iters: 200, epoch: 8 | loss: 0.1753001
	speed: 0.1347s/iter; left time: 101.6700s
	iters: 300, epoch: 8 | loss: 0.2556195
	speed: 0.1259s/iter; left time: 82.4452s
Epoch: 8 cost time: 41.73629403114319
Epoch: 8, Steps: 318 | Train Loss: 0.1768445 Vali Loss: 0.1849620 Test Loss: 0.1634850
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1677762
	speed: 0.1407s/iter; left time: 75.5653s
	iters: 200, epoch: 9 | loss: 0.2336926
	speed: 0.1356s/iter; left time: 59.2401s
	iters: 300, epoch: 9 | loss: 0.1474888
	speed: 0.1165s/iter; left time: 39.2658s
Epoch: 9 cost time: 41.48833179473877
Epoch: 9, Steps: 318 | Train Loss: 0.1768695 Vali Loss: 0.1845484 Test Loss: 0.1635659
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.35779619216918945, mae:0.46197453141212463, rmse:0.5981606841087341, mape:0.016527151688933372, mspe:0.00046444288454949856, rse:0.4189257323741913, r2_score:0.8001856606848534, acc:0.9834728483110666
corr: [36.734467 36.686207 36.793377 36.888412 36.747673 36.924767 37.176785
 37.207146 37.44572  37.50842  37.233562 37.138992 37.410313 37.042816
 37.237602]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4409913
	speed: 0.1657s/iter; left time: 510.6266s
	iters: 200, epoch: 1 | loss: 0.2240106
	speed: 0.1420s/iter; left time: 423.4328s
	iters: 300, epoch: 1 | loss: 0.2451793
	speed: 0.1444s/iter; left time: 416.0136s
Epoch: 1 cost time: 48.06041431427002
Epoch: 1, Steps: 318 | Train Loss: 0.3856046 Vali Loss: 0.2149102 Test Loss: 0.2149031
Validation loss decreased (inf --> 0.214910).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2043851
	speed: 0.1599s/iter; left time: 441.7656s
	iters: 200, epoch: 2 | loss: 0.1927005
	speed: 0.1437s/iter; left time: 382.6562s
	iters: 300, epoch: 2 | loss: 0.2363594
	speed: 0.1442s/iter; left time: 369.5122s
Epoch: 2 cost time: 47.32060408592224
Epoch: 2, Steps: 318 | Train Loss: 0.2151746 Vali Loss: 0.2075584 Test Loss: 0.1922642
Validation loss decreased (0.214910 --> 0.207558).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2021744
	speed: 0.1550s/iter; left time: 379.0943s
	iters: 200, epoch: 3 | loss: 0.1811703
	speed: 0.1559s/iter; left time: 365.5434s
	iters: 300, epoch: 3 | loss: 0.1896494
	speed: 0.1450s/iter; left time: 325.5128s
Epoch: 3 cost time: 48.306063413619995
Epoch: 3, Steps: 318 | Train Loss: 0.2023575 Vali Loss: 0.2004060 Test Loss: 0.1917350
Validation loss decreased (0.207558 --> 0.200406).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1984463
	speed: 0.1614s/iter; left time: 343.2285s
	iters: 200, epoch: 4 | loss: 0.2805909
	speed: 0.1485s/iter; left time: 301.0365s
	iters: 300, epoch: 4 | loss: 0.1814822
	speed: 0.1404s/iter; left time: 270.5528s
Epoch: 4 cost time: 47.7855429649353
Epoch: 4, Steps: 318 | Train Loss: 0.1974776 Vali Loss: 0.2047903 Test Loss: 0.1863627
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1737277
	speed: 0.1692s/iter; left time: 306.1146s
	iters: 200, epoch: 5 | loss: 0.1761222
	speed: 0.1481s/iter; left time: 253.0603s
	iters: 300, epoch: 5 | loss: 0.1900581
	speed: 0.1484s/iter; left time: 238.8163s
Epoch: 5 cost time: 49.387577056884766
Epoch: 5, Steps: 318 | Train Loss: 0.1950509 Vali Loss: 0.2032832 Test Loss: 0.1887431
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1698226
	speed: 0.1637s/iter; left time: 244.1372s
	iters: 200, epoch: 6 | loss: 0.1858952
	speed: 0.1621s/iter; left time: 225.4560s
	iters: 300, epoch: 6 | loss: 0.1652649
	speed: 0.1386s/iter; left time: 178.8683s
Epoch: 6 cost time: 49.16671538352966
Epoch: 6, Steps: 318 | Train Loss: 0.1938366 Vali Loss: 0.2069746 Test Loss: 0.1871100
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.42075493931770325, mae:0.5103170275688171, rmse:0.6486562490463257, mape:0.01822493225336075, mspe:0.000543458154425025, rse:0.45402753353118896, r2_score:0.7586788004713951, acc:0.9817750677466393
corr: [37.02437  37.084995 37.215305 37.168037 37.282944 37.100773 37.07426
 37.270145 37.147934 37.298725 37.43658  37.351597 37.30926  37.439445
 37.310375 37.397945 37.45323  37.783512 37.728573 37.678688]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3042521
	speed: 0.1387s/iter; left time: 427.2973s
	iters: 200, epoch: 1 | loss: 0.2684245
	speed: 0.0999s/iter; left time: 297.7292s
	iters: 300, epoch: 1 | loss: 0.2834210
	speed: 0.0975s/iter; left time: 281.0343s
Epoch: 1 cost time: 35.31283688545227
Epoch: 1, Steps: 318 | Train Loss: 0.3769231 Vali Loss: 0.2213479 Test Loss: 0.2331295
Validation loss decreased (inf --> 0.221348).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2841915
	speed: 0.1076s/iter; left time: 297.3959s
	iters: 200, epoch: 2 | loss: 0.1698345
	speed: 0.0951s/iter; left time: 253.3716s
	iters: 300, epoch: 2 | loss: 0.1974630
	speed: 0.0979s/iter; left time: 250.9781s
Epoch: 2 cost time: 31.92423987388611
Epoch: 2, Steps: 318 | Train Loss: 0.2307417 Vali Loss: 0.2102073 Test Loss: 0.2069243
Validation loss decreased (0.221348 --> 0.210207).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2009360
	speed: 0.1065s/iter; left time: 260.4221s
	iters: 200, epoch: 3 | loss: 0.2331014
	speed: 0.1058s/iter; left time: 248.0927s
	iters: 300, epoch: 3 | loss: 0.1768077
	speed: 0.0937s/iter; left time: 210.4074s
Epoch: 3 cost time: 32.42188024520874
Epoch: 3, Steps: 318 | Train Loss: 0.2146472 Vali Loss: 0.2097670 Test Loss: 0.2043572
Validation loss decreased (0.210207 --> 0.209767).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2317651
	speed: 0.1169s/iter; left time: 248.6643s
	iters: 200, epoch: 4 | loss: 0.2566542
	speed: 0.0999s/iter; left time: 202.5753s
	iters: 300, epoch: 4 | loss: 0.1487313
	speed: 0.1057s/iter; left time: 203.7754s
Epoch: 4 cost time: 34.206915616989136
Epoch: 4, Steps: 318 | Train Loss: 0.2092405 Vali Loss: 0.2093046 Test Loss: 0.1958155
Validation loss decreased (0.209767 --> 0.209305).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2075390
	speed: 0.1126s/iter; left time: 203.7238s
	iters: 200, epoch: 5 | loss: 0.1695301
	speed: 0.1025s/iter; left time: 175.0898s
	iters: 300, epoch: 5 | loss: 0.2159393
	speed: 0.1001s/iter; left time: 161.1000s
Epoch: 5 cost time: 33.22331786155701
Epoch: 5, Steps: 318 | Train Loss: 0.2070609 Vali Loss: 0.2092465 Test Loss: 0.1961052
Validation loss decreased (0.209305 --> 0.209247).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1766397
	speed: 0.1117s/iter; left time: 166.5584s
	iters: 200, epoch: 6 | loss: 0.1751151
	speed: 0.1053s/iter; left time: 146.4219s
	iters: 300, epoch: 6 | loss: 0.1566144
	speed: 0.0950s/iter; left time: 122.6562s
Epoch: 6 cost time: 32.974042892456055
Epoch: 6, Steps: 318 | Train Loss: 0.2055821 Vali Loss: 0.2089361 Test Loss: 0.1950777
Validation loss decreased (0.209247 --> 0.208936).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2175467
	speed: 0.1091s/iter; left time: 127.9776s
	iters: 200, epoch: 7 | loss: 0.2366076
	speed: 0.1060s/iter; left time: 113.7800s
	iters: 300, epoch: 7 | loss: 0.1757459
	speed: 0.0966s/iter; left time: 94.0239s
Epoch: 7 cost time: 33.09163165092468
Epoch: 7, Steps: 318 | Train Loss: 0.2053706 Vali Loss: 0.2084366 Test Loss: 0.1953516
Validation loss decreased (0.208936 --> 0.208437).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1813808
	speed: 0.1154s/iter; left time: 98.6261s
	iters: 200, epoch: 8 | loss: 0.2100362
	speed: 0.1071s/iter; left time: 80.8268s
	iters: 300, epoch: 8 | loss: 0.1844146
	speed: 0.0955s/iter; left time: 62.5582s
Epoch: 8 cost time: 33.505669593811035
Epoch: 8, Steps: 318 | Train Loss: 0.2046993 Vali Loss: 0.2089915 Test Loss: 0.1951658
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1919110
	speed: 0.1067s/iter; left time: 57.2814s
	iters: 200, epoch: 9 | loss: 0.2573189
	speed: 0.0974s/iter; left time: 42.5696s
	iters: 300, epoch: 9 | loss: 0.2037235
	speed: 0.0932s/iter; left time: 31.3981s
Epoch: 9 cost time: 31.621711254119873
Epoch: 9, Steps: 318 | Train Loss: 0.2042511 Vali Loss: 0.2073748 Test Loss: 0.1949316
Validation loss decreased (0.208437 --> 0.207375).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1679406
	speed: 0.1080s/iter; left time: 23.6540s
	iters: 200, epoch: 10 | loss: 0.1906398
	speed: 0.0953s/iter; left time: 11.3404s
	iters: 300, epoch: 10 | loss: 0.2128325
	speed: 0.1016s/iter; left time: 1.9305s
Epoch: 10 cost time: 32.575029373168945
Epoch: 10, Steps: 318 | Train Loss: 0.2043250 Vali Loss: 0.2090870 Test Loss: 0.1948439
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.42776986956596375, mae:0.5098680257797241, rmse:0.6540411710739136, mape:0.018265696242451668, mspe:0.0005577721167355776, rse:0.4574909210205078, r2_score:0.7555922270258266, acc:0.9817343037575483
corr: [37.66005  37.787136 37.788597 37.826786 37.66655  37.606033 37.667885
 37.596703 37.69754  37.575718 37.53843  37.693314 37.697777 37.617058
 37.6821   37.541073 37.793823 37.810783 37.96776  37.870403 37.800735
 37.86273  37.841152 38.005447 37.79362 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3789652
	speed: 0.1939s/iter; left time: 595.4186s
	iters: 200, epoch: 1 | loss: 0.3038787
	speed: 0.2006s/iter; left time: 596.0651s
	iters: 300, epoch: 1 | loss: 0.2222284
	speed: 0.1591s/iter; left time: 456.7771s
Epoch: 1 cost time: 58.15036463737488
Epoch: 1, Steps: 317 | Train Loss: 0.4094340 Vali Loss: 0.2553553 Test Loss: 0.2571801
Validation loss decreased (inf --> 0.255355).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2313457
	speed: 0.1903s/iter; left time: 524.0931s
	iters: 200, epoch: 2 | loss: 0.2974317
	speed: 0.1870s/iter; left time: 496.3121s
	iters: 300, epoch: 2 | loss: 0.2335700
	speed: 0.1935s/iter; left time: 494.0717s
Epoch: 2 cost time: 60.65842270851135
Epoch: 2, Steps: 317 | Train Loss: 0.2476564 Vali Loss: 0.2223011 Test Loss: 0.2294246
Validation loss decreased (0.255355 --> 0.222301).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2337730
	speed: 0.2161s/iter; left time: 526.5588s
	iters: 200, epoch: 3 | loss: 0.2276751
	speed: 0.2035s/iter; left time: 475.5543s
	iters: 300, epoch: 3 | loss: 0.2457588
	speed: 0.1989s/iter; left time: 444.8565s
Epoch: 3 cost time: 65.34799337387085
Epoch: 3, Steps: 317 | Train Loss: 0.2259094 Vali Loss: 0.2100100 Test Loss: 0.2194638
Validation loss decreased (0.222301 --> 0.210010).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2605937
	speed: 0.1957s/iter; left time: 414.9807s
	iters: 200, epoch: 4 | loss: 0.1904749
	speed: 0.2031s/iter; left time: 410.1934s
	iters: 300, epoch: 4 | loss: 0.1889144
	speed: 0.2036s/iter; left time: 390.8205s
Epoch: 4 cost time: 63.527292251586914
Epoch: 4, Steps: 317 | Train Loss: 0.2174417 Vali Loss: 0.2132269 Test Loss: 0.2233954
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1869186
	speed: 0.2156s/iter; left time: 388.6684s
	iters: 200, epoch: 5 | loss: 0.2405206
	speed: 0.1984s/iter; left time: 337.9359s
	iters: 300, epoch: 5 | loss: 0.2263139
	speed: 0.1967s/iter; left time: 315.3721s
Epoch: 5 cost time: 64.2337372303009
Epoch: 5, Steps: 317 | Train Loss: 0.2138847 Vali Loss: 0.2122992 Test Loss: 0.2221299
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2013195
	speed: 0.2050s/iter; left time: 304.5698s
	iters: 200, epoch: 6 | loss: 0.2267422
	speed: 0.1984s/iter; left time: 274.9217s
	iters: 300, epoch: 6 | loss: 0.2151663
	speed: 0.2042s/iter; left time: 262.6319s
Epoch: 6 cost time: 63.93217086791992
Epoch: 6, Steps: 317 | Train Loss: 0.2116438 Vali Loss: 0.2115450 Test Loss: 0.2205640
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.48160475492477417, mae:0.5426540970802307, rmse:0.6939774751663208, mape:0.01941509358584881, mspe:0.0006257907371036708, rse:0.4850771427154541, r2_score:0.7243790849981049, acc:0.9805849064141512
corr: [36.925896 36.842022 36.9199   37.038456 36.798923 37.011967 36.932606
 37.13594  37.201363 37.32356  37.324867 37.372643 37.068466 37.139606
 37.44408  37.383064 37.209373 37.574783 37.572025 37.335854 37.24665
 37.420044 37.664093 37.707726 37.31911  37.43042  37.772694 37.72143
 37.83153  37.677307]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4352542
	speed: 0.1522s/iter; left time: 468.9236s
	iters: 200, epoch: 1 | loss: 0.2401516
	speed: 0.1209s/iter; left time: 360.3604s
	iters: 300, epoch: 1 | loss: 0.2973992
	speed: 0.1299s/iter; left time: 374.1734s
Epoch: 1 cost time: 43.00441765785217
Epoch: 1, Steps: 318 | Train Loss: 0.3890934 Vali Loss: 0.1999635 Test Loss: 0.2158729
Validation loss decreased (inf --> 0.199964).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1712609
	speed: 0.1671s/iter; left time: 461.7892s
	iters: 200, epoch: 2 | loss: 0.2272871
	speed: 0.1566s/iter; left time: 417.1477s
	iters: 300, epoch: 2 | loss: 0.1595636
	speed: 0.1608s/iter; left time: 412.0392s
Epoch: 2 cost time: 51.691508054733276
Epoch: 2, Steps: 318 | Train Loss: 0.2080435 Vali Loss: 0.1784302 Test Loss: 0.1762015
Validation loss decreased (0.199964 --> 0.178430).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1725677
	speed: 0.1696s/iter; left time: 414.7183s
	iters: 200, epoch: 3 | loss: 0.1821450
	speed: 0.1576s/iter; left time: 369.5534s
	iters: 300, epoch: 3 | loss: 0.1785463
	speed: 0.1593s/iter; left time: 357.7014s
Epoch: 3 cost time: 51.57085728645325
Epoch: 3, Steps: 318 | Train Loss: 0.1851767 Vali Loss: 0.1738387 Test Loss: 0.1624492
Validation loss decreased (0.178430 --> 0.173839).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1853317
	speed: 0.1625s/iter; left time: 345.6905s
	iters: 200, epoch: 4 | loss: 0.1448116
	speed: 0.1624s/iter; left time: 329.2095s
	iters: 300, epoch: 4 | loss: 0.1634442
	speed: 0.1465s/iter; left time: 282.2093s
Epoch: 4 cost time: 50.03375744819641
Epoch: 4, Steps: 318 | Train Loss: 0.1766036 Vali Loss: 0.1713726 Test Loss: 0.1587828
Validation loss decreased (0.173839 --> 0.171373).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1741747
	speed: 0.1656s/iter; left time: 299.4925s
	iters: 200, epoch: 5 | loss: 0.1786582
	speed: 0.1677s/iter; left time: 286.6196s
	iters: 300, epoch: 5 | loss: 0.1793061
	speed: 0.1634s/iter; left time: 262.9661s
Epoch: 5 cost time: 52.45467209815979
Epoch: 5, Steps: 318 | Train Loss: 0.1735978 Vali Loss: 0.1704751 Test Loss: 0.1561905
Validation loss decreased (0.171373 --> 0.170475).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1457317
	speed: 0.1680s/iter; left time: 250.4786s
	iters: 200, epoch: 6 | loss: 0.1541507
	speed: 0.1590s/iter; left time: 221.1390s
	iters: 300, epoch: 6 | loss: 0.1690158
	speed: 0.1539s/iter; left time: 198.7374s
Epoch: 6 cost time: 51.134132862091064
Epoch: 6, Steps: 318 | Train Loss: 0.1717631 Vali Loss: 0.1701601 Test Loss: 0.1558981
Validation loss decreased (0.170475 --> 0.170160).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1973920
	speed: 0.1653s/iter; left time: 193.8998s
	iters: 200, epoch: 7 | loss: 0.1624180
	speed: 0.1578s/iter; left time: 169.3268s
	iters: 300, epoch: 7 | loss: 0.2003283
	speed: 0.1673s/iter; left time: 162.7816s
Epoch: 7 cost time: 51.96681499481201
Epoch: 7, Steps: 318 | Train Loss: 0.1715385 Vali Loss: 0.1701602 Test Loss: 0.1558246
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1647745
	speed: 0.1759s/iter; left time: 150.3680s
	iters: 200, epoch: 8 | loss: 0.1770101
	speed: 0.1594s/iter; left time: 120.3769s
	iters: 300, epoch: 8 | loss: 0.1531558
	speed: 0.1568s/iter; left time: 102.7349s
Epoch: 8 cost time: 51.94553208351135
Epoch: 8, Steps: 318 | Train Loss: 0.1705510 Vali Loss: 0.1686607 Test Loss: 0.1559362
Validation loss decreased (0.170160 --> 0.168661).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1861149
	speed: 0.1723s/iter; left time: 92.5012s
	iters: 200, epoch: 9 | loss: 0.1357644
	speed: 0.1711s/iter; left time: 74.7492s
	iters: 300, epoch: 9 | loss: 0.1594306
	speed: 0.1645s/iter; left time: 55.4317s
Epoch: 9 cost time: 54.038655519485474
Epoch: 9, Steps: 318 | Train Loss: 0.1709151 Vali Loss: 0.1695284 Test Loss: 0.1557802
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1870427
	speed: 0.1692s/iter; left time: 37.0578s
	iters: 200, epoch: 10 | loss: 0.2045624
	speed: 0.1622s/iter; left time: 19.3042s
	iters: 300, epoch: 10 | loss: 0.1623393
	speed: 0.1631s/iter; left time: 3.0987s
Epoch: 10 cost time: 52.904038190841675
Epoch: 10, Steps: 318 | Train Loss: 0.1703102 Vali Loss: 0.1704208 Test Loss: 0.1556898
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.34219592809677124, mae:0.45536404848098755, rmse:0.5849751234054565, mape:0.016292346641421318, mspe:0.0004433977010194212, rse:0.40988415479660034, r2_score:0.8106831442874931, acc:0.9837076533585787
corr: [37.457195 37.40401  37.41054  37.348522 37.3714   37.66339  37.586258
 37.71113  38.020454 38.105003]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3695053
	speed: 0.1473s/iter; left time: 453.7994s
	iters: 200, epoch: 1 | loss: 0.1927340
	speed: 0.1350s/iter; left time: 402.5761s
	iters: 300, epoch: 1 | loss: 0.1996811
	speed: 0.1120s/iter; left time: 322.5986s
Epoch: 1 cost time: 41.542967557907104
Epoch: 1, Steps: 318 | Train Loss: 0.3635616 Vali Loss: 0.2024001 Test Loss: 0.2065395
Validation loss decreased (inf --> 0.202400).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2175906
	speed: 0.1248s/iter; left time: 344.9232s
	iters: 200, epoch: 2 | loss: 0.2057035
	speed: 0.1398s/iter; left time: 372.3181s
	iters: 300, epoch: 2 | loss: 0.1817775
	speed: 0.1307s/iter; left time: 334.8910s
Epoch: 2 cost time: 41.96324014663696
Epoch: 2, Steps: 318 | Train Loss: 0.2053286 Vali Loss: 0.1901496 Test Loss: 0.1738710
Validation loss decreased (0.202400 --> 0.190150).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1977094
	speed: 0.1381s/iter; left time: 337.6703s
	iters: 200, epoch: 3 | loss: 0.1805916
	speed: 0.1283s/iter; left time: 300.8238s
	iters: 300, epoch: 3 | loss: 0.1521767
	speed: 0.1419s/iter; left time: 318.5186s
Epoch: 3 cost time: 43.52991461753845
Epoch: 3, Steps: 318 | Train Loss: 0.1881009 Vali Loss: 0.1886570 Test Loss: 0.1647997
Validation loss decreased (0.190150 --> 0.188657).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2208419
	speed: 0.1422s/iter; left time: 302.5590s
	iters: 200, epoch: 4 | loss: 0.2375399
	speed: 0.1468s/iter; left time: 297.5561s
	iters: 300, epoch: 4 | loss: 0.1886258
	speed: 0.1368s/iter; left time: 263.6433s
Epoch: 4 cost time: 45.121299743652344
Epoch: 4, Steps: 318 | Train Loss: 0.1821666 Vali Loss: 0.1850463 Test Loss: 0.1662700
Validation loss decreased (0.188657 --> 0.185046).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1632926
	speed: 0.1427s/iter; left time: 258.0795s
	iters: 200, epoch: 5 | loss: 0.1518136
	speed: 0.1369s/iter; left time: 233.9436s
	iters: 300, epoch: 5 | loss: 0.1537962
	speed: 0.1391s/iter; left time: 223.8735s
Epoch: 5 cost time: 44.331987619400024
Epoch: 5, Steps: 318 | Train Loss: 0.1800550 Vali Loss: 0.1859813 Test Loss: 0.1658137
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1651870
	speed: 0.1468s/iter; left time: 218.9316s
	iters: 200, epoch: 6 | loss: 0.1834277
	speed: 0.1353s/iter; left time: 188.2679s
	iters: 300, epoch: 6 | loss: 0.1674116
	speed: 0.1339s/iter; left time: 172.8554s
Epoch: 6 cost time: 44.78603267669678
Epoch: 6, Steps: 318 | Train Loss: 0.1782102 Vali Loss: 0.1845022 Test Loss: 0.1639067
Validation loss decreased (0.185046 --> 0.184502).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1665957
	speed: 0.1362s/iter; left time: 159.7902s
	iters: 200, epoch: 7 | loss: 0.1777631
	speed: 0.1333s/iter; left time: 143.0216s
	iters: 300, epoch: 7 | loss: 0.1772089
	speed: 0.1449s/iter; left time: 141.0282s
Epoch: 7 cost time: 44.257014989852905
Epoch: 7, Steps: 318 | Train Loss: 0.1776499 Vali Loss: 0.1856275 Test Loss: 0.1643632
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1605568
	speed: 0.1372s/iter; left time: 117.3208s
	iters: 200, epoch: 8 | loss: 0.1778366
	speed: 0.1404s/iter; left time: 106.0227s
	iters: 300, epoch: 8 | loss: 0.2553034
	speed: 0.1386s/iter; left time: 90.8121s
Epoch: 8 cost time: 44.45074796676636
Epoch: 8, Steps: 318 | Train Loss: 0.1778031 Vali Loss: 0.1860450 Test Loss: 0.1643379
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1699580
	speed: 0.1370s/iter; left time: 73.5927s
	iters: 200, epoch: 9 | loss: 0.2338155
	speed: 0.1319s/iter; left time: 57.6219s
	iters: 300, epoch: 9 | loss: 0.1479775
	speed: 0.1487s/iter; left time: 50.1015s
Epoch: 9 cost time: 44.35363173484802
Epoch: 9, Steps: 318 | Train Loss: 0.1778779 Vali Loss: 0.1856465 Test Loss: 0.1644103
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.35968688130378723, mae:0.46364519000053406, rmse:0.5997390151023865, mape:0.01658804528415203, mspe:0.0004669921472668648, rse:0.4200311303138733, r2_score:0.7995596571458171, acc:0.983411954715848
corr: [36.7049   36.644917 36.7908   36.90261  36.751923 36.905025 37.178406
 37.231976 37.45889  37.52855  37.25683  37.168777 37.423748 37.035362
 37.183407]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4248632
	speed: 0.2238s/iter; left time: 689.5364s
	iters: 200, epoch: 1 | loss: 0.2138558
	speed: 0.2298s/iter; left time: 685.1215s
	iters: 300, epoch: 1 | loss: 0.2581903
	speed: 0.2210s/iter; left time: 636.6051s
Epoch: 1 cost time: 71.68903374671936
Epoch: 1, Steps: 318 | Train Loss: 0.3837370 Vali Loss: 0.2070863 Test Loss: 0.2140274
Validation loss decreased (inf --> 0.207086).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2131569
	speed: 0.2465s/iter; left time: 681.2045s
	iters: 200, epoch: 2 | loss: 0.1818260
	speed: 0.2280s/iter; left time: 607.1664s
	iters: 300, epoch: 2 | loss: 0.2338812
	speed: 0.2352s/iter; left time: 602.9399s
Epoch: 2 cost time: 75.3127224445343
Epoch: 2, Steps: 318 | Train Loss: 0.2139956 Vali Loss: 0.2004987 Test Loss: 0.1919406
Validation loss decreased (0.207086 --> 0.200499).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2076505
	speed: 0.2485s/iter; left time: 607.6263s
	iters: 200, epoch: 3 | loss: 0.1897625
	speed: 0.2502s/iter; left time: 586.7511s
	iters: 300, epoch: 3 | loss: 0.1807929
	speed: 0.2345s/iter; left time: 526.4506s
Epoch: 3 cost time: 77.72459244728088
Epoch: 3, Steps: 318 | Train Loss: 0.2015827 Vali Loss: 0.1994494 Test Loss: 0.1869887
Validation loss decreased (0.200499 --> 0.199449).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1886713
	speed: 0.2523s/iter; left time: 536.7351s
	iters: 200, epoch: 4 | loss: 0.2820482
	speed: 0.2464s/iter; left time: 499.4308s
	iters: 300, epoch: 4 | loss: 0.1689401
	speed: 0.2398s/iter; left time: 462.0356s
Epoch: 4 cost time: 78.7365071773529
Epoch: 4, Steps: 318 | Train Loss: 0.1970608 Vali Loss: 0.2000431 Test Loss: 0.1836814
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1707551
	speed: 0.2423s/iter; left time: 438.2683s
	iters: 200, epoch: 5 | loss: 0.1878935
	speed: 0.2528s/iter; left time: 432.0132s
	iters: 300, epoch: 5 | loss: 0.1755085
	speed: 0.2252s/iter; left time: 362.3043s
Epoch: 5 cost time: 76.37155151367188
Epoch: 5, Steps: 318 | Train Loss: 0.1948370 Vali Loss: 0.1982795 Test Loss: 0.1845788
Validation loss decreased (0.199449 --> 0.198279).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1726942
	speed: 0.2412s/iter; left time: 359.6079s
	iters: 200, epoch: 6 | loss: 0.2091780
	speed: 0.2381s/iter; left time: 331.1884s
	iters: 300, epoch: 6 | loss: 0.1786368
	speed: 0.2266s/iter; left time: 292.5110s
Epoch: 6 cost time: 74.66918730735779
Epoch: 6, Steps: 318 | Train Loss: 0.1937709 Vali Loss: 0.2004556 Test Loss: 0.1830277
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1781157
	speed: 0.2462s/iter; left time: 288.7471s
	iters: 200, epoch: 7 | loss: 0.2250535
	speed: 0.2428s/iter; left time: 260.4868s
	iters: 300, epoch: 7 | loss: 0.2371231
	speed: 0.2369s/iter; left time: 230.4836s
Epoch: 7 cost time: 76.48622226715088
Epoch: 7, Steps: 318 | Train Loss: 0.1934074 Vali Loss: 0.2009433 Test Loss: 0.1832636
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1956099
	speed: 0.2595s/iter; left time: 221.8960s
	iters: 200, epoch: 8 | loss: 0.2465485
	speed: 0.2489s/iter; left time: 187.9335s
	iters: 300, epoch: 8 | loss: 0.2082373
	speed: 0.2344s/iter; left time: 153.5555s
Epoch: 8 cost time: 78.09362387657166
Epoch: 8, Steps: 318 | Train Loss: 0.1930442 Vali Loss: 0.1995965 Test Loss: 0.1829455
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.40505096316337585, mae:0.4969986379146576, rmse:0.6364361643791199, mape:0.01776917278766632, mspe:0.0005245237844064832, rse:0.4454740881919861, r2_score:0.7689868714298569, acc:0.9822308272123337
corr: [36.917126 37.0843   37.2102   37.15521  37.223267 37.038216 37.11005
 37.25598  37.04723  37.24933  37.476013 37.40562  37.36288  37.437645
 37.302982 37.428886 37.412067 37.714035 37.709312 37.744102]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3030748
	speed: 0.1511s/iter; left time: 465.5573s
	iters: 200, epoch: 1 | loss: 0.2675830
	speed: 0.1092s/iter; left time: 325.6601s
	iters: 300, epoch: 1 | loss: 0.2844128
	speed: 0.1063s/iter; left time: 306.1778s
Epoch: 1 cost time: 38.8814594745636
Epoch: 1, Steps: 318 | Train Loss: 0.3792368 Vali Loss: 0.2209132 Test Loss: 0.2317461
Validation loss decreased (inf --> 0.220913).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2845406
	speed: 0.1248s/iter; left time: 344.7364s
	iters: 200, epoch: 2 | loss: 0.1699990
	speed: 0.1149s/iter; left time: 305.9540s
	iters: 300, epoch: 2 | loss: 0.1986356
	speed: 0.1179s/iter; left time: 302.1404s
Epoch: 2 cost time: 38.03623032569885
Epoch: 2, Steps: 318 | Train Loss: 0.2307916 Vali Loss: 0.2104655 Test Loss: 0.2068455
Validation loss decreased (0.220913 --> 0.210465).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1992004
	speed: 0.1327s/iter; left time: 324.5072s
	iters: 200, epoch: 3 | loss: 0.2317293
	speed: 0.1287s/iter; left time: 301.8558s
	iters: 300, epoch: 3 | loss: 0.1774488
	speed: 0.1217s/iter; left time: 273.1553s
Epoch: 3 cost time: 40.33887314796448
Epoch: 3, Steps: 318 | Train Loss: 0.2141945 Vali Loss: 0.2106856 Test Loss: 0.2046158
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2071542
	speed: 0.1317s/iter; left time: 280.1316s
	iters: 200, epoch: 4 | loss: 0.2594986
	speed: 0.1212s/iter; left time: 245.7407s
	iters: 300, epoch: 4 | loss: 0.1491148
	speed: 0.1101s/iter; left time: 212.1359s
Epoch: 4 cost time: 38.48615074157715
Epoch: 4, Steps: 318 | Train Loss: 0.2086309 Vali Loss: 0.2082335 Test Loss: 0.1995237
Validation loss decreased (0.210465 --> 0.208234).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2054732
	speed: 0.1318s/iter; left time: 238.4894s
	iters: 200, epoch: 5 | loss: 0.1682975
	speed: 0.1094s/iter; left time: 187.0302s
	iters: 300, epoch: 5 | loss: 0.2169712
	speed: 0.1093s/iter; left time: 175.8252s
Epoch: 5 cost time: 37.20602226257324
Epoch: 5, Steps: 318 | Train Loss: 0.2062449 Vali Loss: 0.2102844 Test Loss: 0.1968762
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1759657
	speed: 0.1348s/iter; left time: 201.0043s
	iters: 200, epoch: 6 | loss: 0.1723625
	speed: 0.1188s/iter; left time: 165.2281s
	iters: 300, epoch: 6 | loss: 0.1568672
	speed: 0.1257s/iter; left time: 162.2878s
Epoch: 6 cost time: 40.52224683761597
Epoch: 6, Steps: 318 | Train Loss: 0.2048115 Vali Loss: 0.2096499 Test Loss: 0.1956897
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2178587
	speed: 0.1415s/iter; left time: 165.9362s
	iters: 200, epoch: 7 | loss: 0.2374131
	speed: 0.1179s/iter; left time: 126.5506s
	iters: 300, epoch: 7 | loss: 0.1750257
	speed: 0.1321s/iter; left time: 128.5314s
Epoch: 7 cost time: 41.75971508026123
Epoch: 7, Steps: 318 | Train Loss: 0.2044529 Vali Loss: 0.2091277 Test Loss: 0.1959776
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.43784698843955994, mae:0.5137576460838318, rmse:0.6617000699043274, mape:0.018424341455101967, mspe:0.0005737280589528382, rse:0.4628481864929199, r2_score:0.7444474352312233, acc:0.981575658544898
corr: [37.57188  37.684093 37.665382 37.75313  37.535404 37.499115 37.566933
 37.448097 37.58695  37.499302 37.412292 37.554943 37.49479  37.465622
 37.528088 37.385166 37.635887 37.61302  37.810818 37.720394 37.61475
 37.68649  37.695053 37.850613 37.656364]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3843651
	speed: 0.2117s/iter; left time: 650.0776s
	iters: 200, epoch: 1 | loss: 0.3062311
	speed: 0.2777s/iter; left time: 825.1410s
	iters: 300, epoch: 1 | loss: 0.2318020
	speed: 0.2389s/iter; left time: 685.7485s
Epoch: 1 cost time: 77.27139663696289
Epoch: 1, Steps: 317 | Train Loss: 0.4079283 Vali Loss: 0.2518966 Test Loss: 0.2593235
Validation loss decreased (inf --> 0.251897).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2246049
	speed: 0.2787s/iter; left time: 767.5867s
	iters: 200, epoch: 2 | loss: 0.3031107
	speed: 0.2628s/iter; left time: 697.3547s
	iters: 300, epoch: 2 | loss: 0.2167784
	speed: 0.2163s/iter; left time: 552.5228s
Epoch: 2 cost time: 79.28498816490173
Epoch: 2, Steps: 317 | Train Loss: 0.2450308 Vali Loss: 0.2319853 Test Loss: 0.2310067
Validation loss decreased (0.251897 --> 0.231985).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2437219
	speed: 0.2764s/iter; left time: 673.5285s
	iters: 200, epoch: 3 | loss: 0.2334741
	speed: 0.2576s/iter; left time: 602.0930s
	iters: 300, epoch: 3 | loss: 0.2192323
	speed: 0.2426s/iter; left time: 542.6712s
Epoch: 3 cost time: 81.96400785446167
Epoch: 3, Steps: 317 | Train Loss: 0.2237591 Vali Loss: 0.2155505 Test Loss: 0.2230747
Validation loss decreased (0.231985 --> 0.215550).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2558537
	speed: 0.3034s/iter; left time: 643.1974s
	iters: 200, epoch: 4 | loss: 0.1996804
	speed: 0.2526s/iter; left time: 510.2107s
	iters: 300, epoch: 4 | loss: 0.1834863
	speed: 0.2493s/iter; left time: 478.7173s
Epoch: 4 cost time: 84.81989336013794
Epoch: 4, Steps: 317 | Train Loss: 0.2151360 Vali Loss: 0.2220267 Test Loss: 0.2214008
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1899778
	speed: 0.2895s/iter; left time: 522.0454s
	iters: 200, epoch: 5 | loss: 0.2444405
	speed: 0.2391s/iter; left time: 407.1181s
	iters: 300, epoch: 5 | loss: 0.2182576
	speed: 0.2436s/iter; left time: 390.4710s
Epoch: 5 cost time: 81.47904467582703
Epoch: 5, Steps: 317 | Train Loss: 0.2116170 Vali Loss: 0.2194121 Test Loss: 0.2233528
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1955928
	speed: 0.2770s/iter; left time: 411.6783s
	iters: 200, epoch: 6 | loss: 0.2285849
	speed: 0.2376s/iter; left time: 329.2689s
	iters: 300, epoch: 6 | loss: 0.2178589
	speed: 0.2575s/iter; left time: 331.1181s
Epoch: 6 cost time: 81.66220116615295
Epoch: 6, Steps: 317 | Train Loss: 0.2096747 Vali Loss: 0.2186764 Test Loss: 0.2198195
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.4895287752151489, mae:0.548096239566803, rmse:0.6996633410453796, mape:0.01960090361535549, mspe:0.000635460193734616, rse:0.4890514612197876, r2_score:0.7181941338913324, acc:0.9803990963846445
corr: [36.88957  36.78947  36.940536 37.018257 36.71154  36.941086 36.859776
 36.98249  37.031944 37.25269  37.201725 37.24624  36.93811  36.8675
 37.20533  37.137726 37.019188 37.31623  37.385498 37.212494 37.06928
 37.219093 37.491455 37.4365   37.11336  37.205063 37.57986  37.571518
 37.685925 37.47128 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4375619
	speed: 0.2094s/iter; left time: 645.0546s
	iters: 200, epoch: 1 | loss: 0.2100458
	speed: 0.2132s/iter; left time: 635.4785s
	iters: 300, epoch: 1 | loss: 0.2673880
	speed: 0.2672s/iter; left time: 769.8049s
Epoch: 1 cost time: 73.78235101699829
Epoch: 1, Steps: 318 | Train Loss: 0.3761006 Vali Loss: 0.1922628 Test Loss: 0.2005082
Validation loss decreased (inf --> 0.192263).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1687703
	speed: 0.2091s/iter; left time: 577.7942s
	iters: 200, epoch: 2 | loss: 0.2204926
	speed: 0.2279s/iter; left time: 606.9097s
	iters: 300, epoch: 2 | loss: 0.1557730
	speed: 0.2251s/iter; left time: 576.8698s
Epoch: 2 cost time: 70.88800978660583
Epoch: 2, Steps: 318 | Train Loss: 0.1993036 Vali Loss: 0.1757654 Test Loss: 0.1690252
Validation loss decreased (0.192263 --> 0.175765).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1689727
	speed: 0.2566s/iter; left time: 627.3513s
	iters: 200, epoch: 3 | loss: 0.1763673
	speed: 0.2345s/iter; left time: 549.9537s
	iters: 300, epoch: 3 | loss: 0.1760595
	speed: 0.2350s/iter; left time: 527.5564s
Epoch: 3 cost time: 77.36601400375366
Epoch: 3, Steps: 318 | Train Loss: 0.1804550 Vali Loss: 0.1708549 Test Loss: 0.1583585
Validation loss decreased (0.175765 --> 0.170855).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1793941
	speed: 0.2584s/iter; left time: 549.6788s
	iters: 200, epoch: 4 | loss: 0.1379509
	speed: 0.2464s/iter; left time: 499.3854s
	iters: 300, epoch: 4 | loss: 0.1590522
	speed: 0.2803s/iter; left time: 540.0810s
Epoch: 4 cost time: 83.95189642906189
Epoch: 4, Steps: 318 | Train Loss: 0.1727863 Vali Loss: 0.1687160 Test Loss: 0.1552455
Validation loss decreased (0.170855 --> 0.168716).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1709760
	speed: 0.2750s/iter; left time: 497.4855s
	iters: 200, epoch: 5 | loss: 0.1691786
	speed: 0.2394s/iter; left time: 409.1316s
	iters: 300, epoch: 5 | loss: 0.1752530
	speed: 0.2525s/iter; left time: 406.2422s
Epoch: 5 cost time: 80.87978482246399
Epoch: 5, Steps: 318 | Train Loss: 0.1699949 Vali Loss: 0.1679714 Test Loss: 0.1527146
Validation loss decreased (0.168716 --> 0.167971).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1432340
	speed: 0.2547s/iter; left time: 379.7327s
	iters: 200, epoch: 6 | loss: 0.1465143
	speed: 0.2665s/iter; left time: 370.6966s
	iters: 300, epoch: 6 | loss: 0.1657078
	speed: 0.2736s/iter; left time: 353.1768s
Epoch: 6 cost time: 83.74200582504272
Epoch: 6, Steps: 318 | Train Loss: 0.1683639 Vali Loss: 0.1675746 Test Loss: 0.1525005
Validation loss decreased (0.167971 --> 0.167575).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1944056
	speed: 0.2793s/iter; left time: 327.6595s
	iters: 200, epoch: 7 | loss: 0.1578675
	speed: 0.2486s/iter; left time: 266.7447s
	iters: 300, epoch: 7 | loss: 0.2011591
	speed: 0.2596s/iter; left time: 252.5742s
Epoch: 7 cost time: 83.61191272735596
Epoch: 7, Steps: 318 | Train Loss: 0.1682017 Vali Loss: 0.1675837 Test Loss: 0.1523760
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1633697
	speed: 0.2533s/iter; left time: 216.5579s
	iters: 200, epoch: 8 | loss: 0.1721931
	speed: 0.2900s/iter; left time: 218.9716s
	iters: 300, epoch: 8 | loss: 0.1498348
	speed: 0.2440s/iter; left time: 159.8223s
Epoch: 8 cost time: 83.96305775642395
Epoch: 8, Steps: 318 | Train Loss: 0.1672060 Vali Loss: 0.1661538 Test Loss: 0.1526029
Validation loss decreased (0.167575 --> 0.166154).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1818040
	speed: 0.2662s/iter; left time: 142.9709s
	iters: 200, epoch: 9 | loss: 0.1339076
	speed: 0.2344s/iter; left time: 102.4173s
	iters: 300, epoch: 9 | loss: 0.1549728
	speed: 0.2475s/iter; left time: 83.3968s
Epoch: 9 cost time: 79.29310011863708
Epoch: 9, Steps: 318 | Train Loss: 0.1677348 Vali Loss: 0.1668218 Test Loss: 0.1524366
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1846788
	speed: 0.2594s/iter; left time: 56.8162s
	iters: 200, epoch: 10 | loss: 0.2015792
	speed: 0.2206s/iter; left time: 26.2556s
	iters: 300, epoch: 10 | loss: 0.1610582
	speed: 0.2143s/iter; left time: 4.0708s
Epoch: 10 cost time: 73.20738577842712
Epoch: 10, Steps: 318 | Train Loss: 0.1670606 Vali Loss: 0.1676483 Test Loss: 0.1523584
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.33488109707832336, mae:0.44963860511779785, rmse:0.5786890983581543, mape:0.016085002571344376, mspe:0.0004336233250796795, rse:0.4054796099662781, r2_score:0.8140484617913302, acc:0.9839149974286556
corr: [37.35819  37.337563 37.31265  37.24963  37.303135 37.62648  37.53303
 37.652294 37.970608 38.01837 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3699601
	speed: 0.2003s/iter; left time: 616.9870s
	iters: 200, epoch: 1 | loss: 0.1856622
	speed: 0.1775s/iter; left time: 529.0573s
	iters: 300, epoch: 1 | loss: 0.2300764
	speed: 0.1859s/iter; left time: 535.4469s
Epoch: 1 cost time: 60.675682067871094
Epoch: 1, Steps: 318 | Train Loss: 0.3660593 Vali Loss: 0.2149409 Test Loss: 0.2292842
Validation loss decreased (inf --> 0.214941).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2234595
	speed: 0.1999s/iter; left time: 552.3733s
	iters: 200, epoch: 2 | loss: 0.2148608
	speed: 0.2058s/iter; left time: 547.9673s
	iters: 300, epoch: 2 | loss: 0.2041771
	speed: 0.1652s/iter; left time: 423.4373s
Epoch: 2 cost time: 60.20279121398926
Epoch: 2, Steps: 318 | Train Loss: 0.2164120 Vali Loss: 0.2041504 Test Loss: 0.1967176
Validation loss decreased (0.214941 --> 0.204150).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2082052
	speed: 0.2003s/iter; left time: 489.7875s
	iters: 200, epoch: 3 | loss: 0.1902645
	speed: 0.1881s/iter; left time: 441.0945s
	iters: 300, epoch: 3 | loss: 0.1589901
	speed: 0.1783s/iter; left time: 400.3372s
Epoch: 3 cost time: 60.298426389694214
Epoch: 3, Steps: 318 | Train Loss: 0.1958526 Vali Loss: 0.2010349 Test Loss: 0.1886070
Validation loss decreased (0.204150 --> 0.201035).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2256093
	speed: 0.1778s/iter; left time: 378.2551s
	iters: 200, epoch: 4 | loss: 0.2197369
	speed: 0.1708s/iter; left time: 346.1852s
	iters: 300, epoch: 4 | loss: 0.1998664
	speed: 0.1752s/iter; left time: 337.6607s
Epoch: 4 cost time: 55.06702160835266
Epoch: 4, Steps: 318 | Train Loss: 0.1881563 Vali Loss: 0.2012076 Test Loss: 0.1916240
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1735129
	speed: 0.1877s/iter; left time: 339.6329s
	iters: 200, epoch: 5 | loss: 0.1667265
	speed: 0.1919s/iter; left time: 327.9493s
	iters: 300, epoch: 5 | loss: 0.1515999
	speed: 0.1826s/iter; left time: 293.7978s
Epoch: 5 cost time: 59.587952613830566
Epoch: 5, Steps: 318 | Train Loss: 0.1856809 Vali Loss: 0.2021447 Test Loss: 0.1906659
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1893170
	speed: 0.1931s/iter; left time: 287.9485s
	iters: 200, epoch: 6 | loss: 0.1873769
	speed: 0.1799s/iter; left time: 250.2255s
	iters: 300, epoch: 6 | loss: 0.1874440
	speed: 0.1836s/iter; left time: 237.0627s
Epoch: 6 cost time: 59.73835778236389
Epoch: 6, Steps: 318 | Train Loss: 0.1837108 Vali Loss: 0.2012693 Test Loss: 0.1875121
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.41389068961143494, mae:0.5008929371833801, rmse:0.6433433890342712, mape:0.017899220809340477, mspe:0.0005344284581951797, rse:0.4505697190761566, r2_score:0.7674844839657676, acc:0.9821007791906595
corr: [37.26927  37.399548 37.449844 37.308174 37.252056 37.420666 37.267086
 37.373005 37.26942  37.51372  37.400127 37.65094  37.609837 37.589897
 37.758312]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4419482
	speed: 0.2648s/iter; left time: 815.7231s
	iters: 200, epoch: 1 | loss: 0.2108810
	speed: 0.2282s/iter; left time: 680.1769s
	iters: 300, epoch: 1 | loss: 0.2566144
	speed: 0.2826s/iter; left time: 814.1665s
Epoch: 1 cost time: 81.75430917739868
Epoch: 1, Steps: 318 | Train Loss: 0.3870692 Vali Loss: 0.2039019 Test Loss: 0.2221006
Validation loss decreased (inf --> 0.203902).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2257512
	speed: 0.2938s/iter; left time: 811.8964s
	iters: 200, epoch: 2 | loss: 0.1928038
	speed: 0.2597s/iter; left time: 691.5559s
	iters: 300, epoch: 2 | loss: 0.2290991
	speed: 0.2662s/iter; left time: 682.3509s
Epoch: 2 cost time: 88.48467683792114
Epoch: 2, Steps: 318 | Train Loss: 0.2182968 Vali Loss: 0.2038788 Test Loss: 0.1961214
Validation loss decreased (0.203902 --> 0.203879).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2090409
	speed: 0.2926s/iter; left time: 715.4154s
	iters: 200, epoch: 3 | loss: 0.1863423
	speed: 0.2694s/iter; left time: 631.6266s
	iters: 300, epoch: 3 | loss: 0.1815879
	speed: 0.2626s/iter; left time: 589.6219s
Epoch: 3 cost time: 88.39256358146667
Epoch: 3, Steps: 318 | Train Loss: 0.2046942 Vali Loss: 0.1987788 Test Loss: 0.1923194
Validation loss decreased (0.203879 --> 0.198779).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1976523
	speed: 0.2519s/iter; left time: 535.8544s
	iters: 200, epoch: 4 | loss: 0.2850896
	speed: 0.2944s/iter; left time: 596.7118s
	iters: 300, epoch: 4 | loss: 0.1896429
	speed: 0.2718s/iter; left time: 523.7343s
Epoch: 4 cost time: 87.1606068611145
Epoch: 4, Steps: 318 | Train Loss: 0.1987757 Vali Loss: 0.2000374 Test Loss: 0.1895252
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1733503
	speed: 0.3021s/iter; left time: 546.4166s
	iters: 200, epoch: 5 | loss: 0.1954092
	speed: 0.2649s/iter; left time: 452.7584s
	iters: 300, epoch: 5 | loss: 0.1777106
	speed: 0.2628s/iter; left time: 422.8103s
Epoch: 5 cost time: 88.21855401992798
Epoch: 5, Steps: 318 | Train Loss: 0.1959539 Vali Loss: 0.1985180 Test Loss: 0.1901309
Validation loss decreased (0.198779 --> 0.198518).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1662032
	speed: 0.2355s/iter; left time: 351.2010s
	iters: 200, epoch: 6 | loss: 0.2013070
	speed: 0.3012s/iter; left time: 418.9622s
	iters: 300, epoch: 6 | loss: 0.1597794
	speed: 0.2660s/iter; left time: 343.4127s
Epoch: 6 cost time: 85.32965469360352
Epoch: 6, Steps: 318 | Train Loss: 0.1944805 Vali Loss: 0.2006824 Test Loss: 0.1893132
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1768457
	speed: 0.2936s/iter; left time: 344.4512s
	iters: 200, epoch: 7 | loss: 0.2057851
	speed: 0.2413s/iter; left time: 258.9385s
	iters: 300, epoch: 7 | loss: 0.2165457
	speed: 0.2756s/iter; left time: 268.1533s
Epoch: 7 cost time: 85.9142575263977
Epoch: 7, Steps: 318 | Train Loss: 0.1938146 Vali Loss: 0.2012115 Test Loss: 0.1892848
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1926924
	speed: 0.3097s/iter; left time: 264.7685s
	iters: 200, epoch: 8 | loss: 0.2397412
	speed: 0.2582s/iter; left time: 194.9226s
	iters: 300, epoch: 8 | loss: 0.1912473
	speed: 0.2658s/iter; left time: 174.0693s
Epoch: 8 cost time: 87.74076676368713
Epoch: 8, Steps: 318 | Train Loss: 0.1929600 Vali Loss: 0.1999910 Test Loss: 0.1889772
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.417234867811203, mae:0.5037302374839783, rmse:0.6459372043609619, mape:0.01800728216767311, mspe:0.0005398375215008855, rse:0.45212432742118835, r2_score:0.7668237171429044, acc:0.9819927178323269
corr: [36.970165 37.106102 37.257214 37.21667  37.291218 37.08848  37.09087
 37.260292 37.080986 37.30211  37.44849  37.436226 37.38089  37.443096
 37.299137 37.42992  37.389423 37.69833  37.691956 37.71037 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3033058
	speed: 0.1690s/iter; left time: 520.8321s
	iters: 200, epoch: 1 | loss: 0.2681627
	speed: 0.1551s/iter; left time: 462.4740s
	iters: 300, epoch: 1 | loss: 0.2887388
	speed: 0.1389s/iter; left time: 400.1264s
Epoch: 1 cost time: 49.13175344467163
Epoch: 1, Steps: 318 | Train Loss: 0.3782191 Vali Loss: 0.2213671 Test Loss: 0.2327256
Validation loss decreased (inf --> 0.221367).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2836855
	speed: 0.1614s/iter; left time: 445.9072s
	iters: 200, epoch: 2 | loss: 0.1706977
	speed: 0.1730s/iter; left time: 460.6278s
	iters: 300, epoch: 2 | loss: 0.1980588
	speed: 0.1728s/iter; left time: 442.7583s
Epoch: 2 cost time: 53.94043803215027
Epoch: 2, Steps: 318 | Train Loss: 0.2313208 Vali Loss: 0.2106619 Test Loss: 0.2069746
Validation loss decreased (0.221367 --> 0.210662).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1989599
	speed: 0.1914s/iter; left time: 467.9019s
	iters: 200, epoch: 3 | loss: 0.2310625
	speed: 0.1572s/iter; left time: 368.5998s
	iters: 300, epoch: 3 | loss: 0.1773857
	speed: 0.1649s/iter; left time: 370.2444s
Epoch: 3 cost time: 54.55047392845154
Epoch: 3, Steps: 318 | Train Loss: 0.2147299 Vali Loss: 0.2103747 Test Loss: 0.2045156
Validation loss decreased (0.210662 --> 0.210375).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2315066
	speed: 0.1793s/iter; left time: 381.3149s
	iters: 200, epoch: 4 | loss: 0.2565233
	speed: 0.1442s/iter; left time: 292.2438s
	iters: 300, epoch: 4 | loss: 0.1480381
	speed: 0.1324s/iter; left time: 255.1271s
Epoch: 4 cost time: 47.670685052871704
Epoch: 4, Steps: 318 | Train Loss: 0.2088113 Vali Loss: 0.2099215 Test Loss: 0.1959072
Validation loss decreased (0.210375 --> 0.209921).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2053839
	speed: 0.1269s/iter; left time: 229.6396s
	iters: 200, epoch: 5 | loss: 0.1673702
	speed: 0.1290s/iter; left time: 220.4988s
	iters: 300, epoch: 5 | loss: 0.2158187
	speed: 0.1226s/iter; left time: 197.2079s
Epoch: 5 cost time: 40.045222759246826
Epoch: 5, Steps: 318 | Train Loss: 0.2063571 Vali Loss: 0.2109400 Test Loss: 0.1963457
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1778130
	speed: 0.1274s/iter; left time: 189.9579s
	iters: 200, epoch: 6 | loss: 0.1700780
	speed: 0.1246s/iter; left time: 173.3075s
	iters: 300, epoch: 6 | loss: 0.1564929
	speed: 0.1251s/iter; left time: 161.5331s
Epoch: 6 cost time: 39.82245445251465
Epoch: 6, Steps: 318 | Train Loss: 0.2047048 Vali Loss: 0.2099216 Test Loss: 0.1950808
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2193359
	speed: 0.1279s/iter; left time: 150.0630s
	iters: 200, epoch: 7 | loss: 0.2332658
	speed: 0.1278s/iter; left time: 137.1667s
	iters: 300, epoch: 7 | loss: 0.1734637
	speed: 0.1313s/iter; left time: 127.7776s
Epoch: 7 cost time: 41.277809619903564
Epoch: 7, Steps: 318 | Train Loss: 0.2043625 Vali Loss: 0.2096873 Test Loss: 0.1954631
Validation loss decreased (0.209921 --> 0.209687).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1774494
	speed: 0.1212s/iter; left time: 103.5957s
	iters: 200, epoch: 8 | loss: 0.2112371
	speed: 0.1196s/iter; left time: 90.2960s
	iters: 300, epoch: 8 | loss: 0.1880406
	speed: 0.1181s/iter; left time: 77.3562s
Epoch: 8 cost time: 38.227153062820435
Epoch: 8, Steps: 318 | Train Loss: 0.2036638 Vali Loss: 0.2104229 Test Loss: 0.1954188
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1918660
	speed: 0.1375s/iter; left time: 73.8395s
	iters: 200, epoch: 9 | loss: 0.2576208
	speed: 0.1097s/iter; left time: 47.9589s
	iters: 300, epoch: 9 | loss: 0.2041457
	speed: 0.1133s/iter; left time: 38.1703s
Epoch: 9 cost time: 38.299118518829346
Epoch: 9, Steps: 318 | Train Loss: 0.2033231 Vali Loss: 0.2088524 Test Loss: 0.1951198
Validation loss decreased (0.209687 --> 0.208852).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1709613
	speed: 0.1242s/iter; left time: 27.2096s
	iters: 200, epoch: 10 | loss: 0.1945943
	speed: 0.1342s/iter; left time: 15.9643s
	iters: 300, epoch: 10 | loss: 0.2044844
	speed: 0.1321s/iter; left time: 2.5103s
Epoch: 10 cost time: 41.21003603935242
Epoch: 10, Steps: 318 | Train Loss: 0.2031691 Vali Loss: 0.2105623 Test Loss: 0.1950354
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.4281827509403229, mae:0.5106579065322876, rmse:0.6543567180633545, mape:0.01829700917005539, mspe:0.0005586117622442544, rse:0.4577116370201111, r2_score:0.7585586939884631, acc:0.9817029908299446
corr: [37.73215  37.855198 37.84385  37.87311  37.74283  37.67654  37.734707
 37.688522 37.795067 37.679756 37.637978 37.806664 37.775936 37.698086
 37.74837  37.605003 37.843662 37.826214 37.989666 37.895676 37.844624
 37.92047  37.868324 38.041    37.840023]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3901527
	speed: 0.1900s/iter; left time: 583.5188s
	iters: 200, epoch: 1 | loss: 0.3040921
	speed: 0.1724s/iter; left time: 512.1383s
	iters: 300, epoch: 1 | loss: 0.2213136
	speed: 0.1631s/iter; left time: 468.2183s
Epoch: 1 cost time: 55.117775678634644
Epoch: 1, Steps: 317 | Train Loss: 0.4154209 Vali Loss: 0.2621030 Test Loss: 0.2667681
Validation loss decreased (inf --> 0.262103).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2372843
	speed: 0.1511s/iter; left time: 416.2271s
	iters: 200, epoch: 2 | loss: 0.2963063
	speed: 0.1673s/iter; left time: 444.0444s
	iters: 300, epoch: 2 | loss: 0.2502112
	speed: 0.1658s/iter; left time: 423.5632s
Epoch: 2 cost time: 51.541255950927734
Epoch: 2, Steps: 317 | Train Loss: 0.2525450 Vali Loss: 0.2433229 Test Loss: 0.2430884
Validation loss decreased (0.262103 --> 0.243323).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2218741
	speed: 0.1511s/iter; left time: 368.3220s
	iters: 200, epoch: 3 | loss: 0.2265600
	speed: 0.1550s/iter; left time: 362.1653s
	iters: 300, epoch: 3 | loss: 0.2262339
	speed: 0.1367s/iter; left time: 305.8069s
Epoch: 3 cost time: 47.00990080833435
Epoch: 3, Steps: 317 | Train Loss: 0.2301576 Vali Loss: 0.2269600 Test Loss: 0.2290641
Validation loss decreased (0.243323 --> 0.226960).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2498744
	speed: 0.1606s/iter; left time: 340.4137s
	iters: 200, epoch: 4 | loss: 0.1913090
	speed: 0.1820s/iter; left time: 367.5960s
	iters: 300, epoch: 4 | loss: 0.1851898
	speed: 0.1545s/iter; left time: 296.6706s
Epoch: 4 cost time: 52.573344469070435
Epoch: 4, Steps: 317 | Train Loss: 0.2203481 Vali Loss: 0.2285628 Test Loss: 0.2301797
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2007870
	speed: 0.1744s/iter; left time: 314.4219s
	iters: 200, epoch: 5 | loss: 0.2334435
	speed: 0.1738s/iter; left time: 295.9866s
	iters: 300, epoch: 5 | loss: 0.2120034
	speed: 0.1768s/iter; left time: 283.4455s
Epoch: 5 cost time: 54.89725470542908
Epoch: 5, Steps: 317 | Train Loss: 0.2165583 Vali Loss: 0.2279797 Test Loss: 0.2297842
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2071792
	speed: 0.1709s/iter; left time: 253.9717s
	iters: 200, epoch: 6 | loss: 0.2251903
	speed: 0.1692s/iter; left time: 234.4661s
	iters: 300, epoch: 6 | loss: 0.2194958
	speed: 0.1772s/iter; left time: 227.8267s
Epoch: 6 cost time: 54.72996997833252
Epoch: 6, Steps: 317 | Train Loss: 0.2146305 Vali Loss: 0.2249579 Test Loss: 0.2257827
Validation loss decreased (0.226960 --> 0.224958).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1830438
	speed: 0.1897s/iter; left time: 221.7073s
	iters: 200, epoch: 7 | loss: 0.2243253
	speed: 0.1716s/iter; left time: 183.4878s
	iters: 300, epoch: 7 | loss: 0.2381528
	speed: 0.1792s/iter; left time: 173.6117s
Epoch: 7 cost time: 56.77532744407654
Epoch: 7, Steps: 317 | Train Loss: 0.2137631 Vali Loss: 0.2254894 Test Loss: 0.2257386
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2434170
	speed: 0.1960s/iter; left time: 166.9612s
	iters: 200, epoch: 8 | loss: 0.1955970
	speed: 0.1677s/iter; left time: 126.1197s
	iters: 300, epoch: 8 | loss: 0.1850461
	speed: 0.1779s/iter; left time: 115.9684s
Epoch: 8 cost time: 57.01529574394226
Epoch: 8, Steps: 317 | Train Loss: 0.2133283 Vali Loss: 0.2246854 Test Loss: 0.2252916
Validation loss decreased (0.224958 --> 0.224685).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2560353
	speed: 0.1887s/iter; left time: 100.9729s
	iters: 200, epoch: 9 | loss: 0.1691652
	speed: 0.1816s/iter; left time: 78.9775s
	iters: 300, epoch: 9 | loss: 0.1737279
	speed: 0.1888s/iter; left time: 63.2331s
Epoch: 9 cost time: 59.42719841003418
Epoch: 9, Steps: 317 | Train Loss: 0.2132187 Vali Loss: 0.2251653 Test Loss: 0.2250216
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2559210
	speed: 0.1893s/iter; left time: 41.2769s
	iters: 200, epoch: 10 | loss: 0.1902990
	speed: 0.1759s/iter; left time: 20.7568s
	iters: 300, epoch: 10 | loss: 0.1964893
	speed: 0.1723s/iter; left time: 3.1006s
Epoch: 10 cost time: 57.554423332214355
Epoch: 10, Steps: 317 | Train Loss: 0.2125214 Vali Loss: 0.2240762 Test Loss: 0.2249410
Validation loss decreased (0.224685 --> 0.224076).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.49362415075302124, mae:0.5508907437324524, rmse:0.702583909034729, mape:0.019695453345775604, mspe:0.000639969075564295, rse:0.49109286069869995, r2_score:0.7170315930244623, acc:0.9803045466542244
corr: [36.689865 36.629387 36.809975 36.721455 36.50324  36.649017 36.755245
 36.823864 36.777588 36.907433 36.959805 37.01932  36.691376 36.753483
 37.236694 37.138702 36.882523 37.104546 37.275387 37.079163 37.02647
 37.377827 37.572086 37.687943 37.442932 37.469334 37.784767 37.798824
 37.921436 38.05672 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2916636
	speed: 0.1011s/iter; left time: 311.5543s
	iters: 200, epoch: 1 | loss: 0.2902129
	speed: 0.0956s/iter; left time: 284.9139s
	iters: 300, epoch: 1 | loss: 0.2178218
	speed: 0.0832s/iter; left time: 239.6878s
Epoch: 1 cost time: 29.534340620040894
Epoch: 1, Steps: 318 | Train Loss: 0.3422229 Vali Loss: 0.1933573 Test Loss: 0.2060890
Validation loss decreased (inf --> 0.193357).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1687988
	speed: 0.0899s/iter; left time: 248.4212s
	iters: 200, epoch: 2 | loss: 0.1574566
	speed: 0.0810s/iter; left time: 215.6038s
	iters: 300, epoch: 2 | loss: 0.1736695
	speed: 0.0881s/iter; left time: 225.7264s
Epoch: 2 cost time: 27.557565212249756
Epoch: 2, Steps: 318 | Train Loss: 0.1812415 Vali Loss: 0.1658798 Test Loss: 0.1855726
Validation loss decreased (0.193357 --> 0.165880).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1393399
	speed: 0.0857s/iter; left time: 209.5282s
	iters: 200, epoch: 3 | loss: 0.1444236
	speed: 0.0822s/iter; left time: 192.8523s
	iters: 300, epoch: 3 | loss: 0.2135960
	speed: 0.0729s/iter; left time: 163.6501s
Epoch: 3 cost time: 25.435109615325928
Epoch: 3, Steps: 318 | Train Loss: 0.1639335 Vali Loss: 0.1595920 Test Loss: 0.1844500
Validation loss decreased (0.165880 --> 0.159592).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1886936
	speed: 0.0840s/iter; left time: 178.7351s
	iters: 200, epoch: 4 | loss: 0.1730687
	speed: 0.0626s/iter; left time: 126.7917s
	iters: 300, epoch: 4 | loss: 0.2113698
	speed: 0.0853s/iter; left time: 164.3512s
Epoch: 4 cost time: 24.617582082748413
Epoch: 4, Steps: 318 | Train Loss: 0.1582218 Vali Loss: 0.1563421 Test Loss: 0.1806498
Validation loss decreased (0.159592 --> 0.156342).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1984110
	speed: 0.0852s/iter; left time: 154.0496s
	iters: 200, epoch: 5 | loss: 0.1121705
	speed: 0.0729s/iter; left time: 124.6147s
	iters: 300, epoch: 5 | loss: 0.1431448
	speed: 0.0683s/iter; left time: 109.8413s
Epoch: 5 cost time: 24.08353853225708
Epoch: 5, Steps: 318 | Train Loss: 0.1550066 Vali Loss: 0.1567852 Test Loss: 0.1816824
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1336164
	speed: 0.0876s/iter; left time: 130.6626s
	iters: 200, epoch: 6 | loss: 0.1662730
	speed: 0.0745s/iter; left time: 103.6469s
	iters: 300, epoch: 6 | loss: 0.1424901
	speed: 0.0787s/iter; left time: 101.6287s
Epoch: 6 cost time: 25.47659969329834
Epoch: 6, Steps: 318 | Train Loss: 0.1541804 Vali Loss: 0.1572466 Test Loss: 0.1819470
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1580043
	speed: 0.0789s/iter; left time: 92.5035s
	iters: 200, epoch: 7 | loss: 0.1386304
	speed: 0.0712s/iter; left time: 76.3790s
	iters: 300, epoch: 7 | loss: 0.1318350
	speed: 0.0792s/iter; left time: 77.0831s
Epoch: 7 cost time: 24.553500413894653
Epoch: 7, Steps: 318 | Train Loss: 0.1528565 Vali Loss: 0.1561356 Test Loss: 0.1813064
Validation loss decreased (0.156342 --> 0.156136).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1417110
	speed: 0.0747s/iter; left time: 63.8858s
	iters: 200, epoch: 8 | loss: 0.1106144
	speed: 0.0795s/iter; left time: 59.9886s
	iters: 300, epoch: 8 | loss: 0.1621438
	speed: 0.0798s/iter; left time: 52.2502s
Epoch: 8 cost time: 24.999125719070435
Epoch: 8, Steps: 318 | Train Loss: 0.1522960 Vali Loss: 0.1557365 Test Loss: 0.1808355
Validation loss decreased (0.156136 --> 0.155737).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1272707
	speed: 0.0747s/iter; left time: 40.1285s
	iters: 200, epoch: 9 | loss: 0.1349001
	speed: 0.0789s/iter; left time: 34.4580s
	iters: 300, epoch: 9 | loss: 0.1394985
	speed: 0.0737s/iter; left time: 24.8355s
Epoch: 9 cost time: 24.293535709381104
Epoch: 9, Steps: 318 | Train Loss: 0.1522179 Vali Loss: 0.1566421 Test Loss: 0.1807411
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1502882
	speed: 0.0830s/iter; left time: 18.1670s
	iters: 200, epoch: 10 | loss: 0.1346042
	speed: 0.0781s/iter; left time: 9.2925s
	iters: 300, epoch: 10 | loss: 0.1316181
	speed: 0.0874s/iter; left time: 1.6612s
Epoch: 10 cost time: 26.593830108642578
Epoch: 10, Steps: 318 | Train Loss: 0.1526411 Vali Loss: 0.1552319 Test Loss: 0.1804498
Validation loss decreased (0.155737 --> 0.155232).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.3959899842739105, mae:0.4799809753894806, rmse:0.6292773485183716, mape:0.017141297459602356, mspe:0.0005098788533359766, rse:0.44092613458633423, r2_score:0.7863852860218575, acc:0.9828587025403976
corr: [36.426613 36.4006   36.453117 36.710648 36.54192  36.48815  36.42101
 36.29358  36.599422 36.25818 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3170200
	speed: 0.0851s/iter; left time: 262.0466s
	iters: 200, epoch: 1 | loss: 0.2677162
	speed: 0.0823s/iter; left time: 245.4537s
	iters: 300, epoch: 1 | loss: 0.2083604
	speed: 0.0809s/iter; left time: 233.0442s
Epoch: 1 cost time: 26.314826250076294
Epoch: 1, Steps: 318 | Train Loss: 0.3332400 Vali Loss: 0.1914198 Test Loss: 0.2352710
Validation loss decreased (inf --> 0.191420).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1671862
	speed: 0.0835s/iter; left time: 230.5933s
	iters: 200, epoch: 2 | loss: 0.1961145
	speed: 0.0832s/iter; left time: 221.6542s
	iters: 300, epoch: 2 | loss: 0.1858847
	speed: 0.0809s/iter; left time: 207.4586s
Epoch: 2 cost time: 26.234981775283813
Epoch: 2, Steps: 318 | Train Loss: 0.1894604 Vali Loss: 0.1810745 Test Loss: 0.2374268
Validation loss decreased (0.191420 --> 0.181074).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1793692
	speed: 0.0700s/iter; left time: 171.0408s
	iters: 200, epoch: 3 | loss: 0.2041123
	speed: 0.0648s/iter; left time: 151.9741s
	iters: 300, epoch: 3 | loss: 0.1882716
	speed: 0.0719s/iter; left time: 161.4595s
Epoch: 3 cost time: 22.179197549819946
Epoch: 3, Steps: 318 | Train Loss: 0.1789143 Vali Loss: 0.1749557 Test Loss: 0.2063268
Validation loss decreased (0.181074 --> 0.174956).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1536097
	speed: 0.0851s/iter; left time: 181.0540s
	iters: 200, epoch: 4 | loss: 0.1259419
	speed: 0.0735s/iter; left time: 148.9140s
	iters: 300, epoch: 4 | loss: 0.1776779
	speed: 0.0786s/iter; left time: 151.5260s
Epoch: 4 cost time: 24.86637234687805
Epoch: 4, Steps: 318 | Train Loss: 0.1736993 Vali Loss: 0.1729679 Test Loss: 0.2021521
Validation loss decreased (0.174956 --> 0.172968).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1453071
	speed: 0.0620s/iter; left time: 112.2406s
	iters: 200, epoch: 5 | loss: 0.1656700
	speed: 0.0795s/iter; left time: 135.8973s
	iters: 300, epoch: 5 | loss: 0.2237536
	speed: 0.0751s/iter; left time: 120.8519s
Epoch: 5 cost time: 23.234286546707153
Epoch: 5, Steps: 318 | Train Loss: 0.1708399 Vali Loss: 0.1742913 Test Loss: 0.2046313
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2025460
	speed: 0.0837s/iter; left time: 124.8203s
	iters: 200, epoch: 6 | loss: 0.1951189
	speed: 0.0738s/iter; left time: 102.7120s
	iters: 300, epoch: 6 | loss: 0.1766247
	speed: 0.0773s/iter; left time: 99.7305s
Epoch: 6 cost time: 24.951515913009644
Epoch: 6, Steps: 318 | Train Loss: 0.1701849 Vali Loss: 0.1737879 Test Loss: 0.2041735
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1596299
	speed: 0.0691s/iter; left time: 81.0065s
	iters: 200, epoch: 7 | loss: 0.2237186
	speed: 0.0760s/iter; left time: 81.4958s
	iters: 300, epoch: 7 | loss: 0.1406969
	speed: 0.0657s/iter; left time: 63.9079s
Epoch: 7 cost time: 22.506901025772095
Epoch: 7, Steps: 318 | Train Loss: 0.1696781 Vali Loss: 0.1720122 Test Loss: 0.2045896
Validation loss decreased (0.172968 --> 0.172012).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1305950
	speed: 0.0829s/iter; left time: 70.9136s
	iters: 200, epoch: 8 | loss: 0.1239006
	speed: 0.0780s/iter; left time: 58.8651s
	iters: 300, epoch: 8 | loss: 0.2035618
	speed: 0.0801s/iter; left time: 52.4552s
Epoch: 8 cost time: 25.479081869125366
Epoch: 8, Steps: 318 | Train Loss: 0.1698419 Vali Loss: 0.1726390 Test Loss: 0.2047820
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1471154
	speed: 0.0884s/iter; left time: 47.4845s
	iters: 200, epoch: 9 | loss: 0.1726500
	speed: 0.0726s/iter; left time: 31.7362s
	iters: 300, epoch: 9 | loss: 0.1346303
	speed: 0.0629s/iter; left time: 21.1934s
Epoch: 9 cost time: 23.5685555934906
Epoch: 9, Steps: 318 | Train Loss: 0.1693074 Vali Loss: 0.1729756 Test Loss: 0.2052140
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1823567
	speed: 0.0877s/iter; left time: 19.2094s
	iters: 200, epoch: 10 | loss: 0.1528758
	speed: 0.0710s/iter; left time: 8.4481s
	iters: 300, epoch: 10 | loss: 0.1621024
	speed: 0.0781s/iter; left time: 1.4835s
Epoch: 10 cost time: 25.28424644470215
Epoch: 10, Steps: 318 | Train Loss: 0.1687100 Vali Loss: 0.1736786 Test Loss: 0.2051842
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.4489637613296509, mae:0.517307460308075, rmse:0.6700475811958313, mape:0.018517358228564262, mspe:0.0005834212643094361, rse:0.4692721962928772, r2_score:0.7282213223260956, acc:0.9814826417714357
corr: [36.01361  36.113567 36.02405  36.13573  36.251614 36.06355  35.810314
 36.11267  36.194466 36.306942 36.06638  36.12899  36.287945 35.88974
 36.444565]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2583658
	speed: 0.0992s/iter; left time: 305.5618s
	iters: 200, epoch: 1 | loss: 0.1742193
	speed: 0.0768s/iter; left time: 228.9021s
	iters: 300, epoch: 1 | loss: 0.2547958
	speed: 0.0724s/iter; left time: 208.7067s
Epoch: 1 cost time: 25.983683347702026
Epoch: 1, Steps: 318 | Train Loss: 0.3705717 Vali Loss: 0.2068932 Test Loss: 0.2135565
Validation loss decreased (inf --> 0.206893).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1857140
	speed: 0.0902s/iter; left time: 249.3533s
	iters: 200, epoch: 2 | loss: 0.1789808
	speed: 0.0788s/iter; left time: 209.7808s
	iters: 300, epoch: 2 | loss: 0.1710093
	speed: 0.0812s/iter; left time: 208.1726s
Epoch: 2 cost time: 26.389869689941406
Epoch: 2, Steps: 318 | Train Loss: 0.1993488 Vali Loss: 0.1975609 Test Loss: 0.2538240
Validation loss decreased (0.206893 --> 0.197561).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1831445
	speed: 0.0735s/iter; left time: 179.6193s
	iters: 200, epoch: 3 | loss: 0.1786642
	speed: 0.0719s/iter; left time: 168.6128s
	iters: 300, epoch: 3 | loss: 0.2771102
	speed: 0.0773s/iter; left time: 173.5321s
Epoch: 3 cost time: 23.6785626411438
Epoch: 3, Steps: 318 | Train Loss: 0.1867710 Vali Loss: 0.2003542 Test Loss: 0.2723957
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2227307
	speed: 0.0864s/iter; left time: 183.8214s
	iters: 200, epoch: 4 | loss: 0.1553934
	speed: 0.0747s/iter; left time: 151.3926s
	iters: 300, epoch: 4 | loss: 0.1708217
	speed: 0.0699s/iter; left time: 134.6305s
Epoch: 4 cost time: 24.16013479232788
Epoch: 4, Steps: 318 | Train Loss: 0.1809569 Vali Loss: 0.1957890 Test Loss: 0.2892252
Validation loss decreased (0.197561 --> 0.195789).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1882462
	speed: 0.0846s/iter; left time: 153.0247s
	iters: 200, epoch: 5 | loss: 0.1727838
	speed: 0.0819s/iter; left time: 139.9380s
	iters: 300, epoch: 5 | loss: 0.1648272
	speed: 0.0769s/iter; left time: 123.7650s
Epoch: 5 cost time: 25.8933002948761
Epoch: 5, Steps: 318 | Train Loss: 0.1784624 Vali Loss: 0.1982456 Test Loss: 0.2959183
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1619658
	speed: 0.0801s/iter; left time: 119.4124s
	iters: 200, epoch: 6 | loss: 0.1643372
	speed: 0.0836s/iter; left time: 116.2741s
	iters: 300, epoch: 6 | loss: 0.1597438
	speed: 0.0692s/iter; left time: 89.3331s
Epoch: 6 cost time: 24.70883274078369
Epoch: 6, Steps: 318 | Train Loss: 0.1768310 Vali Loss: 0.1987564 Test Loss: 0.2978691
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1923703
	speed: 0.0841s/iter; left time: 98.6695s
	iters: 200, epoch: 7 | loss: 0.1412767
	speed: 0.0784s/iter; left time: 84.1039s
	iters: 300, epoch: 7 | loss: 0.1857560
	speed: 0.0754s/iter; left time: 73.3614s
Epoch: 7 cost time: 25.250990629196167
Epoch: 7, Steps: 318 | Train Loss: 0.1765934 Vali Loss: 0.1987552 Test Loss: 0.2995054
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.634693443775177, mae:0.5990843772888184, rmse:0.796676516532898, mape:0.02151898667216301, mspe:0.0008387137786485255, rse:0.5576344728469849, r2_score:0.5825215944161976, acc:0.978481013327837
corr: [35.740902 35.488674 35.663277 35.38789  35.369774 34.85538  34.45788
 34.302494 33.9495   33.9564   33.896687 33.7727   33.009438 32.9756
 33.657276 33.733913 34.21603  34.009624 33.606186 33.359226]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4153273
	speed: 0.1035s/iter; left time: 318.7352s
	iters: 200, epoch: 1 | loss: 0.2969005
	speed: 0.0835s/iter; left time: 248.8392s
	iters: 300, epoch: 1 | loss: 0.2156472
	speed: 0.0774s/iter; left time: 223.0967s
Epoch: 1 cost time: 27.85716414451599
Epoch: 1, Steps: 318 | Train Loss: 0.3851598 Vali Loss: 0.2269908 Test Loss: 0.2843037
Validation loss decreased (inf --> 0.226991).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2384676
	speed: 0.0852s/iter; left time: 235.4723s
	iters: 200, epoch: 2 | loss: 0.2132547
	speed: 0.0868s/iter; left time: 231.2650s
	iters: 300, epoch: 2 | loss: 0.2264882
	speed: 0.0591s/iter; left time: 151.5031s
Epoch: 2 cost time: 24.466734886169434
Epoch: 2, Steps: 318 | Train Loss: 0.2105796 Vali Loss: 0.2119801 Test Loss: 0.2691066
Validation loss decreased (0.226991 --> 0.211980).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2331892
	speed: 0.0830s/iter; left time: 202.8585s
	iters: 200, epoch: 3 | loss: 0.2334008
	speed: 0.0715s/iter; left time: 167.7758s
	iters: 300, epoch: 3 | loss: 0.2173213
	speed: 0.0769s/iter; left time: 172.7376s
Epoch: 3 cost time: 24.55845284461975
Epoch: 3, Steps: 318 | Train Loss: 0.1973874 Vali Loss: 0.2052246 Test Loss: 0.2588726
Validation loss decreased (0.211980 --> 0.205225).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1489328
	speed: 0.0870s/iter; left time: 184.9604s
	iters: 200, epoch: 4 | loss: 0.1920151
	speed: 0.0849s/iter; left time: 172.1129s
	iters: 300, epoch: 4 | loss: 0.2097863
	speed: 0.0796s/iter; left time: 153.3228s
Epoch: 4 cost time: 26.67008876800537
Epoch: 4, Steps: 318 | Train Loss: 0.1929439 Vali Loss: 0.2051898 Test Loss: 0.2556323
Validation loss decreased (0.205225 --> 0.205190).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1943906
	speed: 0.0831s/iter; left time: 150.3917s
	iters: 200, epoch: 5 | loss: 0.1948026
	speed: 0.0640s/iter; left time: 109.3475s
	iters: 300, epoch: 5 | loss: 0.2497031
	speed: 0.0768s/iter; left time: 123.6490s
Epoch: 5 cost time: 24.002084732055664
Epoch: 5, Steps: 318 | Train Loss: 0.1910795 Vali Loss: 0.2058981 Test Loss: 0.2441212
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2508718
	speed: 0.0883s/iter; left time: 131.6536s
	iters: 200, epoch: 6 | loss: 0.2226253
	speed: 0.0781s/iter; left time: 108.6979s
	iters: 300, epoch: 6 | loss: 0.2251669
	speed: 0.0746s/iter; left time: 96.2597s
Epoch: 6 cost time: 25.620043516159058
Epoch: 6, Steps: 318 | Train Loss: 0.1895694 Vali Loss: 0.2074460 Test Loss: 0.2458010
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1539625
	speed: 0.0932s/iter; left time: 109.3297s
	iters: 200, epoch: 7 | loss: 0.2207837
	speed: 0.0598s/iter; left time: 64.2071s
	iters: 300, epoch: 7 | loss: 0.1939656
	speed: 0.0670s/iter; left time: 65.1493s
Epoch: 7 cost time: 23.89646625518799
Epoch: 7, Steps: 318 | Train Loss: 0.1889018 Vali Loss: 0.2060306 Test Loss: 0.2423543
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.5609750747680664, mae:0.5829602479934692, rmse:0.7489826679229736, mape:0.020710349082946777, mspe:0.0007059574709273875, rse:0.5239009261131287, r2_score:0.6631085673654937, acc:0.9792896509170532
corr: [35.139988 35.306957 35.35443  34.989223 35.655262 35.563843 35.0754
 35.48037  35.66272  35.796467 35.547947 35.729443 36.044342 36.11401
 36.08456  36.273525 36.36558  36.194458 36.34405  36.630733 36.737312
 36.39994  36.76034  37.066036 36.893906]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4120789
	speed: 0.1272s/iter; left time: 390.6877s
	iters: 200, epoch: 1 | loss: 0.3356688
	speed: 0.0710s/iter; left time: 211.0311s
	iters: 300, epoch: 1 | loss: 0.3763495
	speed: 0.0967s/iter; left time: 277.4958s
Epoch: 1 cost time: 31.13639259338379
Epoch: 1, Steps: 317 | Train Loss: 0.4495872 Vali Loss: 0.3038507 Test Loss: 0.3002982
Validation loss decreased (inf --> 0.303851).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.4366459
	speed: 0.1012s/iter; left time: 278.6119s
	iters: 200, epoch: 2 | loss: 0.2202195
	speed: 0.0859s/iter; left time: 227.8719s
	iters: 300, epoch: 2 | loss: 0.2324962
	speed: 0.0816s/iter; left time: 208.4727s
Epoch: 2 cost time: 28.554789304733276
Epoch: 2, Steps: 317 | Train Loss: 0.2858153 Vali Loss: 0.2855881 Test Loss: 0.2674773
Validation loss decreased (0.303851 --> 0.285588).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2492135
	speed: 0.0909s/iter; left time: 221.6217s
	iters: 200, epoch: 3 | loss: 0.2654679
	speed: 0.0843s/iter; left time: 196.9755s
	iters: 300, epoch: 3 | loss: 0.2292801
	speed: 0.0925s/iter; left time: 206.9936s
Epoch: 3 cost time: 28.440099716186523
Epoch: 3, Steps: 317 | Train Loss: 0.2691460 Vali Loss: 0.2824563 Test Loss: 0.2567342
Validation loss decreased (0.285588 --> 0.282456).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2922038
	speed: 0.0820s/iter; left time: 173.7741s
	iters: 200, epoch: 4 | loss: 0.2543475
	speed: 0.0815s/iter; left time: 164.6332s
	iters: 300, epoch: 4 | loss: 0.2800365
	speed: 0.0858s/iter; left time: 164.6945s
Epoch: 4 cost time: 26.561460494995117
Epoch: 4, Steps: 317 | Train Loss: 0.2650723 Vali Loss: 0.2835464 Test Loss: 0.2570238
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2929441
	speed: 0.1005s/iter; left time: 181.1604s
	iters: 200, epoch: 5 | loss: 0.3064767
	speed: 0.0793s/iter; left time: 134.9807s
	iters: 300, epoch: 5 | loss: 0.1964202
	speed: 0.0790s/iter; left time: 126.5796s
Epoch: 5 cost time: 27.487003564834595
Epoch: 5, Steps: 317 | Train Loss: 0.2612650 Vali Loss: 0.2793660 Test Loss: 0.2540373
Validation loss decreased (0.282456 --> 0.279366).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2469719
	speed: 0.0993s/iter; left time: 147.6125s
	iters: 200, epoch: 6 | loss: 0.2433980
	speed: 0.0845s/iter; left time: 117.0831s
	iters: 300, epoch: 6 | loss: 0.2894877
	speed: 0.0881s/iter; left time: 113.2613s
Epoch: 6 cost time: 28.254544019699097
Epoch: 6, Steps: 317 | Train Loss: 0.2600146 Vali Loss: 0.2848164 Test Loss: 0.2539057
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2609031
	speed: 0.0904s/iter; left time: 105.6340s
	iters: 200, epoch: 7 | loss: 0.1994687
	speed: 0.0785s/iter; left time: 83.8870s
	iters: 300, epoch: 7 | loss: 0.3346256
	speed: 0.0935s/iter; left time: 90.6487s
Epoch: 7 cost time: 27.92589235305786
Epoch: 7, Steps: 317 | Train Loss: 0.2586510 Vali Loss: 0.2816674 Test Loss: 0.2524210
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2982095
	speed: 0.0915s/iter; left time: 77.9212s
	iters: 200, epoch: 8 | loss: 0.2535192
	speed: 0.0757s/iter; left time: 56.8889s
	iters: 300, epoch: 8 | loss: 0.2713370
	speed: 0.0786s/iter; left time: 51.2217s
Epoch: 8 cost time: 26.122312545776367
Epoch: 8, Steps: 317 | Train Loss: 0.2595595 Vali Loss: 0.2785260 Test Loss: 0.2521155
Validation loss decreased (0.279366 --> 0.278526).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1958391
	speed: 0.0994s/iter; left time: 53.1655s
	iters: 200, epoch: 9 | loss: 0.1984692
	speed: 0.0881s/iter; left time: 38.3330s
	iters: 300, epoch: 9 | loss: 0.2442836
	speed: 0.0775s/iter; left time: 25.9689s
Epoch: 9 cost time: 27.958277940750122
Epoch: 9, Steps: 317 | Train Loss: 0.2584196 Vali Loss: 0.2845417 Test Loss: 0.2518301
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.3098983
	speed: 0.0902s/iter; left time: 19.6658s
	iters: 200, epoch: 10 | loss: 0.3024946
	speed: 0.0883s/iter; left time: 10.4224s
	iters: 300, epoch: 10 | loss: 0.3011108
	speed: 0.0840s/iter; left time: 1.5120s
Epoch: 10 cost time: 27.964441061019897
Epoch: 10, Steps: 317 | Train Loss: 0.2584902 Vali Loss: 0.2818936 Test Loss: 0.2517443
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.5532577037811279, mae:0.5721208453178406, rmse:0.7438129782676697, mape:0.020546521991491318, mspe:0.0007315156399272382, rse:0.5199111700057983, r2_score:0.6884200608618743, acc:0.9794534780085087
corr: [37.21989  37.267525 37.185337 37.422226 37.379932 37.034756 37.13895
 36.99963  37.307007 37.28021  37.271347 37.24206  37.34304  37.375145
 37.18674  37.228424 36.966938 37.201065 37.20984  37.31323  37.192986
 37.42721  37.661854 37.78364  37.678703 37.901768 37.659023 37.323353
 37.097626 36.833885]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2910263
	speed: 0.1153s/iter; left time: 355.1856s
	iters: 200, epoch: 1 | loss: 0.2755897
	speed: 0.0882s/iter; left time: 262.8395s
	iters: 300, epoch: 1 | loss: 0.2027644
	speed: 0.1056s/iter; left time: 304.2209s
Epoch: 1 cost time: 32.633381366729736
Epoch: 1, Steps: 318 | Train Loss: 0.3199776 Vali Loss: 0.1909396 Test Loss: 0.1667866
Validation loss decreased (inf --> 0.190940).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1787248
	speed: 0.1004s/iter; left time: 277.2684s
	iters: 200, epoch: 2 | loss: 0.1648403
	speed: 0.0982s/iter; left time: 261.5778s
	iters: 300, epoch: 2 | loss: 0.1832071
	speed: 0.1093s/iter; left time: 280.1962s
Epoch: 2 cost time: 33.22938561439514
Epoch: 2, Steps: 318 | Train Loss: 0.1772266 Vali Loss: 0.1664662 Test Loss: 0.1444770
Validation loss decreased (0.190940 --> 0.166466).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1389769
	speed: 0.1132s/iter; left time: 276.8631s
	iters: 200, epoch: 3 | loss: 0.1478727
	speed: 0.0989s/iter; left time: 231.8347s
	iters: 300, epoch: 3 | loss: 0.2147427
	speed: 0.1058s/iter; left time: 237.6045s
Epoch: 3 cost time: 34.073368072509766
Epoch: 3, Steps: 318 | Train Loss: 0.1625512 Vali Loss: 0.1602355 Test Loss: 0.1393670
Validation loss decreased (0.166466 --> 0.160235).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1668820
	speed: 0.1094s/iter; left time: 232.7621s
	iters: 200, epoch: 4 | loss: 0.1725850
	speed: 0.1038s/iter; left time: 210.3028s
	iters: 300, epoch: 4 | loss: 0.2012357
	speed: 0.1145s/iter; left time: 220.7305s
Epoch: 4 cost time: 34.878376483917236
Epoch: 4, Steps: 318 | Train Loss: 0.1573277 Vali Loss: 0.1561432 Test Loss: 0.1355078
Validation loss decreased (0.160235 --> 0.156143).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1981256
	speed: 0.1130s/iter; left time: 204.4981s
	iters: 200, epoch: 5 | loss: 0.1096409
	speed: 0.1114s/iter; left time: 190.3343s
	iters: 300, epoch: 5 | loss: 0.1489339
	speed: 0.1067s/iter; left time: 171.7401s
Epoch: 5 cost time: 35.44647192955017
Epoch: 5, Steps: 318 | Train Loss: 0.1550648 Vali Loss: 0.1564699 Test Loss: 0.1351319
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1339825
	speed: 0.1276s/iter; left time: 190.2537s
	iters: 200, epoch: 6 | loss: 0.1597324
	speed: 0.1105s/iter; left time: 153.6658s
	iters: 300, epoch: 6 | loss: 0.1592698
	speed: 0.1086s/iter; left time: 140.1610s
Epoch: 6 cost time: 36.590248346328735
Epoch: 6, Steps: 318 | Train Loss: 0.1535706 Vali Loss: 0.1562454 Test Loss: 0.1360435
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1639203
	speed: 0.1159s/iter; left time: 135.9445s
	iters: 200, epoch: 7 | loss: 0.1443707
	speed: 0.0972s/iter; left time: 104.2940s
	iters: 300, epoch: 7 | loss: 0.1290032
	speed: 0.1035s/iter; left time: 100.7215s
Epoch: 7 cost time: 33.66593050956726
Epoch: 7, Steps: 318 | Train Loss: 0.1531079 Vali Loss: 0.1550221 Test Loss: 0.1351216
Validation loss decreased (0.156143 --> 0.155022).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1623670
	speed: 0.1039s/iter; left time: 88.8366s
	iters: 200, epoch: 8 | loss: 0.1104412
	speed: 0.1165s/iter; left time: 87.9371s
	iters: 300, epoch: 8 | loss: 0.1766502
	speed: 0.1155s/iter; left time: 75.6353s
Epoch: 8 cost time: 35.74412393569946
Epoch: 8, Steps: 318 | Train Loss: 0.1523400 Vali Loss: 0.1542672 Test Loss: 0.1349175
Validation loss decreased (0.155022 --> 0.154267).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1179368
	speed: 0.1252s/iter; left time: 67.2463s
	iters: 200, epoch: 9 | loss: 0.1318860
	speed: 0.1284s/iter; left time: 56.1203s
	iters: 300, epoch: 9 | loss: 0.1285089
	speed: 0.1190s/iter; left time: 40.0994s
Epoch: 9 cost time: 39.22973656654358
Epoch: 9, Steps: 318 | Train Loss: 0.1521537 Vali Loss: 0.1552000 Test Loss: 0.1349033
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1548968
	speed: 0.1312s/iter; left time: 28.7375s
	iters: 200, epoch: 10 | loss: 0.1402639
	speed: 0.1284s/iter; left time: 15.2842s
	iters: 300, epoch: 10 | loss: 0.1286176
	speed: 0.1242s/iter; left time: 2.3596s
Epoch: 10 cost time: 40.321401834487915
Epoch: 10, Steps: 318 | Train Loss: 0.1524538 Vali Loss: 0.1542025 Test Loss: 0.1349223
Validation loss decreased (0.154267 --> 0.154202).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.29608169198036194, mae:0.42331069707870483, rmse:0.5441339015960693, mape:0.015133802779018879, mspe:0.0003831869689747691, rse:0.381267249584198, r2_score:0.8391568870270392, acc:0.9848661972209811
corr: [36.719604 36.67796  36.86778  36.90489  36.794155 36.934937 37.008854
 37.15892  37.14287  37.290318]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3060204
	speed: 0.1141s/iter; left time: 351.6849s
	iters: 200, epoch: 1 | loss: 0.2481126
	speed: 0.1042s/iter; left time: 310.6295s
	iters: 300, epoch: 1 | loss: 0.2267177
	speed: 0.1084s/iter; left time: 312.2776s
Epoch: 1 cost time: 34.53261137008667
Epoch: 1, Steps: 318 | Train Loss: 0.3092436 Vali Loss: 0.1927959 Test Loss: 0.1771544
Validation loss decreased (inf --> 0.192796).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1689319
	speed: 0.1207s/iter; left time: 333.5381s
	iters: 200, epoch: 2 | loss: 0.2175804
	speed: 0.0979s/iter; left time: 260.6934s
	iters: 300, epoch: 2 | loss: 0.1784973
	speed: 0.0846s/iter; left time: 216.8556s
Epoch: 2 cost time: 32.04122018814087
Epoch: 2, Steps: 318 | Train Loss: 0.1891969 Vali Loss: 0.1863989 Test Loss: 0.1698783
Validation loss decreased (0.192796 --> 0.186399).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1787411
	speed: 0.0998s/iter; left time: 244.0524s
	iters: 200, epoch: 3 | loss: 0.1979288
	speed: 0.0902s/iter; left time: 211.5895s
	iters: 300, epoch: 3 | loss: 0.1775351
	speed: 0.0937s/iter; left time: 210.2823s
Epoch: 3 cost time: 30.441035985946655
Epoch: 3, Steps: 318 | Train Loss: 0.1750563 Vali Loss: 0.1763045 Test Loss: 0.1559728
Validation loss decreased (0.186399 --> 0.176304).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1413896
	speed: 0.1016s/iter; left time: 216.0584s
	iters: 200, epoch: 4 | loss: 0.1256901
	speed: 0.0928s/iter; left time: 188.0157s
	iters: 300, epoch: 4 | loss: 0.1598188
	speed: 0.0967s/iter; left time: 186.2562s
Epoch: 4 cost time: 30.87098526954651
Epoch: 4, Steps: 318 | Train Loss: 0.1694687 Vali Loss: 0.1742192 Test Loss: 0.1569576
Validation loss decreased (0.176304 --> 0.174219).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1464619
	speed: 0.0963s/iter; left time: 174.1522s
	iters: 200, epoch: 5 | loss: 0.1577721
	speed: 0.0953s/iter; left time: 162.7983s
	iters: 300, epoch: 5 | loss: 0.2237728
	speed: 0.0955s/iter; left time: 153.6777s
Epoch: 5 cost time: 30.694188594818115
Epoch: 5, Steps: 318 | Train Loss: 0.1661279 Vali Loss: 0.1748273 Test Loss: 0.1563077
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1778808
	speed: 0.1105s/iter; left time: 164.7326s
	iters: 200, epoch: 6 | loss: 0.1973386
	speed: 0.1079s/iter; left time: 150.1369s
	iters: 300, epoch: 6 | loss: 0.1624401
	speed: 0.0994s/iter; left time: 128.3649s
Epoch: 6 cost time: 34.04829812049866
Epoch: 6, Steps: 318 | Train Loss: 0.1651449 Vali Loss: 0.1747015 Test Loss: 0.1558239
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1538362
	speed: 0.1057s/iter; left time: 124.0445s
	iters: 200, epoch: 7 | loss: 0.2233879
	speed: 0.0849s/iter; left time: 91.0478s
	iters: 300, epoch: 7 | loss: 0.1421360
	speed: 0.0924s/iter; left time: 89.9319s
Epoch: 7 cost time: 30.11467170715332
Epoch: 7, Steps: 318 | Train Loss: 0.1644644 Vali Loss: 0.1732837 Test Loss: 0.1560468
Validation loss decreased (0.174219 --> 0.173284).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1291254
	speed: 0.1021s/iter; left time: 87.2679s
	iters: 200, epoch: 8 | loss: 0.1154999
	speed: 0.0993s/iter; left time: 74.9382s
	iters: 300, epoch: 8 | loss: 0.1871963
	speed: 0.0995s/iter; left time: 65.1911s
Epoch: 8 cost time: 31.889968156814575
Epoch: 8, Steps: 318 | Train Loss: 0.1644847 Vali Loss: 0.1741082 Test Loss: 0.1565929
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1377382
	speed: 0.1006s/iter; left time: 54.0037s
	iters: 200, epoch: 9 | loss: 0.1678622
	speed: 0.0919s/iter; left time: 40.1593s
	iters: 300, epoch: 9 | loss: 0.1385041
	speed: 0.0973s/iter; left time: 32.7968s
Epoch: 9 cost time: 30.885009288787842
Epoch: 9, Steps: 318 | Train Loss: 0.1634953 Vali Loss: 0.1743684 Test Loss: 0.1565970
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1817705
	speed: 0.0981s/iter; left time: 21.4785s
	iters: 200, epoch: 10 | loss: 0.1456162
	speed: 0.0950s/iter; left time: 11.3089s
	iters: 300, epoch: 10 | loss: 0.1418494
	speed: 0.1046s/iter; left time: 1.9866s
Epoch: 10 cost time: 31.39889430999756
Epoch: 10, Steps: 318 | Train Loss: 0.1633795 Vali Loss: 0.1751424 Test Loss: 0.1564766
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.34243854880332947, mae:0.4529475271701813, rmse:0.5851824879646301, mape:0.01620190031826496, mspe:0.00044470178545452654, rse:0.40983638167381287, r2_score:0.8170996906183183, acc:0.983798099681735
corr: [36.769363 36.69985  36.629074 36.664806 36.72318  36.62727  36.985275
 36.912544 37.039288 37.31559  37.197792 37.02049  37.053898 36.769287
 36.952667]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2796561
	speed: 0.1358s/iter; left time: 418.4611s
	iters: 200, epoch: 1 | loss: 0.1773796
	speed: 0.1412s/iter; left time: 421.0656s
	iters: 300, epoch: 1 | loss: 0.2623886
	speed: 0.1466s/iter; left time: 422.3271s
Epoch: 1 cost time: 45.25218391418457
Epoch: 1, Steps: 318 | Train Loss: 0.3540172 Vali Loss: 0.2182105 Test Loss: 0.1992016
Validation loss decreased (inf --> 0.218210).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2087294
	speed: 0.1545s/iter; left time: 426.8090s
	iters: 200, epoch: 2 | loss: 0.1727902
	speed: 0.1482s/iter; left time: 394.6523s
	iters: 300, epoch: 2 | loss: 0.1913126
	speed: 0.1470s/iter; left time: 376.7280s
Epoch: 2 cost time: 47.41578435897827
Epoch: 2, Steps: 318 | Train Loss: 0.2085598 Vali Loss: 0.2095066 Test Loss: 0.1834190
Validation loss decreased (0.218210 --> 0.209507).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1750038
	speed: 0.1567s/iter; left time: 383.0189s
	iters: 200, epoch: 3 | loss: 0.1760565
	speed: 0.1488s/iter; left time: 349.0256s
	iters: 300, epoch: 3 | loss: 0.2923930
	speed: 0.1370s/iter; left time: 307.6355s
Epoch: 3 cost time: 46.58392691612244
Epoch: 3, Steps: 318 | Train Loss: 0.1945006 Vali Loss: 0.2072638 Test Loss: 0.1792784
Validation loss decreased (0.209507 --> 0.207264).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2140439
	speed: 0.1368s/iter; left time: 290.8694s
	iters: 200, epoch: 4 | loss: 0.1516595
	speed: 0.1371s/iter; left time: 277.8006s
	iters: 300, epoch: 4 | loss: 0.1718014
	speed: 0.1374s/iter; left time: 264.8592s
Epoch: 4 cost time: 43.8231725692749
Epoch: 4, Steps: 318 | Train Loss: 0.1887802 Vali Loss: 0.2025819 Test Loss: 0.1788954
Validation loss decreased (0.207264 --> 0.202582).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1977240
	speed: 0.1458s/iter; left time: 263.7494s
	iters: 200, epoch: 5 | loss: 0.1828721
	speed: 0.1373s/iter; left time: 234.6159s
	iters: 300, epoch: 5 | loss: 0.1748784
	speed: 0.1333s/iter; left time: 214.4859s
Epoch: 5 cost time: 43.963972091674805
Epoch: 5, Steps: 318 | Train Loss: 0.1855524 Vali Loss: 0.2059229 Test Loss: 0.1791660
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1551922
	speed: 0.1408s/iter; left time: 209.9158s
	iters: 200, epoch: 6 | loss: 0.1789298
	speed: 0.1308s/iter; left time: 181.9443s
	iters: 300, epoch: 6 | loss: 0.1546952
	speed: 0.1318s/iter; left time: 170.1018s
Epoch: 6 cost time: 42.71009945869446
Epoch: 6, Steps: 318 | Train Loss: 0.1844038 Vali Loss: 0.2062243 Test Loss: 0.1789797
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1930951
	speed: 0.1465s/iter; left time: 171.8794s
	iters: 200, epoch: 7 | loss: 0.1464750
	speed: 0.1388s/iter; left time: 148.9252s
	iters: 300, epoch: 7 | loss: 0.1784444
	speed: 0.1429s/iter; left time: 139.0574s
Epoch: 7 cost time: 45.3839385509491
Epoch: 7, Steps: 318 | Train Loss: 0.1838261 Vali Loss: 0.2057896 Test Loss: 0.1789025
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.39257895946502686, mae:0.48602059483528137, rmse:0.6265612244606018, mape:0.01739637367427349, mspe:0.0005114550585858524, rse:0.4385620951652527, r2_score:0.7801391848596853, acc:0.9826036263257265
corr: [36.940735 37.035393 36.89239  36.866234 36.968742 36.974415 36.91817
 36.996605 37.11218  37.08903  36.910107 36.96837  37.095833 37.084133
 37.112602 37.60369  37.271297 37.38986  37.300648 37.55054 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4263826
	speed: 0.1371s/iter; left time: 422.5286s
	iters: 200, epoch: 1 | loss: 0.3162180
	speed: 0.0996s/iter; left time: 296.8385s
	iters: 300, epoch: 1 | loss: 0.2441762
	speed: 0.0915s/iter; left time: 263.7093s
Epoch: 1 cost time: 34.546294927597046
Epoch: 1, Steps: 318 | Train Loss: 0.3831213 Vali Loss: 0.2487187 Test Loss: 0.2504725
Validation loss decreased (inf --> 0.248719).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2419371
	speed: 0.1044s/iter; left time: 288.5364s
	iters: 200, epoch: 2 | loss: 0.2180086
	speed: 0.1013s/iter; left time: 269.7288s
	iters: 300, epoch: 2 | loss: 0.2485652
	speed: 0.0969s/iter; left time: 248.4452s
Epoch: 2 cost time: 32.11853051185608
Epoch: 2, Steps: 318 | Train Loss: 0.2261461 Vali Loss: 0.2201019 Test Loss: 0.2077951
Validation loss decreased (0.248719 --> 0.220102).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2017485
	speed: 0.1050s/iter; left time: 256.6148s
	iters: 200, epoch: 3 | loss: 0.2103407
	speed: 0.0903s/iter; left time: 211.6732s
	iters: 300, epoch: 3 | loss: 0.2155651
	speed: 0.0921s/iter; left time: 206.7117s
Epoch: 3 cost time: 30.552780866622925
Epoch: 3, Steps: 318 | Train Loss: 0.2034891 Vali Loss: 0.2087478 Test Loss: 0.1944838
Validation loss decreased (0.220102 --> 0.208748).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1523774
	speed: 0.1077s/iter; left time: 229.0768s
	iters: 200, epoch: 4 | loss: 0.1966530
	speed: 0.0960s/iter; left time: 194.5608s
	iters: 300, epoch: 4 | loss: 0.1996494
	speed: 0.0986s/iter; left time: 190.0454s
Epoch: 4 cost time: 31.88725447654724
Epoch: 4, Steps: 318 | Train Loss: 0.1967874 Vali Loss: 0.2078033 Test Loss: 0.1899282
Validation loss decreased (0.208748 --> 0.207803).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1847837
	speed: 0.1025s/iter; left time: 185.3485s
	iters: 200, epoch: 5 | loss: 0.2009412
	speed: 0.0975s/iter; left time: 166.7015s
	iters: 300, epoch: 5 | loss: 0.2644911
	speed: 0.0971s/iter; left time: 156.1845s
Epoch: 5 cost time: 31.456921815872192
Epoch: 5, Steps: 318 | Train Loss: 0.1940243 Vali Loss: 0.2086200 Test Loss: 0.1894079
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2342739
	speed: 0.0991s/iter; left time: 147.8303s
	iters: 200, epoch: 6 | loss: 0.2412275
	speed: 0.0963s/iter; left time: 134.0082s
	iters: 300, epoch: 6 | loss: 0.2212397
	speed: 0.0840s/iter; left time: 108.4941s
Epoch: 6 cost time: 29.92900013923645
Epoch: 6, Steps: 318 | Train Loss: 0.1925171 Vali Loss: 0.2090147 Test Loss: 0.1893562
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1518400
	speed: 0.1013s/iter; left time: 118.7684s
	iters: 200, epoch: 7 | loss: 0.2248540
	speed: 0.0949s/iter; left time: 101.8075s
	iters: 300, epoch: 7 | loss: 0.2045356
	speed: 0.0931s/iter; left time: 90.5384s
Epoch: 7 cost time: 30.535643815994263
Epoch: 7, Steps: 318 | Train Loss: 0.1919759 Vali Loss: 0.2078111 Test Loss: 0.1880587
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.4167901277542114, mae:0.5029101371765137, rmse:0.6455928683280945, mape:0.01801231876015663, mspe:0.0005440283566713333, rse:0.4515814483165741, r2_score:0.7600076255510203, acc:0.9819876812398434
corr: [36.536095 36.58915  36.539852 36.742596 36.60983  36.65754  36.59819
 36.6703   36.66764  36.805416 36.895336 36.854885 36.90567  37.085464
 36.966232 36.97211  37.056236 37.139732 37.07875  37.007084 37.206852
 37.233784 37.275204 37.47732  37.722004]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4264289
	speed: 0.1690s/iter; left time: 518.8459s
	iters: 200, epoch: 1 | loss: 0.2815636
	speed: 0.1437s/iter; left time: 426.8572s
	iters: 300, epoch: 1 | loss: 0.3264450
	speed: 0.1366s/iter; left time: 392.1461s
Epoch: 1 cost time: 47.25803208351135
Epoch: 1, Steps: 317 | Train Loss: 0.3645992 Vali Loss: 0.2485176 Test Loss: 0.2329277
Validation loss decreased (inf --> 0.248518).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2982189
	speed: 0.1466s/iter; left time: 403.7915s
	iters: 200, epoch: 2 | loss: 0.1952928
	speed: 0.1368s/iter; left time: 363.1241s
	iters: 300, epoch: 2 | loss: 0.1993218
	speed: 0.1364s/iter; left time: 348.3939s
Epoch: 2 cost time: 44.13042330741882
Epoch: 2, Steps: 317 | Train Loss: 0.2212973 Vali Loss: 0.2411877 Test Loss: 0.2172753
Validation loss decreased (0.248518 --> 0.241188).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2513410
	speed: 0.1431s/iter; left time: 348.7464s
	iters: 200, epoch: 3 | loss: 0.2128888
	speed: 0.1417s/iter; left time: 331.1929s
	iters: 300, epoch: 3 | loss: 0.2068093
	speed: 0.1407s/iter; left time: 314.7171s
Epoch: 3 cost time: 44.90370273590088
Epoch: 3, Steps: 317 | Train Loss: 0.2039372 Vali Loss: 0.2360874 Test Loss: 0.2102067
Validation loss decreased (0.241188 --> 0.236087).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1685408
	speed: 0.1439s/iter; left time: 305.0473s
	iters: 200, epoch: 4 | loss: 0.1927352
	speed: 0.1344s/iter; left time: 271.5802s
	iters: 300, epoch: 4 | loss: 0.1944460
	speed: 0.1326s/iter; left time: 254.5911s
Epoch: 4 cost time: 43.465654134750366
Epoch: 4, Steps: 317 | Train Loss: 0.1964557 Vali Loss: 0.2381183 Test Loss: 0.2146163
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1943235
	speed: 0.1466s/iter; left time: 264.3772s
	iters: 200, epoch: 5 | loss: 0.2210671
	speed: 0.1299s/iter; left time: 221.1609s
	iters: 300, epoch: 5 | loss: 0.1701557
	speed: 0.1305s/iter; left time: 209.1484s
Epoch: 5 cost time: 43.25295400619507
Epoch: 5, Steps: 317 | Train Loss: 0.1936676 Vali Loss: 0.2375075 Test Loss: 0.2097280
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1871105
	speed: 0.1541s/iter; left time: 229.0026s
	iters: 200, epoch: 6 | loss: 0.1614354
	speed: 0.1302s/iter; left time: 180.4179s
	iters: 300, epoch: 6 | loss: 0.2019301
	speed: 0.1353s/iter; left time: 173.9506s
Epoch: 6 cost time: 44.20446062088013
Epoch: 6, Steps: 317 | Train Loss: 0.1917807 Vali Loss: 0.2385487 Test Loss: 0.2120120
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.46129027009010315, mae:0.5297694802284241, rmse:0.6791835427284241, mape:0.018950514495372772, mspe:0.0005999968270771205, rse:0.4747364819049835, r2_score:0.744301877401226, acc:0.9810494855046272
corr: [36.95196  37.018456 36.93506  37.00672  37.005604 36.95804  36.908928
 36.923035 37.119106 37.12268  36.920097 36.89533  36.983475 36.93695
 36.78001  37.020275 36.884697 36.982525 36.931576 36.956345 36.93924
 36.826027 36.95378  37.143486 37.044582 36.85081  36.841732 36.720104
 37.10827  37.027645]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2906328
	speed: 0.1331s/iter; left time: 410.2317s
	iters: 200, epoch: 1 | loss: 0.2761711
	speed: 0.1196s/iter; left time: 356.5656s
	iters: 300, epoch: 1 | loss: 0.2021353
	speed: 0.1293s/iter; left time: 372.6564s
Epoch: 1 cost time: 40.47734093666077
Epoch: 1, Steps: 318 | Train Loss: 0.3209403 Vali Loss: 0.1911071 Test Loss: 0.1666354
Validation loss decreased (inf --> 0.191107).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1786830
	speed: 0.1421s/iter; left time: 392.6456s
	iters: 200, epoch: 2 | loss: 0.1650171
	speed: 0.1139s/iter; left time: 303.4376s
	iters: 300, epoch: 2 | loss: 0.1833464
	speed: 0.1099s/iter; left time: 281.6219s
Epoch: 2 cost time: 38.69544267654419
Epoch: 2, Steps: 318 | Train Loss: 0.1772752 Vali Loss: 0.1663829 Test Loss: 0.1443466
Validation loss decreased (0.191107 --> 0.166383).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1397177
	speed: 0.1336s/iter; left time: 326.5370s
	iters: 200, epoch: 3 | loss: 0.1478051
	speed: 0.1276s/iter; left time: 299.2834s
	iters: 300, epoch: 3 | loss: 0.2144033
	speed: 0.1134s/iter; left time: 254.5545s
Epoch: 3 cost time: 39.65258979797363
Epoch: 3, Steps: 318 | Train Loss: 0.1625029 Vali Loss: 0.1601299 Test Loss: 0.1392254
Validation loss decreased (0.166383 --> 0.160130).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1663840
	speed: 0.1237s/iter; left time: 263.1720s
	iters: 200, epoch: 4 | loss: 0.1724215
	speed: 0.1194s/iter; left time: 241.9315s
	iters: 300, epoch: 4 | loss: 0.2019534
	speed: 0.1177s/iter; left time: 226.7666s
Epoch: 4 cost time: 38.24655270576477
Epoch: 4, Steps: 318 | Train Loss: 0.1572309 Vali Loss: 0.1561165 Test Loss: 0.1354342
Validation loss decreased (0.160130 --> 0.156116).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1971211
	speed: 0.1291s/iter; left time: 233.5163s
	iters: 200, epoch: 5 | loss: 0.1098102
	speed: 0.1229s/iter; left time: 209.9511s
	iters: 300, epoch: 5 | loss: 0.1488736
	speed: 0.1235s/iter; left time: 198.7898s
Epoch: 5 cost time: 39.81691241264343
Epoch: 5, Steps: 318 | Train Loss: 0.1549403 Vali Loss: 0.1564497 Test Loss: 0.1350855
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1333902
	speed: 0.1240s/iter; left time: 184.8497s
	iters: 200, epoch: 6 | loss: 0.1593176
	speed: 0.1216s/iter; left time: 169.1749s
	iters: 300, epoch: 6 | loss: 0.1589723
	speed: 0.1256s/iter; left time: 162.1204s
Epoch: 6 cost time: 39.15891981124878
Epoch: 6, Steps: 318 | Train Loss: 0.1534365 Vali Loss: 0.1561926 Test Loss: 0.1359955
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1644450
	speed: 0.1382s/iter; left time: 162.1572s
	iters: 200, epoch: 7 | loss: 0.1450293
	speed: 0.1128s/iter; left time: 120.9957s
	iters: 300, epoch: 7 | loss: 0.1284921
	speed: 0.1066s/iter; left time: 103.7018s
Epoch: 7 cost time: 38.23300504684448
Epoch: 7, Steps: 318 | Train Loss: 0.1530085 Vali Loss: 0.1549883 Test Loss: 0.1350452
Validation loss decreased (0.156116 --> 0.154988).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1620179
	speed: 0.1332s/iter; left time: 113.8559s
	iters: 200, epoch: 8 | loss: 0.1113082
	speed: 0.1283s/iter; left time: 96.8814s
	iters: 300, epoch: 8 | loss: 0.1752541
	speed: 0.1087s/iter; left time: 71.1751s
Epoch: 8 cost time: 39.29630208015442
Epoch: 8, Steps: 318 | Train Loss: 0.1522166 Vali Loss: 0.1542299 Test Loss: 0.1348561
Validation loss decreased (0.154988 --> 0.154230).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1179603
	speed: 0.1275s/iter; left time: 68.4854s
	iters: 200, epoch: 9 | loss: 0.1311671
	speed: 0.1268s/iter; left time: 55.3929s
	iters: 300, epoch: 9 | loss: 0.1288702
	speed: 0.1124s/iter; left time: 37.8919s
Epoch: 9 cost time: 38.508973598480225
Epoch: 9, Steps: 318 | Train Loss: 0.1520814 Vali Loss: 0.1551539 Test Loss: 0.1348381
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1540269
	speed: 0.1360s/iter; left time: 29.7882s
	iters: 200, epoch: 10 | loss: 0.1403030
	speed: 0.1226s/iter; left time: 14.5930s
	iters: 300, epoch: 10 | loss: 0.1283101
	speed: 0.1195s/iter; left time: 2.2700s
Epoch: 10 cost time: 40.096455097198486
Epoch: 10, Steps: 318 | Train Loss: 0.1523726 Vali Loss: 0.1541720 Test Loss: 0.1348566
Validation loss decreased (0.154230 --> 0.154172).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.29593753814697266, mae:0.42305687069892883, rmse:0.5440014004707336, mape:0.015123900957405567, mspe:0.00038295789272524416, rse:0.38117438554763794, r2_score:0.8394673830177348, acc:0.9848760990425944
corr: [36.696156 36.665318 36.842426 36.86969  36.7712   36.904236 36.988785
 37.131626 37.123848 37.28575 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3072616
	speed: 0.1307s/iter; left time: 402.7743s
	iters: 200, epoch: 1 | loss: 0.2633962
	speed: 0.1045s/iter; left time: 311.5642s
	iters: 300, epoch: 1 | loss: 0.2118452
	speed: 0.1115s/iter; left time: 321.1326s
Epoch: 1 cost time: 36.72410440444946
Epoch: 1, Steps: 318 | Train Loss: 0.3258555 Vali Loss: 0.2022368 Test Loss: 0.1789392
Validation loss decreased (inf --> 0.202237).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1720778
	speed: 0.1077s/iter; left time: 297.7040s
	iters: 200, epoch: 2 | loss: 0.2101128
	speed: 0.1141s/iter; left time: 303.9121s
	iters: 300, epoch: 2 | loss: 0.1813173
	speed: 0.1111s/iter; left time: 284.6756s
Epoch: 2 cost time: 34.95209717750549
Epoch: 2, Steps: 318 | Train Loss: 0.1893314 Vali Loss: 0.1789383 Test Loss: 0.1617296
Validation loss decreased (0.202237 --> 0.178938).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1826590
	speed: 0.1142s/iter; left time: 279.1151s
	iters: 200, epoch: 3 | loss: 0.2111655
	speed: 0.1084s/iter; left time: 254.2759s
	iters: 300, epoch: 3 | loss: 0.1913261
	speed: 0.1053s/iter; left time: 236.3475s
Epoch: 3 cost time: 34.53402924537659
Epoch: 3, Steps: 318 | Train Loss: 0.1766071 Vali Loss: 0.1736239 Test Loss: 0.1549452
Validation loss decreased (0.178938 --> 0.173624).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1544251
	speed: 0.1141s/iter; left time: 242.7430s
	iters: 200, epoch: 4 | loss: 0.1241910
	speed: 0.1076s/iter; left time: 218.0263s
	iters: 300, epoch: 4 | loss: 0.1696916
	speed: 0.1091s/iter; left time: 210.2324s
Epoch: 4 cost time: 35.03505754470825
Epoch: 4, Steps: 318 | Train Loss: 0.1721585 Vali Loss: 0.1713311 Test Loss: 0.1561963
Validation loss decreased (0.173624 --> 0.171331).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1504214
	speed: 0.1118s/iter; left time: 202.2997s
	iters: 200, epoch: 5 | loss: 0.1455838
	speed: 0.1063s/iter; left time: 181.7293s
	iters: 300, epoch: 5 | loss: 0.2288056
	speed: 0.1046s/iter; left time: 168.2285s
Epoch: 5 cost time: 34.0499312877655
Epoch: 5, Steps: 318 | Train Loss: 0.1692452 Vali Loss: 0.1716485 Test Loss: 0.1555202
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1846078
	speed: 0.1101s/iter; left time: 164.1902s
	iters: 200, epoch: 6 | loss: 0.1881756
	speed: 0.1092s/iter; left time: 151.8313s
	iters: 300, epoch: 6 | loss: 0.1691520
	speed: 0.1086s/iter; left time: 140.1472s
Epoch: 6 cost time: 34.66112470626831
Epoch: 6, Steps: 318 | Train Loss: 0.1680455 Vali Loss: 0.1710872 Test Loss: 0.1548104
Validation loss decreased (0.171331 --> 0.171087).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1576673
	speed: 0.1167s/iter; left time: 136.9090s
	iters: 200, epoch: 7 | loss: 0.2212666
	speed: 0.1042s/iter; left time: 111.8573s
	iters: 300, epoch: 7 | loss: 0.1370651
	speed: 0.1091s/iter; left time: 106.1457s
Epoch: 7 cost time: 34.9338641166687
Epoch: 7, Steps: 318 | Train Loss: 0.1680177 Vali Loss: 0.1699011 Test Loss: 0.1549393
Validation loss decreased (0.171087 --> 0.169901).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1345076
	speed: 0.1099s/iter; left time: 93.9818s
	iters: 200, epoch: 8 | loss: 0.1308125
	speed: 0.1036s/iter; left time: 78.2014s
	iters: 300, epoch: 8 | loss: 0.2161822
	speed: 0.1061s/iter; left time: 69.4788s
Epoch: 8 cost time: 34.2280957698822
Epoch: 8, Steps: 318 | Train Loss: 0.1676439 Vali Loss: 0.1705111 Test Loss: 0.1554017
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1452132
	speed: 0.1079s/iter; left time: 57.9439s
	iters: 200, epoch: 9 | loss: 0.1770662
	speed: 0.1012s/iter; left time: 44.2322s
	iters: 300, epoch: 9 | loss: 0.1327810
	speed: 0.1124s/iter; left time: 37.8679s
Epoch: 9 cost time: 34.301957845687866
Epoch: 9, Steps: 318 | Train Loss: 0.1672221 Vali Loss: 0.1705246 Test Loss: 0.1555249
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1929987
	speed: 0.1228s/iter; left time: 26.9021s
	iters: 200, epoch: 10 | loss: 0.1449132
	speed: 0.1260s/iter; left time: 14.9989s
	iters: 300, epoch: 10 | loss: 0.1539477
	speed: 0.1214s/iter; left time: 2.3075s
Epoch: 10 cost time: 39.33522605895996
Epoch: 10, Steps: 318 | Train Loss: 0.1668447 Vali Loss: 0.1714105 Test Loss: 0.1554530
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.34000831842422485, mae:0.4514695703983307, rmse:0.5831023454666138, mape:0.01615951955318451, mspe:0.00044273282401263714, rse:0.40837952494621277, r2_score:0.8128142988386126, acc:0.9838404804468155
corr: [36.74817  36.87904  36.86804  36.92104  36.761414 36.92399  36.931767
 36.99563  36.905926 36.98664  37.125454 37.093857 37.08451  37.003094
 37.277008]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2801994
	speed: 0.1927s/iter; left time: 593.7390s
	iters: 200, epoch: 1 | loss: 0.1775381
	speed: 0.1627s/iter; left time: 485.0124s
	iters: 300, epoch: 1 | loss: 0.2635344
	speed: 0.1759s/iter; left time: 506.8598s
Epoch: 1 cost time: 56.91457915306091
Epoch: 1, Steps: 318 | Train Loss: 0.3577758 Vali Loss: 0.2197915 Test Loss: 0.1978971
Validation loss decreased (inf --> 0.219791).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2049867
	speed: 0.1801s/iter; left time: 497.6070s
	iters: 200, epoch: 2 | loss: 0.1745763
	speed: 0.1719s/iter; left time: 457.8679s
	iters: 300, epoch: 2 | loss: 0.1916113
	speed: 0.1841s/iter; left time: 471.8610s
Epoch: 2 cost time: 56.757760524749756
Epoch: 2, Steps: 318 | Train Loss: 0.2081589 Vali Loss: 0.2121422 Test Loss: 0.1832003
Validation loss decreased (0.219791 --> 0.212142).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1752726
	speed: 0.1737s/iter; left time: 424.7869s
	iters: 200, epoch: 3 | loss: 0.1750550
	speed: 0.1620s/iter; left time: 379.9492s
	iters: 300, epoch: 3 | loss: 0.2929831
	speed: 0.1620s/iter; left time: 363.6103s
Epoch: 3 cost time: 53.03254318237305
Epoch: 3, Steps: 318 | Train Loss: 0.1944303 Vali Loss: 0.2107545 Test Loss: 0.1795608
Validation loss decreased (0.212142 --> 0.210755).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2143023
	speed: 0.1728s/iter; left time: 367.6174s
	iters: 200, epoch: 4 | loss: 0.1503536
	speed: 0.1678s/iter; left time: 340.0862s
	iters: 300, epoch: 4 | loss: 0.1736465
	speed: 0.1592s/iter; left time: 306.8075s
Epoch: 4 cost time: 52.60790538787842
Epoch: 4, Steps: 318 | Train Loss: 0.1887730 Vali Loss: 0.2063530 Test Loss: 0.1791729
Validation loss decreased (0.210755 --> 0.206353).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1983215
	speed: 0.1764s/iter; left time: 319.1287s
	iters: 200, epoch: 5 | loss: 0.1848529
	speed: 0.1617s/iter; left time: 276.3278s
	iters: 300, epoch: 5 | loss: 0.1768589
	speed: 0.1579s/iter; left time: 254.0110s
Epoch: 5 cost time: 52.528998613357544
Epoch: 5, Steps: 318 | Train Loss: 0.1855134 Vali Loss: 0.2097703 Test Loss: 0.1795845
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1568578
	speed: 0.1745s/iter; left time: 260.1261s
	iters: 200, epoch: 6 | loss: 0.1787705
	speed: 0.1658s/iter; left time: 230.6887s
	iters: 300, epoch: 6 | loss: 0.1547238
	speed: 0.1533s/iter; left time: 197.8923s
Epoch: 6 cost time: 52.1646614074707
Epoch: 6, Steps: 318 | Train Loss: 0.1844558 Vali Loss: 0.2100513 Test Loss: 0.1794180
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1930122
	speed: 0.1691s/iter; left time: 198.3710s
	iters: 200, epoch: 7 | loss: 0.1465914
	speed: 0.1566s/iter; left time: 168.0355s
	iters: 300, epoch: 7 | loss: 0.1790869
	speed: 0.1631s/iter; left time: 158.6764s
Epoch: 7 cost time: 52.06921362876892
Epoch: 7, Steps: 318 | Train Loss: 0.1838360 Vali Loss: 0.2096129 Test Loss: 0.1792565
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.3931879997253418, mae:0.4866213798522949, rmse:0.627047061920166, mape:0.01741894893348217, mspe:0.0005121835856698453, rse:0.4389021694660187, r2_score:0.7797645762865502, acc:0.9825810510665178
corr: [36.961025 37.03804  36.889713 36.855026 36.95593  36.986393 36.87084
 36.96701  37.10459  37.04424  36.88454  36.945225 37.0655   37.06951
 37.074406 37.60098  37.283222 37.361813 37.295013 37.554653]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4226118
	speed: 0.1795s/iter; left time: 553.0094s
	iters: 200, epoch: 1 | loss: 0.3137040
	speed: 0.1205s/iter; left time: 359.1142s
	iters: 300, epoch: 1 | loss: 0.2397949
	speed: 0.1363s/iter; left time: 392.5476s
Epoch: 1 cost time: 46.024439573287964
Epoch: 1, Steps: 318 | Train Loss: 0.3799084 Vali Loss: 0.2433086 Test Loss: 0.2446865
Validation loss decreased (inf --> 0.243309).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2379872
	speed: 0.1307s/iter; left time: 360.9989s
	iters: 200, epoch: 2 | loss: 0.2129311
	speed: 0.1338s/iter; left time: 356.2712s
	iters: 300, epoch: 2 | loss: 0.2437224
	speed: 0.1278s/iter; left time: 327.5489s
Epoch: 2 cost time: 41.76578998565674
Epoch: 2, Steps: 318 | Train Loss: 0.2234447 Vali Loss: 0.2164412 Test Loss: 0.2036654
Validation loss decreased (0.243309 --> 0.216441).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2014761
	speed: 0.1363s/iter; left time: 333.1808s
	iters: 200, epoch: 3 | loss: 0.2090088
	speed: 0.1310s/iter; left time: 307.1971s
	iters: 300, epoch: 3 | loss: 0.2156528
	speed: 0.1334s/iter; left time: 299.4585s
Epoch: 3 cost time: 42.56800413131714
Epoch: 3, Steps: 318 | Train Loss: 0.2022270 Vali Loss: 0.2058541 Test Loss: 0.1910396
Validation loss decreased (0.216441 --> 0.205854).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1527672
	speed: 0.1306s/iter; left time: 277.8748s
	iters: 200, epoch: 4 | loss: 0.1948041
	speed: 0.1339s/iter; left time: 271.4913s
	iters: 300, epoch: 4 | loss: 0.1999038
	speed: 0.1330s/iter; left time: 256.2072s
Epoch: 4 cost time: 42.40559148788452
Epoch: 4, Steps: 318 | Train Loss: 0.1960464 Vali Loss: 0.2054108 Test Loss: 0.1871027
Validation loss decreased (0.205854 --> 0.205411).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1848280
	speed: 0.1490s/iter; left time: 269.4704s
	iters: 200, epoch: 5 | loss: 0.2020967
	speed: 0.1451s/iter; left time: 247.9200s
	iters: 300, epoch: 5 | loss: 0.2594401
	speed: 0.1261s/iter; left time: 202.8867s
Epoch: 5 cost time: 44.27746605873108
Epoch: 5, Steps: 318 | Train Loss: 0.1934714 Vali Loss: 0.2063001 Test Loss: 0.1868672
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2347433
	speed: 0.1470s/iter; left time: 219.1519s
	iters: 200, epoch: 6 | loss: 0.2379216
	speed: 0.1130s/iter; left time: 157.1535s
	iters: 300, epoch: 6 | loss: 0.2221645
	speed: 0.1173s/iter; left time: 151.4573s
Epoch: 6 cost time: 40.10094428062439
Epoch: 6, Steps: 318 | Train Loss: 0.1920245 Vali Loss: 0.2068090 Test Loss: 0.1867479
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1506538
	speed: 0.1343s/iter; left time: 157.4894s
	iters: 200, epoch: 7 | loss: 0.2223703
	speed: 0.1172s/iter; left time: 125.7287s
	iters: 300, epoch: 7 | loss: 0.2042269
	speed: 0.1271s/iter; left time: 123.6681s
Epoch: 7 cost time: 40.04965662956238
Epoch: 7, Steps: 318 | Train Loss: 0.1914626 Vali Loss: 0.2057522 Test Loss: 0.1853765
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.4105895757675171, mae:0.49913322925567627, rmse:0.6407726407051086, mape:0.017873991280794144, mspe:0.0005353432497940958, rse:0.4482097923755646, r2_score:0.7644466866717654, acc:0.9821260087192059
corr: [36.569828 36.63529  36.57108  36.815926 36.65959  36.697872 36.67743
 36.774242 36.719364 36.85514  36.92832  36.922546 36.9717   37.1321
 37.07604  37.04129  37.123478 37.198196 37.16399  37.080948 37.26148
 37.25604  37.329742 37.54788  37.78788 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4106718
	speed: 0.2544s/iter; left time: 781.3379s
	iters: 200, epoch: 1 | loss: 0.2604805
	speed: 0.2209s/iter; left time: 656.3757s
	iters: 300, epoch: 1 | loss: 0.3246456
	speed: 0.2376s/iter; left time: 682.2768s
Epoch: 1 cost time: 75.4998631477356
Epoch: 1, Steps: 317 | Train Loss: 0.3551092 Vali Loss: 0.2402014 Test Loss: 0.2152016
Validation loss decreased (inf --> 0.240201).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2828451
	speed: 0.2497s/iter; left time: 687.6237s
	iters: 200, epoch: 2 | loss: 0.1790431
	speed: 0.2483s/iter; left time: 659.0932s
	iters: 300, epoch: 2 | loss: 0.1877592
	speed: 0.2438s/iter; left time: 622.6594s
Epoch: 2 cost time: 78.29774594306946
Epoch: 2, Steps: 317 | Train Loss: 0.2165950 Vali Loss: 0.2283252 Test Loss: 0.2074332
Validation loss decreased (0.240201 --> 0.228325).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2551750
	speed: 0.2284s/iter; left time: 556.5085s
	iters: 200, epoch: 3 | loss: 0.2260304
	speed: 0.2431s/iter; left time: 568.1856s
	iters: 300, epoch: 3 | loss: 0.2075144
	speed: 0.2267s/iter; left time: 507.1197s
Epoch: 3 cost time: 75.03341889381409
Epoch: 3, Steps: 317 | Train Loss: 0.2038820 Vali Loss: 0.2234812 Test Loss: 0.1991930
Validation loss decreased (0.228325 --> 0.223481).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1695540
	speed: 0.2429s/iter; left time: 514.9681s
	iters: 200, epoch: 4 | loss: 0.1882641
	speed: 0.2337s/iter; left time: 472.0289s
	iters: 300, epoch: 4 | loss: 0.1946337
	speed: 0.2326s/iter; left time: 446.5819s
Epoch: 4 cost time: 74.86047768592834
Epoch: 4, Steps: 317 | Train Loss: 0.1982313 Vali Loss: 0.2238316 Test Loss: 0.2012680
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1907390
	speed: 0.2542s/iter; left time: 458.2359s
	iters: 200, epoch: 5 | loss: 0.2202702
	speed: 0.2373s/iter; left time: 404.0920s
	iters: 300, epoch: 5 | loss: 0.1677544
	speed: 0.2394s/iter; left time: 383.7787s
Epoch: 5 cost time: 76.97502756118774
Epoch: 5, Steps: 317 | Train Loss: 0.1958119 Vali Loss: 0.2207480 Test Loss: 0.1975051
Validation loss decreased (0.223481 --> 0.220748).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1798638
	speed: 0.2470s/iter; left time: 367.0399s
	iters: 200, epoch: 6 | loss: 0.1695773
	speed: 0.2555s/iter; left time: 354.1505s
	iters: 300, epoch: 6 | loss: 0.2213470
	speed: 0.2279s/iter; left time: 293.0938s
Epoch: 6 cost time: 76.90120387077332
Epoch: 6, Steps: 317 | Train Loss: 0.1943169 Vali Loss: 0.2215400 Test Loss: 0.1990226
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2100337
	speed: 0.2544s/iter; left time: 297.3580s
	iters: 200, epoch: 7 | loss: 0.1920047
	speed: 0.2282s/iter; left time: 243.8945s
	iters: 300, epoch: 7 | loss: 0.2736672
	speed: 0.2377s/iter; left time: 230.2949s
Epoch: 7 cost time: 76.35640525817871
Epoch: 7, Steps: 317 | Train Loss: 0.1937735 Vali Loss: 0.2224718 Test Loss: 0.1988409
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2238608
	speed: 0.2511s/iter; left time: 213.9699s
	iters: 200, epoch: 8 | loss: 0.1979776
	speed: 0.2402s/iter; left time: 180.6042s
	iters: 300, epoch: 8 | loss: 0.2044217
	speed: 0.2344s/iter; left time: 152.8613s
Epoch: 8 cost time: 76.29724407196045
Epoch: 8, Steps: 317 | Train Loss: 0.1932581 Vali Loss: 0.2220579 Test Loss: 0.1979262
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.43341711163520813, mae:0.5101011991500854, rmse:0.6583442091941833, mape:0.018259532749652863, mspe:0.0005647840443998575, rse:0.460170179605484, r2_score:0.762374411766394, acc:0.9817404672503471
corr: [37.120808 37.10354  37.07801  37.056015 37.017952 36.97732  37.005238
 36.965313 37.079727 37.180805 37.190582 37.098213 37.15173  37.06805
 36.972336 37.135185 37.068153 37.11128  37.09529  37.076954 37.06468
 37.007748 37.06489  37.269432 37.186672 37.117462 37.079853 37.014717
 37.270813 37.299107]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2933257
	speed: 0.1797s/iter; left time: 553.7097s
	iters: 200, epoch: 1 | loss: 0.2783078
	speed: 0.1688s/iter; left time: 503.0716s
	iters: 300, epoch: 1 | loss: 0.2019008
	speed: 0.1584s/iter; left time: 456.4390s
Epoch: 1 cost time: 53.56618595123291
Epoch: 1, Steps: 318 | Train Loss: 0.3220732 Vali Loss: 0.1919441 Test Loss: 0.1677314
Validation loss decreased (inf --> 0.191944).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1817348
	speed: 0.1549s/iter; left time: 428.0398s
	iters: 200, epoch: 2 | loss: 0.1648856
	speed: 0.1480s/iter; left time: 394.2110s
	iters: 300, epoch: 2 | loss: 0.1805597
	speed: 0.1305s/iter; left time: 334.4892s
Epoch: 2 cost time: 46.13043022155762
Epoch: 2, Steps: 318 | Train Loss: 0.1779455 Vali Loss: 0.1683504 Test Loss: 0.1457239
Validation loss decreased (0.191944 --> 0.168350).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1408673
	speed: 0.1529s/iter; left time: 373.9439s
	iters: 200, epoch: 3 | loss: 0.1463487
	speed: 0.1364s/iter; left time: 319.9416s
	iters: 300, epoch: 3 | loss: 0.2082774
	speed: 0.1370s/iter; left time: 307.5947s
Epoch: 3 cost time: 45.32763075828552
Epoch: 3, Steps: 318 | Train Loss: 0.1621564 Vali Loss: 0.1612799 Test Loss: 0.1400073
Validation loss decreased (0.168350 --> 0.161280).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1651690
	speed: 0.1627s/iter; left time: 346.0969s
	iters: 200, epoch: 4 | loss: 0.1716017
	speed: 0.1447s/iter; left time: 293.2389s
	iters: 300, epoch: 4 | loss: 0.1985170
	speed: 0.1342s/iter; left time: 258.6832s
Epoch: 4 cost time: 46.8394091129303
Epoch: 4, Steps: 318 | Train Loss: 0.1560002 Vali Loss: 0.1577565 Test Loss: 0.1361082
Validation loss decreased (0.161280 --> 0.157757).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2001570
	speed: 0.1595s/iter; left time: 288.4675s
	iters: 200, epoch: 5 | loss: 0.1058810
	speed: 0.1524s/iter; left time: 260.4864s
	iters: 300, epoch: 5 | loss: 0.1495552
	speed: 0.1294s/iter; left time: 208.1571s
Epoch: 5 cost time: 46.46654200553894
Epoch: 5, Steps: 318 | Train Loss: 0.1533491 Vali Loss: 0.1581389 Test Loss: 0.1359301
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1256282
	speed: 0.1504s/iter; left time: 224.1888s
	iters: 200, epoch: 6 | loss: 0.1531045
	speed: 0.1496s/iter; left time: 208.1419s
	iters: 300, epoch: 6 | loss: 0.1544315
	speed: 0.1355s/iter; left time: 174.8681s
Epoch: 6 cost time: 46.18721342086792
Epoch: 6, Steps: 318 | Train Loss: 0.1513835 Vali Loss: 0.1588029 Test Loss: 0.1372214
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1623115
	speed: 0.1484s/iter; left time: 174.1202s
	iters: 200, epoch: 7 | loss: 0.1373165
	speed: 0.1340s/iter; left time: 143.8228s
	iters: 300, epoch: 7 | loss: 0.1293898
	speed: 0.1484s/iter; left time: 144.4337s
Epoch: 7 cost time: 45.404380321502686
Epoch: 7, Steps: 318 | Train Loss: 0.1509750 Vali Loss: 0.1570129 Test Loss: 0.1359333
Validation loss decreased (0.157757 --> 0.157013).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1629327
	speed: 0.1412s/iter; left time: 120.6861s
	iters: 200, epoch: 8 | loss: 0.1138605
	speed: 0.1513s/iter; left time: 114.2614s
	iters: 300, epoch: 8 | loss: 0.1595685
	speed: 0.1439s/iter; left time: 94.2776s
Epoch: 8 cost time: 46.354674100875854
Epoch: 8, Steps: 318 | Train Loss: 0.1501116 Vali Loss: 0.1565260 Test Loss: 0.1357803
Validation loss decreased (0.157013 --> 0.156526).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1190819
	speed: 0.1567s/iter; left time: 84.1272s
	iters: 200, epoch: 9 | loss: 0.1355436
	speed: 0.1518s/iter; left time: 66.3284s
	iters: 300, epoch: 9 | loss: 0.1297527
	speed: 0.1531s/iter; left time: 51.5917s
Epoch: 9 cost time: 48.64788198471069
Epoch: 9, Steps: 318 | Train Loss: 0.1499428 Vali Loss: 0.1575031 Test Loss: 0.1358183
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1432131
	speed: 0.1677s/iter; left time: 36.7223s
	iters: 200, epoch: 10 | loss: 0.1411329
	speed: 0.1511s/iter; left time: 17.9859s
	iters: 300, epoch: 10 | loss: 0.1292225
	speed: 0.1658s/iter; left time: 3.1510s
Epoch: 10 cost time: 51.48934769630432
Epoch: 10, Steps: 318 | Train Loss: 0.1501234 Vali Loss: 0.1565171 Test Loss: 0.1358688
Validation loss decreased (0.156526 --> 0.156517).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.29815879464149475, mae:0.4242565333843231, rmse:0.5460391640663147, mape:0.015167311765253544, mspe:0.00038561373366974294, rse:0.3826022744178772, r2_score:0.8389925772025695, acc:0.9848326882347465
corr: [36.7615   36.723995 36.916565 36.91774  36.81823  36.94842  37.01824
 37.15125  37.14669  37.295303]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3152694
	speed: 0.1975s/iter; left time: 608.4034s
	iters: 200, epoch: 1 | loss: 0.2761850
	speed: 0.1361s/iter; left time: 405.7308s
	iters: 300, epoch: 1 | loss: 0.2121023
	speed: 0.1508s/iter; left time: 434.3537s
Epoch: 1 cost time: 51.54718089103699
Epoch: 1, Steps: 318 | Train Loss: 0.3304883 Vali Loss: 0.2132175 Test Loss: 0.1849397
Validation loss decreased (inf --> 0.213218).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1701834
	speed: 0.2051s/iter; left time: 566.6611s
	iters: 200, epoch: 2 | loss: 0.2104788
	speed: 0.1966s/iter; left time: 523.6446s
	iters: 300, epoch: 2 | loss: 0.1826783
	speed: 0.1800s/iter; left time: 461.3632s
Epoch: 2 cost time: 62.02206492424011
Epoch: 2, Steps: 318 | Train Loss: 0.1915212 Vali Loss: 0.1852366 Test Loss: 0.1642552
Validation loss decreased (0.213218 --> 0.185237).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1855348
	speed: 0.1912s/iter; left time: 467.3695s
	iters: 200, epoch: 3 | loss: 0.2153212
	speed: 0.1826s/iter; left time: 428.0947s
	iters: 300, epoch: 3 | loss: 0.1933544
	speed: 0.1875s/iter; left time: 420.9663s
Epoch: 3 cost time: 59.30005693435669
Epoch: 3, Steps: 318 | Train Loss: 0.1774839 Vali Loss: 0.1790161 Test Loss: 0.1570075
Validation loss decreased (0.185237 --> 0.179016).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1549953
	speed: 0.2008s/iter; left time: 427.1948s
	iters: 200, epoch: 4 | loss: 0.1230782
	speed: 0.1831s/iter; left time: 371.1911s
	iters: 300, epoch: 4 | loss: 0.1726790
	speed: 0.1811s/iter; left time: 348.9191s
Epoch: 4 cost time: 59.71951222419739
Epoch: 4, Steps: 318 | Train Loss: 0.1725720 Vali Loss: 0.1762794 Test Loss: 0.1581517
Validation loss decreased (0.179016 --> 0.176279).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1518937
	speed: 0.1835s/iter; left time: 331.9728s
	iters: 200, epoch: 5 | loss: 0.1457816
	speed: 0.1887s/iter; left time: 322.5404s
	iters: 300, epoch: 5 | loss: 0.2265184
	speed: 0.1861s/iter; left time: 299.4947s
Epoch: 5 cost time: 59.491467237472534
Epoch: 5, Steps: 318 | Train Loss: 0.1696743 Vali Loss: 0.1767187 Test Loss: 0.1573941
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1851317
	speed: 0.1731s/iter; left time: 258.0605s
	iters: 200, epoch: 6 | loss: 0.1886751
	speed: 0.1647s/iter; left time: 229.1405s
	iters: 300, epoch: 6 | loss: 0.1725417
	speed: 0.1673s/iter; left time: 216.0024s
Epoch: 6 cost time: 53.97466802597046
Epoch: 6, Steps: 318 | Train Loss: 0.1685065 Vali Loss: 0.1760622 Test Loss: 0.1567068
Validation loss decreased (0.176279 --> 0.176062).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1597890
	speed: 0.1862s/iter; left time: 218.4438s
	iters: 200, epoch: 7 | loss: 0.2213050
	speed: 0.1655s/iter; left time: 177.5511s
	iters: 300, epoch: 7 | loss: 0.1379371
	speed: 0.1610s/iter; left time: 156.6287s
Epoch: 7 cost time: 54.09384274482727
Epoch: 7, Steps: 318 | Train Loss: 0.1684094 Vali Loss: 0.1747306 Test Loss: 0.1568398
Validation loss decreased (0.176062 --> 0.174731).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1348184
	speed: 0.1652s/iter; left time: 141.2274s
	iters: 200, epoch: 8 | loss: 0.1315148
	speed: 0.1602s/iter; left time: 120.9366s
	iters: 300, epoch: 8 | loss: 0.2150972
	speed: 0.1664s/iter; left time: 108.9917s
Epoch: 8 cost time: 51.81348156929016
Epoch: 8, Steps: 318 | Train Loss: 0.1679807 Vali Loss: 0.1752747 Test Loss: 0.1573394
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1430815
	speed: 0.1781s/iter; left time: 95.6464s
	iters: 200, epoch: 9 | loss: 0.1766445
	speed: 0.1654s/iter; left time: 72.2724s
	iters: 300, epoch: 9 | loss: 0.1316862
	speed: 0.1648s/iter; left time: 55.5218s
Epoch: 9 cost time: 53.74528622627258
Epoch: 9, Steps: 318 | Train Loss: 0.1675998 Vali Loss: 0.1753790 Test Loss: 0.1574912
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1896167
	speed: 0.1824s/iter; left time: 39.9520s
	iters: 200, epoch: 10 | loss: 0.1437363
	speed: 0.1823s/iter; left time: 21.6956s
	iters: 300, epoch: 10 | loss: 0.1556502
	speed: 0.1837s/iter; left time: 3.4908s
Epoch: 10 cost time: 58.48124027252197
Epoch: 10, Steps: 318 | Train Loss: 0.1673320 Vali Loss: 0.1762628 Test Loss: 0.1574041
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.3441786766052246, mae:0.4548957943916321, rmse:0.5866674184799194, mape:0.01628163829445839, mspe:0.00044813862768933177, rse:0.41087639331817627, r2_score:0.810253527369195, acc:0.9837183617055416
corr: [36.806038 36.90644  36.87307  36.916794 36.79405  36.95188  36.944866
 36.98279  36.908382 37.003304 37.121246 37.070976 37.08804  37.000187
 37.266056]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2797881
	speed: 0.2389s/iter; left time: 736.1059s
	iters: 200, epoch: 1 | loss: 0.1756144
	speed: 0.2617s/iter; left time: 780.1529s
	iters: 300, epoch: 1 | loss: 0.2635201
	speed: 0.2429s/iter; left time: 699.7561s
Epoch: 1 cost time: 79.42764925956726
Epoch: 1, Steps: 318 | Train Loss: 0.3550878 Vali Loss: 0.2168177 Test Loss: 0.1947934
Validation loss decreased (inf --> 0.216818).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2000134
	speed: 0.2501s/iter; left time: 690.9692s
	iters: 200, epoch: 2 | loss: 0.1749669
	speed: 0.2500s/iter; left time: 665.7085s
	iters: 300, epoch: 2 | loss: 0.1932211
	speed: 0.2274s/iter; left time: 582.9013s
Epoch: 2 cost time: 77.47651290893555
Epoch: 2, Steps: 318 | Train Loss: 0.2065448 Vali Loss: 0.2083459 Test Loss: 0.1814781
Validation loss decreased (0.216818 --> 0.208346).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1775363
	speed: 0.2579s/iter; left time: 630.4966s
	iters: 200, epoch: 3 | loss: 0.1747003
	speed: 0.2160s/iter; left time: 506.5469s
	iters: 300, epoch: 3 | loss: 0.2874307
	speed: 0.1801s/iter; left time: 404.4048s
Epoch: 3 cost time: 69.18275022506714
Epoch: 3, Steps: 318 | Train Loss: 0.1934963 Vali Loss: 0.2069212 Test Loss: 0.1777895
Validation loss decreased (0.208346 --> 0.206921).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2106919
	speed: 0.2097s/iter; left time: 446.1001s
	iters: 200, epoch: 4 | loss: 0.1472451
	speed: 0.2111s/iter; left time: 427.9488s
	iters: 300, epoch: 4 | loss: 0.1695330
	speed: 0.1854s/iter; left time: 357.2023s
Epoch: 4 cost time: 64.29242849349976
Epoch: 4, Steps: 318 | Train Loss: 0.1880331 Vali Loss: 0.2024380 Test Loss: 0.1776303
Validation loss decreased (0.206921 --> 0.202438).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2011133
	speed: 0.2101s/iter; left time: 380.0340s
	iters: 200, epoch: 5 | loss: 0.1856015
	speed: 0.1959s/iter; left time: 334.7510s
	iters: 300, epoch: 5 | loss: 0.1765308
	speed: 0.1918s/iter; left time: 308.5309s
Epoch: 5 cost time: 63.418994426727295
Epoch: 5, Steps: 318 | Train Loss: 0.1849592 Vali Loss: 0.2059557 Test Loss: 0.1781280
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1601751
	speed: 0.2353s/iter; left time: 350.9008s
	iters: 200, epoch: 6 | loss: 0.1791423
	speed: 0.2342s/iter; left time: 325.7648s
	iters: 300, epoch: 6 | loss: 0.1549954
	speed: 0.2072s/iter; left time: 267.4661s
Epoch: 6 cost time: 71.64767384529114
Epoch: 6, Steps: 318 | Train Loss: 0.1839549 Vali Loss: 0.2062905 Test Loss: 0.1778482
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1918168
	speed: 0.2236s/iter; left time: 262.3292s
	iters: 200, epoch: 7 | loss: 0.1451939
	speed: 0.2126s/iter; left time: 228.0701s
	iters: 300, epoch: 7 | loss: 0.1770805
	speed: 0.2055s/iter; left time: 200.0000s
Epoch: 7 cost time: 67.41075396537781
Epoch: 7, Steps: 318 | Train Loss: 0.1833355 Vali Loss: 0.2058387 Test Loss: 0.1777216
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.3898027539253235, mae:0.4843968152999878, rmse:0.6243418455123901, mape:0.017338203266263008, mspe:0.0005077607929706573, rse:0.43700867891311646, r2_score:0.7811619740524726, acc:0.982661796733737
corr: [36.979095 37.05173  36.8892   36.821156 36.919075 36.93208  36.802517
 36.921337 37.040146 36.97861  36.836906 36.890812 37.013783 37.048634
 37.002266 37.515003 37.249546 37.30134  37.224564 37.46919 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4236257
	speed: 0.1860s/iter; left time: 573.0757s
	iters: 200, epoch: 1 | loss: 0.3132245
	speed: 0.1658s/iter; left time: 494.2103s
	iters: 300, epoch: 1 | loss: 0.2464752
	speed: 0.1625s/iter; left time: 468.2211s
Epoch: 1 cost time: 54.16704297065735
Epoch: 1, Steps: 318 | Train Loss: 0.3824607 Vali Loss: 0.2550196 Test Loss: 0.2377648
Validation loss decreased (inf --> 0.255020).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2185458
	speed: 0.1749s/iter; left time: 483.2842s
	iters: 200, epoch: 2 | loss: 0.2146215
	speed: 0.1649s/iter; left time: 439.1049s
	iters: 300, epoch: 2 | loss: 0.2440799
	speed: 0.1638s/iter; left time: 419.8763s
Epoch: 2 cost time: 53.24848008155823
Epoch: 2, Steps: 318 | Train Loss: 0.2234368 Vali Loss: 0.2286753 Test Loss: 0.2089890
Validation loss decreased (0.255020 --> 0.228675).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1973663
	speed: 0.1846s/iter; left time: 451.3730s
	iters: 200, epoch: 3 | loss: 0.2041485
	speed: 0.1593s/iter; left time: 373.4878s
	iters: 300, epoch: 3 | loss: 0.2173695
	speed: 0.1869s/iter; left time: 419.5252s
Epoch: 3 cost time: 55.942551136016846
Epoch: 3, Steps: 318 | Train Loss: 0.2009702 Vali Loss: 0.2194565 Test Loss: 0.2037999
Validation loss decreased (0.228675 --> 0.219456).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1476444
	speed: 0.1701s/iter; left time: 361.8043s
	iters: 200, epoch: 4 | loss: 0.1927512
	speed: 0.1454s/iter; left time: 294.6621s
	iters: 300, epoch: 4 | loss: 0.1891565
	speed: 0.1355s/iter; left time: 261.0288s
Epoch: 4 cost time: 47.61865162849426
Epoch: 4, Steps: 318 | Train Loss: 0.1933998 Vali Loss: 0.2215551 Test Loss: 0.2013389
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1734283
	speed: 0.1873s/iter; left time: 338.7374s
	iters: 200, epoch: 5 | loss: 0.1939419
	speed: 0.1630s/iter; left time: 278.6180s
	iters: 300, epoch: 5 | loss: 0.2357056
	speed: 0.1664s/iter; left time: 267.7272s
Epoch: 5 cost time: 54.200584411621094
Epoch: 5, Steps: 318 | Train Loss: 0.1898422 Vali Loss: 0.2217628 Test Loss: 0.2019504
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2353776
	speed: 0.1818s/iter; left time: 271.0596s
	iters: 200, epoch: 6 | loss: 0.2411497
	speed: 0.1647s/iter; left time: 229.0700s
	iters: 300, epoch: 6 | loss: 0.2181730
	speed: 0.1644s/iter; left time: 212.1971s
Epoch: 6 cost time: 54.672290563583374
Epoch: 6, Steps: 318 | Train Loss: 0.1876495 Vali Loss: 0.2227815 Test Loss: 0.2027808
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.4472309648990631, mae:0.5186823606491089, rmse:0.6687532663345337, mape:0.018564078956842422, mspe:0.000583111250307411, rse:0.46778181195259094, r2_score:0.7568520440995694, acc:0.9814359210431576
corr: [36.062828 36.23223  36.18929  36.434685 36.35637  36.25145  36.1285
 36.35067  36.398945 36.530167 36.61484  36.55061  36.591152 36.59479
 36.65328  36.773483 36.829178 36.789818 36.899616 36.911106 36.911335
 36.935078 37.15022  37.288246 37.44549 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4094617
	speed: 0.3188s/iter; left time: 978.8974s
	iters: 200, epoch: 1 | loss: 0.2667732
	speed: 0.3167s/iter; left time: 940.7910s
	iters: 300, epoch: 1 | loss: 0.3293941
	speed: 0.3163s/iter; left time: 908.1161s
Epoch: 1 cost time: 100.79746460914612
Epoch: 1, Steps: 317 | Train Loss: 0.3588669 Vali Loss: 0.2502410 Test Loss: 0.2256517
Validation loss decreased (inf --> 0.250241).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.3048893
	speed: 0.3174s/iter; left time: 874.0616s
	iters: 200, epoch: 2 | loss: 0.1802377
	speed: 0.2995s/iter; left time: 794.7815s
	iters: 300, epoch: 2 | loss: 0.1899070
	speed: 0.3156s/iter; left time: 806.0559s
Epoch: 2 cost time: 98.06878781318665
Epoch: 2, Steps: 317 | Train Loss: 0.2210145 Vali Loss: 0.2367158 Test Loss: 0.2114997
Validation loss decreased (0.250241 --> 0.236716).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2575507
	speed: 0.3115s/iter; left time: 759.2113s
	iters: 200, epoch: 3 | loss: 0.2313206
	speed: 0.3425s/iter; left time: 800.4514s
	iters: 300, epoch: 3 | loss: 0.2066059
	speed: 0.3098s/iter; left time: 692.9500s
Epoch: 3 cost time: 101.28918671607971
Epoch: 3, Steps: 317 | Train Loss: 0.2061406 Vali Loss: 0.2298279 Test Loss: 0.2019400
Validation loss decreased (0.236716 --> 0.229828).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1696858
	speed: 0.3292s/iter; left time: 697.8117s
	iters: 200, epoch: 4 | loss: 0.1910231
	speed: 0.3102s/iter; left time: 626.6218s
	iters: 300, epoch: 4 | loss: 0.1979879
	speed: 0.2987s/iter; left time: 573.4781s
Epoch: 4 cost time: 98.63328790664673
Epoch: 4, Steps: 317 | Train Loss: 0.1999938 Vali Loss: 0.2302594 Test Loss: 0.2033502
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2021078
	speed: 0.3190s/iter; left time: 575.1785s
	iters: 200, epoch: 5 | loss: 0.2197856
	speed: 0.3299s/iter; left time: 561.7372s
	iters: 300, epoch: 5 | loss: 0.1707149
	speed: 0.3075s/iter; left time: 492.9675s
Epoch: 5 cost time: 100.62707376480103
Epoch: 5, Steps: 317 | Train Loss: 0.1972903 Vali Loss: 0.2263076 Test Loss: 0.1991614
Validation loss decreased (0.229828 --> 0.226308).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1787931
	speed: 0.3272s/iter; left time: 486.1937s
	iters: 200, epoch: 6 | loss: 0.1703247
	speed: 0.3045s/iter; left time: 421.9939s
	iters: 300, epoch: 6 | loss: 0.2266686
	speed: 0.3022s/iter; left time: 388.6275s
Epoch: 6 cost time: 99.05077147483826
Epoch: 6, Steps: 317 | Train Loss: 0.1956340 Vali Loss: 0.2270640 Test Loss: 0.2003714
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2078435
	speed: 0.3125s/iter; left time: 365.2697s
	iters: 200, epoch: 7 | loss: 0.1910150
	speed: 0.3473s/iter; left time: 371.2383s
	iters: 300, epoch: 7 | loss: 0.2731633
	speed: 0.3808s/iter; left time: 368.9957s
Epoch: 7 cost time: 110.57027268409729
Epoch: 7, Steps: 317 | Train Loss: 0.1951503 Vali Loss: 0.2278458 Test Loss: 0.2000528
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2274758
	speed: 0.3269s/iter; left time: 278.5580s
	iters: 200, epoch: 8 | loss: 0.2029704
	speed: 0.3595s/iter; left time: 270.3283s
	iters: 300, epoch: 8 | loss: 0.2013088
	speed: 0.3740s/iter; left time: 243.8396s
Epoch: 8 cost time: 113.39054322242737
Epoch: 8, Steps: 317 | Train Loss: 0.1946214 Vali Loss: 0.2274731 Test Loss: 0.1990818
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.437051922082901, mae:0.5131708979606628, rmse:0.6610990166664124, mape:0.01836507022380829, mspe:0.0005689109675586224, rse:0.4620957374572754, r2_score:0.7605832509365723, acc:0.9816349297761917
corr: [37.179806 37.134766 37.104305 37.056034 37.036816 36.963    37.004704
 36.979145 37.054947 37.14459  37.11755  37.010185 37.08691  36.99929
 36.896805 37.096542 37.049137 37.12769  37.150806 37.13589  37.174934
 37.1036   37.161266 37.360252 37.28124  37.131447 37.139297 37.125637
 37.44536  37.555378]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2926168
	speed: 0.1916s/iter; left time: 590.4353s
	iters: 200, epoch: 1 | loss: 0.2728711
	speed: 0.1765s/iter; left time: 526.1111s
	iters: 300, epoch: 1 | loss: 0.2022952
	speed: 0.1707s/iter; left time: 491.6945s
Epoch: 1 cost time: 56.81447124481201
Epoch: 1, Steps: 318 | Train Loss: 0.3229812 Vali Loss: 0.1910534 Test Loss: 0.1665645
Validation loss decreased (inf --> 0.191053).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1784745
	speed: 0.1863s/iter; left time: 514.7245s
	iters: 200, epoch: 2 | loss: 0.1648261
	speed: 0.1826s/iter; left time: 486.2509s
	iters: 300, epoch: 2 | loss: 0.1834081
	speed: 0.1640s/iter; left time: 420.4193s
Epoch: 2 cost time: 56.28104376792908
Epoch: 2, Steps: 318 | Train Loss: 0.1774075 Vali Loss: 0.1671525 Test Loss: 0.1446055
Validation loss decreased (0.191053 --> 0.167152).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1396089
	speed: 0.1647s/iter; left time: 402.7852s
	iters: 200, epoch: 3 | loss: 0.1477174
	speed: 0.1744s/iter; left time: 409.0309s
	iters: 300, epoch: 3 | loss: 0.2149621
	speed: 0.1736s/iter; left time: 389.6850s
Epoch: 3 cost time: 53.88562846183777
Epoch: 3, Steps: 318 | Train Loss: 0.1623958 Vali Loss: 0.1608421 Test Loss: 0.1392311
Validation loss decreased (0.167152 --> 0.160842).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1660066
	speed: 0.1764s/iter; left time: 375.1196s
	iters: 200, epoch: 4 | loss: 0.1732809
	speed: 0.1685s/iter; left time: 341.5074s
	iters: 300, epoch: 4 | loss: 0.1994565
	speed: 0.1663s/iter; left time: 320.3892s
Epoch: 4 cost time: 54.22962498664856
Epoch: 4, Steps: 318 | Train Loss: 0.1570661 Vali Loss: 0.1570148 Test Loss: 0.1357164
Validation loss decreased (0.160842 --> 0.157015).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1982592
	speed: 0.1732s/iter; left time: 313.4033s
	iters: 200, epoch: 5 | loss: 0.1104583
	speed: 0.1725s/iter; left time: 294.7566s
	iters: 300, epoch: 5 | loss: 0.1489691
	speed: 0.1754s/iter; left time: 282.2228s
Epoch: 5 cost time: 55.01010322570801
Epoch: 5, Steps: 318 | Train Loss: 0.1544223 Vali Loss: 0.1575536 Test Loss: 0.1359582
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1336089
	speed: 0.1775s/iter; left time: 264.6964s
	iters: 200, epoch: 6 | loss: 0.1586452
	speed: 0.1732s/iter; left time: 240.9766s
	iters: 300, epoch: 6 | loss: 0.1568358
	speed: 0.1733s/iter; left time: 223.7351s
Epoch: 6 cost time: 55.82976984977722
Epoch: 6, Steps: 318 | Train Loss: 0.1527154 Vali Loss: 0.1571987 Test Loss: 0.1368776
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1599755
	speed: 0.1831s/iter; left time: 214.7804s
	iters: 200, epoch: 7 | loss: 0.1439046
	speed: 0.1753s/iter; left time: 188.0527s
	iters: 300, epoch: 7 | loss: 0.1290933
	speed: 0.1668s/iter; left time: 162.3417s
Epoch: 7 cost time: 55.86442542076111
Epoch: 7, Steps: 318 | Train Loss: 0.1522096 Vali Loss: 0.1559582 Test Loss: 0.1358797
Validation loss decreased (0.157015 --> 0.155958).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1623027
	speed: 0.1751s/iter; left time: 149.7194s
	iters: 200, epoch: 8 | loss: 0.1093139
	speed: 0.1679s/iter; left time: 126.7638s
	iters: 300, epoch: 8 | loss: 0.1719837
	speed: 0.1713s/iter; left time: 112.2182s
Epoch: 8 cost time: 54.669097900390625
Epoch: 8, Steps: 318 | Train Loss: 0.1513857 Vali Loss: 0.1552708 Test Loss: 0.1357094
Validation loss decreased (0.155958 --> 0.155271).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1188014
	speed: 0.1823s/iter; left time: 97.8922s
	iters: 200, epoch: 9 | loss: 0.1330076
	speed: 0.1755s/iter; left time: 76.7025s
	iters: 300, epoch: 9 | loss: 0.1280839
	speed: 0.1718s/iter; left time: 57.8998s
Epoch: 9 cost time: 55.94674825668335
Epoch: 9, Steps: 318 | Train Loss: 0.1512192 Vali Loss: 0.1561992 Test Loss: 0.1357970
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1498906
	speed: 0.1918s/iter; left time: 42.0093s
	iters: 200, epoch: 10 | loss: 0.1394158
	speed: 0.1624s/iter; left time: 19.3210s
	iters: 300, epoch: 10 | loss: 0.1281051
	speed: 0.1739s/iter; left time: 3.3050s
Epoch: 10 cost time: 55.80285167694092
Epoch: 10, Steps: 318 | Train Loss: 0.1514548 Vali Loss: 0.1552466 Test Loss: 0.1358950
Validation loss decreased (0.155271 --> 0.155247).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.2982162833213806, mae:0.42426371574401855, rmse:0.5460918545722961, mape:0.01516610849648714, mspe:0.0003856749681290239, rse:0.3826391398906708, r2_score:0.8383726054832747, acc:0.9848338915035129
corr: [36.733315 36.68049  36.876728 36.90258  36.79625  36.919083 36.99245
 37.153122 37.129284 37.276245]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3172260
	speed: 0.1720s/iter; left time: 529.9555s
	iters: 200, epoch: 1 | loss: 0.2808299
	speed: 0.1469s/iter; left time: 438.0285s
	iters: 300, epoch: 1 | loss: 0.2156514
	speed: 0.1610s/iter; left time: 463.7133s
Epoch: 1 cost time: 51.3186616897583
Epoch: 1, Steps: 318 | Train Loss: 0.3326495 Vali Loss: 0.2238810 Test Loss: 0.1894924
Validation loss decreased (inf --> 0.223881).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1703268
	speed: 0.1823s/iter; left time: 503.8184s
	iters: 200, epoch: 2 | loss: 0.2093329
	speed: 0.1708s/iter; left time: 454.8737s
	iters: 300, epoch: 2 | loss: 0.1846422
	speed: 0.1637s/iter; left time: 419.4392s
Epoch: 2 cost time: 55.39536809921265
Epoch: 2, Steps: 318 | Train Loss: 0.1933663 Vali Loss: 0.1909622 Test Loss: 0.1660491
Validation loss decreased (0.223881 --> 0.190962).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1877660
	speed: 0.1778s/iter; left time: 434.6150s
	iters: 200, epoch: 3 | loss: 0.2187762
	speed: 0.1731s/iter; left time: 405.8841s
	iters: 300, epoch: 3 | loss: 0.1932546
	speed: 0.1815s/iter; left time: 407.5131s
Epoch: 3 cost time: 56.18918323516846
Epoch: 3, Steps: 318 | Train Loss: 0.1782661 Vali Loss: 0.1838337 Test Loss: 0.1584723
Validation loss decreased (0.190962 --> 0.183834).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1561749
	speed: 0.1832s/iter; left time: 389.6066s
	iters: 200, epoch: 4 | loss: 0.1224960
	speed: 0.1762s/iter; left time: 357.2456s
	iters: 300, epoch: 4 | loss: 0.1747952
	speed: 0.1712s/iter; left time: 329.9157s
Epoch: 4 cost time: 56.45112919807434
Epoch: 4, Steps: 318 | Train Loss: 0.1730341 Vali Loss: 0.1804712 Test Loss: 0.1594003
Validation loss decreased (0.183834 --> 0.180471).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1523783
	speed: 0.1800s/iter; left time: 325.6075s
	iters: 200, epoch: 5 | loss: 0.1433475
	speed: 0.1622s/iter; left time: 277.1609s
	iters: 300, epoch: 5 | loss: 0.2259888
	speed: 0.1889s/iter; left time: 303.9724s
Epoch: 5 cost time: 56.13882541656494
Epoch: 5, Steps: 318 | Train Loss: 0.1700502 Vali Loss: 0.1809498 Test Loss: 0.1587083
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1856256
	speed: 0.1843s/iter; left time: 274.8412s
	iters: 200, epoch: 6 | loss: 0.1890174
	speed: 0.1770s/iter; left time: 246.1565s
	iters: 300, epoch: 6 | loss: 0.1717500
	speed: 0.1746s/iter; left time: 225.3833s
Epoch: 6 cost time: 56.62822937965393
Epoch: 6, Steps: 318 | Train Loss: 0.1688519 Vali Loss: 0.1801861 Test Loss: 0.1580462
Validation loss decreased (0.180471 --> 0.180186).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1634739
	speed: 0.1802s/iter; left time: 211.4159s
	iters: 200, epoch: 7 | loss: 0.2225522
	speed: 0.1670s/iter; left time: 179.2362s
	iters: 300, epoch: 7 | loss: 0.1374817
	speed: 0.1790s/iter; left time: 174.1652s
Epoch: 7 cost time: 55.6287899017334
Epoch: 7, Steps: 318 | Train Loss: 0.1686776 Vali Loss: 0.1787238 Test Loss: 0.1581363
Validation loss decreased (0.180186 --> 0.178724).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1342417
	speed: 0.1836s/iter; left time: 157.0143s
	iters: 200, epoch: 8 | loss: 0.1296110
	speed: 0.1727s/iter; left time: 130.3992s
	iters: 300, epoch: 8 | loss: 0.2160235
	speed: 0.1696s/iter; left time: 111.0704s
Epoch: 8 cost time: 55.77226114273071
Epoch: 8, Steps: 318 | Train Loss: 0.1681708 Vali Loss: 0.1793383 Test Loss: 0.1586952
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1413537
	speed: 0.1828s/iter; left time: 98.1528s
	iters: 200, epoch: 9 | loss: 0.1766554
	speed: 0.1814s/iter; left time: 79.2687s
	iters: 300, epoch: 9 | loss: 0.1316670
	speed: 0.1619s/iter; left time: 54.5558s
Epoch: 9 cost time: 55.92376470565796
Epoch: 9, Steps: 318 | Train Loss: 0.1678739 Vali Loss: 0.1793675 Test Loss: 0.1588466
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1905359
	speed: 0.1741s/iter; left time: 38.1316s
	iters: 200, epoch: 10 | loss: 0.1417187
	speed: 0.1756s/iter; left time: 20.8938s
	iters: 300, epoch: 10 | loss: 0.1564474
	speed: 0.1705s/iter; left time: 3.2386s
Epoch: 10 cost time: 55.39736247062683
Epoch: 10, Steps: 318 | Train Loss: 0.1676032 Vali Loss: 0.1803805 Test Loss: 0.1587516
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.34702402353286743, mae:0.4564773440361023, rmse:0.5890874266624451, mape:0.01634005829691887, mspe:0.00045212029363028705, rse:0.4125712513923645, r2_score:0.8086844264362529, acc:0.9836599417030811
corr: [36.818493 36.900635 36.843845 36.950024 36.800533 36.934196 36.93819
 36.979717 36.899273 36.98937  37.10341  37.046284 37.071693 36.967922
 37.207565]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2814099
	speed: 0.2137s/iter; left time: 658.3672s
	iters: 200, epoch: 1 | loss: 0.1693830
	speed: 0.2163s/iter; left time: 644.8362s
	iters: 300, epoch: 1 | loss: 0.2640025
	speed: 0.2333s/iter; left time: 672.2655s
Epoch: 1 cost time: 70.41716241836548
Epoch: 1, Steps: 318 | Train Loss: 0.3515750 Vali Loss: 0.2072675 Test Loss: 0.1867191
Validation loss decreased (inf --> 0.207268).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1898701
	speed: 0.2038s/iter; left time: 563.1437s
	iters: 200, epoch: 2 | loss: 0.1755477
	speed: 0.2152s/iter; left time: 573.0413s
	iters: 300, epoch: 2 | loss: 0.1918840
	speed: 0.2173s/iter; left time: 557.0261s
Epoch: 2 cost time: 67.52421426773071
Epoch: 2, Steps: 318 | Train Loss: 0.2031565 Vali Loss: 0.1978558 Test Loss: 0.1756210
Validation loss decreased (0.207268 --> 0.197856).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1845629
	speed: 0.2454s/iter; left time: 599.8850s
	iters: 200, epoch: 3 | loss: 0.1728053
	speed: 0.1971s/iter; left time: 462.1777s
	iters: 300, epoch: 3 | loss: 0.2789341
	speed: 0.2074s/iter; left time: 465.5132s
Epoch: 3 cost time: 69.49871945381165
Epoch: 3, Steps: 318 | Train Loss: 0.1913063 Vali Loss: 0.1968768 Test Loss: 0.1726346
Validation loss decreased (0.197856 --> 0.196877).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2004394
	speed: 0.2326s/iter; left time: 494.6801s
	iters: 200, epoch: 4 | loss: 0.1391654
	speed: 0.2137s/iter; left time: 433.2496s
	iters: 300, epoch: 4 | loss: 0.1636019
	speed: 0.2057s/iter; left time: 396.4403s
Epoch: 4 cost time: 68.80468368530273
Epoch: 4, Steps: 318 | Train Loss: 0.1860364 Vali Loss: 0.1928365 Test Loss: 0.1718029
Validation loss decreased (0.196877 --> 0.192836).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2045828
	speed: 0.2174s/iter; left time: 393.3498s
	iters: 200, epoch: 5 | loss: 0.1827083
	speed: 0.2117s/iter; left time: 361.7149s
	iters: 300, epoch: 5 | loss: 0.1773616
	speed: 0.1991s/iter; left time: 320.2905s
Epoch: 5 cost time: 66.56911611557007
Epoch: 5, Steps: 318 | Train Loss: 0.1831734 Vali Loss: 0.1956711 Test Loss: 0.1723560
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1647932
	speed: 0.2122s/iter; left time: 316.4063s
	iters: 200, epoch: 6 | loss: 0.1802559
	speed: 0.2017s/iter; left time: 280.4957s
	iters: 300, epoch: 6 | loss: 0.1627301
	speed: 0.2034s/iter; left time: 262.5998s
Epoch: 6 cost time: 65.29725551605225
Epoch: 6, Steps: 318 | Train Loss: 0.1820859 Vali Loss: 0.1966995 Test Loss: 0.1725640
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1938378
	speed: 0.2162s/iter; left time: 253.6219s
	iters: 200, epoch: 7 | loss: 0.1395540
	speed: 0.2067s/iter; left time: 221.7746s
	iters: 300, epoch: 7 | loss: 0.1749973
	speed: 0.2086s/iter; left time: 202.9708s
Epoch: 7 cost time: 66.80066275596619
Epoch: 7, Steps: 318 | Train Loss: 0.1816893 Vali Loss: 0.1958450 Test Loss: 0.1723327
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.3770148456096649, mae:0.4767497479915619, rmse:0.6140153408050537, mape:0.01706596277654171, mspe:0.0004912100848741829, rse:0.42978063225746155, r2_score:0.7870748757365387, acc:0.9829340372234583
corr: [36.90503  36.947235 36.789032 36.802753 36.83137  36.880695 36.684452
 36.880943 36.931614 36.91239  36.772026 36.870983 36.966087 37.014294
 36.946617 37.425602 37.22716  37.232845 37.14557  37.32905 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4252744
	speed: 0.2100s/iter; left time: 646.9995s
	iters: 200, epoch: 1 | loss: 0.3158376
	speed: 0.1852s/iter; left time: 552.1872s
	iters: 300, epoch: 1 | loss: 0.2336843
	speed: 0.1703s/iter; left time: 490.5268s
Epoch: 1 cost time: 60.77607870101929
Epoch: 1, Steps: 318 | Train Loss: 0.3830207 Vali Loss: 0.2413168 Test Loss: 0.2454637
Validation loss decreased (inf --> 0.241317).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2384274
	speed: 0.1869s/iter; left time: 516.4644s
	iters: 200, epoch: 2 | loss: 0.2147663
	speed: 0.1838s/iter; left time: 489.5595s
	iters: 300, epoch: 2 | loss: 0.2454902
	speed: 0.1848s/iter; left time: 473.5890s
Epoch: 2 cost time: 59.64082312583923
Epoch: 2, Steps: 318 | Train Loss: 0.2242225 Vali Loss: 0.2147388 Test Loss: 0.2050564
Validation loss decreased (0.241317 --> 0.214739).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2014926
	speed: 0.2012s/iter; left time: 491.9859s
	iters: 200, epoch: 3 | loss: 0.2123713
	speed: 0.2005s/iter; left time: 470.2748s
	iters: 300, epoch: 3 | loss: 0.2142214
	speed: 0.1897s/iter; left time: 425.9248s
Epoch: 3 cost time: 63.0878529548645
Epoch: 3, Steps: 318 | Train Loss: 0.2030677 Vali Loss: 0.2035835 Test Loss: 0.1930941
Validation loss decreased (0.214739 --> 0.203584).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1511100
	speed: 0.2015s/iter; left time: 428.5898s
	iters: 200, epoch: 4 | loss: 0.1967216
	speed: 0.1936s/iter; left time: 392.5120s
	iters: 300, epoch: 4 | loss: 0.2028499
	speed: 0.1971s/iter; left time: 379.8415s
Epoch: 4 cost time: 63.12488055229187
Epoch: 4, Steps: 318 | Train Loss: 0.1967729 Vali Loss: 0.2034450 Test Loss: 0.1887229
Validation loss decreased (0.203584 --> 0.203445).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1874166
	speed: 0.2081s/iter; left time: 376.4743s
	iters: 200, epoch: 5 | loss: 0.2013164
	speed: 0.2056s/iter; left time: 351.3006s
	iters: 300, epoch: 5 | loss: 0.2622016
	speed: 0.1806s/iter; left time: 290.6232s
Epoch: 5 cost time: 63.285849809646606
Epoch: 5, Steps: 318 | Train Loss: 0.1941268 Vali Loss: 0.2041791 Test Loss: 0.1882395
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2334762
	speed: 0.2064s/iter; left time: 307.8104s
	iters: 200, epoch: 6 | loss: 0.2407380
	speed: 0.1954s/iter; left time: 271.7633s
	iters: 300, epoch: 6 | loss: 0.2212866
	speed: 0.2083s/iter; left time: 268.9413s
Epoch: 6 cost time: 64.78492522239685
Epoch: 6, Steps: 318 | Train Loss: 0.1926365 Vali Loss: 0.2045153 Test Loss: 0.1880159
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1510607
	speed: 0.2016s/iter; left time: 236.4186s
	iters: 200, epoch: 7 | loss: 0.2283139
	speed: 0.2002s/iter; left time: 214.8136s
	iters: 300, epoch: 7 | loss: 0.2038841
	speed: 0.2041s/iter; left time: 198.5629s
Epoch: 7 cost time: 64.86277556419373
Epoch: 7, Steps: 318 | Train Loss: 0.1919680 Vali Loss: 0.2035068 Test Loss: 0.1867354
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.4141451120376587, mae:0.501395046710968, rmse:0.6435410976409912, mape:0.017957234755158424, mspe:0.000540488341357559, rse:0.45014628767967224, r2_score:0.7603179999452473, acc:0.9820427652448416
corr: [36.469227 36.515415 36.50423  36.71077  36.565292 36.618034 36.580486
 36.683285 36.647064 36.774853 36.8844   36.85116  36.89926  37.072235
 36.99594  36.979702 37.038532 37.14522  37.115578 37.028847 37.22564
 37.244595 37.298138 37.502983 37.73145 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4071999
	speed: 0.2918s/iter; left time: 895.9916s
	iters: 200, epoch: 1 | loss: 0.2677246
	speed: 0.3108s/iter; left time: 923.2507s
	iters: 300, epoch: 1 | loss: 0.3292271
	speed: 0.2738s/iter; left time: 786.2154s
Epoch: 1 cost time: 92.69354391098022
Epoch: 1, Steps: 317 | Train Loss: 0.3586680 Vali Loss: 0.2521244 Test Loss: 0.2256562
Validation loss decreased (inf --> 0.252124).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.3041964
	speed: 0.3253s/iter; left time: 895.7908s
	iters: 200, epoch: 2 | loss: 0.1804946
	speed: 0.3041s/iter; left time: 807.1046s
	iters: 300, epoch: 2 | loss: 0.1896637
	speed: 0.3270s/iter; left time: 835.0540s
Epoch: 2 cost time: 100.29135227203369
Epoch: 2, Steps: 317 | Train Loss: 0.2209006 Vali Loss: 0.2381976 Test Loss: 0.2112493
Validation loss decreased (0.252124 --> 0.238198).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2565866
	speed: 0.3331s/iter; left time: 811.7137s
	iters: 200, epoch: 3 | loss: 0.2307396
	speed: 0.2968s/iter; left time: 693.5701s
	iters: 300, epoch: 3 | loss: 0.2069514
	speed: 0.3294s/iter; left time: 736.8471s
Epoch: 3 cost time: 102.61255097389221
Epoch: 3, Steps: 317 | Train Loss: 0.2060596 Vali Loss: 0.2309648 Test Loss: 0.2018703
Validation loss decreased (0.238198 --> 0.230965).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1698214
	speed: 0.3317s/iter; left time: 703.2670s
	iters: 200, epoch: 4 | loss: 0.1903873
	speed: 0.3271s/iter; left time: 660.8379s
	iters: 300, epoch: 4 | loss: 0.1965356
	speed: 0.3216s/iter; left time: 617.4191s
Epoch: 4 cost time: 103.95896649360657
Epoch: 4, Steps: 317 | Train Loss: 0.1999216 Vali Loss: 0.2312099 Test Loss: 0.2033624
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2016371
	speed: 0.3263s/iter; left time: 588.3121s
	iters: 200, epoch: 5 | loss: 0.2201602
	speed: 0.3075s/iter; left time: 523.7182s
	iters: 300, epoch: 5 | loss: 0.1692572
	speed: 0.3111s/iter; left time: 498.7145s
Epoch: 5 cost time: 99.82789731025696
Epoch: 5, Steps: 317 | Train Loss: 0.1972213 Vali Loss: 0.2272273 Test Loss: 0.1992738
Validation loss decreased (0.230965 --> 0.227227).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1787334
	speed: 0.3175s/iter; left time: 471.8020s
	iters: 200, epoch: 6 | loss: 0.1702772
	speed: 0.3078s/iter; left time: 426.6376s
	iters: 300, epoch: 6 | loss: 0.2279676
	speed: 0.2897s/iter; left time: 372.5555s
Epoch: 6 cost time: 96.71445107460022
Epoch: 6, Steps: 317 | Train Loss: 0.1956163 Vali Loss: 0.2280424 Test Loss: 0.2005067
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2093930
	speed: 0.2920s/iter; left time: 341.3118s
	iters: 200, epoch: 7 | loss: 0.1918800
	speed: 0.3154s/iter; left time: 337.1402s
	iters: 300, epoch: 7 | loss: 0.2719502
	speed: 0.2874s/iter; left time: 278.4864s
Epoch: 7 cost time: 95.18693327903748
Epoch: 7, Steps: 317 | Train Loss: 0.1950721 Vali Loss: 0.2287867 Test Loss: 0.2002350
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2268678
	speed: 0.4065s/iter; left time: 346.3337s
	iters: 200, epoch: 8 | loss: 0.2029632
	speed: 0.3493s/iter; left time: 262.6957s
	iters: 300, epoch: 8 | loss: 0.1996798
	speed: 0.3607s/iter; left time: 235.1852s
Epoch: 8 cost time: 118.6166787147522
Epoch: 8, Steps: 317 | Train Loss: 0.1945739 Vali Loss: 0.2284613 Test Loss: 0.1992691
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.43729859590530396, mae:0.5132094621658325, rmse:0.6612855792045593, mape:0.01836668886244297, mspe:0.0005691882106475532, rse:0.46222612261772156, r2_score:0.760005649360438, acc:0.981633311137557
corr: [37.1759   37.132145 37.089676 37.048126 37.01401  36.945503 36.98412
 36.95258  37.032932 37.138443 37.119186 37.015594 37.08622  37.00988
 36.903362 37.094936 37.044518 37.132385 37.140785 37.13037  37.17704
 37.091    37.157856 37.370846 37.279175 37.13767  37.144623 37.123962
 37.456703 37.582893]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3821342
	speed: 0.1299s/iter; left time: 400.2558s
	iters: 200, epoch: 1 | loss: 0.2284217
	speed: 0.1047s/iter; left time: 312.2297s
	iters: 300, epoch: 1 | loss: 0.2347252
	speed: 0.1009s/iter; left time: 290.7555s
Epoch: 1 cost time: 35.55097413063049
Epoch: 1, Steps: 318 | Train Loss: 0.3062442 Vali Loss: 0.1672142 Test Loss: 0.1568869
Validation loss decreased (inf --> 0.167214).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1812919
	speed: 0.1060s/iter; left time: 292.9592s
	iters: 200, epoch: 2 | loss: 0.1328688
	speed: 0.1053s/iter; left time: 280.4282s
	iters: 300, epoch: 2 | loss: 0.1877188
	speed: 0.1002s/iter; left time: 256.7261s
Epoch: 2 cost time: 33.02466416358948
Epoch: 2, Steps: 318 | Train Loss: 0.1665156 Vali Loss: 0.1512085 Test Loss: 0.1411126
Validation loss decreased (0.167214 --> 0.151209).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1496205
	speed: 0.1175s/iter; left time: 287.1979s
	iters: 200, epoch: 3 | loss: 0.1925114
	speed: 0.0962s/iter; left time: 225.5296s
	iters: 300, epoch: 3 | loss: 0.1083048
	speed: 0.1008s/iter; left time: 226.3369s
Epoch: 3 cost time: 33.198378562927246
Epoch: 3, Steps: 318 | Train Loss: 0.1516094 Vali Loss: 0.1469578 Test Loss: 0.1369347
Validation loss decreased (0.151209 --> 0.146958).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1745192
	speed: 0.1146s/iter; left time: 243.8078s
	iters: 200, epoch: 4 | loss: 0.1424376
	speed: 0.1037s/iter; left time: 210.2988s
	iters: 300, epoch: 4 | loss: 0.1618036
	speed: 0.0960s/iter; left time: 184.9916s
Epoch: 4 cost time: 33.2488214969635
Epoch: 4, Steps: 318 | Train Loss: 0.1461864 Vali Loss: 0.1429494 Test Loss: 0.1355664
Validation loss decreased (0.146958 --> 0.142949).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1857382
	speed: 0.1040s/iter; left time: 188.1968s
	iters: 200, epoch: 5 | loss: 0.1126669
	speed: 0.1012s/iter; left time: 172.9409s
	iters: 300, epoch: 5 | loss: 0.1094653
	speed: 0.1068s/iter; left time: 171.9117s
Epoch: 5 cost time: 33.62247705459595
Epoch: 5, Steps: 318 | Train Loss: 0.1430970 Vali Loss: 0.1434962 Test Loss: 0.1334817
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1418556
	speed: 0.1076s/iter; left time: 160.3826s
	iters: 200, epoch: 6 | loss: 0.1412019
	speed: 0.1009s/iter; left time: 140.3372s
	iters: 300, epoch: 6 | loss: 0.1372816
	speed: 0.0998s/iter; left time: 128.8418s
Epoch: 6 cost time: 32.512558937072754
Epoch: 6, Steps: 318 | Train Loss: 0.1413277 Vali Loss: 0.1426832 Test Loss: 0.1333350
Validation loss decreased (0.142949 --> 0.142683).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1404389
	speed: 0.1125s/iter; left time: 131.9091s
	iters: 200, epoch: 7 | loss: 0.2155539
	speed: 0.1079s/iter; left time: 115.7722s
	iters: 300, epoch: 7 | loss: 0.1286201
	speed: 0.1011s/iter; left time: 98.3240s
Epoch: 7 cost time: 34.260451555252075
Epoch: 7, Steps: 318 | Train Loss: 0.1409350 Vali Loss: 0.1422316 Test Loss: 0.1331319
Validation loss decreased (0.142683 --> 0.142232).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1632757
	speed: 0.1079s/iter; left time: 92.2466s
	iters: 200, epoch: 8 | loss: 0.1552937
	speed: 0.0964s/iter; left time: 72.7803s
	iters: 300, epoch: 8 | loss: 0.1043734
	speed: 0.1068s/iter; left time: 69.9659s
Epoch: 8 cost time: 33.22456741333008
Epoch: 8, Steps: 318 | Train Loss: 0.1397848 Vali Loss: 0.1420358 Test Loss: 0.1328021
Validation loss decreased (0.142232 --> 0.142036).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1310823
	speed: 0.1143s/iter; left time: 61.3945s
	iters: 200, epoch: 9 | loss: 0.1591322
	speed: 0.1094s/iter; left time: 47.8296s
	iters: 300, epoch: 9 | loss: 0.1548077
	speed: 0.1016s/iter; left time: 34.2329s
Epoch: 9 cost time: 34.39891529083252
Epoch: 9, Steps: 318 | Train Loss: 0.1402326 Vali Loss: 0.1422352 Test Loss: 0.1327234
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1770571
	speed: 0.1056s/iter; left time: 23.1194s
	iters: 200, epoch: 10 | loss: 0.1079361
	speed: 0.0994s/iter; left time: 11.8334s
	iters: 300, epoch: 10 | loss: 0.2038589
	speed: 0.1050s/iter; left time: 1.9955s
Epoch: 10 cost time: 33.01982831954956
Epoch: 10, Steps: 318 | Train Loss: 0.1401142 Vali Loss: 0.1420251 Test Loss: 0.1326663
Validation loss decreased (0.142036 --> 0.142025).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.29113107919692993, mae:0.41669461131095886, rmse:0.5395656228065491, mape:0.014880052767693996, mspe:0.00037444441113620996, rse:0.37806636095046997, r2_score:0.8448484116073478, acc:0.985119947232306
corr: [36.556995 36.650406 36.490417 36.31585  36.28945  36.284477 36.41325
 36.46095  36.683105 36.8898  ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4075619
	speed: 0.1291s/iter; left time: 397.7565s
	iters: 200, epoch: 1 | loss: 0.3356422
	speed: 0.1100s/iter; left time: 327.8436s
	iters: 300, epoch: 1 | loss: 0.2341440
	speed: 0.1057s/iter; left time: 304.6494s
Epoch: 1 cost time: 36.51397347450256
Epoch: 1, Steps: 318 | Train Loss: 0.3956174 Vali Loss: 0.2658671 Test Loss: 0.2668575
Validation loss decreased (inf --> 0.265867).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.3015249
	speed: 0.1196s/iter; left time: 330.3810s
	iters: 200, epoch: 2 | loss: 0.2633845
	speed: 0.1130s/iter; left time: 301.0503s
	iters: 300, epoch: 2 | loss: 0.1946861
	speed: 0.1058s/iter; left time: 271.1934s
Epoch: 2 cost time: 35.57496190071106
Epoch: 2, Steps: 318 | Train Loss: 0.2412577 Vali Loss: 0.2439917 Test Loss: 0.2486208
Validation loss decreased (0.265867 --> 0.243992).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2028907
	speed: 0.1102s/iter; left time: 269.3508s
	iters: 200, epoch: 3 | loss: 0.1981751
	speed: 0.1110s/iter; left time: 260.3809s
	iters: 300, epoch: 3 | loss: 0.2101612
	speed: 0.1070s/iter; left time: 240.2975s
Epoch: 3 cost time: 35.030784606933594
Epoch: 3, Steps: 318 | Train Loss: 0.2254273 Vali Loss: 0.2281836 Test Loss: 0.2419474
Validation loss decreased (0.243992 --> 0.228184).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1993704
	speed: 0.1099s/iter; left time: 233.7238s
	iters: 200, epoch: 4 | loss: 0.2307585
	speed: 0.1123s/iter; left time: 227.7323s
	iters: 300, epoch: 4 | loss: 0.3006446
	speed: 0.1015s/iter; left time: 195.6829s
Epoch: 4 cost time: 34.63411045074463
Epoch: 4, Steps: 318 | Train Loss: 0.2181741 Vali Loss: 0.2215939 Test Loss: 0.2410980
Validation loss decreased (0.228184 --> 0.221594).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1879697
	speed: 0.1149s/iter; left time: 207.8498s
	iters: 200, epoch: 5 | loss: 0.1662914
	speed: 0.1119s/iter; left time: 191.1995s
	iters: 300, epoch: 5 | loss: 0.1932649
	speed: 0.1074s/iter; left time: 172.8747s
Epoch: 5 cost time: 35.355717182159424
Epoch: 5, Steps: 318 | Train Loss: 0.2142069 Vali Loss: 0.2194306 Test Loss: 0.2394995
Validation loss decreased (0.221594 --> 0.219431).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1828307
	speed: 0.1167s/iter; left time: 173.9910s
	iters: 200, epoch: 6 | loss: 0.2263064
	speed: 0.1020s/iter; left time: 141.9234s
	iters: 300, epoch: 6 | loss: 0.2528941
	speed: 0.1021s/iter; left time: 131.7988s
Epoch: 6 cost time: 34.3961615562439
Epoch: 6, Steps: 318 | Train Loss: 0.2126873 Vali Loss: 0.2158937 Test Loss: 0.2389495
Validation loss decreased (0.219431 --> 0.215894).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2535781
	speed: 0.1131s/iter; left time: 132.6489s
	iters: 200, epoch: 7 | loss: 0.1761572
	speed: 0.1155s/iter; left time: 123.8988s
	iters: 300, epoch: 7 | loss: 0.2680644
	speed: 0.1136s/iter; left time: 110.5069s
Epoch: 7 cost time: 36.48967003822327
Epoch: 7, Steps: 318 | Train Loss: 0.2118496 Vali Loss: 0.2165765 Test Loss: 0.2388064
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2068148
	speed: 0.1153s/iter; left time: 98.5885s
	iters: 200, epoch: 8 | loss: 0.2694647
	speed: 0.1018s/iter; left time: 76.8657s
	iters: 300, epoch: 8 | loss: 0.2142322
	speed: 0.1122s/iter; left time: 73.5047s
Epoch: 8 cost time: 34.909632205963135
Epoch: 8, Steps: 318 | Train Loss: 0.2114723 Vali Loss: 0.2179685 Test Loss: 0.2391449
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2226811
	speed: 0.1174s/iter; left time: 63.0487s
	iters: 200, epoch: 9 | loss: 0.1624445
	speed: 0.1031s/iter; left time: 45.0505s
	iters: 300, epoch: 9 | loss: 0.2324003
	speed: 0.1021s/iter; left time: 34.4122s
Epoch: 9 cost time: 34.215402126312256
Epoch: 9, Steps: 318 | Train Loss: 0.2112243 Vali Loss: 0.2170339 Test Loss: 0.2387509
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.524365246295929, mae:0.5544716119766235, rmse:0.7241306900978088, mape:0.019893545657396317, mspe:0.0006860566209070385, rse:0.5071496367454529, r2_score:0.7201893858954656, acc:0.9801064543426037
corr: [36.153328 36.318615 36.2272   35.990223 36.074684 36.39805  36.425503
 36.75307  36.349037 36.66106  36.489475 36.770042 36.44326  36.71786
 36.647514]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3617184
	speed: 0.1451s/iter; left time: 447.1420s
	iters: 200, epoch: 1 | loss: 0.2110967
	speed: 0.1394s/iter; left time: 415.5698s
	iters: 300, epoch: 1 | loss: 0.2303953
	speed: 0.1562s/iter; left time: 449.8805s
Epoch: 1 cost time: 47.21853733062744
Epoch: 1, Steps: 318 | Train Loss: 0.3151891 Vali Loss: 0.1993478 Test Loss: 0.2161814
Validation loss decreased (inf --> 0.199348).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1757435
	speed: 0.1546s/iter; left time: 427.0791s
	iters: 200, epoch: 2 | loss: 0.2593023
	speed: 0.1531s/iter; left time: 407.7368s
	iters: 300, epoch: 2 | loss: 0.1706945
	speed: 0.1445s/iter; left time: 370.4071s
Epoch: 2 cost time: 47.93878674507141
Epoch: 2, Steps: 318 | Train Loss: 0.1937243 Vali Loss: 0.1910616 Test Loss: 0.1812156
Validation loss decreased (0.199348 --> 0.191062).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2214117
	speed: 0.1552s/iter; left time: 379.5759s
	iters: 200, epoch: 3 | loss: 0.1384737
	speed: 0.1490s/iter; left time: 349.3205s
	iters: 300, epoch: 3 | loss: 0.1950139
	speed: 0.1437s/iter; left time: 322.5778s
Epoch: 3 cost time: 46.28221249580383
Epoch: 3, Steps: 318 | Train Loss: 0.1786582 Vali Loss: 0.1781492 Test Loss: 0.1767187
Validation loss decreased (0.191062 --> 0.178149).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1963405
	speed: 0.1594s/iter; left time: 339.1413s
	iters: 200, epoch: 4 | loss: 0.1335049
	speed: 0.1483s/iter; left time: 300.5549s
	iters: 300, epoch: 4 | loss: 0.1796753
	speed: 0.1551s/iter; left time: 298.9591s
Epoch: 4 cost time: 48.62841176986694
Epoch: 4, Steps: 318 | Train Loss: 0.1721888 Vali Loss: 0.1837679 Test Loss: 0.1756895
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1460554
	speed: 0.1428s/iter; left time: 258.2689s
	iters: 200, epoch: 5 | loss: 0.1900750
	speed: 0.1358s/iter; left time: 232.0117s
	iters: 300, epoch: 5 | loss: 0.1194581
	speed: 0.1553s/iter; left time: 249.8211s
Epoch: 5 cost time: 45.92716026306152
Epoch: 5, Steps: 318 | Train Loss: 0.1685810 Vali Loss: 0.1834047 Test Loss: 0.1740356
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1457691
	speed: 0.1560s/iter; left time: 232.6621s
	iters: 200, epoch: 6 | loss: 0.2055090
	speed: 0.1492s/iter; left time: 207.5781s
	iters: 300, epoch: 6 | loss: 0.1946109
	speed: 0.1527s/iter; left time: 197.1222s
Epoch: 6 cost time: 48.046485900878906
Epoch: 6, Steps: 318 | Train Loss: 0.1667225 Vali Loss: 0.1842826 Test Loss: 0.1739504
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.38780227303504944, mae:0.481049120426178, rmse:0.62273770570755, mape:0.01721000112593174, mspe:0.0005035161739215255, rse:0.43588587641716003, r2_score:0.7994131906428674, acc:0.9827899988740683
corr: [36.684105 36.852283 36.595253 36.68583  36.629433 36.74661  36.64326
 36.59181  36.646137 36.64057  36.691338 36.653255 36.796387 36.734756
 37.039394 37.06528  37.112957 37.208447 37.164185 37.444954]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3359454
	speed: 0.1365s/iter; left time: 420.4441s
	iters: 200, epoch: 1 | loss: 0.3123689
	speed: 0.1100s/iter; left time: 327.9985s
	iters: 300, epoch: 1 | loss: 0.1746939
	speed: 0.1060s/iter; left time: 305.3238s
Epoch: 1 cost time: 37.681288719177246
Epoch: 1, Steps: 318 | Train Loss: 0.3678030 Vali Loss: 0.2691905 Test Loss: 0.3796879
Validation loss decreased (inf --> 0.269191).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2119528
	speed: 0.1147s/iter; left time: 316.9136s
	iters: 200, epoch: 2 | loss: 0.2626011
	speed: 0.1069s/iter; left time: 284.6265s
	iters: 300, epoch: 2 | loss: 0.2737648
	speed: 0.1088s/iter; left time: 278.9638s
Epoch: 2 cost time: 35.07685351371765
Epoch: 2, Steps: 318 | Train Loss: 0.2496822 Vali Loss: 0.2380424 Test Loss: 0.3343813
Validation loss decreased (0.269191 --> 0.238042).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2239847
	speed: 0.1120s/iter; left time: 273.9564s
	iters: 200, epoch: 3 | loss: 0.2180386
	speed: 0.1120s/iter; left time: 262.6877s
	iters: 300, epoch: 3 | loss: 0.2101394
	speed: 0.1100s/iter; left time: 246.8791s
Epoch: 3 cost time: 35.39327359199524
Epoch: 3, Steps: 318 | Train Loss: 0.2282433 Vali Loss: 0.2237469 Test Loss: 0.3369376
Validation loss decreased (0.238042 --> 0.223747).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2053009
	speed: 0.1162s/iter; left time: 247.1961s
	iters: 200, epoch: 4 | loss: 0.2273715
	speed: 0.1069s/iter; left time: 216.7357s
	iters: 300, epoch: 4 | loss: 0.2167045
	speed: 0.1065s/iter; left time: 205.3127s
Epoch: 4 cost time: 34.41389584541321
Epoch: 4, Steps: 318 | Train Loss: 0.2197756 Vali Loss: 0.2165686 Test Loss: 0.3266090
Validation loss decreased (0.223747 --> 0.216569).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2023970
	speed: 0.1161s/iter; left time: 209.9651s
	iters: 200, epoch: 5 | loss: 0.2304882
	speed: 0.1120s/iter; left time: 191.4158s
	iters: 300, epoch: 5 | loss: 0.2100342
	speed: 0.1125s/iter; left time: 181.0255s
Epoch: 5 cost time: 36.00657105445862
Epoch: 5, Steps: 318 | Train Loss: 0.2169195 Vali Loss: 0.2159038 Test Loss: 0.3306261
Validation loss decreased (0.216569 --> 0.215904).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1744557
	speed: 0.1057s/iter; left time: 157.5850s
	iters: 200, epoch: 6 | loss: 0.1886370
	speed: 0.1003s/iter; left time: 139.4655s
	iters: 300, epoch: 6 | loss: 0.2737910
	speed: 0.1117s/iter; left time: 144.1636s
Epoch: 6 cost time: 33.621034145355225
Epoch: 6, Steps: 318 | Train Loss: 0.2146211 Vali Loss: 0.2149984 Test Loss: 0.3292552
Validation loss decreased (0.215904 --> 0.214998).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1796668
	speed: 0.1191s/iter; left time: 139.7553s
	iters: 200, epoch: 7 | loss: 0.1841415
	speed: 0.1106s/iter; left time: 118.6953s
	iters: 300, epoch: 7 | loss: 0.1963791
	speed: 0.1063s/iter; left time: 103.4705s
Epoch: 7 cost time: 35.40161848068237
Epoch: 7, Steps: 318 | Train Loss: 0.2140494 Vali Loss: 0.2145725 Test Loss: 0.3289732
Validation loss decreased (0.214998 --> 0.214572).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2075646
	speed: 0.1160s/iter; left time: 99.2123s
	iters: 200, epoch: 8 | loss: 0.1926759
	speed: 0.1101s/iter; left time: 83.1079s
	iters: 300, epoch: 8 | loss: 0.1761235
	speed: 0.1097s/iter; left time: 71.8209s
Epoch: 8 cost time: 35.07459616661072
Epoch: 8, Steps: 318 | Train Loss: 0.2134118 Vali Loss: 0.2149109 Test Loss: 0.3294509
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2440673
	speed: 0.1050s/iter; left time: 56.3904s
	iters: 200, epoch: 9 | loss: 0.1989756
	speed: 0.1091s/iter; left time: 47.6835s
	iters: 300, epoch: 9 | loss: 0.1713191
	speed: 0.1145s/iter; left time: 38.5777s
Epoch: 9 cost time: 35.10515761375427
Epoch: 9, Steps: 318 | Train Loss: 0.2129744 Vali Loss: 0.2146419 Test Loss: 0.3292990
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1822866
	speed: 0.1137s/iter; left time: 24.8995s
	iters: 200, epoch: 10 | loss: 0.2079272
	speed: 0.1081s/iter; left time: 12.8683s
	iters: 300, epoch: 10 | loss: 0.2167441
	speed: 0.1040s/iter; left time: 1.9761s
Epoch: 10 cost time: 34.725789070129395
Epoch: 10, Steps: 318 | Train Loss: 0.2130034 Vali Loss: 0.2128387 Test Loss: 0.3291119
Validation loss decreased (0.214572 --> 0.212839).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.7222231030464172, mae:0.6308171153068542, rmse:0.8498371243476868, mape:0.022474421188235283, mspe:0.0009188741678372025, rse:0.5944468975067139, r2_score:0.678332210797026, acc:0.9775255788117647
corr: [36.764896 36.82786  36.809517 36.94899  37.156464 37.24317  37.24969
 36.90885  37.23554  37.35449  37.262547 36.78318  37.360867 36.988876
 37.352398 37.46969  37.05793  37.73134  37.821316 37.978195 37.972538
 38.24664  38.45115  38.646347 38.709194]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4097835
	speed: 0.1413s/iter; left time: 433.8428s
	iters: 200, epoch: 1 | loss: 0.3259304
	speed: 0.1169s/iter; left time: 347.1952s
	iters: 300, epoch: 1 | loss: 0.2891625
	speed: 0.1262s/iter; left time: 362.3735s
Epoch: 1 cost time: 40.37950873374939
Epoch: 1, Steps: 317 | Train Loss: 0.4025479 Vali Loss: 0.2715921 Test Loss: 0.3043942
Validation loss decreased (inf --> 0.271592).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2366541
	speed: 0.1302s/iter; left time: 358.6234s
	iters: 200, epoch: 2 | loss: 0.2749344
	speed: 0.1216s/iter; left time: 322.7235s
	iters: 300, epoch: 2 | loss: 0.2226820
	speed: 0.1043s/iter; left time: 266.2576s
Epoch: 2 cost time: 37.34643793106079
Epoch: 2, Steps: 317 | Train Loss: 0.2585096 Vali Loss: 0.2542861 Test Loss: 0.2807514
Validation loss decreased (0.271592 --> 0.254286).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1860382
	speed: 0.1259s/iter; left time: 306.7197s
	iters: 200, epoch: 3 | loss: 0.2592850
	speed: 0.1153s/iter; left time: 269.5366s
	iters: 300, epoch: 3 | loss: 0.2352768
	speed: 0.1178s/iter; left time: 263.4658s
Epoch: 3 cost time: 37.90877151489258
Epoch: 3, Steps: 317 | Train Loss: 0.2424040 Vali Loss: 0.2372151 Test Loss: 0.2532819
Validation loss decreased (0.254286 --> 0.237215).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1817174
	speed: 0.1129s/iter; left time: 239.2546s
	iters: 200, epoch: 4 | loss: 0.2758449
	speed: 0.1013s/iter; left time: 204.6935s
	iters: 300, epoch: 4 | loss: 0.2092845
	speed: 0.1102s/iter; left time: 211.5446s
Epoch: 4 cost time: 34.55026388168335
Epoch: 4, Steps: 317 | Train Loss: 0.2356890 Vali Loss: 0.2368985 Test Loss: 0.2553828
Validation loss decreased (0.237215 --> 0.236898).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2104037
	speed: 0.1187s/iter; left time: 214.1041s
	iters: 200, epoch: 5 | loss: 0.2093158
	speed: 0.1087s/iter; left time: 185.0944s
	iters: 300, epoch: 5 | loss: 0.2427243
	speed: 0.1112s/iter; left time: 178.2466s
Epoch: 5 cost time: 36.051613092422485
Epoch: 5, Steps: 317 | Train Loss: 0.2327267 Vali Loss: 0.2320652 Test Loss: 0.2439922
Validation loss decreased (0.236898 --> 0.232065).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2621006
	speed: 0.1176s/iter; left time: 174.7071s
	iters: 200, epoch: 6 | loss: 0.2017986
	speed: 0.1183s/iter; left time: 163.9181s
	iters: 300, epoch: 6 | loss: 0.2779238
	speed: 0.1137s/iter; left time: 146.2481s
Epoch: 6 cost time: 36.824116945266724
Epoch: 6, Steps: 317 | Train Loss: 0.2310329 Vali Loss: 0.2334637 Test Loss: 0.2454132
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2303776
	speed: 0.1115s/iter; left time: 130.3633s
	iters: 200, epoch: 7 | loss: 0.1877023
	speed: 0.1118s/iter; left time: 119.4860s
	iters: 300, epoch: 7 | loss: 0.2379874
	speed: 0.1107s/iter; left time: 107.2262s
Epoch: 7 cost time: 35.70874762535095
Epoch: 7, Steps: 317 | Train Loss: 0.2298094 Vali Loss: 0.2323118 Test Loss: 0.2446996
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2183981
	speed: 0.1193s/iter; left time: 101.6224s
	iters: 200, epoch: 8 | loss: 0.2042127
	speed: 0.1172s/iter; left time: 88.1310s
	iters: 300, epoch: 8 | loss: 0.1965415
	speed: 0.1108s/iter; left time: 72.2402s
Epoch: 8 cost time: 37.008904695510864
Epoch: 8, Steps: 317 | Train Loss: 0.2294272 Vali Loss: 0.2315861 Test Loss: 0.2439854
Validation loss decreased (0.232065 --> 0.231586).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2728738
	speed: 0.1192s/iter; left time: 63.7674s
	iters: 200, epoch: 9 | loss: 0.2158107
	speed: 0.1184s/iter; left time: 51.4985s
	iters: 300, epoch: 9 | loss: 0.2751290
	speed: 0.1093s/iter; left time: 36.6240s
Epoch: 9 cost time: 36.05078482627869
Epoch: 9, Steps: 317 | Train Loss: 0.2286499 Vali Loss: 0.2312992 Test Loss: 0.2438706
Validation loss decreased (0.231586 --> 0.231299).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2142754
	speed: 0.1115s/iter; left time: 24.3008s
	iters: 200, epoch: 10 | loss: 0.3332868
	speed: 0.1084s/iter; left time: 12.7926s
	iters: 300, epoch: 10 | loss: 0.2223070
	speed: 0.1173s/iter; left time: 2.1105s
Epoch: 10 cost time: 35.870797634124756
Epoch: 10, Steps: 317 | Train Loss: 0.2291751 Vali Loss: 0.2308762 Test Loss: 0.2436541
Validation loss decreased (0.231299 --> 0.230876).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.5346893072128296, mae:0.5787159204483032, rmse:0.731224536895752, mape:0.020672369748353958, mspe:0.0006903540343046188, rse:0.5111120939254761, r2_score:0.7168697921110907, acc:0.979327630251646
corr: [37.21828  37.267967 37.262394 37.007084 36.941044 37.05805  37.20316
 37.41207  37.316696 37.347927 37.31815  37.43066  37.317215 37.324215
 37.262722 37.589184 37.766575 37.535114 37.54771  37.68594  37.3661
 37.645874 37.71022  37.638702 37.91284  37.907356 37.92744  37.968193
 38.039394 38.19551 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2095412
	speed: 0.1582s/iter; left time: 487.3312s
	iters: 200, epoch: 1 | loss: 0.1804784
	speed: 0.1360s/iter; left time: 405.5494s
	iters: 300, epoch: 1 | loss: 0.2266523
	speed: 0.1283s/iter; left time: 369.7183s
Epoch: 1 cost time: 44.66913318634033
Epoch: 1, Steps: 318 | Train Loss: 0.2601416 Vali Loss: 0.1621305 Test Loss: 0.1461418
Validation loss decreased (inf --> 0.162131).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1721801
	speed: 0.1384s/iter; left time: 382.5130s
	iters: 200, epoch: 2 | loss: 0.1282078
	speed: 0.1245s/iter; left time: 331.6259s
	iters: 300, epoch: 2 | loss: 0.1519177
	speed: 0.1286s/iter; left time: 329.6355s
Epoch: 2 cost time: 41.096922159194946
Epoch: 2, Steps: 318 | Train Loss: 0.1514512 Vali Loss: 0.1470482 Test Loss: 0.1345870
Validation loss decreased (0.162131 --> 0.147048).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1312990
	speed: 0.1362s/iter; left time: 332.9582s
	iters: 200, epoch: 3 | loss: 0.1681204
	speed: 0.1271s/iter; left time: 298.1438s
	iters: 300, epoch: 3 | loss: 0.1099819
	speed: 0.1279s/iter; left time: 287.0458s
Epoch: 3 cost time: 41.4409863948822
Epoch: 3, Steps: 318 | Train Loss: 0.1357535 Vali Loss: 0.1508546 Test Loss: 0.1351084
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1613824
	speed: 0.1321s/iter; left time: 281.0584s
	iters: 200, epoch: 4 | loss: 0.1133258
	speed: 0.1351s/iter; left time: 273.8820s
	iters: 300, epoch: 4 | loss: 0.1208701
	speed: 0.1237s/iter; left time: 238.2887s
Epoch: 4 cost time: 41.60416388511658
Epoch: 4, Steps: 318 | Train Loss: 0.1282936 Vali Loss: 0.1479667 Test Loss: 0.1341211
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1558895
	speed: 0.1371s/iter; left time: 247.9387s
	iters: 200, epoch: 5 | loss: 0.0958543
	speed: 0.1157s/iter; left time: 197.7869s
	iters: 300, epoch: 5 | loss: 0.1142582
	speed: 0.1241s/iter; left time: 199.6578s
Epoch: 5 cost time: 39.955941915512085
Epoch: 5, Steps: 318 | Train Loss: 0.1243036 Vali Loss: 0.1467575 Test Loss: 0.1317597
Validation loss decreased (0.147048 --> 0.146758).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1193176
	speed: 0.1331s/iter; left time: 198.4156s
	iters: 200, epoch: 6 | loss: 0.1148147
	speed: 0.1214s/iter; left time: 168.8943s
	iters: 300, epoch: 6 | loss: 0.1113822
	speed: 0.1271s/iter; left time: 164.1386s
Epoch: 6 cost time: 40.42245030403137
Epoch: 6, Steps: 318 | Train Loss: 0.1223917 Vali Loss: 0.1479726 Test Loss: 0.1323566
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1315721
	speed: 0.1221s/iter; left time: 143.2294s
	iters: 200, epoch: 7 | loss: 0.1677965
	speed: 0.1227s/iter; left time: 131.6221s
	iters: 300, epoch: 7 | loss: 0.1022393
	speed: 0.1272s/iter; left time: 123.8097s
Epoch: 7 cost time: 39.55828881263733
Epoch: 7, Steps: 318 | Train Loss: 0.1207780 Vali Loss: 0.1469342 Test Loss: 0.1320955
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1514733
	speed: 0.1329s/iter; left time: 113.6508s
	iters: 200, epoch: 8 | loss: 0.1211506
	speed: 0.1258s/iter; left time: 94.9789s
	iters: 300, epoch: 8 | loss: 0.0925306
	speed: 0.1229s/iter; left time: 80.5065s
Epoch: 8 cost time: 40.21045160293579
Epoch: 8, Steps: 318 | Train Loss: 0.1202407 Vali Loss: 0.1469428 Test Loss: 0.1316004
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.2891414761543274, mae:0.41682133078575134, rmse:0.5377187728881836, mape:0.014900686219334602, mspe:0.00037318532122299075, rse:0.3767722547054291, r2_score:0.851726727220368, acc:0.9850993137806654
corr: [36.594322 36.542892 36.583214 36.45744  36.373558 36.496765 36.644608
 36.753006 36.763153 36.86934 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2809385
	speed: 0.1878s/iter; left time: 578.6959s
	iters: 200, epoch: 1 | loss: 0.2456381
	speed: 0.1612s/iter; left time: 480.4724s
	iters: 300, epoch: 1 | loss: 0.1990219
	speed: 0.1549s/iter; left time: 446.2289s
Epoch: 1 cost time: 53.113651275634766
Epoch: 1, Steps: 318 | Train Loss: 0.2953851 Vali Loss: 0.1911026 Test Loss: 0.1758891
Validation loss decreased (inf --> 0.191103).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2044297
	speed: 0.1510s/iter; left time: 417.1151s
	iters: 200, epoch: 2 | loss: 0.1726758
	speed: 0.1405s/iter; left time: 374.1715s
	iters: 300, epoch: 2 | loss: 0.1344676
	speed: 0.1431s/iter; left time: 366.7654s
Epoch: 2 cost time: 46.030524015426636
Epoch: 2, Steps: 318 | Train Loss: 0.1760642 Vali Loss: 0.1872643 Test Loss: 0.1719745
Validation loss decreased (0.191103 --> 0.187264).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1331298
	speed: 0.1414s/iter; left time: 345.6454s
	iters: 200, epoch: 3 | loss: 0.1576029
	speed: 0.1279s/iter; left time: 299.8140s
	iters: 300, epoch: 3 | loss: 0.1479844
	speed: 0.1311s/iter; left time: 294.3915s
Epoch: 3 cost time: 42.46629762649536
Epoch: 3, Steps: 318 | Train Loss: 0.1608780 Vali Loss: 0.1798275 Test Loss: 0.1679886
Validation loss decreased (0.187264 --> 0.179827).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1405375
	speed: 0.1338s/iter; left time: 284.6187s
	iters: 200, epoch: 4 | loss: 0.1796047
	speed: 0.1302s/iter; left time: 263.8374s
	iters: 300, epoch: 4 | loss: 0.1733862
	speed: 0.1314s/iter; left time: 253.2574s
Epoch: 4 cost time: 41.859416484832764
Epoch: 4, Steps: 318 | Train Loss: 0.1532755 Vali Loss: 0.1775136 Test Loss: 0.1650183
Validation loss decreased (0.179827 --> 0.177514).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1424430
	speed: 0.1303s/iter; left time: 235.7247s
	iters: 200, epoch: 5 | loss: 0.1397232
	speed: 0.1337s/iter; left time: 228.5725s
	iters: 300, epoch: 5 | loss: 0.1518822
	speed: 0.1317s/iter; left time: 211.8724s
Epoch: 5 cost time: 41.93884086608887
Epoch: 5, Steps: 318 | Train Loss: 0.1496323 Vali Loss: 0.1804558 Test Loss: 0.1639876
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1304677
	speed: 0.1404s/iter; left time: 209.2698s
	iters: 200, epoch: 6 | loss: 0.1598835
	speed: 0.1290s/iter; left time: 179.3768s
	iters: 300, epoch: 6 | loss: 0.1670343
	speed: 0.1252s/iter; left time: 161.6950s
Epoch: 6 cost time: 42.259496450424194
Epoch: 6, Steps: 318 | Train Loss: 0.1476301 Vali Loss: 0.1780775 Test Loss: 0.1626868
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1582784
	speed: 0.1347s/iter; left time: 158.0437s
	iters: 200, epoch: 7 | loss: 0.1305074
	speed: 0.1283s/iter; left time: 137.6924s
	iters: 300, epoch: 7 | loss: 0.1671558
	speed: 0.1333s/iter; left time: 129.7141s
Epoch: 7 cost time: 42.45996594429016
Epoch: 7, Steps: 318 | Train Loss: 0.1467057 Vali Loss: 0.1781532 Test Loss: 0.1633727
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.3621262013912201, mae:0.46553611755371094, rmse:0.601769208908081, mape:0.01665262132883072, mspe:0.00046911966637708247, rse:0.42145299911499023, r2_score:0.8086327476570643, acc:0.9833473786711693
corr: [36.947884 36.681835 36.721962 36.692158 36.698746 36.831306 36.79431
 36.66595  36.6967   36.735134 36.81342  36.774883 36.879284 36.78119
 36.909943]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2750103
	speed: 0.1704s/iter; left time: 525.0553s
	iters: 200, epoch: 1 | loss: 0.1885924
	speed: 0.1391s/iter; left time: 414.5269s
	iters: 300, epoch: 1 | loss: 0.2154280
	speed: 0.1409s/iter; left time: 405.9316s
Epoch: 1 cost time: 47.46851921081543
Epoch: 1, Steps: 318 | Train Loss: 0.2818781 Vali Loss: 0.2051415 Test Loss: 0.2147542
Validation loss decreased (inf --> 0.205142).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1899718
	speed: 0.1300s/iter; left time: 359.0944s
	iters: 200, epoch: 2 | loss: 0.2268758
	speed: 0.1281s/iter; left time: 341.2417s
	iters: 300, epoch: 2 | loss: 0.1698204
	speed: 0.1272s/iter; left time: 326.1102s
Epoch: 2 cost time: 40.71719026565552
Epoch: 2, Steps: 318 | Train Loss: 0.1817537 Vali Loss: 0.1985754 Test Loss: 0.1850849
Validation loss decreased (0.205142 --> 0.198575).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1940294
	speed: 0.1355s/iter; left time: 331.2598s
	iters: 200, epoch: 3 | loss: 0.1330800
	speed: 0.1363s/iter; left time: 319.6794s
	iters: 300, epoch: 3 | loss: 0.1883049
	speed: 0.1310s/iter; left time: 294.1281s
Epoch: 3 cost time: 42.40219831466675
Epoch: 3, Steps: 318 | Train Loss: 0.1636907 Vali Loss: 0.1933925 Test Loss: 0.1845052
Validation loss decreased (0.198575 --> 0.193392).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1674084
	speed: 0.1367s/iter; left time: 290.7807s
	iters: 200, epoch: 4 | loss: 0.1322182
	speed: 0.1293s/iter; left time: 262.0784s
	iters: 300, epoch: 4 | loss: 0.1545338
	speed: 0.1261s/iter; left time: 242.9015s
Epoch: 4 cost time: 41.3430335521698
Epoch: 4, Steps: 318 | Train Loss: 0.1550882 Vali Loss: 0.1955537 Test Loss: 0.1836403
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1272452
	speed: 0.1384s/iter; left time: 250.4254s
	iters: 200, epoch: 5 | loss: 0.1454112
	speed: 0.1253s/iter; left time: 214.0556s
	iters: 300, epoch: 5 | loss: 0.1031220
	speed: 0.1269s/iter; left time: 204.1169s
Epoch: 5 cost time: 40.88608431816101
Epoch: 5, Steps: 318 | Train Loss: 0.1501411 Vali Loss: 0.1979744 Test Loss: 0.1818304
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1475350
	speed: 0.1367s/iter; left time: 203.8624s
	iters: 200, epoch: 6 | loss: 0.1748112
	speed: 0.1219s/iter; left time: 169.5315s
	iters: 300, epoch: 6 | loss: 0.1614401
	speed: 0.1275s/iter; left time: 164.5863s
Epoch: 6 cost time: 41.23304080963135
Epoch: 6, Steps: 318 | Train Loss: 0.1478914 Vali Loss: 0.1985391 Test Loss: 0.1824708
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.4048894941806793, mae:0.4966697692871094, rmse:0.6363092660903931, mape:0.017772473394870758, mspe:0.0005254394491203129, rse:0.44538524746894836, r2_score:0.7957086209576494, acc:0.9822275266051292
corr: [36.571404 36.475643 36.32741  36.28065  36.300503 36.3877   36.414604
 36.374485 36.41439  36.416042 36.36394  36.326    36.54779  36.70293
 36.73134  36.920265 36.862503 36.940807 36.882633 37.087486]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2535921
	speed: 0.1633s/iter; left time: 503.0165s
	iters: 200, epoch: 1 | loss: 0.2364873
	speed: 0.1443s/iter; left time: 430.1026s
	iters: 300, epoch: 1 | loss: 0.1345370
	speed: 0.1641s/iter; left time: 472.6803s
Epoch: 1 cost time: 50.357038259506226
Epoch: 1, Steps: 318 | Train Loss: 0.2886633 Vali Loss: 0.2146200 Test Loss: 0.2072123
Validation loss decreased (inf --> 0.214620).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1842583
	speed: 0.1730s/iter; left time: 478.0612s
	iters: 200, epoch: 2 | loss: 0.1914384
	speed: 0.1602s/iter; left time: 426.5593s
	iters: 300, epoch: 2 | loss: 0.2293392
	speed: 0.1597s/iter; left time: 409.3964s
Epoch: 2 cost time: 52.0778443813324
Epoch: 2, Steps: 318 | Train Loss: 0.1938563 Vali Loss: 0.2100653 Test Loss: 0.1955930
Validation loss decreased (0.214620 --> 0.210065).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1877709
	speed: 0.1651s/iter; left time: 403.6119s
	iters: 200, epoch: 3 | loss: 0.1737779
	speed: 0.1581s/iter; left time: 370.7940s
	iters: 300, epoch: 3 | loss: 0.1791059
	speed: 0.1667s/iter; left time: 374.1368s
Epoch: 3 cost time: 51.85128450393677
Epoch: 3, Steps: 318 | Train Loss: 0.1787428 Vali Loss: 0.2079813 Test Loss: 0.1921525
Validation loss decreased (0.210065 --> 0.207981).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1556395
	speed: 0.1760s/iter; left time: 374.4173s
	iters: 200, epoch: 4 | loss: 0.1744489
	speed: 0.1732s/iter; left time: 351.0475s
	iters: 300, epoch: 4 | loss: 0.1672323
	speed: 0.1617s/iter; left time: 311.5766s
Epoch: 4 cost time: 53.78528380393982
Epoch: 4, Steps: 318 | Train Loss: 0.1693268 Vali Loss: 0.2079463 Test Loss: 0.1925526
Validation loss decreased (0.207981 --> 0.207946).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1511754
	speed: 0.1609s/iter; left time: 291.1352s
	iters: 200, epoch: 5 | loss: 0.1674898
	speed: 0.1505s/iter; left time: 257.2844s
	iters: 300, epoch: 5 | loss: 0.1450450
	speed: 0.1267s/iter; left time: 203.8412s
Epoch: 5 cost time: 46.03593587875366
Epoch: 5, Steps: 318 | Train Loss: 0.1658013 Vali Loss: 0.2084440 Test Loss: 0.1910728
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1357578
	speed: 0.1580s/iter; left time: 235.6380s
	iters: 200, epoch: 6 | loss: 0.1387630
	speed: 0.1477s/iter; left time: 205.3887s
	iters: 300, epoch: 6 | loss: 0.1952637
	speed: 0.1540s/iter; left time: 198.8070s
Epoch: 6 cost time: 48.760708808898926
Epoch: 6, Steps: 318 | Train Loss: 0.1631762 Vali Loss: 0.2097970 Test Loss: 0.1930511
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1482409
	speed: 0.1571s/iter; left time: 184.2924s
	iters: 200, epoch: 7 | loss: 0.1730383
	speed: 0.1401s/iter; left time: 150.3793s
	iters: 300, epoch: 7 | loss: 0.1483544
	speed: 0.1394s/iter; left time: 135.6114s
Epoch: 7 cost time: 46.50062942504883
Epoch: 7, Steps: 318 | Train Loss: 0.1618109 Vali Loss: 0.2090452 Test Loss: 0.1924060
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.4225492775440216, mae:0.5038345456123352, rmse:0.6500378847122192, mape:0.018059968948364258, mspe:0.0005536137032322586, rse:0.45469069480895996, r2_score:0.7746754355751009, acc:0.9819400310516357
corr: [36.39952  36.24605  36.41472  36.446007 36.394764 36.614834 36.229446
 36.363335 36.317627 36.37572  36.448994 36.474144 36.685337 36.599224
 36.420574 36.57936  36.693035 36.537918 36.722046 37.062904 36.801327
 37.032524 37.03613  36.969948 37.15673 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3568534
	speed: 0.2402s/iter; left time: 737.6993s
	iters: 200, epoch: 1 | loss: 0.2666668
	speed: 0.2244s/iter; left time: 666.6623s
	iters: 300, epoch: 1 | loss: 0.2096208
	speed: 0.2342s/iter; left time: 672.4645s
Epoch: 1 cost time: 73.48892450332642
Epoch: 1, Steps: 317 | Train Loss: 0.3114952 Vali Loss: 0.2352341 Test Loss: 0.2352968
Validation loss decreased (inf --> 0.235234).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1959946
	speed: 0.2289s/iter; left time: 630.4027s
	iters: 200, epoch: 2 | loss: 0.2198311
	speed: 0.2109s/iter; left time: 559.6221s
	iters: 300, epoch: 2 | loss: 0.1734514
	speed: 0.2101s/iter; left time: 536.6664s
Epoch: 2 cost time: 68.79883337020874
Epoch: 2, Steps: 317 | Train Loss: 0.2004806 Vali Loss: 0.2140789 Test Loss: 0.2089057
Validation loss decreased (0.235234 --> 0.214079).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1691803
	speed: 0.2189s/iter; left time: 533.3543s
	iters: 200, epoch: 3 | loss: 0.1881632
	speed: 0.2068s/iter; left time: 483.3978s
	iters: 300, epoch: 3 | loss: 0.1959752
	speed: 0.2124s/iter; left time: 475.1210s
Epoch: 3 cost time: 67.17277717590332
Epoch: 3, Steps: 317 | Train Loss: 0.1847845 Vali Loss: 0.2080799 Test Loss: 0.2063732
Validation loss decreased (0.214079 --> 0.208080).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1456593
	speed: 0.2187s/iter; left time: 463.5572s
	iters: 200, epoch: 4 | loss: 0.1874664
	speed: 0.2087s/iter; left time: 421.5371s
	iters: 300, epoch: 4 | loss: 0.1603064
	speed: 0.2254s/iter; left time: 432.7514s
Epoch: 4 cost time: 69.17324495315552
Epoch: 4, Steps: 317 | Train Loss: 0.1763016 Vali Loss: 0.2049296 Test Loss: 0.2012779
Validation loss decreased (0.208080 --> 0.204930).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1407923
	speed: 0.2043s/iter; left time: 368.3869s
	iters: 200, epoch: 5 | loss: 0.1580678
	speed: 0.2173s/iter; left time: 370.0754s
	iters: 300, epoch: 5 | loss: 0.1754770
	speed: 0.2049s/iter; left time: 328.3790s
Epoch: 5 cost time: 66.32144141197205
Epoch: 5, Steps: 317 | Train Loss: 0.1711633 Vali Loss: 0.2062220 Test Loss: 0.1993370
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1704645
	speed: 0.2099s/iter; left time: 311.9633s
	iters: 200, epoch: 6 | loss: 0.1678164
	speed: 0.2151s/iter; left time: 298.1222s
	iters: 300, epoch: 6 | loss: 0.1520388
	speed: 0.2155s/iter; left time: 277.1862s
Epoch: 6 cost time: 67.49976658821106
Epoch: 6, Steps: 317 | Train Loss: 0.1691995 Vali Loss: 0.2069745 Test Loss: 0.2002269
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1585027
	speed: 0.2112s/iter; left time: 246.9272s
	iters: 200, epoch: 7 | loss: 0.1414450
	speed: 0.2110s/iter; left time: 225.5735s
	iters: 300, epoch: 7 | loss: 0.1765044
	speed: 0.2187s/iter; left time: 211.9509s
Epoch: 7 cost time: 67.82916212081909
Epoch: 7, Steps: 317 | Train Loss: 0.1680248 Vali Loss: 0.2068207 Test Loss: 0.1998881
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.44169655442237854, mae:0.5162819027900696, rmse:0.6646025776863098, mape:0.018494442105293274, mspe:0.0005768091068603098, rse:0.4645446240901947, r2_score:0.7770019288303056, acc:0.9815055578947067
corr: [36.58163  36.633827 36.50409  36.502483 36.57956  36.55691  36.577286
 36.644695 36.623363 36.490623 36.54832  36.571854 36.514736 36.531963
 36.61151  36.704144 36.724026 36.861237 36.98141  37.237392 37.059105
 36.924213 37.049187 36.868942 36.83479  36.824883 36.87946  36.816364
 36.674637 36.804005]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2141459
	speed: 0.1871s/iter; left time: 576.6066s
	iters: 200, epoch: 1 | loss: 0.1801169
	speed: 0.1611s/iter; left time: 480.1392s
	iters: 300, epoch: 1 | loss: 0.2278370
	speed: 0.1733s/iter; left time: 499.2014s
Epoch: 1 cost time: 55.14640784263611
Epoch: 1, Steps: 318 | Train Loss: 0.2614218 Vali Loss: 0.1628796 Test Loss: 0.1467533
Validation loss decreased (inf --> 0.162880).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1736711
	speed: 0.1666s/iter; left time: 460.3767s
	iters: 200, epoch: 2 | loss: 0.1286617
	speed: 0.1624s/iter; left time: 432.3837s
	iters: 300, epoch: 2 | loss: 0.1532352
	speed: 0.1659s/iter; left time: 425.1136s
Epoch: 2 cost time: 52.32709717750549
Epoch: 2, Steps: 318 | Train Loss: 0.1518003 Vali Loss: 0.1482033 Test Loss: 0.1351886
Validation loss decreased (0.162880 --> 0.148203).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1307946
	speed: 0.1633s/iter; left time: 399.3533s
	iters: 200, epoch: 3 | loss: 0.1684984
	speed: 0.1459s/iter; left time: 342.1763s
	iters: 300, epoch: 3 | loss: 0.1095002
	speed: 0.1489s/iter; left time: 334.3212s
Epoch: 3 cost time: 48.82829403877258
Epoch: 3, Steps: 318 | Train Loss: 0.1360260 Vali Loss: 0.1519089 Test Loss: 0.1355727
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1610881
	speed: 0.1528s/iter; left time: 325.0149s
	iters: 200, epoch: 4 | loss: 0.1128360
	speed: 0.1481s/iter; left time: 300.1388s
	iters: 300, epoch: 4 | loss: 0.1207711
	speed: 0.1484s/iter; left time: 285.9064s
Epoch: 4 cost time: 47.80749797821045
Epoch: 4, Steps: 318 | Train Loss: 0.1284910 Vali Loss: 0.1488634 Test Loss: 0.1345256
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1560679
	speed: 0.1697s/iter; left time: 307.0712s
	iters: 200, epoch: 5 | loss: 0.0956933
	speed: 0.1503s/iter; left time: 256.9213s
	iters: 300, epoch: 5 | loss: 0.1137478
	speed: 0.1495s/iter; left time: 240.5037s
Epoch: 5 cost time: 49.514556884765625
Epoch: 5, Steps: 318 | Train Loss: 0.1245708 Vali Loss: 0.1476469 Test Loss: 0.1321295
Validation loss decreased (0.148203 --> 0.147647).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1189366
	speed: 0.1739s/iter; left time: 259.3580s
	iters: 200, epoch: 6 | loss: 0.1141364
	speed: 0.1714s/iter; left time: 238.4284s
	iters: 300, epoch: 6 | loss: 0.1115891
	speed: 0.1688s/iter; left time: 217.8660s
Epoch: 6 cost time: 54.68498396873474
Epoch: 6, Steps: 318 | Train Loss: 0.1226242 Vali Loss: 0.1486611 Test Loss: 0.1326436
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1317065
	speed: 0.1792s/iter; left time: 210.1857s
	iters: 200, epoch: 7 | loss: 0.1688623
	speed: 0.1624s/iter; left time: 174.2389s
	iters: 300, epoch: 7 | loss: 0.1024993
	speed: 0.1720s/iter; left time: 167.3823s
Epoch: 7 cost time: 54.40105962753296
Epoch: 7, Steps: 318 | Train Loss: 0.1210352 Vali Loss: 0.1476541 Test Loss: 0.1324551
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1531583
	speed: 0.1837s/iter; left time: 157.0991s
	iters: 200, epoch: 8 | loss: 0.1215546
	speed: 0.1762s/iter; left time: 133.0158s
	iters: 300, epoch: 8 | loss: 0.0919086
	speed: 0.1694s/iter; left time: 110.9595s
Epoch: 8 cost time: 55.832361459732056
Epoch: 8, Steps: 318 | Train Loss: 0.1205223 Vali Loss: 0.1476546 Test Loss: 0.1319294
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.28995299339294434, mae:0.4174797236919403, rmse:0.5384728312492371, mape:0.014923459850251675, mspe:0.00037415637052617967, rse:0.3773006200790405, r2_score:0.8513012973493777, acc:0.9850765401497483
corr: [36.573475 36.536446 36.575848 36.45553  36.364517 36.504314 36.63981
 36.75059  36.74955  36.863   ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2923764
	speed: 0.2580s/iter; left time: 794.8611s
	iters: 200, epoch: 1 | loss: 0.2190475
	speed: 0.2132s/iter; left time: 635.4658s
	iters: 300, epoch: 1 | loss: 0.2173429
	speed: 0.2027s/iter; left time: 583.9979s
Epoch: 1 cost time: 71.56065487861633
Epoch: 1, Steps: 318 | Train Loss: 0.2923666 Vali Loss: 0.1756174 Test Loss: 0.1654194
Validation loss decreased (inf --> 0.175617).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1961142
	speed: 0.2182s/iter; left time: 602.9248s
	iters: 200, epoch: 2 | loss: 0.1533450
	speed: 0.2101s/iter; left time: 559.5551s
	iters: 300, epoch: 2 | loss: 0.1309518
	speed: 0.2204s/iter; left time: 564.8607s
Epoch: 2 cost time: 69.11744976043701
Epoch: 2, Steps: 318 | Train Loss: 0.1688574 Vali Loss: 0.1714964 Test Loss: 0.1572174
Validation loss decreased (0.175617 --> 0.171496).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1214769
	speed: 0.2236s/iter; left time: 546.7307s
	iters: 200, epoch: 3 | loss: 0.1461321
	speed: 0.2193s/iter; left time: 514.2181s
	iters: 300, epoch: 3 | loss: 0.1231271
	speed: 0.2143s/iter; left time: 481.2007s
Epoch: 3 cost time: 69.5048348903656
Epoch: 3, Steps: 318 | Train Loss: 0.1548467 Vali Loss: 0.1702165 Test Loss: 0.1550252
Validation loss decreased (0.171496 --> 0.170216).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1332834
	speed: 0.2105s/iter; left time: 447.8069s
	iters: 200, epoch: 4 | loss: 0.1581963
	speed: 0.2139s/iter; left time: 433.4764s
	iters: 300, epoch: 4 | loss: 0.1676967
	speed: 0.2281s/iter; left time: 439.6109s
Epoch: 4 cost time: 69.22656798362732
Epoch: 4, Steps: 318 | Train Loss: 0.1470452 Vali Loss: 0.1709176 Test Loss: 0.1570780
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1214039
	speed: 0.2220s/iter; left time: 401.6027s
	iters: 200, epoch: 5 | loss: 0.1302107
	speed: 0.2279s/iter; left time: 389.5468s
	iters: 300, epoch: 5 | loss: 0.1408664
	speed: 0.2188s/iter; left time: 352.0053s
Epoch: 5 cost time: 70.97495436668396
Epoch: 5, Steps: 318 | Train Loss: 0.1435128 Vali Loss: 0.1749255 Test Loss: 0.1579981
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1271682
	speed: 0.2310s/iter; left time: 344.3985s
	iters: 200, epoch: 6 | loss: 0.1527564
	speed: 0.2248s/iter; left time: 312.7227s
	iters: 300, epoch: 6 | loss: 0.1408691
	speed: 0.2248s/iter; left time: 290.2441s
Epoch: 6 cost time: 72.45017647743225
Epoch: 6, Steps: 318 | Train Loss: 0.1414099 Vali Loss: 0.1713335 Test Loss: 0.1540370
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.3401966691017151, mae:0.45005398988723755, rmse:0.5832638144493103, mape:0.016076937317848206, mspe:0.00043871416710317135, rse:0.4084925949573517, r2_score:0.8164586932713588, acc:0.9839230626821518
corr: [36.566242 36.3734   36.43696  36.456505 36.459305 36.503433 36.711742
 36.662617 36.711906 36.727745 36.827305 36.887627 37.18988  37.20013
 37.144688]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2758664
	speed: 0.2194s/iter; left time: 675.8973s
	iters: 200, epoch: 1 | loss: 0.1875374
	speed: 0.1857s/iter; left time: 553.5497s
	iters: 300, epoch: 1 | loss: 0.2148893
	speed: 0.1866s/iter; left time: 537.6687s
Epoch: 1 cost time: 62.73901438713074
Epoch: 1, Steps: 318 | Train Loss: 0.2822726 Vali Loss: 0.2048648 Test Loss: 0.2146046
Validation loss decreased (inf --> 0.204865).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1903745
	speed: 0.1903s/iter; left time: 525.9319s
	iters: 200, epoch: 2 | loss: 0.2265421
	speed: 0.1716s/iter; left time: 457.0709s
	iters: 300, epoch: 2 | loss: 0.1714912
	speed: 0.1877s/iter; left time: 481.0947s
Epoch: 2 cost time: 57.852781772613525
Epoch: 2, Steps: 318 | Train Loss: 0.1820968 Vali Loss: 0.1986375 Test Loss: 0.1851094
Validation loss decreased (0.204865 --> 0.198638).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1936012
	speed: 0.1731s/iter; left time: 423.2845s
	iters: 200, epoch: 3 | loss: 0.1335063
	speed: 0.1819s/iter; left time: 426.5288s
	iters: 300, epoch: 3 | loss: 0.1889214
	speed: 0.1675s/iter; left time: 375.9323s
Epoch: 3 cost time: 55.40711283683777
Epoch: 3, Steps: 318 | Train Loss: 0.1640440 Vali Loss: 0.1930385 Test Loss: 0.1842983
Validation loss decreased (0.198638 --> 0.193038).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1670586
	speed: 0.1894s/iter; left time: 402.8293s
	iters: 200, epoch: 4 | loss: 0.1312978
	speed: 0.1786s/iter; left time: 361.9731s
	iters: 300, epoch: 4 | loss: 0.1561479
	speed: 0.1768s/iter; left time: 340.7585s
Epoch: 4 cost time: 57.93300223350525
Epoch: 4, Steps: 318 | Train Loss: 0.1553759 Vali Loss: 0.1955593 Test Loss: 0.1837262
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1268692
	speed: 0.1870s/iter; left time: 338.1927s
	iters: 200, epoch: 5 | loss: 0.1455414
	speed: 0.1806s/iter; left time: 308.5681s
	iters: 300, epoch: 5 | loss: 0.1044030
	speed: 0.1897s/iter; left time: 305.2509s
Epoch: 5 cost time: 58.965569496154785
Epoch: 5, Steps: 318 | Train Loss: 0.1504199 Vali Loss: 0.1978300 Test Loss: 0.1819628
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1483826
	speed: 0.1913s/iter; left time: 285.2542s
	iters: 200, epoch: 6 | loss: 0.1746172
	speed: 0.1776s/iter; left time: 246.9814s
	iters: 300, epoch: 6 | loss: 0.1622123
	speed: 0.1831s/iter; left time: 236.4402s
Epoch: 6 cost time: 58.66501522064209
Epoch: 6, Steps: 318 | Train Loss: 0.1481778 Vali Loss: 0.1981918 Test Loss: 0.1825389
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.40443548560142517, mae:0.495943158864975, rmse:0.6359524130821228, mape:0.017747895792126656, mspe:0.000525064067915082, rse:0.4451355040073395, r2_score:0.7950929309704966, acc:0.9822521042078733
corr: [36.559673 36.461426 36.32015  36.268604 36.27398  36.365425 36.40199
 36.35846  36.401596 36.404446 36.35532  36.315784 36.544094 36.688366
 36.72077  36.922085 36.863518 36.943233 36.873924 37.074394]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2520616
	speed: 0.1916s/iter; left time: 590.4661s
	iters: 200, epoch: 1 | loss: 0.2356591
	speed: 0.1654s/iter; left time: 493.1970s
	iters: 300, epoch: 1 | loss: 0.1354847
	speed: 0.1771s/iter; left time: 510.3607s
Epoch: 1 cost time: 56.491166830062866
Epoch: 1, Steps: 318 | Train Loss: 0.2895384 Vali Loss: 0.2118393 Test Loss: 0.2058091
Validation loss decreased (inf --> 0.211839).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1865102
	speed: 0.1906s/iter; left time: 526.6744s
	iters: 200, epoch: 2 | loss: 0.1908789
	speed: 0.1729s/iter; left time: 460.3336s
	iters: 300, epoch: 2 | loss: 0.2307588
	speed: 0.1821s/iter; left time: 466.7257s
Epoch: 2 cost time: 57.768837690353394
Epoch: 2, Steps: 318 | Train Loss: 0.1942175 Vali Loss: 0.2068817 Test Loss: 0.1940261
Validation loss decreased (0.211839 --> 0.206882).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1896089
	speed: 0.1776s/iter; left time: 434.1540s
	iters: 200, epoch: 3 | loss: 0.1737180
	speed: 0.1791s/iter; left time: 419.9919s
	iters: 300, epoch: 3 | loss: 0.1747061
	speed: 0.1707s/iter; left time: 383.1169s
Epoch: 3 cost time: 55.990227699279785
Epoch: 3, Steps: 318 | Train Loss: 0.1796438 Vali Loss: 0.2056221 Test Loss: 0.1905717
Validation loss decreased (0.206882 --> 0.205622).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1596703
	speed: 0.1799s/iter; left time: 382.5686s
	iters: 200, epoch: 4 | loss: 0.1819471
	speed: 0.1658s/iter; left time: 336.1369s
	iters: 300, epoch: 4 | loss: 0.1666611
	speed: 0.1775s/iter; left time: 342.1123s
Epoch: 4 cost time: 55.82707071304321
Epoch: 4, Steps: 318 | Train Loss: 0.1703336 Vali Loss: 0.2046466 Test Loss: 0.1906617
Validation loss decreased (0.205622 --> 0.204647).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1503199
	speed: 0.1828s/iter; left time: 330.6927s
	iters: 200, epoch: 5 | loss: 0.1705408
	speed: 0.1785s/iter; left time: 305.0508s
	iters: 300, epoch: 5 | loss: 0.1464062
	speed: 0.1684s/iter; left time: 270.9255s
Epoch: 5 cost time: 56.01459217071533
Epoch: 5, Steps: 318 | Train Loss: 0.1669956 Vali Loss: 0.2051319 Test Loss: 0.1890101
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1357251
	speed: 0.1772s/iter; left time: 264.1925s
	iters: 200, epoch: 6 | loss: 0.1380495
	speed: 0.1704s/iter; left time: 236.9753s
	iters: 300, epoch: 6 | loss: 0.1939346
	speed: 0.1762s/iter; left time: 227.4261s
Epoch: 6 cost time: 55.26244282722473
Epoch: 6, Steps: 318 | Train Loss: 0.1642704 Vali Loss: 0.2066184 Test Loss: 0.1908544
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1432134
	speed: 0.1795s/iter; left time: 210.5407s
	iters: 200, epoch: 7 | loss: 0.1711863
	speed: 0.1705s/iter; left time: 182.9724s
	iters: 300, epoch: 7 | loss: 0.1500186
	speed: 0.1746s/iter; left time: 169.8392s
Epoch: 7 cost time: 55.93318510055542
Epoch: 7, Steps: 318 | Train Loss: 0.1630212 Vali Loss: 0.2057457 Test Loss: 0.1901632
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.4183996617794037, mae:0.5010569095611572, rmse:0.6468381881713867, mape:0.017964014783501625, mspe:0.0005485491128638387, rse:0.45245254039764404, r2_score:0.7766431970875763, acc:0.9820359852164984
corr: [36.294025 36.142807 36.34058  36.367203 36.348392 36.558514 36.21401
 36.379826 36.322163 36.390812 36.483524 36.506233 36.706898 36.645775
 36.455647 36.624054 36.72729  36.566864 36.76012  37.083378 36.805626
 37.03644  37.044144 37.015408 37.17115 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3362030
	speed: 0.2712s/iter; left time: 832.8621s
	iters: 200, epoch: 1 | loss: 0.2695880
	speed: 0.2472s/iter; left time: 734.5445s
	iters: 300, epoch: 1 | loss: 0.2275685
	speed: 0.2343s/iter; left time: 672.7012s
Epoch: 1 cost time: 79.38330101966858
Epoch: 1, Steps: 317 | Train Loss: 0.3224711 Vali Loss: 0.2454651 Test Loss: 0.2249662
Validation loss decreased (inf --> 0.245465).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1941065
	speed: 0.2561s/iter; left time: 705.1963s
	iters: 200, epoch: 2 | loss: 0.2077031
	speed: 0.2822s/iter; left time: 748.8317s
	iters: 300, epoch: 2 | loss: 0.1779640
	speed: 0.2675s/iter; left time: 683.1152s
Epoch: 2 cost time: 85.2411961555481
Epoch: 2, Steps: 317 | Train Loss: 0.2062258 Vali Loss: 0.2283479 Test Loss: 0.2085867
Validation loss decreased (0.245465 --> 0.228348).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1860097
	speed: 0.2710s/iter; left time: 660.3628s
	iters: 200, epoch: 3 | loss: 0.1975883
	speed: 0.2766s/iter; left time: 646.3913s
	iters: 300, epoch: 3 | loss: 0.1833499
	speed: 0.2961s/iter; left time: 662.4341s
Epoch: 3 cost time: 88.95647549629211
Epoch: 3, Steps: 317 | Train Loss: 0.1870906 Vali Loss: 0.2223130 Test Loss: 0.2055018
Validation loss decreased (0.228348 --> 0.222313).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1360129
	speed: 0.2901s/iter; left time: 615.0583s
	iters: 200, epoch: 4 | loss: 0.1937695
	speed: 0.2615s/iter; left time: 528.2998s
	iters: 300, epoch: 4 | loss: 0.1545702
	speed: 0.2606s/iter; left time: 500.3680s
Epoch: 4 cost time: 85.72937846183777
Epoch: 4, Steps: 317 | Train Loss: 0.1777782 Vali Loss: 0.2237166 Test Loss: 0.2050423
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1559920
	speed: 0.2615s/iter; left time: 471.3958s
	iters: 200, epoch: 5 | loss: 0.1663028
	speed: 0.2639s/iter; left time: 449.4584s
	iters: 300, epoch: 5 | loss: 0.1739562
	speed: 0.2703s/iter; left time: 433.2562s
Epoch: 5 cost time: 84.01286149024963
Epoch: 5, Steps: 317 | Train Loss: 0.1725695 Vali Loss: 0.2239287 Test Loss: 0.2041076
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1761370
	speed: 0.2727s/iter; left time: 405.1931s
	iters: 200, epoch: 6 | loss: 0.1854597
	speed: 0.2786s/iter; left time: 386.1681s
	iters: 300, epoch: 6 | loss: 0.1594569
	speed: 0.2659s/iter; left time: 341.9200s
Epoch: 6 cost time: 86.97486162185669
Epoch: 6, Steps: 317 | Train Loss: 0.1705134 Vali Loss: 0.2249651 Test Loss: 0.2059505
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.45096564292907715, mae:0.5253421068191528, rmse:0.6715397834777832, mape:0.01878909207880497, mspe:0.0005844784318469465, rse:0.4693935811519623, r2_score:0.7748504892885903, acc:0.981210907921195
corr: [36.908554 36.895676 36.753624 36.652527 36.68078  36.785007 36.930492
 36.965683 36.901123 36.76037  36.77846  36.831543 36.921886 36.940437
 36.971336 36.97173  37.142223 37.225754 37.42004  37.291557 37.175076
 37.073452 37.23394  37.158318 37.163425 37.1267   37.29671  37.17042
 37.097366 37.36145 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2162791
	speed: 0.2171s/iter; left time: 668.7751s
	iters: 200, epoch: 1 | loss: 0.1928256
	speed: 0.2040s/iter; left time: 608.2260s
	iters: 300, epoch: 1 | loss: 0.2317181
	speed: 0.1970s/iter; left time: 567.6112s
Epoch: 1 cost time: 65.19572472572327
Epoch: 1, Steps: 318 | Train Loss: 0.2763071 Vali Loss: 0.1762249 Test Loss: 0.1525141
Validation loss decreased (inf --> 0.176225).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1745937
	speed: 0.2000s/iter; left time: 552.5051s
	iters: 200, epoch: 2 | loss: 0.1307642
	speed: 0.2119s/iter; left time: 564.1633s
	iters: 300, epoch: 2 | loss: 0.1612566
	speed: 0.2043s/iter; left time: 523.7202s
Epoch: 2 cost time: 65.11768913269043
Epoch: 2, Steps: 318 | Train Loss: 0.1524617 Vali Loss: 0.1605011 Test Loss: 0.1437346
Validation loss decreased (0.176225 --> 0.160501).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1289285
	speed: 0.1953s/iter; left time: 477.4785s
	iters: 200, epoch: 3 | loss: 0.1534114
	speed: 0.1978s/iter; left time: 463.7731s
	iters: 300, epoch: 3 | loss: 0.1189718
	speed: 0.1913s/iter; left time: 429.4793s
Epoch: 3 cost time: 62.08953523635864
Epoch: 3, Steps: 318 | Train Loss: 0.1337555 Vali Loss: 0.1627832 Test Loss: 0.1436700
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1439492
	speed: 0.2033s/iter; left time: 432.4975s
	iters: 200, epoch: 4 | loss: 0.1143228
	speed: 0.1856s/iter; left time: 376.1833s
	iters: 300, epoch: 4 | loss: 0.1230676
	speed: 0.1877s/iter; left time: 361.6536s
Epoch: 4 cost time: 61.18441677093506
Epoch: 4, Steps: 318 | Train Loss: 0.1252079 Vali Loss: 0.1654017 Test Loss: 0.1450739
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1403337
	speed: 0.1950s/iter; left time: 352.7787s
	iters: 200, epoch: 5 | loss: 0.0997120
	speed: 0.1984s/iter; left time: 339.0672s
	iters: 300, epoch: 5 | loss: 0.0959807
	speed: 0.1951s/iter; left time: 313.9815s
Epoch: 5 cost time: 62.411826848983765
Epoch: 5, Steps: 318 | Train Loss: 0.1207876 Vali Loss: 0.1651897 Test Loss: 0.1432134
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000031
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.3154200315475464, mae:0.4356131851673126, rmse:0.561622679233551, mape:0.015552370809018612, mspe:0.0004047524998895824, rse:0.39352136850357056, r2_score:0.8337351716710011, acc:0.9844476291909814
corr: [36.887325 36.78214  36.854973 36.76919  36.677025 36.74574  36.800434
 36.89392  37.004616 37.144226]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2959571
	speed: 0.2832s/iter; left time: 872.4119s
	iters: 200, epoch: 1 | loss: 0.2211195
	speed: 0.2640s/iter; left time: 787.0021s
	iters: 300, epoch: 1 | loss: 0.2146299
	speed: 0.2623s/iter; left time: 755.7132s
Epoch: 1 cost time: 84.68475914001465
Epoch: 1, Steps: 318 | Train Loss: 0.2917323 Vali Loss: 0.1755294 Test Loss: 0.1653717
Validation loss decreased (inf --> 0.175529).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1942157
	speed: 0.2651s/iter; left time: 732.4769s
	iters: 200, epoch: 2 | loss: 0.1538113
	speed: 0.2330s/iter; left time: 620.3893s
	iters: 300, epoch: 2 | loss: 0.1314366
	speed: 0.2363s/iter; left time: 605.5314s
Epoch: 2 cost time: 76.85617804527283
Epoch: 2, Steps: 318 | Train Loss: 0.1693174 Vali Loss: 0.1726746 Test Loss: 0.1581433
Validation loss decreased (0.175529 --> 0.172675).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1229778
	speed: 0.2409s/iter; left time: 588.8958s
	iters: 200, epoch: 3 | loss: 0.1482856
	speed: 0.2027s/iter; left time: 475.2969s
	iters: 300, epoch: 3 | loss: 0.1239611
	speed: 0.2347s/iter; left time: 526.8806s
Epoch: 3 cost time: 72.30674004554749
Epoch: 3, Steps: 318 | Train Loss: 0.1548635 Vali Loss: 0.1717096 Test Loss: 0.1559790
Validation loss decreased (0.172675 --> 0.171710).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1349012
	speed: 0.2573s/iter; left time: 547.3625s
	iters: 200, epoch: 4 | loss: 0.1585069
	speed: 0.2636s/iter; left time: 534.3008s
	iters: 300, epoch: 4 | loss: 0.1696323
	speed: 0.2650s/iter; left time: 510.5788s
Epoch: 4 cost time: 83.63158750534058
Epoch: 4, Steps: 318 | Train Loss: 0.1469331 Vali Loss: 0.1730712 Test Loss: 0.1582793
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1220084
	speed: 0.2669s/iter; left time: 482.8051s
	iters: 200, epoch: 5 | loss: 0.1318749
	speed: 0.2546s/iter; left time: 435.1409s
	iters: 300, epoch: 5 | loss: 0.1396378
	speed: 0.2539s/iter; left time: 408.6051s
Epoch: 5 cost time: 82.26614356040955
Epoch: 5, Steps: 318 | Train Loss: 0.1433740 Vali Loss: 0.1780085 Test Loss: 0.1600608
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1278955
	speed: 0.2668s/iter; left time: 397.8732s
	iters: 200, epoch: 6 | loss: 0.1513417
	speed: 0.2539s/iter; left time: 353.2273s
	iters: 300, epoch: 6 | loss: 0.1411072
	speed: 0.2581s/iter; left time: 333.2671s
Epoch: 6 cost time: 82.89176654815674
Epoch: 6, Steps: 318 | Train Loss: 0.1412673 Vali Loss: 0.1737457 Test Loss: 0.1554164
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.3422898054122925, mae:0.4516445994377136, rmse:0.585055410861969, mape:0.016135063022375107, mspe:0.00044159917160868645, rse:0.4097473621368408, r2_score:0.8166696260164633, acc:0.9838649369776249
corr: [36.64032  36.41475  36.460182 36.494164 36.510635 36.558273 36.756363
 36.69804  36.7589   36.778145 36.855934 36.88021  37.234177 37.27169
 37.212162]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2765587
	speed: 0.3300s/iter; left time: 1016.6041s
	iters: 200, epoch: 1 | loss: 0.1847108
	speed: 0.3046s/iter; left time: 908.1124s
	iters: 300, epoch: 1 | loss: 0.2150504
	speed: 0.3093s/iter; left time: 891.1469s
Epoch: 1 cost time: 100.03269743919373
Epoch: 1, Steps: 318 | Train Loss: 0.2844621 Vali Loss: 0.2036220 Test Loss: 0.2145496
Validation loss decreased (inf --> 0.203622).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1901130
	speed: 0.2997s/iter; left time: 827.9823s
	iters: 200, epoch: 2 | loss: 0.2274159
	speed: 0.2908s/iter; left time: 774.3111s
	iters: 300, epoch: 2 | loss: 0.1736116
	speed: 0.3016s/iter; left time: 773.0794s
Epoch: 2 cost time: 94.46840310096741
Epoch: 2, Steps: 318 | Train Loss: 0.1833413 Vali Loss: 0.1979101 Test Loss: 0.1845476
Validation loss decreased (0.203622 --> 0.197910).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1956514
	speed: 0.2987s/iter; left time: 730.3172s
	iters: 200, epoch: 3 | loss: 0.1351180
	speed: 0.2970s/iter; left time: 696.4866s
	iters: 300, epoch: 3 | loss: 0.1930168
	speed: 0.2848s/iter; left time: 639.2982s
Epoch: 3 cost time: 93.70450949668884
Epoch: 3, Steps: 318 | Train Loss: 0.1654478 Vali Loss: 0.1925541 Test Loss: 0.1832685
Validation loss decreased (0.197910 --> 0.192554).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1663399
	speed: 0.3059s/iter; left time: 650.6051s
	iters: 200, epoch: 4 | loss: 0.1287773
	speed: 0.2813s/iter; left time: 570.1418s
	iters: 300, epoch: 4 | loss: 0.1573261
	speed: 0.2868s/iter; left time: 552.7535s
Epoch: 4 cost time: 92.16436219215393
Epoch: 4, Steps: 318 | Train Loss: 0.1566470 Vali Loss: 0.1954276 Test Loss: 0.1829163
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1286349
	speed: 0.3005s/iter; left time: 543.6444s
	iters: 200, epoch: 5 | loss: 0.1471384
	speed: 0.2986s/iter; left time: 510.2930s
	iters: 300, epoch: 5 | loss: 0.1051439
	speed: 0.2774s/iter; left time: 446.3533s
Epoch: 5 cost time: 92.54777455329895
Epoch: 5, Steps: 318 | Train Loss: 0.1514867 Vali Loss: 0.1970293 Test Loss: 0.1811737
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1520719
	speed: 0.2893s/iter; left time: 431.2775s
	iters: 200, epoch: 6 | loss: 0.1748184
	speed: 0.2960s/iter; left time: 411.7349s
	iters: 300, epoch: 6 | loss: 0.1684670
	speed: 0.2835s/iter; left time: 365.9696s
Epoch: 6 cost time: 92.73168992996216
Epoch: 6, Steps: 318 | Train Loss: 0.1491900 Vali Loss: 0.1971550 Test Loss: 0.1820691
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.40217554569244385, mae:0.4928598999977112, rmse:0.6341730952262878, mape:0.017642492428421974, mspe:0.0005228586960583925, rse:0.4438900947570801, r2_score:0.7950887275403347, acc:0.982357507571578
corr: [36.564743 36.4606   36.344585 36.295456 36.289894 36.352135 36.428993
 36.380566 36.429264 36.435276 36.401897 36.34932  36.578953 36.705048
 36.75589  36.95106  36.89207  37.006954 36.927402 37.124516]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2464352
	speed: 0.2468s/iter; left time: 760.3597s
	iters: 200, epoch: 1 | loss: 0.2349521
	speed: 0.2221s/iter; left time: 662.0985s
	iters: 300, epoch: 1 | loss: 0.1332527
	speed: 0.2071s/iter; left time: 596.6728s
Epoch: 1 cost time: 71.27969646453857
Epoch: 1, Steps: 318 | Train Loss: 0.2907651 Vali Loss: 0.2174756 Test Loss: 0.2043362
Validation loss decreased (inf --> 0.217476).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1798020
	speed: 0.2500s/iter; left time: 690.6822s
	iters: 200, epoch: 2 | loss: 0.1890352
	speed: 0.2464s/iter; left time: 656.1139s
	iters: 300, epoch: 2 | loss: 0.2316951
	speed: 0.2595s/iter; left time: 665.1159s
Epoch: 2 cost time: 80.42206931114197
Epoch: 2, Steps: 318 | Train Loss: 0.1939029 Vali Loss: 0.2101989 Test Loss: 0.1910223
Validation loss decreased (0.217476 --> 0.210199).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1817129
	speed: 0.2646s/iter; left time: 646.9054s
	iters: 200, epoch: 3 | loss: 0.1837463
	speed: 0.2520s/iter; left time: 590.8454s
	iters: 300, epoch: 3 | loss: 0.1697913
	speed: 0.2419s/iter; left time: 543.0349s
Epoch: 3 cost time: 80.26239848136902
Epoch: 3, Steps: 318 | Train Loss: 0.1778128 Vali Loss: 0.2121368 Test Loss: 0.1930429
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1577457
	speed: 0.2495s/iter; left time: 530.7310s
	iters: 200, epoch: 4 | loss: 0.1777055
	speed: 0.2475s/iter; left time: 501.7579s
	iters: 300, epoch: 4 | loss: 0.1649489
	speed: 0.2527s/iter; left time: 487.0320s
Epoch: 4 cost time: 79.31294178962708
Epoch: 4, Steps: 318 | Train Loss: 0.1682757 Vali Loss: 0.2131071 Test Loss: 0.1914864
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1485397
	speed: 0.2403s/iter; left time: 434.7022s
	iters: 200, epoch: 5 | loss: 0.1698175
	speed: 0.2602s/iter; left time: 444.6215s
	iters: 300, epoch: 5 | loss: 0.1517231
	speed: 0.2423s/iter; left time: 389.9127s
Epoch: 5 cost time: 79.0073938369751
Epoch: 5, Steps: 318 | Train Loss: 0.1645543 Vali Loss: 0.2130479 Test Loss: 0.1907385
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000031
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.419190913438797, mae:0.500174880027771, rmse:0.6474495530128479, mape:0.01794975996017456, mspe:0.0005514537915587425, rse:0.45288020372390747, r2_score:0.7611722711803088, acc:0.9820502400398254
corr: [36.470333 36.24014  36.57137  36.568863 36.56438  36.748734 36.426285
 36.388607 36.628574 36.67713  36.722435 36.6634   36.934727 36.897114
 36.779922 36.956432 36.96909  36.843464 36.97882  37.26987  37.087597
 37.272377 37.247112 37.151184 37.33464 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3530480
	speed: 0.2889s/iter; left time: 887.2929s
	iters: 200, epoch: 1 | loss: 0.2674599
	speed: 0.2926s/iter; left time: 869.3265s
	iters: 300, epoch: 1 | loss: 0.2364974
	speed: 0.2916s/iter; left time: 837.1396s
Epoch: 1 cost time: 92.4032928943634
Epoch: 1, Steps: 317 | Train Loss: 0.3209891 Vali Loss: 0.2455748 Test Loss: 0.2300327
Validation loss decreased (inf --> 0.245575).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1879152
	speed: 0.3044s/iter; left time: 838.2259s
	iters: 200, epoch: 2 | loss: 0.2105405
	speed: 0.2968s/iter; left time: 787.6576s
	iters: 300, epoch: 2 | loss: 0.1749562
	speed: 0.2988s/iter; left time: 763.1117s
Epoch: 2 cost time: 95.29639601707458
Epoch: 2, Steps: 317 | Train Loss: 0.2029081 Vali Loss: 0.2300818 Test Loss: 0.2184355
Validation loss decreased (0.245575 --> 0.230082).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1771787
	speed: 0.3028s/iter; left time: 738.0008s
	iters: 200, epoch: 3 | loss: 0.1922522
	speed: 0.3025s/iter; left time: 706.8529s
	iters: 300, epoch: 3 | loss: 0.1933240
	speed: 0.2944s/iter; left time: 658.6287s
Epoch: 3 cost time: 95.83134365081787
Epoch: 3, Steps: 317 | Train Loss: 0.1830555 Vali Loss: 0.2262821 Test Loss: 0.2136732
Validation loss decreased (0.230082 --> 0.226282).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1348275
	speed: 0.3294s/iter; left time: 698.3170s
	iters: 200, epoch: 4 | loss: 0.1759403
	speed: 0.2953s/iter; left time: 596.4681s
	iters: 300, epoch: 4 | loss: 0.1452875
	speed: 0.2919s/iter; left time: 560.3665s
Epoch: 4 cost time: 97.87225317955017
Epoch: 4, Steps: 317 | Train Loss: 0.1737123 Vali Loss: 0.2250654 Test Loss: 0.2112702
Validation loss decreased (0.226282 --> 0.225065).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1497439
	speed: 0.3083s/iter; left time: 555.9283s
	iters: 200, epoch: 5 | loss: 0.1533240
	speed: 0.3022s/iter; left time: 514.5969s
	iters: 300, epoch: 5 | loss: 0.1745871
	speed: 0.3094s/iter; left time: 495.9697s
Epoch: 5 cost time: 97.50059413909912
Epoch: 5, Steps: 317 | Train Loss: 0.1683403 Vali Loss: 0.2277755 Test Loss: 0.2096807
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1759753
	speed: 0.2953s/iter; left time: 438.8359s
	iters: 200, epoch: 6 | loss: 0.1633234
	speed: 0.3010s/iter; left time: 417.1177s
	iters: 300, epoch: 6 | loss: 0.1438120
	speed: 0.2745s/iter; left time: 352.9839s
Epoch: 6 cost time: 91.37996292114258
Epoch: 6, Steps: 317 | Train Loss: 0.1663231 Vali Loss: 0.2266048 Test Loss: 0.2110069
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1545315
	speed: 0.3149s/iter; left time: 368.1611s
	iters: 200, epoch: 7 | loss: 0.1426873
	speed: 0.2794s/iter; left time: 298.6646s
	iters: 300, epoch: 7 | loss: 0.1700339
	speed: 0.2702s/iter; left time: 261.8265s
Epoch: 7 cost time: 91.1824197769165
Epoch: 7, Steps: 317 | Train Loss: 0.1648260 Vali Loss: 0.2278255 Test Loss: 0.2106741
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.46362408995628357, mae:0.5296732783317566, rmse:0.6808995008468628, mape:0.0189521387219429, mspe:0.0006014990503899753, rse:0.4759358763694763, r2_score:0.7675606862472261, acc:0.9810478612780571
corr: [36.84732  36.830185 36.633766 36.63034  36.6658   36.71798  36.79685
 36.77588  36.829624 36.719833 36.656418 36.716587 36.82202  36.790268
 36.85694  36.84596  36.93666  37.034725 37.23525  37.327747 37.171906
 37.075005 37.262398 37.138878 37.131298 37.022617 37.159122 37.01563
 36.93731  37.118504]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2069962
	speed: 0.2417s/iter; left time: 744.8199s
	iters: 200, epoch: 1 | loss: 0.1863398
	speed: 0.2397s/iter; left time: 714.5008s
	iters: 300, epoch: 1 | loss: 0.2432846
	speed: 0.2636s/iter; left time: 759.4851s
Epoch: 1 cost time: 79.81469750404358
Epoch: 1, Steps: 318 | Train Loss: 0.2738141 Vali Loss: 0.1768326 Test Loss: 0.1536679
Validation loss decreased (inf --> 0.176833).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1765041
	speed: 0.2847s/iter; left time: 786.5516s
	iters: 200, epoch: 2 | loss: 0.1275226
	speed: 0.2749s/iter; left time: 732.0826s
	iters: 300, epoch: 2 | loss: 0.1463669
	speed: 0.2739s/iter; left time: 702.0222s
Epoch: 2 cost time: 88.30506563186646
Epoch: 2, Steps: 318 | Train Loss: 0.1517137 Vali Loss: 0.1628136 Test Loss: 0.1412893
Validation loss decreased (0.176833 --> 0.162814).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1259502
	speed: 0.2851s/iter; left time: 697.1834s
	iters: 200, epoch: 3 | loss: 0.1596669
	speed: 0.2965s/iter; left time: 695.3303s
	iters: 300, epoch: 3 | loss: 0.1115599
	speed: 0.2854s/iter; left time: 640.6316s
Epoch: 3 cost time: 91.60852074623108
Epoch: 3, Steps: 318 | Train Loss: 0.1340049 Vali Loss: 0.1619098 Test Loss: 0.1425807
Validation loss decreased (0.162814 --> 0.161910).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1437894
	speed: 0.2910s/iter; left time: 618.9731s
	iters: 200, epoch: 4 | loss: 0.1273970
	speed: 0.2788s/iter; left time: 565.1195s
	iters: 300, epoch: 4 | loss: 0.1198824
	speed: 0.3012s/iter; left time: 580.5000s
Epoch: 4 cost time: 91.94912958145142
Epoch: 4, Steps: 318 | Train Loss: 0.1251633 Vali Loss: 0.1628502 Test Loss: 0.1438584
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1398797
	speed: 0.2918s/iter; left time: 527.7872s
	iters: 200, epoch: 5 | loss: 0.0979699
	speed: 0.2891s/iter; left time: 494.0996s
	iters: 300, epoch: 5 | loss: 0.1042656
	speed: 0.2801s/iter; left time: 450.6522s
Epoch: 5 cost time: 90.96918106079102
Epoch: 5, Steps: 318 | Train Loss: 0.1209076 Vali Loss: 0.1616804 Test Loss: 0.1429264
Validation loss decreased (0.161910 --> 0.161680).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1077597
	speed: 0.3075s/iter; left time: 458.5159s
	iters: 200, epoch: 6 | loss: 0.1170632
	speed: 0.2755s/iter; left time: 383.2252s
	iters: 300, epoch: 6 | loss: 0.1148192
	speed: 0.2903s/iter; left time: 374.8287s
Epoch: 6 cost time: 92.55310654640198
Epoch: 6, Steps: 318 | Train Loss: 0.1187391 Vali Loss: 0.1610791 Test Loss: 0.1420547
Validation loss decreased (0.161680 --> 0.161079).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1112093
	speed: 0.2930s/iter; left time: 343.6593s
	iters: 200, epoch: 7 | loss: 0.1478789
	speed: 0.2817s/iter; left time: 302.2941s
	iters: 300, epoch: 7 | loss: 0.1008546
	speed: 0.2800s/iter; left time: 272.4076s
Epoch: 7 cost time: 90.14350652694702
Epoch: 7, Steps: 318 | Train Loss: 0.1172270 Vali Loss: 0.1604520 Test Loss: 0.1424719
Validation loss decreased (0.161079 --> 0.160452).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1494531
	speed: 0.2823s/iter; left time: 241.3331s
	iters: 200, epoch: 8 | loss: 0.1145888
	speed: 0.2854s/iter; left time: 215.4834s
	iters: 300, epoch: 8 | loss: 0.0835168
	speed: 0.2862s/iter; left time: 187.4605s
Epoch: 8 cost time: 91.01686811447144
Epoch: 8, Steps: 318 | Train Loss: 0.1167816 Vali Loss: 0.1607197 Test Loss: 0.1420598
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1034776
	speed: 0.2645s/iter; left time: 142.0248s
	iters: 200, epoch: 9 | loss: 0.1453706
	speed: 0.2734s/iter; left time: 119.4900s
	iters: 300, epoch: 9 | loss: 0.1438822
	speed: 0.2383s/iter; left time: 80.3162s
Epoch: 9 cost time: 81.89119815826416
Epoch: 9, Steps: 318 | Train Loss: 0.1165230 Vali Loss: 0.1597626 Test Loss: 0.1419227
Validation loss decreased (0.160452 --> 0.159763).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1286329
	speed: 0.2690s/iter; left time: 58.9017s
	iters: 200, epoch: 10 | loss: 0.0926925
	speed: 0.2510s/iter; left time: 29.8676s
	iters: 300, epoch: 10 | loss: 0.1604131
	speed: 0.2411s/iter; left time: 4.5803s
Epoch: 10 cost time: 80.74714303016663
Epoch: 10, Steps: 318 | Train Loss: 0.1162260 Vali Loss: 0.1602329 Test Loss: 0.1418328
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.3114436864852905, mae:0.43180984258651733, rmse:0.5580713748931885, mape:0.015408997423946857, mspe:0.00039917154936119914, rse:0.3910330533981323, r2_score:0.8394395893423198, acc:0.9845910025760531
corr: [36.430347 36.433575 36.600475 36.422764 36.423626 36.468765 36.474346
 36.589294 36.547417 36.58207 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2967116
	speed: 0.3373s/iter; left time: 1039.1076s
	iters: 200, epoch: 1 | loss: 0.2209352
	speed: 0.3110s/iter; left time: 927.0867s
	iters: 300, epoch: 1 | loss: 0.2143731
	speed: 0.3038s/iter; left time: 875.2596s
Epoch: 1 cost time: 101.32150053977966
Epoch: 1, Steps: 318 | Train Loss: 0.2866385 Vali Loss: 0.1764797 Test Loss: 0.1641701
Validation loss decreased (inf --> 0.176480).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1874773
	speed: 0.3206s/iter; left time: 885.9213s
	iters: 200, epoch: 2 | loss: 0.1519979
	speed: 0.3011s/iter; left time: 801.8658s
	iters: 300, epoch: 2 | loss: 0.1318274
	speed: 0.3078s/iter; left time: 788.8537s
Epoch: 2 cost time: 98.51185083389282
Epoch: 2, Steps: 318 | Train Loss: 0.1683974 Vali Loss: 0.1716042 Test Loss: 0.1567781
Validation loss decreased (0.176480 --> 0.171604).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1135714
	speed: 0.3084s/iter; left time: 753.9398s
	iters: 200, epoch: 3 | loss: 0.1396898
	speed: 0.3061s/iter; left time: 717.7566s
	iters: 300, epoch: 3 | loss: 0.1217058
	speed: 0.3041s/iter; left time: 682.6657s
Epoch: 3 cost time: 97.45610737800598
Epoch: 3, Steps: 318 | Train Loss: 0.1530319 Vali Loss: 0.1732636 Test Loss: 0.1628205
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1345881
	speed: 0.3151s/iter; left time: 670.1898s
	iters: 200, epoch: 4 | loss: 0.1569735
	speed: 0.3188s/iter; left time: 646.2513s
	iters: 300, epoch: 4 | loss: 0.1661254
	speed: 0.3120s/iter; left time: 601.2851s
Epoch: 4 cost time: 100.17549681663513
Epoch: 4, Steps: 318 | Train Loss: 0.1446136 Vali Loss: 0.1749206 Test Loss: 0.1635549
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1206872
	speed: 0.3191s/iter; left time: 577.2959s
	iters: 200, epoch: 5 | loss: 0.1234119
	speed: 0.3012s/iter; left time: 514.7344s
	iters: 300, epoch: 5 | loss: 0.1405337
	speed: 0.3040s/iter; left time: 489.1092s
Epoch: 5 cost time: 97.97271871566772
Epoch: 5, Steps: 318 | Train Loss: 0.1411556 Vali Loss: 0.1797042 Test Loss: 0.1663542
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000031
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.34404343366622925, mae:0.451140820980072, rmse:0.5865521430969238, mape:0.01614517718553543, mspe:0.0004471748834475875, rse:0.4107956290245056, r2_score:0.8203766789542081, acc:0.9838548228144646
corr: [36.849094 36.641594 36.662254 36.702377 36.697952 36.743286 36.919537
 36.754673 36.785133 36.771297 36.88258  36.892036 37.411835 37.465607
 37.425526]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2729653
	speed: 0.3847s/iter; left time: 1185.1316s
	iters: 200, epoch: 1 | loss: 0.1826341
	speed: 0.3427s/iter; left time: 1021.7363s
	iters: 300, epoch: 1 | loss: 0.2158674
	speed: 0.3297s/iter; left time: 949.7822s
Epoch: 1 cost time: 112.55027294158936
Epoch: 1, Steps: 318 | Train Loss: 0.2875035 Vali Loss: 0.2017848 Test Loss: 0.2142006
Validation loss decreased (inf --> 0.201785).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1904514
	speed: 0.3508s/iter; left time: 969.3867s
	iters: 200, epoch: 2 | loss: 0.2289262
	speed: 0.3510s/iter; left time: 934.6574s
	iters: 300, epoch: 2 | loss: 0.1731056
	speed: 0.3320s/iter; left time: 851.0052s
Epoch: 2 cost time: 109.3095383644104
Epoch: 2, Steps: 318 | Train Loss: 0.1842540 Vali Loss: 0.1954072 Test Loss: 0.1830843
Validation loss decreased (0.201785 --> 0.195407).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1950775
	speed: 0.3176s/iter; left time: 776.4648s
	iters: 200, epoch: 3 | loss: 0.1353740
	speed: 0.3378s/iter; left time: 792.2494s
	iters: 300, epoch: 3 | loss: 0.1969535
	speed: 0.3366s/iter; left time: 755.7749s
Epoch: 3 cost time: 105.83826684951782
Epoch: 3, Steps: 318 | Train Loss: 0.1669431 Vali Loss: 0.1890549 Test Loss: 0.1822182
Validation loss decreased (0.195407 --> 0.189055).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1656381
	speed: 0.3045s/iter; left time: 647.6465s
	iters: 200, epoch: 4 | loss: 0.1301535
	speed: 0.3476s/iter; left time: 704.6664s
	iters: 300, epoch: 4 | loss: 0.1567743
	speed: 0.3516s/iter; left time: 677.5495s
Epoch: 4 cost time: 106.52550530433655
Epoch: 4, Steps: 318 | Train Loss: 0.1585436 Vali Loss: 0.1923307 Test Loss: 0.1813093
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1294233
	speed: 0.3478s/iter; left time: 629.1509s
	iters: 200, epoch: 5 | loss: 0.1541719
	speed: 0.3485s/iter; left time: 595.6370s
	iters: 300, epoch: 5 | loss: 0.1083716
	speed: 0.3586s/iter; left time: 576.9350s
Epoch: 5 cost time: 111.62750124931335
Epoch: 5, Steps: 318 | Train Loss: 0.1536886 Vali Loss: 0.1940970 Test Loss: 0.1802309
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1511772
	speed: 0.3592s/iter; left time: 535.6025s
	iters: 200, epoch: 6 | loss: 0.1758728
	speed: 0.3369s/iter; left time: 468.6456s
	iters: 300, epoch: 6 | loss: 0.1707953
	speed: 0.3293s/iter; left time: 425.0797s
Epoch: 6 cost time: 109.0884439945221
Epoch: 6, Steps: 318 | Train Loss: 0.1515754 Vali Loss: 0.1937967 Test Loss: 0.1806636
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.3998706340789795, mae:0.4903501868247986, rmse:0.6323532462120056, mape:0.017557205632328987, mspe:0.0005204500048421323, rse:0.4426162838935852, r2_score:0.794836849073536, acc:0.982442794367671
corr: [36.52094  36.426178 36.347412 36.29501  36.304535 36.376633 36.43236
 36.40163  36.471237 36.476265 36.417088 36.36345  36.602848 36.66602
 36.761913 36.950825 36.86557  37.021595 36.901527 37.10972 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2501461
	speed: 0.3244s/iter; left time: 999.4116s
	iters: 200, epoch: 1 | loss: 0.2376966
	speed: 0.3145s/iter; left time: 937.5151s
	iters: 300, epoch: 1 | loss: 0.1355271
	speed: 0.3193s/iter; left time: 919.9648s
Epoch: 1 cost time: 101.58904361724854
Epoch: 1, Steps: 318 | Train Loss: 0.2944112 Vali Loss: 0.2164027 Test Loss: 0.2077052
Validation loss decreased (inf --> 0.216403).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1796099
	speed: 0.3247s/iter; left time: 897.1491s
	iters: 200, epoch: 2 | loss: 0.1805566
	speed: 0.3179s/iter; left time: 846.5891s
	iters: 300, epoch: 2 | loss: 0.2313469
	speed: 0.3364s/iter; left time: 862.1298s
Epoch: 2 cost time: 103.69148755073547
Epoch: 2, Steps: 318 | Train Loss: 0.1941723 Vali Loss: 0.2065709 Test Loss: 0.1925441
Validation loss decreased (0.216403 --> 0.206571).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1832903
	speed: 0.3185s/iter; left time: 778.6216s
	iters: 200, epoch: 3 | loss: 0.1879187
	speed: 0.3236s/iter; left time: 758.7672s
	iters: 300, epoch: 3 | loss: 0.1721940
	speed: 0.3318s/iter; left time: 744.9093s
Epoch: 3 cost time: 102.91320037841797
Epoch: 3, Steps: 318 | Train Loss: 0.1786599 Vali Loss: 0.2086920 Test Loss: 0.1921132
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1579535
	speed: 0.3369s/iter; left time: 716.5279s
	iters: 200, epoch: 4 | loss: 0.1778781
	speed: 0.3151s/iter; left time: 638.7777s
	iters: 300, epoch: 4 | loss: 0.1688513
	speed: 0.3225s/iter; left time: 621.4911s
Epoch: 4 cost time: 102.823805809021
Epoch: 4, Steps: 318 | Train Loss: 0.1689173 Vali Loss: 0.2077997 Test Loss: 0.1911988
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1470427
	speed: 0.3269s/iter; left time: 591.4475s
	iters: 200, epoch: 5 | loss: 0.1716865
	speed: 0.3199s/iter; left time: 546.6413s
	iters: 300, epoch: 5 | loss: 0.1469424
	speed: 0.3010s/iter; left time: 484.3458s
Epoch: 5 cost time: 100.13561034202576
Epoch: 5, Steps: 318 | Train Loss: 0.1651671 Vali Loss: 0.2082647 Test Loss: 0.1901172
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000031
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.42253053188323975, mae:0.5026106238365173, rmse:0.6500234603881836, mape:0.01802932471036911, mspe:0.0005549898487515748, rse:0.4546806216239929, r2_score:0.7583266379019499, acc:0.9819706752896309
corr: [36.5076   36.28554  36.598644 36.57367  36.60144  36.770515 36.42483
 36.331657 36.580093 36.660896 36.676155 36.57689  36.888615 36.805252
 36.69268  36.893402 36.859627 36.787205 36.973404 37.232006 36.911297
 37.156998 37.136726 37.05617  37.284878]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3524749
	speed: 0.2487s/iter; left time: 763.6226s
	iters: 200, epoch: 1 | loss: 0.2750109
	speed: 0.2314s/iter; left time: 687.5461s
	iters: 300, epoch: 1 | loss: 0.2026535
	speed: 0.2624s/iter; left time: 753.4261s
Epoch: 1 cost time: 79.03380727767944
Epoch: 1, Steps: 317 | Train Loss: 0.3065388 Vali Loss: 0.2224371 Test Loss: 0.2185187
Validation loss decreased (inf --> 0.222437).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1937094
	speed: 0.2788s/iter; left time: 767.8298s
	iters: 200, epoch: 2 | loss: 0.1957845
	speed: 0.2779s/iter; left time: 737.4433s
	iters: 300, epoch: 2 | loss: 0.1665102
	speed: 0.2860s/iter; left time: 730.5397s
Epoch: 2 cost time: 89.1444320678711
Epoch: 2, Steps: 317 | Train Loss: 0.2006739 Vali Loss: 0.2115866 Test Loss: 0.2077391
Validation loss decreased (0.222437 --> 0.211587).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1788266
	speed: 0.2945s/iter; left time: 717.7730s
	iters: 200, epoch: 3 | loss: 0.2076858
	speed: 0.2824s/iter; left time: 660.0131s
	iters: 300, epoch: 3 | loss: 0.1811792
	speed: 0.2807s/iter; left time: 627.8558s
Epoch: 3 cost time: 90.62254405021667
Epoch: 3, Steps: 317 | Train Loss: 0.1851410 Vali Loss: 0.2089950 Test Loss: 0.2036957
Validation loss decreased (0.211587 --> 0.208995).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1413266
	speed: 0.2760s/iter; left time: 585.1259s
	iters: 200, epoch: 4 | loss: 0.1848536
	speed: 0.2798s/iter; left time: 565.1238s
	iters: 300, epoch: 4 | loss: 0.1461123
	speed: 0.2769s/iter; left time: 531.6513s
Epoch: 4 cost time: 88.11578345298767
Epoch: 4, Steps: 317 | Train Loss: 0.1763475 Vali Loss: 0.2072852 Test Loss: 0.2015347
Validation loss decreased (0.208995 --> 0.207285).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1493148
	speed: 0.2752s/iter; left time: 496.2295s
	iters: 200, epoch: 5 | loss: 0.1625462
	speed: 0.2751s/iter; left time: 468.5569s
	iters: 300, epoch: 5 | loss: 0.1622640
	speed: 0.2724s/iter; left time: 436.6169s
Epoch: 5 cost time: 87.11239576339722
Epoch: 5, Steps: 317 | Train Loss: 0.1713081 Vali Loss: 0.2079033 Test Loss: 0.2005816
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1708797
	speed: 0.2898s/iter; left time: 430.6869s
	iters: 200, epoch: 6 | loss: 0.1674374
	speed: 0.2674s/iter; left time: 370.5790s
	iters: 300, epoch: 6 | loss: 0.1632878
	speed: 0.2737s/iter; left time: 351.9652s
Epoch: 6 cost time: 88.14294099807739
Epoch: 6, Steps: 317 | Train Loss: 0.1687628 Vali Loss: 0.2088113 Test Loss: 0.2013472
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1512999
	speed: 0.2847s/iter; left time: 332.8106s
	iters: 200, epoch: 7 | loss: 0.1376860
	speed: 0.2780s/iter; left time: 297.1345s
	iters: 300, epoch: 7 | loss: 0.1792027
	speed: 0.2573s/iter; left time: 249.3348s
Epoch: 7 cost time: 87.0385673046112
Epoch: 7, Steps: 317 | Train Loss: 0.1673196 Vali Loss: 0.2092347 Test Loss: 0.2016686
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.4422599673271179, mae:0.517813503742218, rmse:0.6650263071060181, mape:0.018555957823991776, mspe:0.0005783612141385674, rse:0.4648407995700836, r2_score:0.7656117422902439, acc:0.9814440421760082
corr: [36.449535 36.349285 36.34712  36.28743  36.44885  36.519234 36.36191
 36.330288 36.476025 36.41062  36.43374  36.53096  36.582752 36.603985
 36.509502 36.535587 36.588486 36.572712 36.63075  36.846188 36.714718
 36.661716 36.84153  36.765488 36.879787 37.013836 37.06095  36.962727
 36.853657 36.983418]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4494213
	speed: 0.1365s/iter; left time: 420.5272s
	iters: 200, epoch: 1 | loss: 0.3600795
	speed: 0.1010s/iter; left time: 301.0466s
	iters: 300, epoch: 1 | loss: 0.1993048
	speed: 0.1122s/iter; left time: 323.3407s
Epoch: 1 cost time: 36.552478551864624
Epoch: 1, Steps: 318 | Train Loss: 0.4212041 Vali Loss: 0.2337397 Test Loss: 0.3365071
Validation loss decreased (inf --> 0.233740).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2345447
	speed: 0.1182s/iter; left time: 326.4588s
	iters: 200, epoch: 2 | loss: 0.1775699
	speed: 0.1072s/iter; left time: 285.3579s
	iters: 300, epoch: 2 | loss: 0.2539807
	speed: 0.1138s/iter; left time: 291.7521s
Epoch: 2 cost time: 35.72133135795593
Epoch: 2, Steps: 318 | Train Loss: 0.2094089 Vali Loss: 0.2160364 Test Loss: 0.3096932
Validation loss decreased (0.233740 --> 0.216036).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1426097
	speed: 0.1156s/iter; left time: 282.5342s
	iters: 200, epoch: 3 | loss: 0.1795645
	speed: 0.1187s/iter; left time: 278.2660s
	iters: 300, epoch: 3 | loss: 0.2006883
	speed: 0.1107s/iter; left time: 248.5272s
Epoch: 3 cost time: 36.43342137336731
Epoch: 3, Steps: 318 | Train Loss: 0.1913341 Vali Loss: 0.2104716 Test Loss: 0.3032604
Validation loss decreased (0.216036 --> 0.210472).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1382457
	speed: 0.1279s/iter; left time: 271.9860s
	iters: 200, epoch: 4 | loss: 0.1811581
	speed: 0.1233s/iter; left time: 249.9616s
	iters: 300, epoch: 4 | loss: 0.2080529
	speed: 0.1102s/iter; left time: 212.4311s
Epoch: 4 cost time: 38.05630707740784
Epoch: 4, Steps: 318 | Train Loss: 0.1855908 Vali Loss: 0.2056588 Test Loss: 0.3030007
Validation loss decreased (0.210472 --> 0.205659).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1825165
	speed: 0.1137s/iter; left time: 205.6427s
	iters: 200, epoch: 5 | loss: 0.1579610
	speed: 0.1153s/iter; left time: 197.0914s
	iters: 300, epoch: 5 | loss: 0.2597761
	speed: 0.1066s/iter; left time: 171.4840s
Epoch: 5 cost time: 35.653868436813354
Epoch: 5, Steps: 318 | Train Loss: 0.1840314 Vali Loss: 0.2040149 Test Loss: 0.2995379
Validation loss decreased (0.205659 --> 0.204015).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1774137
	speed: 0.1318s/iter; left time: 196.5352s
	iters: 200, epoch: 6 | loss: 0.1602417
	speed: 0.1154s/iter; left time: 160.5677s
	iters: 300, epoch: 6 | loss: 0.1716134
	speed: 0.1037s/iter; left time: 133.8423s
Epoch: 6 cost time: 37.041608810424805
Epoch: 6, Steps: 318 | Train Loss: 0.1821023 Vali Loss: 0.2048262 Test Loss: 0.2987091
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1964158
	speed: 0.1069s/iter; left time: 125.3940s
	iters: 200, epoch: 7 | loss: 0.1623718
	speed: 0.1187s/iter; left time: 127.3378s
	iters: 300, epoch: 7 | loss: 0.1958787
	speed: 0.1080s/iter; left time: 105.1081s
Epoch: 7 cost time: 35.53056836128235
Epoch: 7, Steps: 318 | Train Loss: 0.1815775 Vali Loss: 0.2042053 Test Loss: 0.2992699
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1372025
	speed: 0.1256s/iter; left time: 107.4132s
	iters: 200, epoch: 8 | loss: 0.2032595
	speed: 0.1057s/iter; left time: 79.8171s
	iters: 300, epoch: 8 | loss: 0.1486337
	speed: 0.1032s/iter; left time: 67.6083s
Epoch: 8 cost time: 35.71375250816345
Epoch: 8, Steps: 318 | Train Loss: 0.1815502 Vali Loss: 0.2037737 Test Loss: 0.2995236
Validation loss decreased (0.204015 --> 0.203774).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1789177
	speed: 0.1129s/iter; left time: 60.6035s
	iters: 200, epoch: 9 | loss: 0.1781071
	speed: 0.1241s/iter; left time: 54.2224s
	iters: 300, epoch: 9 | loss: 0.1922731
	speed: 0.1110s/iter; left time: 37.4159s
Epoch: 9 cost time: 36.95699667930603
Epoch: 9, Steps: 318 | Train Loss: 0.1812316 Vali Loss: 0.2035785 Test Loss: 0.2993882
Validation loss decreased (0.203774 --> 0.203579).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1774420
	speed: 0.1217s/iter; left time: 26.6480s
	iters: 200, epoch: 10 | loss: 0.1954287
	speed: 0.1069s/iter; left time: 12.7235s
	iters: 300, epoch: 10 | loss: 0.1457391
	speed: 0.1080s/iter; left time: 2.0524s
Epoch: 10 cost time: 35.83039975166321
Epoch: 10, Steps: 318 | Train Loss: 0.1822833 Vali Loss: 0.2038680 Test Loss: 0.2995127
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.6569957137107849, mae:0.6355547308921814, rmse:0.810552716255188, mape:0.02275998890399933, mspe:0.0008516045636497438, rse:0.567943274974823, r2_score:0.599324218069176, acc:0.9772400110960007
corr: [37.183125 37.1454   37.461773 37.78138  37.795597 37.552967 37.92773
 37.78577  37.948757 37.787907]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3329294
	speed: 0.1891s/iter; left time: 582.5132s
	iters: 200, epoch: 1 | loss: 0.2672427
	speed: 0.1775s/iter; left time: 529.0389s
	iters: 300, epoch: 1 | loss: 0.2319051
	speed: 0.1876s/iter; left time: 540.5626s
Epoch: 1 cost time: 58.93718099594116
Epoch: 1, Steps: 318 | Train Loss: 0.4246416 Vali Loss: 0.2255762 Test Loss: 0.2292245
Validation loss decreased (inf --> 0.225576).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2524118
	speed: 0.1874s/iter; left time: 517.6578s
	iters: 200, epoch: 2 | loss: 0.1788862
	speed: 0.1903s/iter; left time: 506.6845s
	iters: 300, epoch: 2 | loss: 0.1799275
	speed: 0.1662s/iter; left time: 425.8819s
Epoch: 2 cost time: 57.82339286804199
Epoch: 2, Steps: 318 | Train Loss: 0.2240682 Vali Loss: 0.2063458 Test Loss: 0.2042456
Validation loss decreased (0.225576 --> 0.206346).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1720771
	speed: 0.1843s/iter; left time: 450.5167s
	iters: 200, epoch: 3 | loss: 0.2296400
	speed: 0.2023s/iter; left time: 474.3044s
	iters: 300, epoch: 3 | loss: 0.1557397
	speed: 0.1949s/iter; left time: 437.4528s
Epoch: 3 cost time: 61.63457918167114
Epoch: 3, Steps: 318 | Train Loss: 0.2044696 Vali Loss: 0.2006074 Test Loss: 0.2014755
Validation loss decreased (0.206346 --> 0.200607).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1941575
	speed: 0.1945s/iter; left time: 413.7434s
	iters: 200, epoch: 4 | loss: 0.1711985
	speed: 0.1699s/iter; left time: 344.3059s
	iters: 300, epoch: 4 | loss: 0.1862016
	speed: 0.1839s/iter; left time: 354.3889s
Epoch: 4 cost time: 58.591763973236084
Epoch: 4, Steps: 318 | Train Loss: 0.1987574 Vali Loss: 0.1964258 Test Loss: 0.2007239
Validation loss decreased (0.200607 --> 0.196426).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1959241
	speed: 0.1994s/iter; left time: 360.6936s
	iters: 200, epoch: 5 | loss: 0.1840320
	speed: 0.2017s/iter; left time: 344.7686s
	iters: 300, epoch: 5 | loss: 0.2949755
	speed: 0.1744s/iter; left time: 280.6122s
Epoch: 5 cost time: 60.59394836425781
Epoch: 5, Steps: 318 | Train Loss: 0.1959195 Vali Loss: 0.1938837 Test Loss: 0.2016049
Validation loss decreased (0.196426 --> 0.193884).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2310162
	speed: 0.1753s/iter; left time: 261.3346s
	iters: 200, epoch: 6 | loss: 0.1871691
	speed: 0.1996s/iter; left time: 277.5840s
	iters: 300, epoch: 6 | loss: 0.2709818
	speed: 0.1861s/iter; left time: 240.2901s
Epoch: 6 cost time: 59.360559940338135
Epoch: 6, Steps: 318 | Train Loss: 0.1940817 Vali Loss: 0.1941852 Test Loss: 0.2016219
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2145148
	speed: 0.1892s/iter; left time: 221.9235s
	iters: 200, epoch: 7 | loss: 0.1641919
	speed: 0.1829s/iter; left time: 196.2317s
	iters: 300, epoch: 7 | loss: 0.2117418
	speed: 0.1956s/iter; left time: 190.3376s
Epoch: 7 cost time: 60.23604106903076
Epoch: 7, Steps: 318 | Train Loss: 0.1935916 Vali Loss: 0.1934839 Test Loss: 0.2012261
Validation loss decreased (0.193884 --> 0.193484).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1426257
	speed: 0.1946s/iter; left time: 166.3481s
	iters: 200, epoch: 8 | loss: 0.2623341
	speed: 0.1900s/iter; left time: 143.4853s
	iters: 300, epoch: 8 | loss: 0.2367907
	speed: 0.1895s/iter; left time: 124.1136s
Epoch: 8 cost time: 60.20111131668091
Epoch: 8, Steps: 318 | Train Loss: 0.1926755 Vali Loss: 0.1929210 Test Loss: 0.2009822
Validation loss decreased (0.193484 --> 0.192921).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1694617
	speed: 0.1910s/iter; left time: 102.5741s
	iters: 200, epoch: 9 | loss: 0.1668012
	speed: 0.1717s/iter; left time: 75.0287s
	iters: 300, epoch: 9 | loss: 0.2000118
	speed: 0.1811s/iter; left time: 61.0455s
Epoch: 9 cost time: 58.05268859863281
Epoch: 9, Steps: 318 | Train Loss: 0.1925594 Vali Loss: 0.1932696 Test Loss: 0.2008792
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2208474
	speed: 0.1978s/iter; left time: 43.3128s
	iters: 200, epoch: 10 | loss: 0.2294956
	speed: 0.2099s/iter; left time: 24.9789s
	iters: 300, epoch: 10 | loss: 0.1864653
	speed: 0.2257s/iter; left time: 4.2880s
Epoch: 10 cost time: 66.89945697784424
Epoch: 10, Steps: 318 | Train Loss: 0.1927472 Vali Loss: 0.1928695 Test Loss: 0.2008547
Validation loss decreased (0.192921 --> 0.192869).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.44076773524284363, mae:0.5110644102096558, rmse:0.6639034152030945, mape:0.018336063250899315, mspe:0.0005814178148284554, rse:0.46496909856796265, r2_score:0.7420914727881117, acc:0.9816639367491007
corr: [36.728374 36.723995 36.75795  36.691994 36.56306  36.5338   36.577835
 36.74213  36.87965  36.912308 36.96838  37.18736  37.374485 37.10735
 37.069942]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3983815
	speed: 0.1364s/iter; left time: 420.3504s
	iters: 200, epoch: 1 | loss: 0.2551329
	speed: 0.0976s/iter; left time: 290.9542s
	iters: 300, epoch: 1 | loss: 0.2474715
	speed: 0.1066s/iter; left time: 307.2457s
Epoch: 1 cost time: 35.85753774642944
Epoch: 1, Steps: 318 | Train Loss: 0.4434663 Vali Loss: 0.2323287 Test Loss: 0.2844337
Validation loss decreased (inf --> 0.232329).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2132458
	speed: 0.1091s/iter; left time: 301.5245s
	iters: 200, epoch: 2 | loss: 0.1914427
	speed: 0.1040s/iter; left time: 277.0454s
	iters: 300, epoch: 2 | loss: 0.1363214
	speed: 0.1153s/iter; left time: 295.6181s
Epoch: 2 cost time: 34.73235750198364
Epoch: 2, Steps: 318 | Train Loss: 0.2203051 Vali Loss: 0.2301031 Test Loss: 0.2612955
Validation loss decreased (0.232329 --> 0.230103).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1786554
	speed: 0.1140s/iter; left time: 278.6905s
	iters: 200, epoch: 3 | loss: 0.1724945
	speed: 0.1042s/iter; left time: 244.3474s
	iters: 300, epoch: 3 | loss: 0.1950948
	speed: 0.1046s/iter; left time: 234.7741s
Epoch: 3 cost time: 34.274277687072754
Epoch: 3, Steps: 318 | Train Loss: 0.2053905 Vali Loss: 0.2237419 Test Loss: 0.2526263
Validation loss decreased (0.230103 --> 0.223742).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2046660
	speed: 0.1172s/iter; left time: 249.2483s
	iters: 200, epoch: 4 | loss: 0.1419775
	speed: 0.1031s/iter; left time: 209.0724s
	iters: 300, epoch: 4 | loss: 0.2346336
	speed: 0.1097s/iter; left time: 211.2970s
Epoch: 4 cost time: 35.345585107803345
Epoch: 4, Steps: 318 | Train Loss: 0.1996016 Vali Loss: 0.2196505 Test Loss: 0.2457835
Validation loss decreased (0.223742 --> 0.219650).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1886113
	speed: 0.1113s/iter; left time: 201.2954s
	iters: 200, epoch: 5 | loss: 0.1912288
	speed: 0.1126s/iter; left time: 192.3496s
	iters: 300, epoch: 5 | loss: 0.2096065
	speed: 0.1142s/iter; left time: 183.7297s
Epoch: 5 cost time: 36.001986026763916
Epoch: 5, Steps: 318 | Train Loss: 0.1964921 Vali Loss: 0.2158054 Test Loss: 0.2433053
Validation loss decreased (0.219650 --> 0.215805).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2114090
	speed: 0.1190s/iter; left time: 177.4724s
	iters: 200, epoch: 6 | loss: 0.2664162
	speed: 0.1033s/iter; left time: 143.6819s
	iters: 300, epoch: 6 | loss: 0.2147083
	speed: 0.0996s/iter; left time: 128.5917s
Epoch: 6 cost time: 34.36547613143921
Epoch: 6, Steps: 318 | Train Loss: 0.1955992 Vali Loss: 0.2167938 Test Loss: 0.2406377
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2103646
	speed: 0.1160s/iter; left time: 136.0899s
	iters: 200, epoch: 7 | loss: 0.1862810
	speed: 0.1011s/iter; left time: 108.4306s
	iters: 300, epoch: 7 | loss: 0.1830866
	speed: 0.1105s/iter; left time: 107.4990s
Epoch: 7 cost time: 34.80053734779358
Epoch: 7, Steps: 318 | Train Loss: 0.1941284 Vali Loss: 0.2163795 Test Loss: 0.2403651
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1895129
	speed: 0.1089s/iter; left time: 93.1107s
	iters: 200, epoch: 8 | loss: 0.1636025
	speed: 0.1150s/iter; left time: 86.8515s
	iters: 300, epoch: 8 | loss: 0.2174842
	speed: 0.1221s/iter; left time: 79.9589s
Epoch: 8 cost time: 36.450785875320435
Epoch: 8, Steps: 318 | Train Loss: 0.1940378 Vali Loss: 0.2174463 Test Loss: 0.2399835
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.5339239835739136, mae:0.5582138895988464, rmse:0.7307010293006897, mape:0.019921397790312767, mspe:0.0006853231461718678, rse:0.5114548802375793, r2_score:0.6961808882668478, acc:0.9800786022096872
corr: [36.43845  36.38842  36.610207 36.349796 36.177536 36.297615 36.288986
 36.331566 36.195942 36.35036  36.297844 36.562706 36.078434 36.148815
 36.535645 36.3958   36.367348 36.122135 36.53696  37.39353 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3373197
	speed: 0.1316s/iter; left time: 405.4445s
	iters: 200, epoch: 1 | loss: 0.2848046
	speed: 0.0959s/iter; left time: 285.9425s
	iters: 300, epoch: 1 | loss: 0.2553425
	speed: 0.1073s/iter; left time: 309.2497s
Epoch: 1 cost time: 35.75895929336548
Epoch: 1, Steps: 318 | Train Loss: 0.4281768 Vali Loss: 0.2462649 Test Loss: 0.2593185
Validation loss decreased (inf --> 0.246265).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2351851
	speed: 0.1178s/iter; left time: 325.5872s
	iters: 200, epoch: 2 | loss: 0.2706507
	speed: 0.1108s/iter; left time: 294.9894s
	iters: 300, epoch: 2 | loss: 0.1930199
	speed: 0.1162s/iter; left time: 297.7307s
Epoch: 2 cost time: 36.623085021972656
Epoch: 2, Steps: 318 | Train Loss: 0.2429241 Vali Loss: 0.2434074 Test Loss: 0.2482502
Validation loss decreased (0.246265 --> 0.243407).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2431046
	speed: 0.1168s/iter; left time: 285.5613s
	iters: 200, epoch: 3 | loss: 0.1898064
	speed: 0.1033s/iter; left time: 242.3442s
	iters: 300, epoch: 3 | loss: 0.2853511
	speed: 0.1057s/iter; left time: 237.2513s
Epoch: 3 cost time: 34.671175479888916
Epoch: 3, Steps: 318 | Train Loss: 0.2265392 Vali Loss: 0.2354291 Test Loss: 0.2463346
Validation loss decreased (0.243407 --> 0.235429).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1968994
	speed: 0.1162s/iter; left time: 247.1173s
	iters: 200, epoch: 4 | loss: 0.2097329
	speed: 0.1203s/iter; left time: 243.7909s
	iters: 300, epoch: 4 | loss: 0.2213006
	speed: 0.1090s/iter; left time: 210.0595s
Epoch: 4 cost time: 36.582260847091675
Epoch: 4, Steps: 318 | Train Loss: 0.2205334 Vali Loss: 0.2361310 Test Loss: 0.2451966
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1995552
	speed: 0.1240s/iter; left time: 224.2808s
	iters: 200, epoch: 5 | loss: 0.2516887
	speed: 0.0983s/iter; left time: 167.9566s
	iters: 300, epoch: 5 | loss: 0.2117935
	speed: 0.0995s/iter; left time: 160.1264s
Epoch: 5 cost time: 34.23315453529358
Epoch: 5, Steps: 318 | Train Loss: 0.2174102 Vali Loss: 0.2347817 Test Loss: 0.2441127
Validation loss decreased (0.235429 --> 0.234782).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1832130
	speed: 0.1197s/iter; left time: 178.4307s
	iters: 200, epoch: 6 | loss: 0.1921394
	speed: 0.1006s/iter; left time: 139.9691s
	iters: 300, epoch: 6 | loss: 0.1978295
	speed: 0.1099s/iter; left time: 141.9263s
Epoch: 6 cost time: 35.23480939865112
Epoch: 6, Steps: 318 | Train Loss: 0.2168748 Vali Loss: 0.2353888 Test Loss: 0.2425015
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2028154
	speed: 0.1135s/iter; left time: 133.0771s
	iters: 200, epoch: 7 | loss: 0.2624112
	speed: 0.1062s/iter; left time: 113.9214s
	iters: 300, epoch: 7 | loss: 0.1597857
	speed: 0.1094s/iter; left time: 106.4592s
Epoch: 7 cost time: 34.989928245544434
Epoch: 7, Steps: 318 | Train Loss: 0.2157689 Vali Loss: 0.2359010 Test Loss: 0.2438390
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2106408
	speed: 0.1169s/iter; left time: 99.9913s
	iters: 200, epoch: 8 | loss: 0.1720728
	speed: 0.1093s/iter; left time: 82.5024s
	iters: 300, epoch: 8 | loss: 0.2018718
	speed: 0.1115s/iter; left time: 73.0214s
Epoch: 8 cost time: 35.68301820755005
Epoch: 8, Steps: 318 | Train Loss: 0.2152551 Vali Loss: 0.2354075 Test Loss: 0.2436156
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.5356959104537964, mae:0.573643147945404, rmse:0.7319124937057495, mape:0.020521609112620354, mspe:0.0006982668419368565, rse:0.5119606256484985, r2_score:0.6824409768147923, acc:0.9794783908873796
corr: [36.728504 36.99537  36.876556 36.66805  36.713684 36.57382  36.476612
 36.606735 36.337166 36.57811  36.56238  36.668034 36.814983 36.71816
 36.779522 36.713993 36.642338 36.534695 36.43507  36.712723 36.636574
 36.814762 36.732918 36.633884 36.90432 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4431003
	speed: 0.1404s/iter; left time: 431.1906s
	iters: 200, epoch: 1 | loss: 0.3044757
	speed: 0.1132s/iter; left time: 336.3990s
	iters: 300, epoch: 1 | loss: 0.2925141
	speed: 0.1144s/iter; left time: 328.5093s
Epoch: 1 cost time: 38.31303572654724
Epoch: 1, Steps: 317 | Train Loss: 0.4890218 Vali Loss: 0.2637887 Test Loss: 0.2993378
Validation loss decreased (inf --> 0.263789).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2196456
	speed: 0.1155s/iter; left time: 318.1501s
	iters: 200, epoch: 2 | loss: 0.2394147
	speed: 0.1027s/iter; left time: 272.6390s
	iters: 300, epoch: 2 | loss: 0.1726094
	speed: 0.1129s/iter; left time: 288.2945s
Epoch: 2 cost time: 35.02610421180725
Epoch: 2, Steps: 317 | Train Loss: 0.2795578 Vali Loss: 0.2456995 Test Loss: 0.2741070
Validation loss decreased (0.263789 --> 0.245699).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2483697
	speed: 0.1148s/iter; left time: 279.8605s
	iters: 200, epoch: 3 | loss: 0.2216470
	speed: 0.1146s/iter; left time: 267.8639s
	iters: 300, epoch: 3 | loss: 0.2166372
	speed: 0.1184s/iter; left time: 264.8283s
Epoch: 3 cost time: 36.741989850997925
Epoch: 3, Steps: 317 | Train Loss: 0.2586078 Vali Loss: 0.2265293 Test Loss: 0.2675446
Validation loss decreased (0.245699 --> 0.226529).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1920070
	speed: 0.1135s/iter; left time: 240.6154s
	iters: 200, epoch: 4 | loss: 0.3388084
	speed: 0.1062s/iter; left time: 214.5785s
	iters: 300, epoch: 4 | loss: 0.2200254
	speed: 0.1170s/iter; left time: 224.5634s
Epoch: 4 cost time: 35.67540740966797
Epoch: 4, Steps: 317 | Train Loss: 0.2438446 Vali Loss: 0.2262398 Test Loss: 0.2657355
Validation loss decreased (0.226529 --> 0.226240).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2394032
	speed: 0.1304s/iter; left time: 235.1708s
	iters: 200, epoch: 5 | loss: 0.2282296
	speed: 0.1306s/iter; left time: 222.3602s
	iters: 300, epoch: 5 | loss: 0.2127703
	speed: 0.1142s/iter; left time: 183.0743s
Epoch: 5 cost time: 39.376863956451416
Epoch: 5, Steps: 317 | Train Loss: 0.2395350 Vali Loss: 0.2275499 Test Loss: 0.2639056
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.3068661
	speed: 0.1199s/iter; left time: 178.1193s
	iters: 200, epoch: 6 | loss: 0.3028117
	speed: 0.1098s/iter; left time: 152.2099s
	iters: 300, epoch: 6 | loss: 0.1916807
	speed: 0.1350s/iter; left time: 173.5467s
Epoch: 6 cost time: 38.5924072265625
Epoch: 6, Steps: 317 | Train Loss: 0.2376222 Vali Loss: 0.2215213 Test Loss: 0.2623685
Validation loss decreased (0.226240 --> 0.221521).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2120478
	speed: 0.1298s/iter; left time: 151.7679s
	iters: 200, epoch: 7 | loss: 0.2148651
	speed: 0.1129s/iter; left time: 120.7127s
	iters: 300, epoch: 7 | loss: 0.2176665
	speed: 0.1270s/iter; left time: 123.1029s
Epoch: 7 cost time: 39.20598864555359
Epoch: 7, Steps: 317 | Train Loss: 0.2350176 Vali Loss: 0.2273084 Test Loss: 0.2617418
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2421973
	speed: 0.1431s/iter; left time: 121.8853s
	iters: 200, epoch: 8 | loss: 0.2037040
	speed: 0.1329s/iter; left time: 99.9326s
	iters: 300, epoch: 8 | loss: 0.2259383
	speed: 0.1126s/iter; left time: 73.4348s
Epoch: 8 cost time: 40.730289936065674
Epoch: 8, Steps: 317 | Train Loss: 0.2363158 Vali Loss: 0.2251356 Test Loss: 0.2615917
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2110614
	speed: 0.1378s/iter; left time: 73.7005s
	iters: 200, epoch: 9 | loss: 0.2336535
	speed: 0.1303s/iter; left time: 56.6934s
	iters: 300, epoch: 9 | loss: 0.2329134
	speed: 0.1390s/iter; left time: 46.5580s
Epoch: 9 cost time: 43.30020833015442
Epoch: 9, Steps: 317 | Train Loss: 0.2348088 Vali Loss: 0.2263024 Test Loss: 0.2616279
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.5757574439048767, mae:0.5991644859313965, rmse:0.7587867975234985, mape:0.02143796719610691, mspe:0.000749903847463429, rse:0.5303776860237122, r2_score:0.6656934419796622, acc:0.9785620328038931
corr: [36.739975 36.628994 36.643974 36.844734 36.88608  36.677746 36.581898
 36.900112 36.889942 36.940666 37.16434  37.16773  37.4278   37.4999
 37.306885 37.739025 37.478733 37.4853   37.759567 37.676292 37.74393
 37.852577 37.986126 37.99611  38.072643 38.33024  38.257515 38.20266
 38.4246   38.365456]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4270666
	speed: 0.1839s/iter; left time: 566.6522s
	iters: 200, epoch: 1 | loss: 0.3353091
	speed: 0.1913s/iter; left time: 570.3652s
	iters: 300, epoch: 1 | loss: 0.2113409
	speed: 0.1494s/iter; left time: 430.4487s
Epoch: 1 cost time: 55.61544585227966
Epoch: 1, Steps: 318 | Train Loss: 0.4212427 Vali Loss: 0.2163665 Test Loss: 0.2164176
Validation loss decreased (inf --> 0.216367).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2526215
	speed: 0.1596s/iter; left time: 440.8844s
	iters: 200, epoch: 2 | loss: 0.1811277
	speed: 0.1558s/iter; left time: 414.9961s
	iters: 300, epoch: 2 | loss: 0.2812036
	speed: 0.1735s/iter; left time: 444.7250s
Epoch: 2 cost time: 52.26495003700256
Epoch: 2, Steps: 318 | Train Loss: 0.2204331 Vali Loss: 0.2029596 Test Loss: 0.1880167
Validation loss decreased (0.216367 --> 0.202960).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1447289
	speed: 0.1690s/iter; left time: 413.1238s
	iters: 200, epoch: 3 | loss: 0.1942341
	speed: 0.1492s/iter; left time: 349.8724s
	iters: 300, epoch: 3 | loss: 0.1953728
	speed: 0.1623s/iter; left time: 364.4097s
Epoch: 3 cost time: 50.406426429748535
Epoch: 3, Steps: 318 | Train Loss: 0.2006360 Vali Loss: 0.1951406 Test Loss: 0.1796634
Validation loss decreased (0.202960 --> 0.195141).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1426353
	speed: 0.1940s/iter; left time: 412.7111s
	iters: 200, epoch: 4 | loss: 0.1814217
	speed: 0.1819s/iter; left time: 368.7954s
	iters: 300, epoch: 4 | loss: 0.2172034
	speed: 0.1871s/iter; left time: 360.5169s
Epoch: 4 cost time: 60.16028881072998
Epoch: 4, Steps: 318 | Train Loss: 0.1930601 Vali Loss: 0.1952542 Test Loss: 0.1784528
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1972441
	speed: 0.1775s/iter; left time: 321.1292s
	iters: 200, epoch: 5 | loss: 0.1695218
	speed: 0.1840s/iter; left time: 314.4298s
	iters: 300, epoch: 5 | loss: 0.2835656
	speed: 0.1902s/iter; left time: 306.0565s
Epoch: 5 cost time: 58.46055555343628
Epoch: 5, Steps: 318 | Train Loss: 0.1910596 Vali Loss: 0.1915434 Test Loss: 0.1754443
Validation loss decreased (0.195141 --> 0.191543).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1981127
	speed: 0.1999s/iter; left time: 298.1015s
	iters: 200, epoch: 6 | loss: 0.1725184
	speed: 0.1870s/iter; left time: 260.0935s
	iters: 300, epoch: 6 | loss: 0.2011015
	speed: 0.1893s/iter; left time: 244.3229s
Epoch: 6 cost time: 61.11880159378052
Epoch: 6, Steps: 318 | Train Loss: 0.1892056 Vali Loss: 0.1915777 Test Loss: 0.1748331
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1728222
	speed: 0.1913s/iter; left time: 224.3430s
	iters: 200, epoch: 7 | loss: 0.1602790
	speed: 0.1905s/iter; left time: 204.4599s
	iters: 300, epoch: 7 | loss: 0.1858528
	speed: 0.1917s/iter; left time: 186.5117s
Epoch: 7 cost time: 60.83315086364746
Epoch: 7, Steps: 318 | Train Loss: 0.1882049 Vali Loss: 0.1915663 Test Loss: 0.1744919
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1410439
	speed: 0.2370s/iter; left time: 202.6242s
	iters: 200, epoch: 8 | loss: 0.2269460
	speed: 0.2156s/iter; left time: 162.7582s
	iters: 300, epoch: 8 | loss: 0.1508672
	speed: 0.2165s/iter; left time: 141.8151s
Epoch: 8 cost time: 70.75366258621216
Epoch: 8, Steps: 318 | Train Loss: 0.1880431 Vali Loss: 0.1906396 Test Loss: 0.1742112
Validation loss decreased (0.191543 --> 0.190640).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1914442
	speed: 0.2256s/iter; left time: 121.1293s
	iters: 200, epoch: 9 | loss: 0.1896460
	speed: 0.2213s/iter; left time: 96.7127s
	iters: 300, epoch: 9 | loss: 0.1874528
	speed: 0.2310s/iter; left time: 77.8547s
Epoch: 9 cost time: 71.65170168876648
Epoch: 9, Steps: 318 | Train Loss: 0.1879134 Vali Loss: 0.1923574 Test Loss: 0.1740806
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1774150
	speed: 0.2294s/iter; left time: 50.2489s
	iters: 200, epoch: 10 | loss: 0.1852423
	speed: 0.2154s/iter; left time: 25.6344s
	iters: 300, epoch: 10 | loss: 0.1579559
	speed: 0.2028s/iter; left time: 3.8534s
Epoch: 10 cost time: 68.61496758460999
Epoch: 10, Steps: 318 | Train Loss: 0.1881434 Vali Loss: 0.1908212 Test Loss: 0.1739973
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.3822996914386749, mae:0.48225194215774536, rmse:0.6183038949966431, mape:0.017287958413362503, mspe:0.0005000078235752881, rse:0.43323713541030884, r2_score:0.7939951152491564, acc:0.9827120415866375
corr: [37.53225  37.577217 37.41188  37.518185 37.48602  37.431168 37.543545
 37.556995 37.50031  37.658722]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3107414
	speed: 0.1576s/iter; left time: 485.6158s
	iters: 200, epoch: 1 | loss: 0.2456760
	speed: 0.1440s/iter; left time: 429.1845s
	iters: 300, epoch: 1 | loss: 0.2419059
	speed: 0.1435s/iter; left time: 413.5260s
Epoch: 1 cost time: 47.58814549446106
Epoch: 1, Steps: 318 | Train Loss: 0.3961159 Vali Loss: 0.2237910 Test Loss: 0.2332074
Validation loss decreased (inf --> 0.223791).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2586379
	speed: 0.1540s/iter; left time: 425.5016s
	iters: 200, epoch: 2 | loss: 0.1788097
	speed: 0.1632s/iter; left time: 434.6419s
	iters: 300, epoch: 2 | loss: 0.1985939
	speed: 0.1514s/iter; left time: 388.0376s
Epoch: 2 cost time: 49.60310220718384
Epoch: 2, Steps: 318 | Train Loss: 0.2223934 Vali Loss: 0.2151487 Test Loss: 0.2009757
Validation loss decreased (0.223791 --> 0.215149).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1699224
	speed: 0.1527s/iter; left time: 373.3130s
	iters: 200, epoch: 3 | loss: 0.2179199
	speed: 0.1472s/iter; left time: 345.2669s
	iters: 300, epoch: 3 | loss: 0.1671046
	speed: 0.1382s/iter; left time: 310.3657s
Epoch: 3 cost time: 46.306580543518066
Epoch: 3, Steps: 318 | Train Loss: 0.1998562 Vali Loss: 0.2116082 Test Loss: 0.1946906
Validation loss decreased (0.215149 --> 0.211608).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2052037
	speed: 0.1697s/iter; left time: 360.9708s
	iters: 200, epoch: 4 | loss: 0.1518643
	speed: 0.1533s/iter; left time: 310.8094s
	iters: 300, epoch: 4 | loss: 0.1598196
	speed: 0.1403s/iter; left time: 270.3443s
Epoch: 4 cost time: 48.90220069885254
Epoch: 4, Steps: 318 | Train Loss: 0.1917838 Vali Loss: 0.2043337 Test Loss: 0.1884784
Validation loss decreased (0.211608 --> 0.204334).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1689636
	speed: 0.1494s/iter; left time: 270.3437s
	iters: 200, epoch: 5 | loss: 0.1581737
	speed: 0.1413s/iter; left time: 241.5459s
	iters: 300, epoch: 5 | loss: 0.2514466
	speed: 0.1364s/iter; left time: 219.5437s
Epoch: 5 cost time: 45.04878902435303
Epoch: 5, Steps: 318 | Train Loss: 0.1878084 Vali Loss: 0.2010135 Test Loss: 0.1881603
Validation loss decreased (0.204334 --> 0.201014).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2306998
	speed: 0.1472s/iter; left time: 219.4214s
	iters: 200, epoch: 6 | loss: 0.1750421
	speed: 0.1519s/iter; left time: 211.3341s
	iters: 300, epoch: 6 | loss: 0.2651003
	speed: 0.1454s/iter; left time: 187.7032s
Epoch: 6 cost time: 46.897188901901245
Epoch: 6, Steps: 318 | Train Loss: 0.1853154 Vali Loss: 0.2021930 Test Loss: 0.1871185
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1932326
	speed: 0.1456s/iter; left time: 170.7900s
	iters: 200, epoch: 7 | loss: 0.1697324
	speed: 0.1454s/iter; left time: 156.0612s
	iters: 300, epoch: 7 | loss: 0.2027531
	speed: 0.1547s/iter; left time: 150.4988s
Epoch: 7 cost time: 47.40653967857361
Epoch: 7, Steps: 318 | Train Loss: 0.1840960 Vali Loss: 0.2016115 Test Loss: 0.1867069
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1418719
	speed: 0.1587s/iter; left time: 135.6983s
	iters: 200, epoch: 8 | loss: 0.2321176
	speed: 0.1489s/iter; left time: 112.4412s
	iters: 300, epoch: 8 | loss: 0.2088713
	speed: 0.1637s/iter; left time: 107.2437s
Epoch: 8 cost time: 50.359206199645996
Epoch: 8, Steps: 318 | Train Loss: 0.1836957 Vali Loss: 0.2010714 Test Loss: 0.1867191
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.4129103720188141, mae:0.5031291842460632, rmse:0.642581045627594, mape:0.01803440786898136, mspe:0.0005402516108006239, rse:0.45003581047058105, r2_score:0.777083465590232, acc:0.9819655921310186
corr: [37.91052  37.78986  37.750393 37.591507 37.780235 37.791412 37.71342
 37.67021  37.617172 37.47479  37.5119   37.456627 37.43566  37.31062
 37.646984]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4658875
	speed: 0.2010s/iter; left time: 619.2904s
	iters: 200, epoch: 1 | loss: 0.3260866
	speed: 0.2143s/iter; left time: 638.7962s
	iters: 300, epoch: 1 | loss: 0.2903981
	speed: 0.2118s/iter; left time: 610.1821s
Epoch: 1 cost time: 65.86530017852783
Epoch: 1, Steps: 318 | Train Loss: 0.4860902 Vali Loss: 0.2686133 Test Loss: 0.2888897
Validation loss decreased (inf --> 0.268613).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2358241
	speed: 0.1893s/iter; left time: 522.9983s
	iters: 200, epoch: 2 | loss: 0.2447442
	speed: 0.1873s/iter; left time: 498.7313s
	iters: 300, epoch: 2 | loss: 0.1672353
	speed: 0.1888s/iter; left time: 483.8095s
Epoch: 2 cost time: 59.9354510307312
Epoch: 2, Steps: 318 | Train Loss: 0.2686736 Vali Loss: 0.2398817 Test Loss: 0.2564173
Validation loss decreased (0.268613 --> 0.239882).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2230502
	speed: 0.1731s/iter; left time: 423.1382s
	iters: 200, epoch: 3 | loss: 0.1951939
	speed: 0.1807s/iter; left time: 423.7653s
	iters: 300, epoch: 3 | loss: 0.2313044
	speed: 0.1831s/iter; left time: 411.1062s
Epoch: 3 cost time: 56.542845487594604
Epoch: 3, Steps: 318 | Train Loss: 0.2429491 Vali Loss: 0.2225948 Test Loss: 0.2418261
Validation loss decreased (0.239882 --> 0.222595).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2446303
	speed: 0.1704s/iter; left time: 362.4266s
	iters: 200, epoch: 4 | loss: 0.1695757
	speed: 0.2114s/iter; left time: 428.5187s
	iters: 300, epoch: 4 | loss: 0.2567988
	speed: 0.2259s/iter; left time: 435.3652s
Epoch: 4 cost time: 65.00153422355652
Epoch: 4, Steps: 318 | Train Loss: 0.2324277 Vali Loss: 0.2202356 Test Loss: 0.2344219
Validation loss decreased (0.222595 --> 0.220236).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2137975
	speed: 0.2433s/iter; left time: 440.1246s
	iters: 200, epoch: 5 | loss: 0.2251770
	speed: 0.2351s/iter; left time: 401.7754s
	iters: 300, epoch: 5 | loss: 0.2578909
	speed: 0.2311s/iter; left time: 371.7924s
Epoch: 5 cost time: 75.15878343582153
Epoch: 5, Steps: 318 | Train Loss: 0.2279557 Vali Loss: 0.2152513 Test Loss: 0.2321163
Validation loss decreased (0.220236 --> 0.215251).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2254118
	speed: 0.2568s/iter; left time: 382.8368s
	iters: 200, epoch: 6 | loss: 0.3118259
	speed: 0.2242s/iter; left time: 311.8374s
	iters: 300, epoch: 6 | loss: 0.2399774
	speed: 0.2408s/iter; left time: 310.9146s
Epoch: 6 cost time: 77.27215194702148
Epoch: 6, Steps: 318 | Train Loss: 0.2261064 Vali Loss: 0.2164237 Test Loss: 0.2300419
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2443267
	speed: 0.2726s/iter; left time: 319.7732s
	iters: 200, epoch: 7 | loss: 0.2123639
	speed: 0.2511s/iter; left time: 269.4689s
	iters: 300, epoch: 7 | loss: 0.2082485
	speed: 0.2384s/iter; left time: 231.9329s
Epoch: 7 cost time: 80.31163573265076
Epoch: 7, Steps: 318 | Train Loss: 0.2252639 Vali Loss: 0.2152284 Test Loss: 0.2295019
Validation loss decreased (0.215251 --> 0.215228).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2080962
	speed: 0.2362s/iter; left time: 201.9900s
	iters: 200, epoch: 8 | loss: 0.2054224
	speed: 0.2293s/iter; left time: 173.1372s
	iters: 300, epoch: 8 | loss: 0.2703563
	speed: 0.2511s/iter; left time: 164.4662s
Epoch: 8 cost time: 75.86675643920898
Epoch: 8, Steps: 318 | Train Loss: 0.2246593 Vali Loss: 0.2164165 Test Loss: 0.2292965
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2202275
	speed: 0.2470s/iter; left time: 132.6602s
	iters: 200, epoch: 9 | loss: 0.1770721
	speed: 0.2286s/iter; left time: 99.8897s
	iters: 300, epoch: 9 | loss: 0.2230644
	speed: 0.2432s/iter; left time: 81.9684s
Epoch: 9 cost time: 76.8408875465393
Epoch: 9, Steps: 318 | Train Loss: 0.2250011 Vali Loss: 0.2154695 Test Loss: 0.2292427
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2184517
	speed: 0.2335s/iter; left time: 51.1446s
	iters: 200, epoch: 10 | loss: 0.2950493
	speed: 0.2482s/iter; left time: 29.5410s
	iters: 300, epoch: 10 | loss: 0.2362800
	speed: 0.2199s/iter; left time: 4.1775s
Epoch: 10 cost time: 74.50384640693665
Epoch: 10, Steps: 318 | Train Loss: 0.2247941 Vali Loss: 0.2149186 Test Loss: 0.2292076
Validation loss decreased (0.215228 --> 0.214919).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.5029872059822083, mae:0.5517403483390808, rmse:0.7092159390449524, mape:0.0197963435202837, mspe:0.0006625778041779995, rse:0.4964163601398468, r2_score:0.7102618416681113, acc:0.9802036564797163
corr: [37.72146  37.404537 37.515934 37.512638 37.4764   37.495712 37.62022
 37.38001  37.51977  37.555183 37.28963  37.506042 37.58243  37.248486
 37.266396 37.856525 37.32824  37.83391  37.8171   37.866627]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3204553
	speed: 0.1864s/iter; left time: 574.2214s
	iters: 200, epoch: 1 | loss: 0.2858375
	speed: 0.1474s/iter; left time: 439.5429s
	iters: 300, epoch: 1 | loss: 0.2730975
	speed: 0.1455s/iter; left time: 419.1807s
Epoch: 1 cost time: 50.57469701766968
Epoch: 1, Steps: 318 | Train Loss: 0.4148538 Vali Loss: 0.2616392 Test Loss: 0.2868429
Validation loss decreased (inf --> 0.261639).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2249094
	speed: 0.1484s/iter; left time: 410.0625s
	iters: 200, epoch: 2 | loss: 0.2569811
	speed: 0.1458s/iter; left time: 388.2063s
	iters: 300, epoch: 2 | loss: 0.2292613
	speed: 0.1594s/iter; left time: 408.5084s
Epoch: 2 cost time: 48.29243540763855
Epoch: 2, Steps: 318 | Train Loss: 0.2540746 Vali Loss: 0.2583083 Test Loss: 0.2714006
Validation loss decreased (0.261639 --> 0.258308).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2199514
	speed: 0.1528s/iter; left time: 373.7123s
	iters: 200, epoch: 3 | loss: 0.2282080
	speed: 0.1627s/iter; left time: 381.6441s
	iters: 300, epoch: 3 | loss: 0.2531372
	speed: 0.1337s/iter; left time: 300.1748s
Epoch: 3 cost time: 47.542020082473755
Epoch: 3, Steps: 318 | Train Loss: 0.2334846 Vali Loss: 0.2505545 Test Loss: 0.2676244
Validation loss decreased (0.258308 --> 0.250555).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1845317
	speed: 0.1513s/iter; left time: 321.9105s
	iters: 200, epoch: 4 | loss: 0.1942155
	speed: 0.1445s/iter; left time: 292.8277s
	iters: 300, epoch: 4 | loss: 0.2283086
	speed: 0.1507s/iter; left time: 290.4888s
Epoch: 4 cost time: 47.61217713356018
Epoch: 4, Steps: 318 | Train Loss: 0.2252330 Vali Loss: 0.2510226 Test Loss: 0.2676822
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2277054
	speed: 0.1605s/iter; left time: 290.3589s
	iters: 200, epoch: 5 | loss: 0.2574365
	speed: 0.1448s/iter; left time: 247.4781s
	iters: 300, epoch: 5 | loss: 0.2213190
	speed: 0.1413s/iter; left time: 227.3379s
Epoch: 5 cost time: 47.148855686187744
Epoch: 5, Steps: 318 | Train Loss: 0.2213511 Vali Loss: 0.2508245 Test Loss: 0.2657248
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2064211
	speed: 0.1601s/iter; left time: 238.7127s
	iters: 200, epoch: 6 | loss: 0.2285392
	speed: 0.1659s/iter; left time: 230.7754s
	iters: 300, epoch: 6 | loss: 0.1999364
	speed: 0.1485s/iter; left time: 191.7011s
Epoch: 6 cost time: 50.107606649398804
Epoch: 6, Steps: 318 | Train Loss: 0.2196106 Vali Loss: 0.2519049 Test Loss: 0.2642949
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.5872913002967834, mae:0.590971052646637, rmse:0.7663493156433105, mape:0.021235544234514236, mspe:0.0007793422555550933, rse:0.5360485911369324, r2_score:0.6607543106647088, acc:0.9787644557654858
corr: [37.4184   37.346584 37.411983 37.42008  37.45653  37.40326  37.500946
 37.51167  37.62136  37.44754  37.167316 37.689804 37.62626  37.314693
 36.96387  37.466606 37.351955 37.336006 37.390034 37.4994   37.44012
 37.629795 37.766502 37.463238 37.638752]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3165203
	speed: 0.1868s/iter; left time: 573.6309s
	iters: 200, epoch: 1 | loss: 0.2937227
	speed: 0.2052s/iter; left time: 609.5338s
	iters: 300, epoch: 1 | loss: 0.2866104
	speed: 0.1767s/iter; left time: 507.2575s
Epoch: 1 cost time: 60.63346719741821
Epoch: 1, Steps: 317 | Train Loss: 0.4091978 Vali Loss: 0.2500636 Test Loss: 0.2572039
Validation loss decreased (inf --> 0.250064).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2233179
	speed: 0.2006s/iter; left time: 552.5452s
	iters: 200, epoch: 2 | loss: 0.2405171
	speed: 0.1689s/iter; left time: 448.1347s
	iters: 300, epoch: 2 | loss: 0.1734926
	speed: 0.1505s/iter; left time: 384.2686s
Epoch: 2 cost time: 54.51632237434387
Epoch: 2, Steps: 317 | Train Loss: 0.2393381 Vali Loss: 0.2252596 Test Loss: 0.2240619
Validation loss decreased (0.250064 --> 0.225260).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2412125
	speed: 0.1672s/iter; left time: 407.3901s
	iters: 200, epoch: 3 | loss: 0.2366023
	speed: 0.1383s/iter; left time: 323.2265s
	iters: 300, epoch: 3 | loss: 0.2092177
	speed: 0.1411s/iter; left time: 315.6186s
Epoch: 3 cost time: 47.12879276275635
Epoch: 3, Steps: 317 | Train Loss: 0.2229027 Vali Loss: 0.2145790 Test Loss: 0.2203460
Validation loss decreased (0.225260 --> 0.214579).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2087950
	speed: 0.1790s/iter; left time: 379.4983s
	iters: 200, epoch: 4 | loss: 0.2143708
	speed: 0.1550s/iter; left time: 313.0218s
	iters: 300, epoch: 4 | loss: 0.2156984
	speed: 0.1419s/iter; left time: 272.5378s
Epoch: 4 cost time: 49.891289472579956
Epoch: 4, Steps: 317 | Train Loss: 0.2147821 Vali Loss: 0.2098825 Test Loss: 0.2195607
Validation loss decreased (0.214579 --> 0.209883).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2261153
	speed: 0.1534s/iter; left time: 276.5962s
	iters: 200, epoch: 5 | loss: 0.1553489
	speed: 0.1460s/iter; left time: 248.6724s
	iters: 300, epoch: 5 | loss: 0.2235326
	speed: 0.1459s/iter; left time: 233.8806s
Epoch: 5 cost time: 47.28163027763367
Epoch: 5, Steps: 317 | Train Loss: 0.2117852 Vali Loss: 0.2133249 Test Loss: 0.2204479
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2369062
	speed: 0.1510s/iter; left time: 224.4190s
	iters: 200, epoch: 6 | loss: 0.2166415
	speed: 0.1398s/iter; left time: 193.8064s
	iters: 300, epoch: 6 | loss: 0.1946272
	speed: 0.1504s/iter; left time: 193.4128s
Epoch: 6 cost time: 47.05864119529724
Epoch: 6, Steps: 317 | Train Loss: 0.2104322 Vali Loss: 0.2111118 Test Loss: 0.2190655
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1932390
	speed: 0.1627s/iter; left time: 190.2214s
	iters: 200, epoch: 7 | loss: 0.2471060
	speed: 0.1553s/iter; left time: 166.0142s
	iters: 300, epoch: 7 | loss: 0.2149032
	speed: 0.1561s/iter; left time: 151.2421s
Epoch: 7 cost time: 49.981833696365356
Epoch: 7, Steps: 317 | Train Loss: 0.2089643 Vali Loss: 0.2117656 Test Loss: 0.2190025
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.4818173348903656, mae:0.5389224290847778, rmse:0.6941306591033936, mape:0.019337845966219902, mspe:0.0006353775970637798, rse:0.4851841926574707, r2_score:0.7376946559537167, acc:0.9806621540337801
corr: [37.32078  37.093647 37.18151  37.147808 37.31546  37.341564 37.276096
 37.311703 37.293907 37.20892  37.11826  37.064514 37.11169  36.994076
 37.259113 37.591118 37.45546  37.391735 37.415367 37.266167 37.355072
 37.389793 37.366848 37.486153 37.771835 37.866478 37.517395 37.649754
 37.361702 37.39598 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4257143
	speed: 0.2347s/iter; left time: 722.9685s
	iters: 200, epoch: 1 | loss: 0.3349038
	speed: 0.2217s/iter; left time: 661.0364s
	iters: 300, epoch: 1 | loss: 0.2152797
	speed: 0.3131s/iter; left time: 902.1205s
Epoch: 1 cost time: 82.51223087310791
Epoch: 1, Steps: 318 | Train Loss: 0.4205477 Vali Loss: 0.2160805 Test Loss: 0.2156937
Validation loss decreased (inf --> 0.216080).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2517484
	speed: 0.2442s/iter; left time: 674.7330s
	iters: 200, epoch: 2 | loss: 0.1810741
	speed: 0.2131s/iter; left time: 567.3919s
	iters: 300, epoch: 2 | loss: 0.2813962
	speed: 0.1929s/iter; left time: 494.3737s
Epoch: 2 cost time: 68.17819476127625
Epoch: 2, Steps: 318 | Train Loss: 0.2199444 Vali Loss: 0.2033758 Test Loss: 0.1878820
Validation loss decreased (0.216080 --> 0.203376).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1438451
	speed: 0.1911s/iter; left time: 467.2532s
	iters: 200, epoch: 3 | loss: 0.1942283
	speed: 0.1776s/iter; left time: 416.4694s
	iters: 300, epoch: 3 | loss: 0.1956056
	speed: 0.1636s/iter; left time: 367.3300s
Epoch: 3 cost time: 56.07594084739685
Epoch: 3, Steps: 318 | Train Loss: 0.2002655 Vali Loss: 0.1948874 Test Loss: 0.1796796
Validation loss decreased (0.203376 --> 0.194887).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1429286
	speed: 0.2161s/iter; left time: 459.6505s
	iters: 200, epoch: 4 | loss: 0.1814400
	speed: 0.2164s/iter; left time: 438.5882s
	iters: 300, epoch: 4 | loss: 0.2083991
	speed: 0.1987s/iter; left time: 382.8755s
Epoch: 4 cost time: 66.59437465667725
Epoch: 4, Steps: 318 | Train Loss: 0.1926718 Vali Loss: 0.1949534 Test Loss: 0.1790811
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2007021
	speed: 0.1891s/iter; left time: 342.0823s
	iters: 200, epoch: 5 | loss: 0.1689431
	speed: 0.1739s/iter; left time: 297.2123s
	iters: 300, epoch: 5 | loss: 0.2826909
	speed: 0.1629s/iter; left time: 262.1312s
Epoch: 5 cost time: 56.13826513290405
Epoch: 5, Steps: 318 | Train Loss: 0.1904612 Vali Loss: 0.1915068 Test Loss: 0.1760867
Validation loss decreased (0.194887 --> 0.191507).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1980888
	speed: 0.1734s/iter; left time: 258.5351s
	iters: 200, epoch: 6 | loss: 0.1712627
	speed: 0.1662s/iter; left time: 231.1206s
	iters: 300, epoch: 6 | loss: 0.2013108
	speed: 0.1669s/iter; left time: 215.4989s
Epoch: 6 cost time: 53.451261043548584
Epoch: 6, Steps: 318 | Train Loss: 0.1884235 Vali Loss: 0.1917797 Test Loss: 0.1756621
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1730245
	speed: 0.1823s/iter; left time: 213.7946s
	iters: 200, epoch: 7 | loss: 0.1581630
	speed: 0.1907s/iter; left time: 204.6277s
	iters: 300, epoch: 7 | loss: 0.1864535
	speed: 0.1661s/iter; left time: 161.5792s
Epoch: 7 cost time: 57.01063656806946
Epoch: 7, Steps: 318 | Train Loss: 0.1873769 Vali Loss: 0.1918317 Test Loss: 0.1753444
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1411457
	speed: 0.1718s/iter; left time: 146.9186s
	iters: 200, epoch: 8 | loss: 0.2262829
	speed: 0.1843s/iter; left time: 139.1522s
	iters: 300, epoch: 8 | loss: 0.1514600
	speed: 0.1860s/iter; left time: 121.8228s
Epoch: 8 cost time: 57.72612690925598
Epoch: 8, Steps: 318 | Train Loss: 0.1872750 Vali Loss: 0.1908481 Test Loss: 0.1750547
Validation loss decreased (0.191507 --> 0.190848).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1917324
	speed: 0.1897s/iter; left time: 101.8592s
	iters: 200, epoch: 9 | loss: 0.1877711
	speed: 0.1737s/iter; left time: 75.8942s
	iters: 300, epoch: 9 | loss: 0.1865080
	speed: 0.1595s/iter; left time: 53.7441s
Epoch: 9 cost time: 55.24109673500061
Epoch: 9, Steps: 318 | Train Loss: 0.1870043 Vali Loss: 0.1925522 Test Loss: 0.1749225
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1778394
	speed: 0.1612s/iter; left time: 35.2935s
	iters: 200, epoch: 10 | loss: 0.1864175
	speed: 0.1712s/iter; left time: 20.3761s
	iters: 300, epoch: 10 | loss: 0.1583152
	speed: 0.1891s/iter; left time: 3.5931s
Epoch: 10 cost time: 55.10985541343689
Epoch: 10, Steps: 318 | Train Loss: 0.1872653 Vali Loss: 0.1910416 Test Loss: 0.1748410
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.3841506540775299, mae:0.48326143622398376, rmse:0.6197988986968994, mape:0.01732468046247959, mspe:0.0005023949779570103, rse:0.43428468704223633, r2_score:0.794190322884807, acc:0.9826753195375204
corr: [37.54475  37.588642 37.43426  37.53332  37.506935 37.44626  37.5623
 37.57416  37.515522 37.664986]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3046316
	speed: 0.2617s/iter; left time: 806.3570s
	iters: 200, epoch: 1 | loss: 0.2313960
	speed: 0.1771s/iter; left time: 527.9934s
	iters: 300, epoch: 1 | loss: 0.2175931
	speed: 0.2631s/iter; left time: 758.0650s
Epoch: 1 cost time: 73.77249479293823
Epoch: 1, Steps: 318 | Train Loss: 0.3598569 Vali Loss: 0.2385691 Test Loss: 0.2228912
Validation loss decreased (inf --> 0.238569).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2610622
	speed: 0.1776s/iter; left time: 490.5725s
	iters: 200, epoch: 2 | loss: 0.1711062
	speed: 0.1722s/iter; left time: 458.4734s
	iters: 300, epoch: 2 | loss: 0.1869025
	speed: 0.1571s/iter; left time: 402.5527s
Epoch: 2 cost time: 53.820292711257935
Epoch: 2, Steps: 318 | Train Loss: 0.2195531 Vali Loss: 0.2191998 Test Loss: 0.1958811
Validation loss decreased (0.238569 --> 0.219200).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1641454
	speed: 0.1959s/iter; left time: 478.9927s
	iters: 200, epoch: 3 | loss: 0.2387103
	speed: 0.1627s/iter; left time: 381.5688s
	iters: 300, epoch: 3 | loss: 0.1586289
	speed: 0.1657s/iter; left time: 372.0744s
Epoch: 3 cost time: 55.93672490119934
Epoch: 3, Steps: 318 | Train Loss: 0.2004819 Vali Loss: 0.2145812 Test Loss: 0.1910198
Validation loss decreased (0.219200 --> 0.214581).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2011890
	speed: 0.1870s/iter; left time: 397.6900s
	iters: 200, epoch: 4 | loss: 0.1572965
	speed: 0.1772s/iter; left time: 359.1785s
	iters: 300, epoch: 4 | loss: 0.1756347
	speed: 0.1701s/iter; left time: 327.8127s
Epoch: 4 cost time: 56.19595551490784
Epoch: 4, Steps: 318 | Train Loss: 0.1932891 Vali Loss: 0.2115274 Test Loss: 0.1876576
Validation loss decreased (0.214581 --> 0.211527).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1708038
	speed: 0.1675s/iter; left time: 302.9308s
	iters: 200, epoch: 5 | loss: 0.1677387
	speed: 0.1705s/iter; left time: 291.3528s
	iters: 300, epoch: 5 | loss: 0.2764272
	speed: 0.1844s/iter; left time: 296.7284s
Epoch: 5 cost time: 55.50072956085205
Epoch: 5, Steps: 318 | Train Loss: 0.1903313 Vali Loss: 0.2078617 Test Loss: 0.1867252
Validation loss decreased (0.211527 --> 0.207862).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2199883
	speed: 0.1687s/iter; left time: 251.5013s
	iters: 200, epoch: 6 | loss: 0.1944868
	speed: 0.1501s/iter; left time: 208.7772s
	iters: 300, epoch: 6 | loss: 0.2738967
	speed: 0.1537s/iter; left time: 198.3658s
Epoch: 6 cost time: 50.07759666442871
Epoch: 6, Steps: 318 | Train Loss: 0.1886733 Vali Loss: 0.2084293 Test Loss: 0.1857540
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2159687
	speed: 0.1717s/iter; left time: 201.3666s
	iters: 200, epoch: 7 | loss: 0.1647557
	speed: 0.1561s/iter; left time: 167.4760s
	iters: 300, epoch: 7 | loss: 0.1999914
	speed: 0.1686s/iter; left time: 164.0277s
Epoch: 7 cost time: 52.65035796165466
Epoch: 7, Steps: 318 | Train Loss: 0.1877486 Vali Loss: 0.2076149 Test Loss: 0.1851475
Validation loss decreased (0.207862 --> 0.207615).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1283084
	speed: 0.1685s/iter; left time: 144.0738s
	iters: 200, epoch: 8 | loss: 0.2450618
	speed: 0.1784s/iter; left time: 134.6666s
	iters: 300, epoch: 8 | loss: 0.2312424
	speed: 0.1633s/iter; left time: 106.9878s
Epoch: 8 cost time: 53.99071550369263
Epoch: 8, Steps: 318 | Train Loss: 0.1868884 Vali Loss: 0.2077499 Test Loss: 0.1851525
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1657346
	speed: 0.1792s/iter; left time: 96.2449s
	iters: 200, epoch: 9 | loss: 0.1720219
	speed: 0.1585s/iter; left time: 69.2444s
	iters: 300, epoch: 9 | loss: 0.2083346
	speed: 0.1606s/iter; left time: 54.1076s
Epoch: 9 cost time: 53.19448661804199
Epoch: 9, Steps: 318 | Train Loss: 0.1868078 Vali Loss: 0.2076781 Test Loss: 0.1850448
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2366768
	speed: 0.1681s/iter; left time: 36.8170s
	iters: 200, epoch: 10 | loss: 0.2018729
	speed: 0.1559s/iter; left time: 18.5559s
	iters: 300, epoch: 10 | loss: 0.1707397
	speed: 0.1765s/iter; left time: 3.3538s
Epoch: 10 cost time: 53.00378918647766
Epoch: 10, Steps: 318 | Train Loss: 0.1867287 Vali Loss: 0.2068766 Test Loss: 0.1850068
Validation loss decreased (0.207615 --> 0.206877).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.40599024295806885, mae:0.4945778250694275, rmse:0.6371736526489258, mape:0.017728285863995552, mspe:0.0005323978257365525, rse:0.44624871015548706, r2_score:0.7812881603018301, acc:0.9822717141360044
corr: [37.62487  37.64223  37.569275 37.55248  37.40138  37.40803  37.450176
 37.360355 37.233356 37.32662  37.39867  37.442226 37.57756  37.66745
 37.78048 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4685371
	speed: 0.2668s/iter; left time: 822.1639s
	iters: 200, epoch: 1 | loss: 0.2920150
	speed: 0.2900s/iter; left time: 864.5058s
	iters: 300, epoch: 1 | loss: 0.2907412
	speed: 0.2675s/iter; left time: 770.7045s
Epoch: 1 cost time: 86.71797895431519
Epoch: 1, Steps: 318 | Train Loss: 0.4797220 Vali Loss: 0.2676454 Test Loss: 0.2880757
Validation loss decreased (inf --> 0.267645).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2324070
	speed: 0.3002s/iter; left time: 829.5739s
	iters: 200, epoch: 2 | loss: 0.2306421
	speed: 0.2990s/iter; left time: 796.2074s
	iters: 300, epoch: 2 | loss: 0.1747504
	speed: 0.2887s/iter; left time: 739.9270s
Epoch: 2 cost time: 94.13533425331116
Epoch: 2, Steps: 318 | Train Loss: 0.2574055 Vali Loss: 0.2446750 Test Loss: 0.2586686
Validation loss decreased (0.267645 --> 0.244675).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2406148
	speed: 0.3042s/iter; left time: 743.8569s
	iters: 200, epoch: 3 | loss: 0.2005810
	speed: 0.2798s/iter; left time: 656.0289s
	iters: 300, epoch: 3 | loss: 0.2091428
	speed: 0.2705s/iter; left time: 607.2593s
Epoch: 3 cost time: 90.62968730926514
Epoch: 3, Steps: 318 | Train Loss: 0.2314181 Vali Loss: 0.2359228 Test Loss: 0.2479291
Validation loss decreased (0.244675 --> 0.235923).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2345280
	speed: 0.2882s/iter; left time: 613.0196s
	iters: 200, epoch: 4 | loss: 0.1929689
	speed: 0.3105s/iter; left time: 629.3383s
	iters: 300, epoch: 4 | loss: 0.2218612
	speed: 0.2833s/iter; left time: 545.9335s
Epoch: 4 cost time: 93.04443907737732
Epoch: 4, Steps: 318 | Train Loss: 0.2197719 Vali Loss: 0.2346540 Test Loss: 0.2466107
Validation loss decreased (0.235923 --> 0.234654).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1778425
	speed: 0.2834s/iter; left time: 512.6617s
	iters: 200, epoch: 5 | loss: 0.2295267
	speed: 0.2816s/iter; left time: 481.1891s
	iters: 300, epoch: 5 | loss: 0.2316414
	speed: 0.2818s/iter; left time: 453.3471s
Epoch: 5 cost time: 89.77579092979431
Epoch: 5, Steps: 318 | Train Loss: 0.2154779 Vali Loss: 0.2317899 Test Loss: 0.2459759
Validation loss decreased (0.234654 --> 0.231790).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1904203
	speed: 0.3267s/iter; left time: 487.1705s
	iters: 200, epoch: 6 | loss: 0.2836952
	speed: 0.2868s/iter; left time: 398.9332s
	iters: 300, epoch: 6 | loss: 0.2290894
	speed: 0.2869s/iter; left time: 370.3650s
Epoch: 6 cost time: 95.20011639595032
Epoch: 6, Steps: 318 | Train Loss: 0.2130758 Vali Loss: 0.2332257 Test Loss: 0.2432777
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2161929
	speed: 0.3335s/iter; left time: 391.2397s
	iters: 200, epoch: 7 | loss: 0.1889807
	speed: 0.3032s/iter; left time: 325.3761s
	iters: 300, epoch: 7 | loss: 0.1928286
	speed: 0.2778s/iter; left time: 270.3117s
Epoch: 7 cost time: 96.6029417514801
Epoch: 7, Steps: 318 | Train Loss: 0.2124003 Vali Loss: 0.2328145 Test Loss: 0.2428120
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2147326
	speed: 0.3292s/iter; left time: 281.4836s
	iters: 200, epoch: 8 | loss: 0.1738245
	speed: 0.2669s/iter; left time: 201.4923s
	iters: 300, epoch: 8 | loss: 0.2271171
	speed: 0.2729s/iter; left time: 178.7305s
Epoch: 8 cost time: 92.04583692550659
Epoch: 8, Steps: 318 | Train Loss: 0.2113963 Vali Loss: 0.2337456 Test Loss: 0.2430090
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.5397844314575195, mae:0.5735430121421814, rmse:0.7347002029418945, mape:0.020568424835801125, mspe:0.0007086914265528321, rse:0.514254093170166, r2_score:0.6942125225291489, acc:0.9794315751641989
corr: [37.592968 37.272404 37.38058  37.341038 37.30983  37.26619  37.407597
 37.22337  37.303562 37.4389   37.164753 37.433475 37.50366  37.24155
 37.30026  37.749214 37.35684  37.729847 37.680256 37.752903]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3277177
	speed: 0.2008s/iter; left time: 618.6056s
	iters: 200, epoch: 1 | loss: 0.2862237
	speed: 0.1972s/iter; left time: 587.7045s
	iters: 300, epoch: 1 | loss: 0.2359045
	speed: 0.2514s/iter; left time: 724.3405s
Epoch: 1 cost time: 68.36407327651978
Epoch: 1, Steps: 318 | Train Loss: 0.4077763 Vali Loss: 0.2518814 Test Loss: 0.2664951
Validation loss decreased (inf --> 0.251881).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2161351
	speed: 0.2198s/iter; left time: 607.2267s
	iters: 200, epoch: 2 | loss: 0.2566778
	speed: 0.2280s/iter; left time: 607.2721s
	iters: 300, epoch: 2 | loss: 0.2479872
	speed: 0.2246s/iter; left time: 575.6859s
Epoch: 2 cost time: 71.17324995994568
Epoch: 2, Steps: 318 | Train Loss: 0.2478976 Vali Loss: 0.2493513 Test Loss: 0.2554332
Validation loss decreased (0.251881 --> 0.249351).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2290250
	speed: 0.2386s/iter; left time: 583.4177s
	iters: 200, epoch: 3 | loss: 0.2044161
	speed: 0.2212s/iter; left time: 518.7899s
	iters: 300, epoch: 3 | loss: 0.2478977
	speed: 0.2477s/iter; left time: 556.0385s
Epoch: 3 cost time: 75.44903492927551
Epoch: 3, Steps: 318 | Train Loss: 0.2261510 Vali Loss: 0.2425322 Test Loss: 0.2496434
Validation loss decreased (0.249351 --> 0.242532).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1875530
	speed: 0.2579s/iter; left time: 548.6240s
	iters: 200, epoch: 4 | loss: 0.2059069
	speed: 0.2352s/iter; left time: 476.7158s
	iters: 300, epoch: 4 | loss: 0.2086136
	speed: 0.2446s/iter; left time: 471.2836s
Epoch: 4 cost time: 78.7654709815979
Epoch: 4, Steps: 318 | Train Loss: 0.2164869 Vali Loss: 0.2430349 Test Loss: 0.2497218
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2131364
	speed: 0.2512s/iter; left time: 454.3720s
	iters: 200, epoch: 5 | loss: 0.2674898
	speed: 0.2520s/iter; left time: 430.6680s
	iters: 300, epoch: 5 | loss: 0.1831329
	speed: 0.2379s/iter; left time: 382.8487s
Epoch: 5 cost time: 79.11941289901733
Epoch: 5, Steps: 318 | Train Loss: 0.2121653 Vali Loss: 0.2414982 Test Loss: 0.2475177
Validation loss decreased (0.242532 --> 0.241498).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1865562
	speed: 0.2733s/iter; left time: 407.5291s
	iters: 200, epoch: 6 | loss: 0.2019020
	speed: 0.2414s/iter; left time: 335.7764s
	iters: 300, epoch: 6 | loss: 0.1698256
	speed: 0.2419s/iter; left time: 312.2324s
Epoch: 6 cost time: 80.02768898010254
Epoch: 6, Steps: 318 | Train Loss: 0.2104054 Vali Loss: 0.2430352 Test Loss: 0.2470854
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2131119
	speed: 0.2792s/iter; left time: 327.5521s
	iters: 200, epoch: 7 | loss: 0.2782827
	speed: 0.2502s/iter; left time: 268.4392s
	iters: 300, epoch: 7 | loss: 0.1696436
	speed: 0.2190s/iter; left time: 213.0663s
Epoch: 7 cost time: 78.7451491355896
Epoch: 7, Steps: 318 | Train Loss: 0.2096671 Vali Loss: 0.2436472 Test Loss: 0.2476497
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2102444
	speed: 0.2589s/iter; left time: 221.3820s
	iters: 200, epoch: 8 | loss: 0.1748834
	speed: 0.2246s/iter; left time: 169.5404s
	iters: 300, epoch: 8 | loss: 0.2000109
	speed: 0.2270s/iter; left time: 148.7043s
Epoch: 8 cost time: 75.80421829223633
Epoch: 8, Steps: 318 | Train Loss: 0.2087355 Vali Loss: 0.2432447 Test Loss: 0.2473384
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.5431679487228394, mae:0.5647901296615601, rmse:0.7369992733001709, mape:0.020289046689867973, mspe:0.0007211334304884076, rse:0.5155187249183655, r2_score:0.7018929728639134, acc:0.979710953310132
corr: [37.45958  37.45254  37.136917 37.2239   37.530277 37.39784  37.15708
 37.285267 37.377872 37.35415  37.044205 37.303196 37.448807 37.339363
 37.077244 37.026928 37.18408  37.256653 37.089527 37.17167  37.30293
 37.59244  37.627087 37.272717 37.373474]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3171817
	speed: 0.2339s/iter; left time: 718.3508s
	iters: 200, epoch: 1 | loss: 0.2896771
	speed: 0.2958s/iter; left time: 878.6960s
	iters: 300, epoch: 1 | loss: 0.2865769
	speed: 0.2360s/iter; left time: 677.5137s
Epoch: 1 cost time: 79.90856146812439
Epoch: 1, Steps: 317 | Train Loss: 0.4111496 Vali Loss: 0.2502922 Test Loss: 0.2565326
Validation loss decreased (inf --> 0.250292).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2233819
	speed: 0.2224s/iter; left time: 612.4004s
	iters: 200, epoch: 2 | loss: 0.2443110
	speed: 0.2169s/iter; left time: 575.6460s
	iters: 300, epoch: 2 | loss: 0.1717871
	speed: 0.2451s/iter; left time: 625.8868s
Epoch: 2 cost time: 71.73894667625427
Epoch: 2, Steps: 317 | Train Loss: 0.2393010 Vali Loss: 0.2283519 Test Loss: 0.2232334
Validation loss decreased (0.250292 --> 0.228352).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2409209
	speed: 0.2705s/iter; left time: 659.1009s
	iters: 200, epoch: 3 | loss: 0.2370121
	speed: 0.2703s/iter; left time: 631.7477s
	iters: 300, epoch: 3 | loss: 0.2061388
	speed: 0.2812s/iter; left time: 629.0526s
Epoch: 3 cost time: 86.53007745742798
Epoch: 3, Steps: 317 | Train Loss: 0.2228346 Vali Loss: 0.2150954 Test Loss: 0.2193247
Validation loss decreased (0.228352 --> 0.215095).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2094907
	speed: 0.2261s/iter; left time: 479.3871s
	iters: 200, epoch: 4 | loss: 0.2164912
	speed: 0.2088s/iter; left time: 421.8536s
	iters: 300, epoch: 4 | loss: 0.2132175
	speed: 0.2344s/iter; left time: 450.0797s
Epoch: 4 cost time: 70.16232585906982
Epoch: 4, Steps: 317 | Train Loss: 0.2145581 Vali Loss: 0.2102090 Test Loss: 0.2181771
Validation loss decreased (0.215095 --> 0.210209).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2151798
	speed: 0.2132s/iter; left time: 384.3539s
	iters: 200, epoch: 5 | loss: 0.1532730
	speed: 0.1789s/iter; left time: 304.7104s
	iters: 300, epoch: 5 | loss: 0.2201346
	speed: 0.1788s/iter; left time: 286.5553s
Epoch: 5 cost time: 59.9700071811676
Epoch: 5, Steps: 317 | Train Loss: 0.2114509 Vali Loss: 0.2114368 Test Loss: 0.2187510
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2414020
	speed: 0.1979s/iter; left time: 294.0111s
	iters: 200, epoch: 6 | loss: 0.2160486
	speed: 0.1807s/iter; left time: 250.5116s
	iters: 300, epoch: 6 | loss: 0.1931015
	speed: 0.1908s/iter; left time: 245.3996s
Epoch: 6 cost time: 61.02533435821533
Epoch: 6, Steps: 317 | Train Loss: 0.2102621 Vali Loss: 0.2089827 Test Loss: 0.2172119
Validation loss decreased (0.210209 --> 0.208983).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1926721
	speed: 0.1988s/iter; left time: 232.4416s
	iters: 200, epoch: 7 | loss: 0.2400139
	speed: 0.1802s/iter; left time: 192.6272s
	iters: 300, epoch: 7 | loss: 0.2094834
	speed: 0.1818s/iter; left time: 176.1635s
Epoch: 7 cost time: 59.244502782821655
Epoch: 7, Steps: 317 | Train Loss: 0.2088963 Vali Loss: 0.2094600 Test Loss: 0.2172838
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2268469
	speed: 0.1976s/iter; left time: 168.3845s
	iters: 200, epoch: 8 | loss: 0.1884845
	speed: 0.1700s/iter; left time: 127.8079s
	iters: 300, epoch: 8 | loss: 0.2109026
	speed: 0.1828s/iter; left time: 119.1777s
Epoch: 8 cost time: 58.26393699645996
Epoch: 8, Steps: 317 | Train Loss: 0.2088728 Vali Loss: 0.2091151 Test Loss: 0.2169144
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2318781
	speed: 0.1976s/iter; left time: 105.7346s
	iters: 200, epoch: 9 | loss: 0.2385865
	speed: 0.1831s/iter; left time: 79.6298s
	iters: 300, epoch: 9 | loss: 0.2320636
	speed: 0.1771s/iter; left time: 59.3218s
Epoch: 9 cost time: 59.47919535636902
Epoch: 9, Steps: 317 | Train Loss: 0.2085424 Vali Loss: 0.2089062 Test Loss: 0.2169548
Validation loss decreased (0.208983 --> 0.208906).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1826859
	speed: 0.1934s/iter; left time: 42.1510s
	iters: 200, epoch: 10 | loss: 0.1942256
	speed: 0.1813s/iter; left time: 21.3969s
	iters: 300, epoch: 10 | loss: 0.2133459
	speed: 0.1874s/iter; left time: 3.3727s
Epoch: 10 cost time: 58.92713379859924
Epoch: 10, Steps: 317 | Train Loss: 0.2083775 Vali Loss: 0.2084508 Test Loss: 0.2168828
Validation loss decreased (0.208906 --> 0.208451).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.4759408235549927, mae:0.5348983407020569, rmse:0.6898846626281738, mape:0.019194064661860466, mspe:0.0006278815562836826, rse:0.48221632838249207, r2_score:0.7424405760783604, acc:0.9808059353381395
corr: [37.26623  37.07481  37.17636  37.134525 37.3055   37.3099   37.273857
 37.30103  37.290604 37.20719  37.170177 37.104973 37.132957 37.015347
 37.26192  37.626198 37.480633 37.431618 37.471737 37.315224 37.39764
 37.440823 37.41669  37.516422 37.78485  37.93912  37.6206   37.726387
 37.497734 37.529823]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4255209
	speed: 0.2639s/iter; left time: 812.9876s
	iters: 200, epoch: 1 | loss: 0.3362524
	speed: 0.2690s/iter; left time: 801.9211s
	iters: 300, epoch: 1 | loss: 0.2104439
	speed: 0.3153s/iter; left time: 908.3951s
Epoch: 1 cost time: 91.85447144508362
Epoch: 1, Steps: 318 | Train Loss: 0.4211969 Vali Loss: 0.2161650 Test Loss: 0.2154995
Validation loss decreased (inf --> 0.216165).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2508664
	speed: 0.3714s/iter; left time: 1026.1868s
	iters: 200, epoch: 2 | loss: 0.1809394
	speed: 0.3693s/iter; left time: 983.5404s
	iters: 300, epoch: 2 | loss: 0.2805973
	speed: 0.3657s/iter; left time: 937.3224s
Epoch: 2 cost time: 117.18959712982178
Epoch: 2, Steps: 318 | Train Loss: 0.2197877 Vali Loss: 0.2029627 Test Loss: 0.1877125
Validation loss decreased (0.216165 --> 0.202963).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1435206
	speed: 0.3686s/iter; left time: 901.2728s
	iters: 200, epoch: 3 | loss: 0.1933659
	speed: 0.3547s/iter; left time: 831.8140s
	iters: 300, epoch: 3 | loss: 0.1949272
	speed: 0.3716s/iter; left time: 834.2891s
Epoch: 3 cost time: 115.88358569145203
Epoch: 3, Steps: 318 | Train Loss: 0.1999941 Vali Loss: 0.1997447 Test Loss: 0.1841266
Validation loss decreased (0.202963 --> 0.199745).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1413445
	speed: 0.4519s/iter; left time: 961.2505s
	iters: 200, epoch: 4 | loss: 0.1805352
	speed: 0.4134s/iter; left time: 838.0434s
	iters: 300, epoch: 4 | loss: 0.2105515
	speed: 0.4318s/iter; left time: 831.9995s
Epoch: 4 cost time: 137.32759022712708
Epoch: 4, Steps: 318 | Train Loss: 0.1918717 Vali Loss: 0.1970851 Test Loss: 0.1805193
Validation loss decreased (0.199745 --> 0.197085).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2031587
	speed: 0.4247s/iter; left time: 768.3477s
	iters: 200, epoch: 5 | loss: 0.1715544
	speed: 0.4664s/iter; left time: 797.0278s
	iters: 300, epoch: 5 | loss: 0.2793624
	speed: 0.4357s/iter; left time: 701.0984s
Epoch: 5 cost time: 139.5280020236969
Epoch: 5, Steps: 318 | Train Loss: 0.1894561 Vali Loss: 0.1938027 Test Loss: 0.1783760
Validation loss decreased (0.197085 --> 0.193803).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1970678
	speed: 0.3719s/iter; left time: 554.5183s
	iters: 200, epoch: 6 | loss: 0.1742588
	speed: 0.3800s/iter; left time: 528.6044s
	iters: 300, epoch: 6 | loss: 0.2021421
	speed: 0.3737s/iter; left time: 482.5088s
Epoch: 6 cost time: 119.09262681007385
Epoch: 6, Steps: 318 | Train Loss: 0.1871859 Vali Loss: 0.1943418 Test Loss: 0.1781922
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1697184
	speed: 0.3806s/iter; left time: 446.5012s
	iters: 200, epoch: 7 | loss: 0.1554391
	speed: 0.4058s/iter; left time: 435.3771s
	iters: 300, epoch: 7 | loss: 0.1853835
	speed: 0.3836s/iter; left time: 373.2075s
Epoch: 7 cost time: 123.20660209655762
Epoch: 7, Steps: 318 | Train Loss: 0.1859972 Vali Loss: 0.1942174 Test Loss: 0.1774693
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1351891
	speed: 0.3781s/iter; left time: 323.3003s
	iters: 200, epoch: 8 | loss: 0.2251342
	speed: 0.3957s/iter; left time: 298.7349s
	iters: 300, epoch: 8 | loss: 0.1489426
	speed: 0.4001s/iter; left time: 262.0578s
Epoch: 8 cost time: 124.20799136161804
Epoch: 8, Steps: 318 | Train Loss: 0.1857900 Vali Loss: 0.1934836 Test Loss: 0.1775029
Validation loss decreased (0.193803 --> 0.193484).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1896143
	speed: 0.4056s/iter; left time: 217.7877s
	iters: 200, epoch: 9 | loss: 0.1880145
	speed: 0.3819s/iter; left time: 166.8874s
	iters: 300, epoch: 9 | loss: 0.1877941
	speed: 0.3742s/iter; left time: 126.1179s
Epoch: 9 cost time: 123.22710657119751
Epoch: 9, Steps: 318 | Train Loss: 0.1855867 Vali Loss: 0.1952405 Test Loss: 0.1773955
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1792846
	speed: 0.3857s/iter; left time: 84.4619s
	iters: 200, epoch: 10 | loss: 0.1846703
	speed: 0.3775s/iter; left time: 44.9204s
	iters: 300, epoch: 10 | loss: 0.1601724
	speed: 0.3824s/iter; left time: 7.2658s
Epoch: 10 cost time: 122.111732006073
Epoch: 10, Steps: 318 | Train Loss: 0.1857223 Vali Loss: 0.1937487 Test Loss: 0.1773346
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.3895231783390045, mae:0.4853372275829315, rmse:0.624117910861969, mape:0.01739230379462242, mspe:0.0005080592236481607, rse:0.43731096386909485, r2_score:0.791409407272449, acc:0.9826076962053776
corr: [37.50447  37.595814 37.418648 37.525547 37.518288 37.426796 37.55976
 37.569023 37.502186 37.648357]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3068719
	speed: 0.2753s/iter; left time: 848.1632s
	iters: 200, epoch: 1 | loss: 0.2283810
	speed: 0.2274s/iter; left time: 677.8373s
	iters: 300, epoch: 1 | loss: 0.2139391
	speed: 0.1839s/iter; left time: 529.8879s
Epoch: 1 cost time: 71.7859251499176
Epoch: 1, Steps: 318 | Train Loss: 0.3577845 Vali Loss: 0.2343209 Test Loss: 0.2202482
Validation loss decreased (inf --> 0.234321).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2601888
	speed: 0.1986s/iter; left time: 548.7548s
	iters: 200, epoch: 2 | loss: 0.1694157
	speed: 0.1736s/iter; left time: 462.4223s
	iters: 300, epoch: 2 | loss: 0.1852203
	speed: 0.1784s/iter; left time: 457.2642s
Epoch: 2 cost time: 58.3093843460083
Epoch: 2, Steps: 318 | Train Loss: 0.2172597 Vali Loss: 0.2159285 Test Loss: 0.1936588
Validation loss decreased (0.234321 --> 0.215929).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1665680
	speed: 0.1823s/iter; left time: 445.7277s
	iters: 200, epoch: 3 | loss: 0.2375410
	speed: 0.1875s/iter; left time: 439.8033s
	iters: 300, epoch: 3 | loss: 0.1568050
	speed: 0.1862s/iter; left time: 418.0718s
Epoch: 3 cost time: 59.12408423423767
Epoch: 3, Steps: 318 | Train Loss: 0.1983974 Vali Loss: 0.2112471 Test Loss: 0.1895198
Validation loss decreased (0.215929 --> 0.211247).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1971210
	speed: 0.2104s/iter; left time: 447.4390s
	iters: 200, epoch: 4 | loss: 0.1553694
	speed: 0.1824s/iter; left time: 369.6647s
	iters: 300, epoch: 4 | loss: 0.1748328
	speed: 0.1775s/iter; left time: 342.1057s
Epoch: 4 cost time: 60.069095611572266
Epoch: 4, Steps: 318 | Train Loss: 0.1915176 Vali Loss: 0.2084219 Test Loss: 0.1863330
Validation loss decreased (0.211247 --> 0.208422).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1707495
	speed: 0.1820s/iter; left time: 329.1627s
	iters: 200, epoch: 5 | loss: 0.1647852
	speed: 0.1824s/iter; left time: 311.7770s
	iters: 300, epoch: 5 | loss: 0.2758635
	speed: 0.1893s/iter; left time: 304.5977s
Epoch: 5 cost time: 59.31120586395264
Epoch: 5, Steps: 318 | Train Loss: 0.1887749 Vali Loss: 0.2042795 Test Loss: 0.1852956
Validation loss decreased (0.208422 --> 0.204280).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2134533
	speed: 0.2008s/iter; left time: 299.4309s
	iters: 200, epoch: 6 | loss: 0.1904976
	speed: 0.1875s/iter; left time: 260.8710s
	iters: 300, epoch: 6 | loss: 0.2719610
	speed: 0.1712s/iter; left time: 221.0300s
Epoch: 6 cost time: 59.00384831428528
Epoch: 6, Steps: 318 | Train Loss: 0.1872052 Vali Loss: 0.2049475 Test Loss: 0.1843766
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2124640
	speed: 0.2021s/iter; left time: 237.0327s
	iters: 200, epoch: 7 | loss: 0.1627847
	speed: 0.1968s/iter; left time: 211.1564s
	iters: 300, epoch: 7 | loss: 0.1999931
	speed: 0.1948s/iter; left time: 189.5309s
Epoch: 7 cost time: 62.839431285858154
Epoch: 7, Steps: 318 | Train Loss: 0.1861038 Vali Loss: 0.2041570 Test Loss: 0.1837831
Validation loss decreased (0.204280 --> 0.204157).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1284821
	speed: 0.2001s/iter; left time: 171.0557s
	iters: 200, epoch: 8 | loss: 0.2500897
	speed: 0.1968s/iter; left time: 148.5590s
	iters: 300, epoch: 8 | loss: 0.2287622
	speed: 0.1906s/iter; left time: 124.8676s
Epoch: 8 cost time: 62.3668327331543
Epoch: 8, Steps: 318 | Train Loss: 0.1854297 Vali Loss: 0.2043852 Test Loss: 0.1838278
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1636186
	speed: 0.2105s/iter; left time: 113.0648s
	iters: 200, epoch: 9 | loss: 0.1694713
	speed: 0.1821s/iter; left time: 79.5819s
	iters: 300, epoch: 9 | loss: 0.2047140
	speed: 0.1850s/iter; left time: 62.3525s
Epoch: 9 cost time: 61.04568409919739
Epoch: 9, Steps: 318 | Train Loss: 0.1852175 Vali Loss: 0.2042765 Test Loss: 0.1837165
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2304034
	speed: 0.1937s/iter; left time: 42.4305s
	iters: 200, epoch: 10 | loss: 0.2001943
	speed: 0.1964s/iter; left time: 23.3676s
	iters: 300, epoch: 10 | loss: 0.1709183
	speed: 0.1948s/iter; left time: 3.7004s
Epoch: 10 cost time: 61.88823962211609
Epoch: 10, Steps: 318 | Train Loss: 0.1852345 Vali Loss: 0.2035504 Test Loss: 0.1836751
Validation loss decreased (0.204157 --> 0.203550).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.40306776762008667, mae:0.4926602244377136, rmse:0.6348761916160583, mape:0.01765899546444416, mspe:0.0005285455845296383, rse:0.4446396827697754, r2_score:0.7838480037061469, acc:0.9823410045355558
corr: [37.523964 37.567482 37.5063   37.550945 37.38221  37.35563  37.397717
 37.323776 37.18757  37.28356  37.355415 37.426258 37.55307  37.61767
 37.758595]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4003957
	speed: 0.3221s/iter; left time: 992.5221s
	iters: 200, epoch: 1 | loss: 0.2974495
	speed: 0.4151s/iter; left time: 1237.5054s
	iters: 300, epoch: 1 | loss: 0.2580341
	speed: 0.3468s/iter; left time: 999.1012s
Epoch: 1 cost time: 113.74901580810547
Epoch: 1, Steps: 318 | Train Loss: 0.4356222 Vali Loss: 0.2411903 Test Loss: 0.2508935
Validation loss decreased (inf --> 0.241190).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2360611
	speed: 0.3388s/iter; left time: 936.0616s
	iters: 200, epoch: 2 | loss: 0.1984859
	speed: 0.3610s/iter; left time: 961.3395s
	iters: 300, epoch: 2 | loss: 0.1424964
	speed: 0.3491s/iter; left time: 894.6680s
Epoch: 2 cost time: 112.0259313583374
Epoch: 2, Steps: 318 | Train Loss: 0.2352564 Vali Loss: 0.2356276 Test Loss: 0.2310441
Validation loss decreased (0.241190 --> 0.235628).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2094719
	speed: 0.4444s/iter; left time: 1086.4993s
	iters: 200, epoch: 3 | loss: 0.1823007
	speed: 0.4323s/iter; left time: 1013.7203s
	iters: 300, epoch: 3 | loss: 0.2238965
	speed: 0.3958s/iter; left time: 888.4699s
Epoch: 3 cost time: 135.84906649589539
Epoch: 3, Steps: 318 | Train Loss: 0.2186396 Vali Loss: 0.2283922 Test Loss: 0.2236042
Validation loss decreased (0.235628 --> 0.228392).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2316229
	speed: 0.4256s/iter; left time: 905.2818s
	iters: 200, epoch: 4 | loss: 0.1527360
	speed: 0.3919s/iter; left time: 794.3893s
	iters: 300, epoch: 4 | loss: 0.2364933
	speed: 0.4389s/iter; left time: 845.7968s
Epoch: 4 cost time: 133.9247534275055
Epoch: 4, Steps: 318 | Train Loss: 0.2114865 Vali Loss: 0.2252246 Test Loss: 0.2180146
Validation loss decreased (0.228392 --> 0.225225).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2034870
	speed: 0.4691s/iter; left time: 848.6706s
	iters: 200, epoch: 5 | loss: 0.1986751
	speed: 0.4660s/iter; left time: 796.3827s
	iters: 300, epoch: 5 | loss: 0.2407586
	speed: 0.4590s/iter; left time: 738.5491s
Epoch: 5 cost time: 147.45516180992126
Epoch: 5, Steps: 318 | Train Loss: 0.2077632 Vali Loss: 0.2207791 Test Loss: 0.2174037
Validation loss decreased (0.225225 --> 0.220779).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2189766
	speed: 0.4552s/iter; left time: 678.6634s
	iters: 200, epoch: 6 | loss: 0.2818640
	speed: 0.4343s/iter; left time: 604.1386s
	iters: 300, epoch: 6 | loss: 0.2271217
	speed: 0.4563s/iter; left time: 589.1039s
Epoch: 6 cost time: 141.31697154045105
Epoch: 6, Steps: 318 | Train Loss: 0.2061817 Vali Loss: 0.2226218 Test Loss: 0.2150654
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2096866
	speed: 0.4181s/iter; left time: 490.4761s
	iters: 200, epoch: 7 | loss: 0.2025679
	speed: 0.4039s/iter; left time: 433.4172s
	iters: 300, epoch: 7 | loss: 0.1969083
	speed: 0.4101s/iter; left time: 399.0290s
Epoch: 7 cost time: 130.0865204334259
Epoch: 7, Steps: 318 | Train Loss: 0.2048988 Vali Loss: 0.2221943 Test Loss: 0.2149234
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2038476
	speed: 0.4336s/iter; left time: 370.7437s
	iters: 200, epoch: 8 | loss: 0.1751459
	speed: 0.4143s/iter; left time: 312.8112s
	iters: 300, epoch: 8 | loss: 0.2091190
	speed: 0.4248s/iter; left time: 278.2408s
Epoch: 8 cost time: 135.30884623527527
Epoch: 8, Steps: 318 | Train Loss: 0.2046450 Vali Loss: 0.2236182 Test Loss: 0.2149948
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.4770839214324951, mae:0.5381191968917847, rmse:0.6907126307487488, mape:0.01930995285511017, mspe:0.0006278294022195041, rse:0.48346495628356934, r2_score:0.7394785674028912, acc:0.9806900471448898
corr: [37.847916 37.57609  37.48841  37.556057 37.50356  37.48517  37.451443
 37.478855 37.444023 37.3575   37.284435 37.35228  37.307785 37.372334
 37.36461  37.435852 37.338203 37.46782  37.386177 37.660896]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3549702
	speed: 0.2879s/iter; left time: 887.0411s
	iters: 200, epoch: 1 | loss: 0.2803407
	speed: 0.2076s/iter; left time: 618.8328s
	iters: 300, epoch: 1 | loss: 0.2467937
	speed: 0.2845s/iter; left time: 819.6286s
Epoch: 1 cost time: 83.43907880783081
Epoch: 1, Steps: 318 | Train Loss: 0.4064529 Vali Loss: 0.2463769 Test Loss: 0.2568879
Validation loss decreased (inf --> 0.246377).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2445860
	speed: 0.2859s/iter; left time: 789.9186s
	iters: 200, epoch: 2 | loss: 0.2636321
	speed: 0.2707s/iter; left time: 720.9898s
	iters: 300, epoch: 2 | loss: 0.2202185
	speed: 0.2705s/iter; left time: 693.3757s
Epoch: 2 cost time: 87.30930757522583
Epoch: 2, Steps: 318 | Train Loss: 0.2431650 Vali Loss: 0.2354586 Test Loss: 0.2372693
Validation loss decreased (0.246377 --> 0.235459).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2096007
	speed: 0.2940s/iter; left time: 718.7827s
	iters: 200, epoch: 3 | loss: 0.2035668
	speed: 0.2550s/iter; left time: 598.0684s
	iters: 300, epoch: 3 | loss: 0.2569806
	speed: 0.2702s/iter; left time: 606.5474s
Epoch: 3 cost time: 86.67720222473145
Epoch: 3, Steps: 318 | Train Loss: 0.2182701 Vali Loss: 0.2202796 Test Loss: 0.2312409
Validation loss decreased (0.235459 --> 0.220280).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1957485
	speed: 0.3061s/iter; left time: 651.0019s
	iters: 200, epoch: 4 | loss: 0.1972744
	speed: 0.3121s/iter; left time: 632.5385s
	iters: 300, epoch: 4 | loss: 0.1831527
	speed: 0.2775s/iter; left time: 534.8316s
Epoch: 4 cost time: 94.17396688461304
Epoch: 4, Steps: 318 | Train Loss: 0.2096802 Vali Loss: 0.2244737 Test Loss: 0.2289968
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1978427
	speed: 0.2589s/iter; left time: 468.3402s
	iters: 200, epoch: 5 | loss: 0.2769407
	speed: 0.2495s/iter; left time: 426.4213s
	iters: 300, epoch: 5 | loss: 0.2072735
	speed: 0.2727s/iter; left time: 438.7831s
Epoch: 5 cost time: 83.65805315971375
Epoch: 5, Steps: 318 | Train Loss: 0.2049515 Vali Loss: 0.2226166 Test Loss: 0.2274441
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1827867
	speed: 0.2820s/iter; left time: 420.5045s
	iters: 200, epoch: 6 | loss: 0.1731765
	speed: 0.2830s/iter; left time: 393.6073s
	iters: 300, epoch: 6 | loss: 0.2354927
	speed: 0.3010s/iter; left time: 388.6530s
Epoch: 6 cost time: 91.95773887634277
Epoch: 6, Steps: 318 | Train Loss: 0.2041368 Vali Loss: 0.2222104 Test Loss: 0.2274392
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.5074490308761597, mae:0.5542598366737366, rmse:0.7123545408248901, mape:0.019882310181856155, mspe:0.000666822656057775, rse:0.4982801675796509, r2_score:0.7124805068277678, acc:0.9801176898181438
corr: [37.561565 37.339542 37.45364  37.355915 37.236065 37.25663  37.59777
 37.398914 37.415413 37.453693 37.066097 37.484627 37.465508 37.113773
 37.24033  37.198524 37.2222   37.362766 37.446915 37.307682 36.974617
 37.300087 37.70598  37.31291  37.331593]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3270254
	speed: 0.2195s/iter; left time: 673.9368s
	iters: 200, epoch: 1 | loss: 0.2890781
	speed: 0.1695s/iter; left time: 503.6782s
	iters: 300, epoch: 1 | loss: 0.2778485
	speed: 0.1808s/iter; left time: 519.0280s
Epoch: 1 cost time: 60.496392488479614
Epoch: 1, Steps: 317 | Train Loss: 0.4163680 Vali Loss: 0.2600945 Test Loss: 0.2700105
Validation loss decreased (inf --> 0.260095).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2329488
	speed: 0.1989s/iter; left time: 547.8723s
	iters: 200, epoch: 2 | loss: 0.2620538
	speed: 0.2090s/iter; left time: 554.7821s
	iters: 300, epoch: 2 | loss: 0.1763021
	speed: 0.1963s/iter; left time: 501.3704s
Epoch: 2 cost time: 63.83607029914856
Epoch: 2, Steps: 317 | Train Loss: 0.2412655 Vali Loss: 0.2495644 Test Loss: 0.2450823
Validation loss decreased (0.260095 --> 0.249564).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2350267
	speed: 0.1906s/iter; left time: 464.3929s
	iters: 200, epoch: 3 | loss: 0.2178860
	speed: 0.1923s/iter; left time: 449.3891s
	iters: 300, epoch: 3 | loss: 0.2036212
	speed: 0.1784s/iter; left time: 399.1073s
Epoch: 3 cost time: 59.2590765953064
Epoch: 3, Steps: 317 | Train Loss: 0.2225736 Vali Loss: 0.2498914 Test Loss: 0.2398400
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2070180
	speed: 0.2057s/iter; left time: 436.1593s
	iters: 200, epoch: 4 | loss: 0.2281149
	speed: 0.1921s/iter; left time: 388.0653s
	iters: 300, epoch: 4 | loss: 0.2418858
	speed: 0.1971s/iter; left time: 378.3935s
Epoch: 4 cost time: 62.92742848396301
Epoch: 4, Steps: 317 | Train Loss: 0.2134651 Vali Loss: 0.2488117 Test Loss: 0.2355447
Validation loss decreased (0.249564 --> 0.248812).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2306553
	speed: 0.2053s/iter; left time: 370.2393s
	iters: 200, epoch: 5 | loss: 0.1684696
	speed: 0.1960s/iter; left time: 333.8505s
	iters: 300, epoch: 5 | loss: 0.2452073
	speed: 0.2012s/iter; left time: 322.5477s
Epoch: 5 cost time: 64.02409505844116
Epoch: 5, Steps: 317 | Train Loss: 0.2106159 Vali Loss: 0.2485386 Test Loss: 0.2336924
Validation loss decreased (0.248812 --> 0.248539).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2134448
	speed: 0.1879s/iter; left time: 279.2301s
	iters: 200, epoch: 6 | loss: 0.2287847
	speed: 0.2005s/iter; left time: 277.9571s
	iters: 300, epoch: 6 | loss: 0.1919005
	speed: 0.1881s/iter; left time: 241.9096s
Epoch: 6 cost time: 61.46855068206787
Epoch: 6, Steps: 317 | Train Loss: 0.2083768 Vali Loss: 0.2498229 Test Loss: 0.2339998
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1927496
	speed: 0.1964s/iter; left time: 229.5624s
	iters: 200, epoch: 7 | loss: 0.2294392
	speed: 0.2011s/iter; left time: 215.0177s
	iters: 300, epoch: 7 | loss: 0.2031116
	speed: 0.1812s/iter; left time: 175.5812s
Epoch: 7 cost time: 61.268543004989624
Epoch: 7, Steps: 317 | Train Loss: 0.2075003 Vali Loss: 0.2487465 Test Loss: 0.2339309
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2282277
	speed: 0.1914s/iter; left time: 163.0666s
	iters: 200, epoch: 8 | loss: 0.1942758
	speed: 0.1863s/iter; left time: 140.0738s
	iters: 300, epoch: 8 | loss: 0.2160164
	speed: 0.1754s/iter; left time: 114.3632s
Epoch: 8 cost time: 58.8273561000824
Epoch: 8, Steps: 317 | Train Loss: 0.2070247 Vali Loss: 0.2495838 Test Loss: 0.2335411
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.5128287672996521, mae:0.5634326934814453, rmse:0.7161206603050232, mape:0.02019793912768364, mspe:0.0006715533672831953, rse:0.5005548000335693, r2_score:0.7225573422882284, acc:0.9798020608723164
corr: [37.14729  37.167145 37.154713 37.34842  37.40614  37.096016 37.14994
 37.102287 37.10618  37.116127 37.017597 37.157642 37.427376 37.225193
 37.0509   37.026726 37.19553  37.33938  37.312695 37.156914 37.279186
 37.24948  37.15388  37.197147 37.343517 37.448063 37.324745 37.277042
 37.075462 36.868877]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3796852
	speed: 0.3122s/iter; left time: 961.8955s
	iters: 200, epoch: 1 | loss: 0.3180699
	speed: 0.3422s/iter; left time: 1020.0447s
	iters: 300, epoch: 1 | loss: 0.1953301
	speed: 0.3161s/iter; left time: 910.7372s
Epoch: 1 cost time: 102.65014982223511
Epoch: 1, Steps: 318 | Train Loss: 0.3823770 Vali Loss: 0.2207608 Test Loss: 0.1881870
Validation loss decreased (inf --> 0.220761).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2111315
	speed: 0.3346s/iter; left time: 924.6098s
	iters: 200, epoch: 2 | loss: 0.1409805
	speed: 0.3317s/iter; left time: 883.2242s
	iters: 300, epoch: 2 | loss: 0.2334242
	speed: 0.3381s/iter; left time: 866.5641s
Epoch: 2 cost time: 106.61341738700867
Epoch: 2, Steps: 318 | Train Loss: 0.1923591 Vali Loss: 0.1906560 Test Loss: 0.1682154
Validation loss decreased (0.220761 --> 0.190656).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1270446
	speed: 0.3446s/iter; left time: 842.4815s
	iters: 200, epoch: 3 | loss: 0.1910628
	speed: 0.3436s/iter; left time: 805.7814s
	iters: 300, epoch: 3 | loss: 0.1965948
	speed: 0.3230s/iter; left time: 725.0281s
Epoch: 3 cost time: 106.64989995956421
Epoch: 3, Steps: 318 | Train Loss: 0.1775016 Vali Loss: 0.1714690 Test Loss: 0.1569953
Validation loss decreased (0.190656 --> 0.171469).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1203126
	speed: 0.3520s/iter; left time: 748.7908s
	iters: 200, epoch: 4 | loss: 0.1577997
	speed: 0.3254s/iter; left time: 659.5457s
	iters: 300, epoch: 4 | loss: 0.2013962
	speed: 0.3459s/iter; left time: 666.5992s
Epoch: 4 cost time: 108.33533668518066
Epoch: 4, Steps: 318 | Train Loss: 0.1720258 Vali Loss: 0.1733223 Test Loss: 0.1586254
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1686441
	speed: 0.3283s/iter; left time: 593.9114s
	iters: 200, epoch: 5 | loss: 0.1634375
	speed: 0.3338s/iter; left time: 570.4980s
	iters: 300, epoch: 5 | loss: 0.2545497
	speed: 0.3514s/iter; left time: 565.4234s
Epoch: 5 cost time: 107.26169538497925
Epoch: 5, Steps: 318 | Train Loss: 0.1703492 Vali Loss: 0.1680732 Test Loss: 0.1561925
Validation loss decreased (0.171469 --> 0.168073).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1623193
	speed: 0.3286s/iter; left time: 489.8948s
	iters: 200, epoch: 6 | loss: 0.1435236
	speed: 0.3057s/iter; left time: 425.1656s
	iters: 300, epoch: 6 | loss: 0.1591687
	speed: 0.3108s/iter; left time: 401.3064s
Epoch: 6 cost time: 99.87226128578186
Epoch: 6, Steps: 318 | Train Loss: 0.1674952 Vali Loss: 0.1678613 Test Loss: 0.1556881
Validation loss decreased (0.168073 --> 0.167861).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1641984
	speed: 0.3429s/iter; left time: 402.2212s
	iters: 200, epoch: 7 | loss: 0.1499928
	speed: 0.3271s/iter; left time: 350.9486s
	iters: 300, epoch: 7 | loss: 0.1713782
	speed: 0.3402s/iter; left time: 331.0183s
Epoch: 7 cost time: 107.42884826660156
Epoch: 7, Steps: 318 | Train Loss: 0.1675689 Vali Loss: 0.1683985 Test Loss: 0.1556933
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1240551
	speed: 0.3423s/iter; left time: 292.6722s
	iters: 200, epoch: 8 | loss: 0.1781893
	speed: 0.3161s/iter; left time: 238.6534s
	iters: 300, epoch: 8 | loss: 0.1436688
	speed: 0.3342s/iter; left time: 218.9275s
Epoch: 8 cost time: 105.76733922958374
Epoch: 8, Steps: 318 | Train Loss: 0.1668579 Vali Loss: 0.1671472 Test Loss: 0.1554234
Validation loss decreased (0.167861 --> 0.167147).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1955103
	speed: 0.3219s/iter; left time: 172.8591s
	iters: 200, epoch: 9 | loss: 0.1660360
	speed: 0.3146s/iter; left time: 137.4869s
	iters: 300, epoch: 9 | loss: 0.1677569
	speed: 0.3107s/iter; left time: 104.7136s
Epoch: 9 cost time: 100.54581308364868
Epoch: 9, Steps: 318 | Train Loss: 0.1674098 Vali Loss: 0.1688007 Test Loss: 0.1552083
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1705394
	speed: 0.3362s/iter; left time: 73.6311s
	iters: 200, epoch: 10 | loss: 0.1734838
	speed: 0.3368s/iter; left time: 40.0763s
	iters: 300, epoch: 10 | loss: 0.1356878
	speed: 0.3148s/iter; left time: 5.9818s
Epoch: 10 cost time: 104.73619866371155
Epoch: 10, Steps: 318 | Train Loss: 0.1678399 Vali Loss: 0.1674375 Test Loss: 0.1551173
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.34107065200805664, mae:0.45441049337387085, rmse:0.5840125679969788, mape:0.01628001220524311, mspe:0.0004452531866263598, rse:0.409209668636322, r2_score:0.8166726542656232, acc:0.9837199877947569
corr: [37.655285 37.597183 37.496284 37.485157 37.461903 37.392296 37.390007
 37.38893  37.449833 37.325348]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3070880
	speed: 0.3381s/iter; left time: 1041.6391s
	iters: 200, epoch: 1 | loss: 0.2295990
	speed: 0.3413s/iter; left time: 1017.3118s
	iters: 300, epoch: 1 | loss: 0.2143683
	speed: 0.2707s/iter; left time: 779.8757s
Epoch: 1 cost time: 99.52719807624817
Epoch: 1, Steps: 318 | Train Loss: 0.3596476 Vali Loss: 0.2345787 Test Loss: 0.2203180
Validation loss decreased (inf --> 0.234579).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2612381
	speed: 0.2504s/iter; left time: 691.9553s
	iters: 200, epoch: 2 | loss: 0.1675363
	speed: 0.2484s/iter; left time: 661.5268s
	iters: 300, epoch: 2 | loss: 0.1851008
	speed: 0.2441s/iter; left time: 625.5098s
Epoch: 2 cost time: 78.42185592651367
Epoch: 2, Steps: 318 | Train Loss: 0.2170845 Vali Loss: 0.2154885 Test Loss: 0.1929888
Validation loss decreased (0.234579 --> 0.215488).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1654640
	speed: 0.2567s/iter; left time: 627.6641s
	iters: 200, epoch: 3 | loss: 0.2361404
	speed: 0.2800s/iter; left time: 656.5051s
	iters: 300, epoch: 3 | loss: 0.1551123
	speed: 0.2618s/iter; left time: 587.8330s
Epoch: 3 cost time: 84.65734219551086
Epoch: 3, Steps: 318 | Train Loss: 0.1981261 Vali Loss: 0.2112209 Test Loss: 0.1892698
Validation loss decreased (0.215488 --> 0.211221).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1977716
	speed: 0.2849s/iter; left time: 605.8991s
	iters: 200, epoch: 4 | loss: 0.1554154
	speed: 0.2758s/iter; left time: 559.0778s
	iters: 300, epoch: 4 | loss: 0.1752804
	speed: 0.2487s/iter; left time: 479.2812s
Epoch: 4 cost time: 85.61655926704407
Epoch: 4, Steps: 318 | Train Loss: 0.1912664 Vali Loss: 0.2083810 Test Loss: 0.1858851
Validation loss decreased (0.211221 --> 0.208381).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1683260
	speed: 0.2742s/iter; left time: 496.0785s
	iters: 200, epoch: 5 | loss: 0.1642345
	speed: 0.2494s/iter; left time: 426.1991s
	iters: 300, epoch: 5 | loss: 0.2755293
	speed: 0.2680s/iter; left time: 431.1409s
Epoch: 5 cost time: 84.83661246299744
Epoch: 5, Steps: 318 | Train Loss: 0.1885621 Vali Loss: 0.2042741 Test Loss: 0.1848686
Validation loss decreased (0.208381 --> 0.204274).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2135305
	speed: 0.2693s/iter; left time: 401.5539s
	iters: 200, epoch: 6 | loss: 0.1908768
	speed: 0.2434s/iter; left time: 338.5247s
	iters: 300, epoch: 6 | loss: 0.2716382
	speed: 0.2527s/iter; left time: 326.2458s
Epoch: 6 cost time: 81.6048035621643
Epoch: 6, Steps: 318 | Train Loss: 0.1870120 Vali Loss: 0.2049079 Test Loss: 0.1839258
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2124977
	speed: 0.2738s/iter; left time: 321.2251s
	iters: 200, epoch: 7 | loss: 0.1623893
	speed: 0.2813s/iter; left time: 301.8466s
	iters: 300, epoch: 7 | loss: 0.1991726
	speed: 0.2524s/iter; left time: 245.5666s
Epoch: 7 cost time: 85.61763620376587
Epoch: 7, Steps: 318 | Train Loss: 0.1858902 Vali Loss: 0.2041414 Test Loss: 0.1833491
Validation loss decreased (0.204274 --> 0.204141).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1283477
	speed: 0.2631s/iter; left time: 224.9734s
	iters: 200, epoch: 8 | loss: 0.2498089
	speed: 0.2668s/iter; left time: 201.4145s
	iters: 300, epoch: 8 | loss: 0.2277806
	speed: 0.2714s/iter; left time: 177.7360s
Epoch: 8 cost time: 84.57020115852356
Epoch: 8, Steps: 318 | Train Loss: 0.1851884 Vali Loss: 0.2044016 Test Loss: 0.1834211
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1639238
	speed: 0.2447s/iter; left time: 131.3876s
	iters: 200, epoch: 9 | loss: 0.1704331
	speed: 0.2532s/iter; left time: 110.6508s
	iters: 300, epoch: 9 | loss: 0.2062339
	speed: 0.2444s/iter; left time: 82.3489s
Epoch: 9 cost time: 78.94307446479797
Epoch: 9, Steps: 318 | Train Loss: 0.1850217 Vali Loss: 0.2042906 Test Loss: 0.1833045
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2285258
	speed: 0.2833s/iter; left time: 62.0410s
	iters: 200, epoch: 10 | loss: 0.1986023
	speed: 0.2842s/iter; left time: 33.8221s
	iters: 300, epoch: 10 | loss: 0.1705693
	speed: 0.2956s/iter; left time: 5.6169s
Epoch: 10 cost time: 90.7316198348999
Epoch: 10, Steps: 318 | Train Loss: 0.1850374 Vali Loss: 0.2035809 Test Loss: 0.1832637
Validation loss decreased (0.204141 --> 0.203581).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.4021650552749634, mae:0.49174872040748596, rmse:0.6341648697853088, mape:0.017627086490392685, mspe:0.0005276010488159955, rse:0.4441414773464203, r2_score:0.7841545089857836, acc:0.9823729135096073
corr: [37.50826  37.569862 37.513996 37.56432  37.389942 37.354736 37.397068
 37.32657  37.185783 37.28729  37.35917  37.431168 37.56255  37.61085
 37.758205]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.4206838
	speed: 0.4032s/iter; left time: 1242.3325s
	iters: 200, epoch: 1 | loss: 0.2758224
	speed: 0.4454s/iter; left time: 1327.6988s
	iters: 300, epoch: 1 | loss: 0.2463614
	speed: 0.4559s/iter; left time: 1313.3564s
Epoch: 1 cost time: 139.7410249710083
Epoch: 1, Steps: 318 | Train Loss: 0.4515221 Vali Loss: 0.2865122 Test Loss: 0.2675884
Validation loss decreased (inf --> 0.286512).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2234532
	speed: 0.4476s/iter; left time: 1236.6988s
	iters: 200, epoch: 2 | loss: 0.2369726
	speed: 0.4926s/iter; left time: 1311.8425s
	iters: 300, epoch: 2 | loss: 0.1633275
	speed: 0.4590s/iter; left time: 1176.3060s
Epoch: 2 cost time: 148.17899417877197
Epoch: 2, Steps: 318 | Train Loss: 0.2444592 Vali Loss: 0.2695110 Test Loss: 0.2444342
Validation loss decreased (0.286512 --> 0.269511).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2243880
	speed: 0.5287s/iter; left time: 1292.6059s
	iters: 200, epoch: 3 | loss: 0.1924797
	speed: 0.5015s/iter; left time: 1176.0113s
	iters: 300, epoch: 3 | loss: 0.1989423
	speed: 0.5328s/iter; left time: 1196.1895s
Epoch: 3 cost time: 165.68541836738586
Epoch: 3, Steps: 318 | Train Loss: 0.2273251 Vali Loss: 0.2634893 Test Loss: 0.2440693
Validation loss decreased (0.269511 --> 0.263489).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2205794
	speed: 0.5553s/iter; left time: 1181.0827s
	iters: 200, epoch: 4 | loss: 0.1638068
	speed: 0.5235s/iter; left time: 1061.0567s
	iters: 300, epoch: 4 | loss: 0.2203200
	speed: 0.5463s/iter; left time: 1052.6266s
Epoch: 4 cost time: 172.3361461162567
Epoch: 4, Steps: 318 | Train Loss: 0.2200742 Vali Loss: 0.2612379 Test Loss: 0.2413163
Validation loss decreased (0.263489 --> 0.261238).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2065980
	speed: 0.5378s/iter; left time: 972.9549s
	iters: 200, epoch: 5 | loss: 0.2227716
	speed: 0.5520s/iter; left time: 943.3686s
	iters: 300, epoch: 5 | loss: 0.2405956
	speed: 0.5321s/iter; left time: 856.1340s
Epoch: 5 cost time: 172.9115104675293
Epoch: 5, Steps: 318 | Train Loss: 0.2155176 Vali Loss: 0.2583750 Test Loss: 0.2397895
Validation loss decreased (0.261238 --> 0.258375).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2166970
	speed: 0.4706s/iter; left time: 701.7005s
	iters: 200, epoch: 6 | loss: 0.2820183
	speed: 0.4670s/iter; left time: 649.5640s
	iters: 300, epoch: 6 | loss: 0.2420142
	speed: 0.4995s/iter; left time: 644.8115s
Epoch: 6 cost time: 151.7973175048828
Epoch: 6, Steps: 318 | Train Loss: 0.2136537 Vali Loss: 0.2605440 Test Loss: 0.2381589
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1925979
	speed: 0.5237s/iter; left time: 614.3462s
	iters: 200, epoch: 7 | loss: 0.2007319
	speed: 0.4999s/iter; left time: 536.4044s
	iters: 300, epoch: 7 | loss: 0.1964910
	speed: 0.5118s/iter; left time: 498.0182s
Epoch: 7 cost time: 162.3711221218109
Epoch: 7, Steps: 318 | Train Loss: 0.2130616 Vali Loss: 0.2587622 Test Loss: 0.2377135
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2201591
	speed: 0.5594s/iter; left time: 478.3157s
	iters: 200, epoch: 8 | loss: 0.1691757
	speed: 0.5095s/iter; left time: 384.6382s
	iters: 300, epoch: 8 | loss: 0.2261625
	speed: 0.5376s/iter; left time: 352.1005s
Epoch: 8 cost time: 169.51256251335144
Epoch: 8, Steps: 318 | Train Loss: 0.2119299 Vali Loss: 0.2600625 Test Loss: 0.2379943
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.5262085795402527, mae:0.5644814968109131, rmse:0.7254023551940918, mape:0.020278051495552063, mspe:0.0006949953967705369, rse:0.5077460408210754, r2_score:0.7112846865456754, acc:0.9797219485044479
corr: [37.36196  37.400524 37.402344 37.467453 37.44137  37.34243  37.335
 37.33322  37.287636 37.22733  37.102364 37.28054  37.19629  37.229626
 37.405273 37.43659  37.251278 37.11643  36.72289  36.965782]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3255046
	speed: 0.1876s/iter; left time: 577.8423s
	iters: 200, epoch: 1 | loss: 0.2436348
	speed: 0.1752s/iter; left time: 522.3555s
	iters: 300, epoch: 1 | loss: 0.2632686
	speed: 0.1745s/iter; left time: 502.8283s
Epoch: 1 cost time: 57.24235129356384
Epoch: 1, Steps: 318 | Train Loss: 0.3821914 Vali Loss: 0.2345761 Test Loss: 0.2334053
Validation loss decreased (inf --> 0.234576).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1998937
	speed: 0.1921s/iter; left time: 530.8010s
	iters: 200, epoch: 2 | loss: 0.2502187
	speed: 0.1827s/iter; left time: 486.5996s
	iters: 300, epoch: 2 | loss: 0.2067854
	speed: 0.1760s/iter; left time: 450.9833s
Epoch: 2 cost time: 58.50405192375183
Epoch: 2, Steps: 318 | Train Loss: 0.2265648 Vali Loss: 0.2286504 Test Loss: 0.2210658
Validation loss decreased (0.234576 --> 0.228650).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2050422
	speed: 0.1875s/iter; left time: 458.3992s
	iters: 200, epoch: 3 | loss: 0.1734553
	speed: 0.1692s/iter; left time: 396.8782s
	iters: 300, epoch: 3 | loss: 0.2624228
	speed: 0.1691s/iter; left time: 379.7415s
Epoch: 3 cost time: 55.29336214065552
Epoch: 3, Steps: 318 | Train Loss: 0.2101575 Vali Loss: 0.2191416 Test Loss: 0.2113172
Validation loss decreased (0.228650 --> 0.219142).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1793176
	speed: 0.1864s/iter; left time: 396.4869s
	iters: 200, epoch: 4 | loss: 0.1898917
	speed: 0.1756s/iter; left time: 356.0275s
	iters: 300, epoch: 4 | loss: 0.1908679
	speed: 0.1834s/iter; left time: 353.3784s
Epoch: 4 cost time: 57.844011068344116
Epoch: 4, Steps: 318 | Train Loss: 0.2029022 Vali Loss: 0.2219511 Test Loss: 0.2115087
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1831983
	speed: 0.1564s/iter; left time: 282.9164s
	iters: 200, epoch: 5 | loss: 0.2385963
	speed: 0.1633s/iter; left time: 279.0921s
	iters: 300, epoch: 5 | loss: 0.1973103
	speed: 0.1742s/iter; left time: 280.3605s
Epoch: 5 cost time: 52.71301221847534
Epoch: 5, Steps: 318 | Train Loss: 0.1992584 Vali Loss: 0.2193112 Test Loss: 0.2086504
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1742052
	speed: 0.1627s/iter; left time: 242.6134s
	iters: 200, epoch: 6 | loss: 0.1796630
	speed: 0.1584s/iter; left time: 220.3094s
	iters: 300, epoch: 6 | loss: 0.2053681
	speed: 0.1694s/iter; left time: 218.6636s
Epoch: 6 cost time: 52.32122087478638
Epoch: 6, Steps: 318 | Train Loss: 0.1978149 Vali Loss: 0.2197511 Test Loss: 0.2079535
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.46372726559638977, mae:0.5283724665641785, rmse:0.6809752583503723, mape:0.018937965855002403, mspe:0.0006095032440498471, rse:0.47633081674575806, r2_score:0.7435727179940791, acc:0.9810620341449976
corr: [37.158524 37.22373  37.153225 37.110924 37.059505 37.075848 37.032093
 37.113914 37.04259  37.049965 37.151226 37.14319  37.09362  37.04183
 37.09898  37.226025 37.27427  37.515057 37.576256 37.443405 37.21415
 37.2416   37.299965 37.279182 37.356148]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3174911
	speed: 0.2547s/iter; left time: 782.3007s
	iters: 200, epoch: 1 | loss: 0.2911473
	speed: 0.3003s/iter; left time: 892.1221s
	iters: 300, epoch: 1 | loss: 0.2848004
	speed: 0.3293s/iter; left time: 945.3858s
Epoch: 1 cost time: 93.98540472984314
Epoch: 1, Steps: 317 | Train Loss: 0.4128800 Vali Loss: 0.2514208 Test Loss: 0.2546829
Validation loss decreased (inf --> 0.251421).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2274254
	speed: 0.3434s/iter; left time: 945.8480s
	iters: 200, epoch: 2 | loss: 0.2427548
	speed: 0.3527s/iter; left time: 935.9340s
	iters: 300, epoch: 2 | loss: 0.1739170
	speed: 0.3281s/iter; left time: 838.0627s
Epoch: 2 cost time: 108.80334210395813
Epoch: 2, Steps: 317 | Train Loss: 0.2395387 Vali Loss: 0.2278109 Test Loss: 0.2198485
Validation loss decreased (0.251421 --> 0.227811).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2382221
	speed: 0.3541s/iter; left time: 862.8443s
	iters: 200, epoch: 3 | loss: 0.2321022
	speed: 0.3512s/iter; left time: 820.8678s
	iters: 300, epoch: 3 | loss: 0.2217565
	speed: 0.3643s/iter; left time: 814.8746s
Epoch: 3 cost time: 113.29930305480957
Epoch: 3, Steps: 317 | Train Loss: 0.2224127 Vali Loss: 0.2195900 Test Loss: 0.2170571
Validation loss decreased (0.227811 --> 0.219590).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2005668
	speed: 0.3690s/iter; left time: 782.2040s
	iters: 200, epoch: 4 | loss: 0.2242988
	speed: 0.3738s/iter; left time: 755.1143s
	iters: 300, epoch: 4 | loss: 0.2169012
	speed: 0.3667s/iter; left time: 704.1559s
Epoch: 4 cost time: 117.22168135643005
Epoch: 4, Steps: 317 | Train Loss: 0.2139396 Vali Loss: 0.2132641 Test Loss: 0.2149282
Validation loss decreased (0.219590 --> 0.213264).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2184525
	speed: 0.3502s/iter; left time: 631.4942s
	iters: 200, epoch: 5 | loss: 0.1557061
	speed: 0.3525s/iter; left time: 600.2413s
	iters: 300, epoch: 5 | loss: 0.2108428
	speed: 0.3670s/iter; left time: 588.3523s
Epoch: 5 cost time: 113.17771625518799
Epoch: 5, Steps: 317 | Train Loss: 0.2109827 Vali Loss: 0.2155798 Test Loss: 0.2155197
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.2459583
	speed: 0.3475s/iter; left time: 516.4490s
	iters: 200, epoch: 6 | loss: 0.2149194
	speed: 0.3543s/iter; left time: 491.0128s
	iters: 300, epoch: 6 | loss: 0.1933337
	speed: 0.3576s/iter; left time: 459.8704s
Epoch: 6 cost time: 112.393563747406
Epoch: 6, Steps: 317 | Train Loss: 0.2094689 Vali Loss: 0.2141119 Test Loss: 0.2150353
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1952160
	speed: 0.4165s/iter; left time: 486.8649s
	iters: 200, epoch: 7 | loss: 0.2326623
	speed: 0.4255s/iter; left time: 454.8638s
	iters: 300, epoch: 7 | loss: 0.2227430
	speed: 0.4478s/iter; left time: 433.9256s
Epoch: 7 cost time: 135.73181247711182
Epoch: 7, Steps: 317 | Train Loss: 0.2080647 Vali Loss: 0.2132339 Test Loss: 0.2142313
Validation loss decreased (0.213264 --> 0.213234).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2223504
	speed: 0.4222s/iter; left time: 359.7365s
	iters: 200, epoch: 8 | loss: 0.1802099
	speed: 0.4318s/iter; left time: 324.7001s
	iters: 300, epoch: 8 | loss: 0.2171189
	speed: 0.3623s/iter; left time: 236.2370s
Epoch: 8 cost time: 128.41863369941711
Epoch: 8, Steps: 317 | Train Loss: 0.2078919 Vali Loss: 0.2133356 Test Loss: 0.2142092
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2247890
	speed: 0.3497s/iter; left time: 187.1156s
	iters: 200, epoch: 9 | loss: 0.2350177
	speed: 0.3686s/iter; left time: 160.3532s
	iters: 300, epoch: 9 | loss: 0.2216788
	speed: 0.3439s/iter; left time: 115.2017s
Epoch: 9 cost time: 111.9127607345581
Epoch: 9, Steps: 317 | Train Loss: 0.2076396 Vali Loss: 0.2127138 Test Loss: 0.2140796
Validation loss decreased (0.213234 --> 0.212714).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1839621
	speed: 0.3501s/iter; left time: 76.3299s
	iters: 200, epoch: 10 | loss: 0.1901061
	speed: 0.3568s/iter; left time: 42.1039s
	iters: 300, epoch: 10 | loss: 0.2130114
	speed: 0.3786s/iter; left time: 6.8145s
Epoch: 10 cost time: 113.88689827919006
Epoch: 10, Steps: 317 | Train Loss: 0.2073242 Vali Loss: 0.2123989 Test Loss: 0.2140159
Validation loss decreased (0.212714 --> 0.212399).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.4696494936943054, mae:0.5316700339317322, rmse:0.6853097677230835, mape:0.01907249353826046, mspe:0.0006190231069922447, rse:0.47901856899261475, r2_score:0.7458694527064285, acc:0.9809275064617395
corr: [37.163784 36.998985 37.13577  37.11358  37.29885  37.262733 37.26391
 37.276573 37.241074 37.202785 37.191357 37.099    37.161858 37.03972
 37.256046 37.628143 37.441906 37.410606 37.411903 37.297012 37.33909
 37.422665 37.38135  37.483078 37.751842 37.953064 37.616024 37.793465
 37.42866  37.396915]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3108238
	speed: 0.1475s/iter; left time: 454.5774s
	iters: 200, epoch: 1 | loss: 0.2274889
	speed: 0.1218s/iter; left time: 363.1466s
	iters: 300, epoch: 1 | loss: 0.1898920
	speed: 0.1258s/iter; left time: 362.3608s
Epoch: 1 cost time: 41.78209161758423
Epoch: 1, Steps: 318 | Train Loss: 0.3396671 Vali Loss: 0.1820261 Test Loss: 0.2826809
Validation loss decreased (inf --> 0.182026).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1869909
	speed: 0.1074s/iter; left time: 296.7022s
	iters: 200, epoch: 2 | loss: 0.1813338
	speed: 0.1054s/iter; left time: 280.5570s
	iters: 300, epoch: 2 | loss: 0.1331847
	speed: 0.1098s/iter; left time: 281.3562s
Epoch: 2 cost time: 34.51879930496216
Epoch: 2, Steps: 318 | Train Loss: 0.1789505 Vali Loss: 0.1627228 Test Loss: 0.2519562
Validation loss decreased (0.182026 --> 0.162723).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1748487
	speed: 0.1397s/iter; left time: 341.6355s
	iters: 200, epoch: 3 | loss: 0.1766078
	speed: 0.1266s/iter; left time: 296.9555s
	iters: 300, epoch: 3 | loss: 0.1268773
	speed: 0.1300s/iter; left time: 291.9550s
Epoch: 3 cost time: 42.203848361968994
Epoch: 3, Steps: 318 | Train Loss: 0.1641527 Vali Loss: 0.1587806 Test Loss: 0.2361481
Validation loss decreased (0.162723 --> 0.158781).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1419707
	speed: 0.1351s/iter; left time: 287.4391s
	iters: 200, epoch: 4 | loss: 0.1435792
	speed: 0.1263s/iter; left time: 256.1107s
	iters: 300, epoch: 4 | loss: 0.1897157
	speed: 0.1228s/iter; left time: 236.5414s
Epoch: 4 cost time: 41.023287296295166
Epoch: 4, Steps: 318 | Train Loss: 0.1585833 Vali Loss: 0.1545445 Test Loss: 0.2339434
Validation loss decreased (0.158781 --> 0.154545).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1894792
	speed: 0.1292s/iter; left time: 233.6795s
	iters: 200, epoch: 5 | loss: 0.1500698
	speed: 0.1342s/iter; left time: 229.4245s
	iters: 300, epoch: 5 | loss: 0.1420591
	speed: 0.1279s/iter; left time: 205.8102s
Epoch: 5 cost time: 41.57034730911255
Epoch: 5, Steps: 318 | Train Loss: 0.1549918 Vali Loss: 0.1526721 Test Loss: 0.2310523
Validation loss decreased (0.154545 --> 0.152672).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1401158
	speed: 0.1350s/iter; left time: 201.2874s
	iters: 200, epoch: 6 | loss: 0.1111633
	speed: 0.1349s/iter; left time: 187.7069s
	iters: 300, epoch: 6 | loss: 0.1625019
	speed: 0.1262s/iter; left time: 162.9395s
Epoch: 6 cost time: 41.716227531433105
Epoch: 6, Steps: 318 | Train Loss: 0.1540555 Vali Loss: 0.1509511 Test Loss: 0.2302112
Validation loss decreased (0.152672 --> 0.150951).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1380655
	speed: 0.1220s/iter; left time: 143.1574s
	iters: 200, epoch: 7 | loss: 0.1650286
	speed: 0.1211s/iter; left time: 129.9749s
	iters: 300, epoch: 7 | loss: 0.1756957
	speed: 0.1241s/iter; left time: 120.7103s
Epoch: 7 cost time: 38.92117500305176
Epoch: 7, Steps: 318 | Train Loss: 0.1534592 Vali Loss: 0.1527232 Test Loss: 0.2289876
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1266367
	speed: 0.1397s/iter; left time: 119.4044s
	iters: 200, epoch: 8 | loss: 0.1130393
	speed: 0.1232s/iter; left time: 92.9934s
	iters: 300, epoch: 8 | loss: 0.1530314
	speed: 0.1297s/iter; left time: 84.9703s
Epoch: 8 cost time: 41.818700313568115
Epoch: 8, Steps: 318 | Train Loss: 0.1526141 Vali Loss: 0.1507696 Test Loss: 0.2291586
Validation loss decreased (0.150951 --> 0.150770).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1727778
	speed: 0.1227s/iter; left time: 65.8876s
	iters: 200, epoch: 9 | loss: 0.1623134
	speed: 0.1135s/iter; left time: 49.5925s
	iters: 300, epoch: 9 | loss: 0.1583144
	speed: 0.1185s/iter; left time: 39.9351s
Epoch: 9 cost time: 37.62414908409119
Epoch: 9, Steps: 318 | Train Loss: 0.1527970 Vali Loss: 0.1513023 Test Loss: 0.2294657
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1144326
	speed: 0.1461s/iter; left time: 31.9890s
	iters: 200, epoch: 10 | loss: 0.2330346
	speed: 0.1323s/iter; left time: 15.7485s
	iters: 300, epoch: 10 | loss: 0.1466957
	speed: 0.1445s/iter; left time: 2.7459s
Epoch: 10 cost time: 44.94279336929321
Epoch: 10, Steps: 318 | Train Loss: 0.1527019 Vali Loss: 0.1500239 Test Loss: 0.2290070
Validation loss decreased (0.150770 --> 0.150024).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.5025469660758972, mae:0.5209028720855713, rmse:0.7089054584503174, mape:0.01856735534965992, mspe:0.0006404232699424028, rse:0.49672043323516846, r2_score:0.7287084558816308, acc:0.9814326446503401
corr: [36.072895 35.84951  36.248264 36.15879  35.91609  36.111748 36.099514
 36.154583 36.33935  36.208347]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3695303
	speed: 0.2918s/iter; left time: 899.0186s
	iters: 200, epoch: 1 | loss: 0.1913672
	speed: 0.2933s/iter; left time: 874.2493s
	iters: 300, epoch: 1 | loss: 0.2761619
	speed: 0.2836s/iter; left time: 816.9519s
Epoch: 1 cost time: 92.32093596458435
Epoch: 1, Steps: 318 | Train Loss: 0.3482314 Vali Loss: 0.2048090 Test Loss: 0.1904866
Validation loss decreased (inf --> 0.204809).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1962823
	speed: 0.2095s/iter; left time: 578.8984s
	iters: 200, epoch: 2 | loss: 0.1222341
	speed: 0.2099s/iter; left time: 558.8709s
	iters: 300, epoch: 2 | loss: 0.1551341
	speed: 0.2091s/iter; left time: 535.8765s
Epoch: 2 cost time: 66.58812499046326
Epoch: 2, Steps: 318 | Train Loss: 0.1963583 Vali Loss: 0.1831330 Test Loss: 0.1720070
Validation loss decreased (0.204809 --> 0.183133).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1862357
	speed: 0.2081s/iter; left time: 508.8214s
	iters: 200, epoch: 3 | loss: 0.2371928
	speed: 0.2142s/iter; left time: 502.3838s
	iters: 300, epoch: 3 | loss: 0.1688118
	speed: 0.2037s/iter; left time: 457.2156s
Epoch: 3 cost time: 66.13131284713745
Epoch: 3, Steps: 318 | Train Loss: 0.1807586 Vali Loss: 0.1820451 Test Loss: 0.1695492
Validation loss decreased (0.183133 --> 0.182045).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1774288
	speed: 0.2242s/iter; left time: 476.9101s
	iters: 200, epoch: 4 | loss: 0.1547840
	speed: 0.2074s/iter; left time: 420.4647s
	iters: 300, epoch: 4 | loss: 0.1795602
	speed: 0.1958s/iter; left time: 377.3557s
Epoch: 4 cost time: 66.69583296775818
Epoch: 4, Steps: 318 | Train Loss: 0.1763181 Vali Loss: 0.1764642 Test Loss: 0.1649331
Validation loss decreased (0.182045 --> 0.176464).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1453277
	speed: 0.2124s/iter; left time: 384.2154s
	iters: 200, epoch: 5 | loss: 0.1284975
	speed: 0.2165s/iter; left time: 370.0611s
	iters: 300, epoch: 5 | loss: 0.1898669
	speed: 0.2298s/iter; left time: 369.7046s
Epoch: 5 cost time: 69.5006456375122
Epoch: 5, Steps: 318 | Train Loss: 0.1733371 Vali Loss: 0.1752791 Test Loss: 0.1641632
Validation loss decreased (0.176464 --> 0.175279).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1841463
	speed: 0.2096s/iter; left time: 312.4827s
	iters: 200, epoch: 6 | loss: 0.2070966
	speed: 0.1969s/iter; left time: 273.9301s
	iters: 300, epoch: 6 | loss: 0.1996156
	speed: 0.1923s/iter; left time: 248.2050s
Epoch: 6 cost time: 63.74158525466919
Epoch: 6, Steps: 318 | Train Loss: 0.1721834 Vali Loss: 0.1730290 Test Loss: 0.1629970
Validation loss decreased (0.175279 --> 0.173029).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1678808
	speed: 0.2118s/iter; left time: 248.4115s
	iters: 200, epoch: 7 | loss: 0.2057594
	speed: 0.2026s/iter; left time: 217.4123s
	iters: 300, epoch: 7 | loss: 0.1475372
	speed: 0.2038s/iter; left time: 198.2906s
Epoch: 7 cost time: 65.44585824012756
Epoch: 7, Steps: 318 | Train Loss: 0.1710145 Vali Loss: 0.1735231 Test Loss: 0.1630716
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1762845
	speed: 0.2127s/iter; left time: 181.8343s
	iters: 200, epoch: 8 | loss: 0.1513854
	speed: 0.2185s/iter; left time: 164.9445s
	iters: 300, epoch: 8 | loss: 0.1822778
	speed: 0.2203s/iter; left time: 144.2977s
Epoch: 8 cost time: 68.91511511802673
Epoch: 8, Steps: 318 | Train Loss: 0.1712415 Vali Loss: 0.1720185 Test Loss: 0.1622128
Validation loss decreased (0.173029 --> 0.172019).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1982731
	speed: 0.2196s/iter; left time: 117.9132s
	iters: 200, epoch: 9 | loss: 0.1459542
	speed: 0.2157s/iter; left time: 94.2433s
	iters: 300, epoch: 9 | loss: 0.1721572
	speed: 0.2204s/iter; left time: 74.2699s
Epoch: 9 cost time: 69.53012943267822
Epoch: 9, Steps: 318 | Train Loss: 0.1706190 Vali Loss: 0.1730591 Test Loss: 0.1622583
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1859247
	speed: 0.2154s/iter; left time: 47.1721s
	iters: 200, epoch: 10 | loss: 0.1672435
	speed: 0.2206s/iter; left time: 26.2561s
	iters: 300, epoch: 10 | loss: 0.1558469
	speed: 0.1976s/iter; left time: 3.7546s
Epoch: 10 cost time: 67.34571862220764
Epoch: 10, Steps: 318 | Train Loss: 0.1706497 Vali Loss: 0.1721340 Test Loss: 0.1622373
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.35596972703933716, mae:0.46452999114990234, rmse:0.5966320037841797, mape:0.016603641211986542, mspe:0.00046076637227088213, rse:0.41785508394241333, r2_score:0.8065283098920829, acc:0.9833963587880135
corr: [36.802444 36.741398 36.947758 37.045555 36.779835 36.828735 36.85795
 36.85974  36.960205 37.042686 37.185608 37.30479  37.57894  37.450184
 37.35036 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2461215
	speed: 0.2225s/iter; left time: 685.4158s
	iters: 200, epoch: 1 | loss: 0.1884351
	speed: 0.2062s/iter; left time: 614.7324s
	iters: 300, epoch: 1 | loss: 0.1609650
	speed: 0.2095s/iter; left time: 603.5025s
Epoch: 1 cost time: 67.57857918739319
Epoch: 1, Steps: 318 | Train Loss: 0.3264258 Vali Loss: 0.2058006 Test Loss: 0.1938484
Validation loss decreased (inf --> 0.205801).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1844739
	speed: 0.2138s/iter; left time: 590.6964s
	iters: 200, epoch: 2 | loss: 0.1849159
	speed: 0.2102s/iter; left time: 559.7408s
	iters: 300, epoch: 2 | loss: 0.3229499
	speed: 0.2119s/iter; left time: 543.0774s
Epoch: 2 cost time: 67.30079174041748
Epoch: 2, Steps: 318 | Train Loss: 0.2014098 Vali Loss: 0.2019893 Test Loss: 0.1857766
Validation loss decreased (0.205801 --> 0.201989).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1641554
	speed: 0.2135s/iter; left time: 521.9342s
	iters: 200, epoch: 3 | loss: 0.2568472
	speed: 0.2105s/iter; left time: 493.5270s
	iters: 300, epoch: 3 | loss: 0.1968945
	speed: 0.2041s/iter; left time: 458.2410s
Epoch: 3 cost time: 66.36620569229126
Epoch: 3, Steps: 318 | Train Loss: 0.1904251 Vali Loss: 0.1925407 Test Loss: 0.1809561
Validation loss decreased (0.201989 --> 0.192541).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1809862
	speed: 0.2834s/iter; left time: 602.7887s
	iters: 200, epoch: 4 | loss: 0.1976561
	speed: 0.2780s/iter; left time: 563.4402s
	iters: 300, epoch: 4 | loss: 0.1663619
	speed: 0.2814s/iter; left time: 542.3398s
Epoch: 4 cost time: 89.13489294052124
Epoch: 4, Steps: 318 | Train Loss: 0.1858808 Vali Loss: 0.1943860 Test Loss: 0.1801395
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1639445
	speed: 0.2982s/iter; left time: 539.4594s
	iters: 200, epoch: 5 | loss: 0.1794758
	speed: 0.2766s/iter; left time: 472.6307s
	iters: 300, epoch: 5 | loss: 0.2562151
	speed: 0.2934s/iter; left time: 472.1124s
Epoch: 5 cost time: 91.86331605911255
Epoch: 5, Steps: 318 | Train Loss: 0.1833876 Vali Loss: 0.1937333 Test Loss: 0.1784734
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1739114
	speed: 0.2908s/iter; left time: 433.5905s
	iters: 200, epoch: 6 | loss: 0.2036387
	speed: 0.2681s/iter; left time: 372.9102s
	iters: 300, epoch: 6 | loss: 0.1767036
	speed: 0.2771s/iter; left time: 357.7022s
Epoch: 6 cost time: 88.9498381614685
Epoch: 6, Steps: 318 | Train Loss: 0.1818754 Vali Loss: 0.1939535 Test Loss: 0.1783954
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.39710113406181335, mae:0.48911502957344055, rmse:0.6301596164703369, mape:0.017484350129961967, mspe:0.0005152533412910998, rse:0.44108080863952637, r2_score:0.7700091748940132, acc:0.982515649870038
corr: [36.441784 36.579884 36.64228  36.558704 36.457664 36.36575  36.417706
 36.504242 36.503136 36.51603  36.69889  36.502148 36.775715 36.63307
 36.71717  36.695343 36.740116 36.788517 37.036938 37.435497]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3334706
	speed: 0.1429s/iter; left time: 440.3529s
	iters: 200, epoch: 1 | loss: 0.2616481
	speed: 0.1207s/iter; left time: 359.7241s
	iters: 300, epoch: 1 | loss: 0.2212272
	speed: 0.1397s/iter; left time: 402.5972s
Epoch: 1 cost time: 42.72061491012573
Epoch: 1, Steps: 318 | Train Loss: 0.3575515 Vali Loss: 0.2027390 Test Loss: 0.2314231
Validation loss decreased (inf --> 0.202739).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2930174
	speed: 0.1426s/iter; left time: 394.1417s
	iters: 200, epoch: 2 | loss: 0.2414742
	speed: 0.1288s/iter; left time: 342.8675s
	iters: 300, epoch: 2 | loss: 0.1848851
	speed: 0.1292s/iter; left time: 331.0284s
Epoch: 2 cost time: 42.46347188949585
Epoch: 2, Steps: 318 | Train Loss: 0.2156121 Vali Loss: 0.1992809 Test Loss: 0.2194568
Validation loss decreased (0.202739 --> 0.199281).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1949505
	speed: 0.1386s/iter; left time: 338.7865s
	iters: 200, epoch: 3 | loss: 0.2284892
	speed: 0.1317s/iter; left time: 308.8982s
	iters: 300, epoch: 3 | loss: 0.2124863
	speed: 0.1327s/iter; left time: 297.8925s
Epoch: 3 cost time: 42.82362222671509
Epoch: 3, Steps: 318 | Train Loss: 0.2019842 Vali Loss: 0.2012344 Test Loss: 0.2189074
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2115324
	speed: 0.1362s/iter; left time: 289.6892s
	iters: 200, epoch: 4 | loss: 0.1902383
	speed: 0.1209s/iter; left time: 245.0432s
	iters: 300, epoch: 4 | loss: 0.2067904
	speed: 0.1293s/iter; left time: 249.2125s
Epoch: 4 cost time: 41.08485507965088
Epoch: 4, Steps: 318 | Train Loss: 0.1963053 Vali Loss: 0.2020502 Test Loss: 0.2198394
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2107635
	speed: 0.1353s/iter; left time: 244.7169s
	iters: 200, epoch: 5 | loss: 0.2245119
	speed: 0.1239s/iter; left time: 211.7437s
	iters: 300, epoch: 5 | loss: 0.1920041
	speed: 0.1322s/iter; left time: 212.7818s
Epoch: 5 cost time: 41.51845407485962
Epoch: 5, Steps: 318 | Train Loss: 0.1939897 Vali Loss: 0.2015602 Test Loss: 0.2213379
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000031
Early stopping
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.4815894663333893, mae:0.5364581942558289, rmse:0.6939664483070374, mape:0.019182521849870682, mspe:0.0006225972902029753, rse:0.4854179918766022, r2_score:0.7305716323905631, acc:0.9808174781501293
corr: [36.282085 36.46179  36.261314 36.377804 36.57251  36.37675  36.3998
 36.41665  36.462578 36.616013 36.577175 36.742737 36.826767 36.747
 36.983326 37.041496 37.050694 36.996223 37.12535  37.07813  37.33618
 37.33698  37.520287 37.378468 37.885696]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3153891
	speed: 0.2147s/iter; left time: 659.2787s
	iters: 200, epoch: 1 | loss: 0.2518439
	speed: 0.2139s/iter; left time: 635.6057s
	iters: 300, epoch: 1 | loss: 0.2102149
	speed: 0.2195s/iter; left time: 630.2660s
Epoch: 1 cost time: 68.61244368553162
Epoch: 1, Steps: 317 | Train Loss: 0.3589784 Vali Loss: 0.2219286 Test Loss: 0.2338074
Validation loss decreased (inf --> 0.221929).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2432663
	speed: 0.2179s/iter; left time: 600.1542s
	iters: 200, epoch: 2 | loss: 0.1656473
	speed: 0.2160s/iter; left time: 573.1620s
	iters: 300, epoch: 2 | loss: 0.2154576
	speed: 0.2126s/iter; left time: 542.9746s
Epoch: 2 cost time: 68.03144335746765
Epoch: 2, Steps: 317 | Train Loss: 0.2263965 Vali Loss: 0.2067060 Test Loss: 0.2682969
Validation loss decreased (0.221929 --> 0.206706).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2153597
	speed: 0.2250s/iter; left time: 548.2694s
	iters: 200, epoch: 3 | loss: 0.1928984
	speed: 0.2275s/iter; left time: 531.5507s
	iters: 300, epoch: 3 | loss: 0.2198459
	speed: 0.2215s/iter; left time: 495.5545s
Epoch: 3 cost time: 71.05082535743713
Epoch: 3, Steps: 317 | Train Loss: 0.2087970 Vali Loss: 0.2051588 Test Loss: 0.2692305
Validation loss decreased (0.206706 --> 0.205159).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1772755
	speed: 0.2238s/iter; left time: 474.5455s
	iters: 200, epoch: 4 | loss: 0.2078155
	speed: 0.2033s/iter; left time: 410.6167s
	iters: 300, epoch: 4 | loss: 0.1914934
	speed: 0.2013s/iter; left time: 386.5708s
Epoch: 4 cost time: 66.660001039505
Epoch: 4, Steps: 317 | Train Loss: 0.2025982 Vali Loss: 0.2026962 Test Loss: 0.2674019
Validation loss decreased (0.205159 --> 0.202696).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1935001
	speed: 0.2012s/iter; left time: 362.7508s
	iters: 200, epoch: 5 | loss: 0.2003450
	speed: 0.2142s/iter; left time: 364.7298s
	iters: 300, epoch: 5 | loss: 0.1807676
	speed: 0.2028s/iter; left time: 325.0840s
Epoch: 5 cost time: 65.37230515480042
Epoch: 5, Steps: 317 | Train Loss: 0.1998121 Vali Loss: 0.2028960 Test Loss: 0.2687913
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1758073
	speed: 0.1946s/iter; left time: 289.1913s
	iters: 200, epoch: 6 | loss: 0.1740200
	speed: 0.1973s/iter; left time: 273.4904s
	iters: 300, epoch: 6 | loss: 0.1760168
	speed: 0.2124s/iter; left time: 273.1055s
Epoch: 6 cost time: 63.81793403625488
Epoch: 6, Steps: 317 | Train Loss: 0.1984048 Vali Loss: 0.2013948 Test Loss: 0.2707361
Validation loss decreased (0.202696 --> 0.201395).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1832964
	speed: 0.2006s/iter; left time: 234.5103s
	iters: 200, epoch: 7 | loss: 0.2111418
	speed: 0.1973s/iter; left time: 210.8651s
	iters: 300, epoch: 7 | loss: 0.1712193
	speed: 0.1894s/iter; left time: 183.5135s
Epoch: 7 cost time: 61.97417330741882
Epoch: 7, Steps: 317 | Train Loss: 0.1980423 Vali Loss: 0.2028120 Test Loss: 0.2701995
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1984208
	speed: 0.1952s/iter; left time: 166.3025s
	iters: 200, epoch: 8 | loss: 0.2276461
	speed: 0.1909s/iter; left time: 143.5388s
	iters: 300, epoch: 8 | loss: 0.2231407
	speed: 0.2658s/iter; left time: 173.2728s
Epoch: 8 cost time: 69.78718686103821
Epoch: 8, Steps: 317 | Train Loss: 0.1970583 Vali Loss: 0.2028068 Test Loss: 0.2706382
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2053223
	speed: 0.2622s/iter; left time: 140.2897s
	iters: 200, epoch: 9 | loss: 0.1915952
	speed: 0.2533s/iter; left time: 110.1806s
	iters: 300, epoch: 9 | loss: 0.1966563
	speed: 0.2627s/iter; left time: 88.0108s
Epoch: 9 cost time: 82.20150232315063
Epoch: 9, Steps: 317 | Train Loss: 0.1974353 Vali Loss: 0.2029285 Test Loss: 0.2715793
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.5941197276115417, mae:0.5937831401824951, rmse:0.7707916498184204, mape:0.021054521203041077, mspe:0.0007447253447026014, rse:0.5387688279151917, r2_score:0.6046533548624242, acc:0.9789454787969589
corr: [35.23723  35.14568  35.0062   35.855717 35.17678  35.273487 35.486076
 35.231926 35.12684  35.254883 35.355255 35.045647 35.299168 35.309116
 35.285618 35.196194 35.983494 35.920998 35.61373  35.814045 36.127995
 36.222164 36.0851   35.88528  35.68634  36.013    35.790016 35.33436
 35.768513 35.17098 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3249156
	speed: 0.1707s/iter; left time: 525.8660s
	iters: 200, epoch: 1 | loss: 0.2335159
	speed: 0.1355s/iter; left time: 403.8231s
	iters: 300, epoch: 1 | loss: 0.2109860
	speed: 0.1447s/iter; left time: 416.8330s
Epoch: 1 cost time: 47.971182346343994
Epoch: 1, Steps: 318 | Train Loss: 0.3356549 Vali Loss: 0.1864997 Test Loss: 0.1670000
Validation loss decreased (inf --> 0.186500).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1898150
	speed: 0.1525s/iter; left time: 421.4823s
	iters: 200, epoch: 2 | loss: 0.1573028
	speed: 0.1469s/iter; left time: 391.2545s
	iters: 300, epoch: 2 | loss: 0.1441592
	speed: 0.1389s/iter; left time: 355.9822s
Epoch: 2 cost time: 46.2628915309906
Epoch: 2, Steps: 318 | Train Loss: 0.1826447 Vali Loss: 0.1645290 Test Loss: 0.1501288
Validation loss decreased (0.186500 --> 0.164529).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1766889
	speed: 0.1426s/iter; left time: 348.7019s
	iters: 200, epoch: 3 | loss: 0.2012328
	speed: 0.1396s/iter; left time: 327.4591s
	iters: 300, epoch: 3 | loss: 0.1313624
	speed: 0.1323s/iter; left time: 297.0150s
Epoch: 3 cost time: 44.06779932975769
Epoch: 3, Steps: 318 | Train Loss: 0.1659144 Vali Loss: 0.1629617 Test Loss: 0.1471384
Validation loss decreased (0.164529 --> 0.162962).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1435551
	speed: 0.1508s/iter; left time: 320.6528s
	iters: 200, epoch: 4 | loss: 0.1490838
	speed: 0.1517s/iter; left time: 307.5448s
	iters: 300, epoch: 4 | loss: 0.1924333
	speed: 0.1382s/iter; left time: 266.2510s
Epoch: 4 cost time: 46.68645882606506
Epoch: 4, Steps: 318 | Train Loss: 0.1603305 Vali Loss: 0.1571805 Test Loss: 0.1402711
Validation loss decreased (0.162962 --> 0.157181).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1804499
	speed: 0.1453s/iter; left time: 262.8435s
	iters: 200, epoch: 5 | loss: 0.1586352
	speed: 0.1461s/iter; left time: 249.6939s
	iters: 300, epoch: 5 | loss: 0.1369358
	speed: 0.1413s/iter; left time: 227.4008s
Epoch: 5 cost time: 45.73348140716553
Epoch: 5, Steps: 318 | Train Loss: 0.1571178 Vali Loss: 0.1565728 Test Loss: 0.1394230
Validation loss decreased (0.157181 --> 0.156573).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1313466
	speed: 0.1452s/iter; left time: 216.4519s
	iters: 200, epoch: 6 | loss: 0.1412229
	speed: 0.1395s/iter; left time: 194.1007s
	iters: 300, epoch: 6 | loss: 0.1606696
	speed: 0.1325s/iter; left time: 171.1013s
Epoch: 6 cost time: 44.15760803222656
Epoch: 6, Steps: 318 | Train Loss: 0.1556788 Vali Loss: 0.1548172 Test Loss: 0.1394367
Validation loss decreased (0.156573 --> 0.154817).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1494650
	speed: 0.1342s/iter; left time: 157.3896s
	iters: 200, epoch: 7 | loss: 0.1734272
	speed: 0.1265s/iter; left time: 135.7088s
	iters: 300, epoch: 7 | loss: 0.1847246
	speed: 0.1420s/iter; left time: 138.1418s
Epoch: 7 cost time: 43.05595135688782
Epoch: 7, Steps: 318 | Train Loss: 0.1553494 Vali Loss: 0.1565680 Test Loss: 0.1392604
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1408655
	speed: 0.1480s/iter; left time: 126.5796s
	iters: 200, epoch: 8 | loss: 0.1224857
	speed: 0.1392s/iter; left time: 105.1236s
	iters: 300, epoch: 8 | loss: 0.1500205
	speed: 0.1321s/iter; left time: 86.5229s
Epoch: 8 cost time: 44.37459659576416
Epoch: 8, Steps: 318 | Train Loss: 0.1540789 Vali Loss: 0.1544980 Test Loss: 0.1388046
Validation loss decreased (0.154817 --> 0.154498).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1817617
	speed: 0.1529s/iter; left time: 82.0870s
	iters: 200, epoch: 9 | loss: 0.1494068
	speed: 0.1427s/iter; left time: 62.3739s
	iters: 300, epoch: 9 | loss: 0.1626576
	speed: 0.1390s/iter; left time: 46.8304s
Epoch: 9 cost time: 46.163405895233154
Epoch: 9, Steps: 318 | Train Loss: 0.1541442 Vali Loss: 0.1549799 Test Loss: 0.1389937
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1226346
	speed: 0.1447s/iter; left time: 31.6805s
	iters: 200, epoch: 10 | loss: 0.2331002
	speed: 0.1455s/iter; left time: 17.3124s
	iters: 300, epoch: 10 | loss: 0.1480257
	speed: 0.1414s/iter; left time: 2.6859s
Epoch: 10 cost time: 45.68449521064758
Epoch: 10, Steps: 318 | Train Loss: 0.1546179 Vali Loss: 0.1538246 Test Loss: 0.1390604
Validation loss decreased (0.154498 --> 0.153825).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.30516260862350464, mae:0.42956387996673584, rmse:0.5524152517318726, mape:0.015354683622717857, mspe:0.00039490516064688563, rse:0.38706985116004944, r2_score:0.8397989821338026, acc:0.9846453163772821
corr: [36.927162 37.02328  37.01443  37.19631  37.129757 37.08959  37.1541
 37.287384 37.38033  37.27056 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3842433
	speed: 0.1951s/iter; left time: 601.1637s
	iters: 200, epoch: 1 | loss: 0.1932948
	speed: 0.1721s/iter; left time: 513.0702s
	iters: 300, epoch: 1 | loss: 0.2783217
	speed: 0.1776s/iter; left time: 511.6500s
Epoch: 1 cost time: 57.28596258163452
Epoch: 1, Steps: 318 | Train Loss: 0.3435388 Vali Loss: 0.1943158 Test Loss: 0.1865511
Validation loss decreased (inf --> 0.194316).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2132622
	speed: 0.1768s/iter; left time: 488.3951s
	iters: 200, epoch: 2 | loss: 0.1156271
	speed: 0.1619s/iter; left time: 431.2135s
	iters: 300, epoch: 2 | loss: 0.1493130
	speed: 0.1572s/iter; left time: 402.9298s
Epoch: 2 cost time: 52.50928044319153
Epoch: 2, Steps: 318 | Train Loss: 0.1933183 Vali Loss: 0.1829645 Test Loss: 0.1671960
Validation loss decreased (0.194316 --> 0.182965).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1713460
	speed: 0.1602s/iter; left time: 391.6627s
	iters: 200, epoch: 3 | loss: 0.2106246
	speed: 0.1491s/iter; left time: 349.7292s
	iters: 300, epoch: 3 | loss: 0.1852643
	speed: 0.1612s/iter; left time: 361.8694s
Epoch: 3 cost time: 49.62725067138672
Epoch: 3, Steps: 318 | Train Loss: 0.1792714 Vali Loss: 0.1793493 Test Loss: 0.1629207
Validation loss decreased (0.182965 --> 0.179349).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1617702
	speed: 0.1769s/iter; left time: 376.3537s
	iters: 200, epoch: 4 | loss: 0.1655489
	speed: 0.1752s/iter; left time: 355.2213s
	iters: 300, epoch: 4 | loss: 0.1793565
	speed: 0.1838s/iter; left time: 354.1370s
Epoch: 4 cost time: 57.29649066925049
Epoch: 4, Steps: 318 | Train Loss: 0.1747393 Vali Loss: 0.1776761 Test Loss: 0.1615428
Validation loss decreased (0.179349 --> 0.177676).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1425369
	speed: 0.1664s/iter; left time: 300.9495s
	iters: 200, epoch: 5 | loss: 0.1268741
	speed: 0.1623s/iter; left time: 277.3029s
	iters: 300, epoch: 5 | loss: 0.1868122
	speed: 0.1634s/iter; left time: 262.9062s
Epoch: 5 cost time: 52.15536618232727
Epoch: 5, Steps: 318 | Train Loss: 0.1720875 Vali Loss: 0.1763301 Test Loss: 0.1597769
Validation loss decreased (0.177676 --> 0.176330).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1801181
	speed: 0.1479s/iter; left time: 220.5700s
	iters: 200, epoch: 6 | loss: 0.2058846
	speed: 0.1613s/iter; left time: 224.4355s
	iters: 300, epoch: 6 | loss: 0.1936553
	speed: 0.1713s/iter; left time: 221.1990s
Epoch: 6 cost time: 51.126033306121826
Epoch: 6, Steps: 318 | Train Loss: 0.1706743 Vali Loss: 0.1744729 Test Loss: 0.1590277
Validation loss decreased (0.176330 --> 0.174473).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1568857
	speed: 0.1513s/iter; left time: 177.5329s
	iters: 200, epoch: 7 | loss: 0.1869606
	speed: 0.1669s/iter; left time: 179.1012s
	iters: 300, epoch: 7 | loss: 0.1554871
	speed: 0.1639s/iter; left time: 159.4923s
Epoch: 7 cost time: 51.060028314590454
Epoch: 7, Steps: 318 | Train Loss: 0.1701820 Vali Loss: 0.1745786 Test Loss: 0.1586681
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1532928
	speed: 0.1710s/iter; left time: 146.2098s
	iters: 200, epoch: 8 | loss: 0.1553914
	speed: 0.1507s/iter; left time: 113.7902s
	iters: 300, epoch: 8 | loss: 0.2002000
	speed: 0.1496s/iter; left time: 97.9693s
Epoch: 8 cost time: 49.808499813079834
Epoch: 8, Steps: 318 | Train Loss: 0.1702427 Vali Loss: 0.1730956 Test Loss: 0.1578286
Validation loss decreased (0.174473 --> 0.173096).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2074780
	speed: 0.1661s/iter; left time: 89.1812s
	iters: 200, epoch: 9 | loss: 0.1486686
	speed: 0.1672s/iter; left time: 73.0621s
	iters: 300, epoch: 9 | loss: 0.1762087
	speed: 0.1449s/iter; left time: 48.8402s
Epoch: 9 cost time: 50.69016790390015
Epoch: 9, Steps: 318 | Train Loss: 0.1701286 Vali Loss: 0.1739643 Test Loss: 0.1579635
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1798485
	speed: 0.1612s/iter; left time: 35.2962s
	iters: 200, epoch: 10 | loss: 0.1704344
	speed: 0.1487s/iter; left time: 17.6904s
	iters: 300, epoch: 10 | loss: 0.1351982
	speed: 0.1742s/iter; left time: 3.3098s
Epoch: 10 cost time: 51.41837978363037
Epoch: 10, Steps: 318 | Train Loss: 0.1697908 Vali Loss: 0.1736480 Test Loss: 0.1579237
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.3463486135005951, mae:0.4573795795440674, rmse:0.5885139107704163, mape:0.016359476372599602, mspe:0.0004495980974752456, rse:0.41216957569122314, r2_score:0.8109246861942708, acc:0.9836405236274004
corr: [36.90989  36.92943  36.963314 37.03697  36.872356 36.89155  36.866966
 36.9928   37.084152 37.154034 37.173954 37.226616 37.352215 37.47748
 37.577328]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2725684
	speed: 0.2238s/iter; left time: 689.5827s
	iters: 200, epoch: 1 | loss: 0.1968053
	speed: 0.1916s/iter; left time: 571.0153s
	iters: 300, epoch: 1 | loss: 0.1654836
	speed: 0.1938s/iter; left time: 558.4787s
Epoch: 1 cost time: 64.62653255462646
Epoch: 1, Steps: 318 | Train Loss: 0.3288260 Vali Loss: 0.2011521 Test Loss: 0.1887465
Validation loss decreased (inf --> 0.201152).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1891826
	speed: 0.2114s/iter; left time: 584.2118s
	iters: 200, epoch: 2 | loss: 0.1769476
	speed: 0.2125s/iter; left time: 565.8569s
	iters: 300, epoch: 2 | loss: 0.3195403
	speed: 0.1965s/iter; left time: 503.5287s
Epoch: 2 cost time: 65.40951609611511
Epoch: 2, Steps: 318 | Train Loss: 0.2059628 Vali Loss: 0.1963052 Test Loss: 0.1810227
Validation loss decreased (0.201152 --> 0.196305).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1755857
	speed: 0.2020s/iter; left time: 493.8760s
	iters: 200, epoch: 3 | loss: 0.2589227
	speed: 0.1907s/iter; left time: 447.2451s
	iters: 300, epoch: 3 | loss: 0.1950493
	speed: 0.1893s/iter; left time: 424.9257s
Epoch: 3 cost time: 61.59740924835205
Epoch: 3, Steps: 318 | Train Loss: 0.1934030 Vali Loss: 0.1881118 Test Loss: 0.1751652
Validation loss decreased (0.196305 --> 0.188112).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1899925
	speed: 0.1948s/iter; left time: 414.3129s
	iters: 200, epoch: 4 | loss: 0.2035648
	speed: 0.2189s/iter; left time: 443.6879s
	iters: 300, epoch: 4 | loss: 0.1599582
	speed: 0.1965s/iter; left time: 378.6059s
Epoch: 4 cost time: 64.36292243003845
Epoch: 4, Steps: 318 | Train Loss: 0.1890210 Vali Loss: 0.1889729 Test Loss: 0.1746347
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1815072
	speed: 0.2103s/iter; left time: 380.5005s
	iters: 200, epoch: 5 | loss: 0.1823831
	speed: 0.1843s/iter; left time: 314.9093s
	iters: 300, epoch: 5 | loss: 0.2712406
	speed: 0.1969s/iter; left time: 316.8769s
Epoch: 5 cost time: 62.680962800979614
Epoch: 5, Steps: 318 | Train Loss: 0.1861638 Vali Loss: 0.1881744 Test Loss: 0.1730465
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1808759
	speed: 0.1953s/iter; left time: 291.1576s
	iters: 200, epoch: 6 | loss: 0.2097891
	speed: 0.2050s/iter; left time: 285.0953s
	iters: 300, epoch: 6 | loss: 0.1829219
	speed: 0.2262s/iter; left time: 291.9795s
Epoch: 6 cost time: 67.06698989868164
Epoch: 6, Steps: 318 | Train Loss: 0.1853416 Vali Loss: 0.1880290 Test Loss: 0.1732414
Validation loss decreased (0.188112 --> 0.188029).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1772282
	speed: 0.2388s/iter; left time: 280.0917s
	iters: 200, epoch: 7 | loss: 0.1585470
	speed: 0.2155s/iter; left time: 231.2645s
	iters: 300, epoch: 7 | loss: 0.1998806
	speed: 0.2319s/iter; left time: 225.6508s
Epoch: 7 cost time: 73.28104615211487
Epoch: 7, Steps: 318 | Train Loss: 0.1849267 Vali Loss: 0.1891462 Test Loss: 0.1734477
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2176277
	speed: 0.2394s/iter; left time: 204.6580s
	iters: 200, epoch: 8 | loss: 0.2090402
	speed: 0.1876s/iter; left time: 141.6744s
	iters: 300, epoch: 8 | loss: 0.2038205
	speed: 0.1789s/iter; left time: 117.1604s
Epoch: 8 cost time: 63.97662854194641
Epoch: 8, Steps: 318 | Train Loss: 0.1843310 Vali Loss: 0.1881052 Test Loss: 0.1734211
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1947418
	speed: 0.1856s/iter; left time: 99.6550s
	iters: 200, epoch: 9 | loss: 0.1627963
	speed: 0.1820s/iter; left time: 79.5538s
	iters: 300, epoch: 9 | loss: 0.1429666
	speed: 0.1843s/iter; left time: 62.0996s
Epoch: 9 cost time: 58.77740263938904
Epoch: 9, Steps: 318 | Train Loss: 0.1832933 Vali Loss: 0.1874448 Test Loss: 0.1735296
Validation loss decreased (0.188029 --> 0.187445).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1711903
	speed: 0.1914s/iter; left time: 41.9250s
	iters: 200, epoch: 10 | loss: 0.1997223
	speed: 0.1941s/iter; left time: 23.0942s
	iters: 300, epoch: 10 | loss: 0.2439130
	speed: 0.1811s/iter; left time: 3.4403s
Epoch: 10 cost time: 60.22616195678711
Epoch: 10, Steps: 318 | Train Loss: 0.1841712 Vali Loss: 0.1882254 Test Loss: 0.1733393
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.3808039426803589, mae:0.4797041118144989, rmse:0.6170931458473206, mape:0.017159419134259224, mspe:0.0004946981207467616, rse:0.4319349229335785, r2_score:0.7916376467367692, acc:0.9828405808657408
corr: [37.105095 36.99772  37.087322 37.189213 36.935986 36.895412 37.01895
 36.95551  36.966084 37.068844 37.0377   37.15593  37.285763 37.211147
 37.22096  37.22028  37.307068 37.406605 37.650387 37.686176]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3361040
	speed: 0.1575s/iter; left time: 485.1761s
	iters: 200, epoch: 1 | loss: 0.2526783
	speed: 0.1326s/iter; left time: 395.3661s
	iters: 300, epoch: 1 | loss: 0.2126257
	speed: 0.1497s/iter; left time: 431.4283s
Epoch: 1 cost time: 46.94811010360718
Epoch: 1, Steps: 318 | Train Loss: 0.3375255 Vali Loss: 0.2145885 Test Loss: 0.2057944
Validation loss decreased (inf --> 0.214588).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2726316
	speed: 0.1609s/iter; left time: 444.6916s
	iters: 200, epoch: 2 | loss: 0.2233835
	speed: 0.1365s/iter; left time: 363.5334s
	iters: 300, epoch: 2 | loss: 0.1878837
	speed: 0.1388s/iter; left time: 355.7534s
Epoch: 2 cost time: 46.04950833320618
Epoch: 2, Steps: 318 | Train Loss: 0.2141778 Vali Loss: 0.2105503 Test Loss: 0.1933083
Validation loss decreased (0.214588 --> 0.210550).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1732067
	speed: 0.1516s/iter; left time: 370.7558s
	iters: 200, epoch: 3 | loss: 0.2161821
	speed: 0.1449s/iter; left time: 339.8781s
	iters: 300, epoch: 3 | loss: 0.2242363
	speed: 0.1397s/iter; left time: 313.6605s
Epoch: 3 cost time: 45.95865321159363
Epoch: 3, Steps: 318 | Train Loss: 0.2012074 Vali Loss: 0.2087109 Test Loss: 0.1911208
Validation loss decreased (0.210550 --> 0.208711).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2272548
	speed: 0.1472s/iter; left time: 313.0422s
	iters: 200, epoch: 4 | loss: 0.1933146
	speed: 0.1307s/iter; left time: 264.9911s
	iters: 300, epoch: 4 | loss: 0.2006371
	speed: 0.1367s/iter; left time: 263.4790s
Epoch: 4 cost time: 44.10020327568054
Epoch: 4, Steps: 318 | Train Loss: 0.1968696 Vali Loss: 0.2102148 Test Loss: 0.1882379
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2018610
	speed: 0.1500s/iter; left time: 271.3326s
	iters: 200, epoch: 5 | loss: 0.2241284
	speed: 0.1384s/iter; left time: 236.6040s
	iters: 300, epoch: 5 | loss: 0.1975890
	speed: 0.1345s/iter; left time: 216.3421s
Epoch: 5 cost time: 44.83372759819031
Epoch: 5, Steps: 318 | Train Loss: 0.1944854 Vali Loss: 0.2084278 Test Loss: 0.1888324
Validation loss decreased (0.208711 --> 0.208428).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1768099
	speed: 0.1544s/iter; left time: 230.1389s
	iters: 200, epoch: 6 | loss: 0.1648598
	speed: 0.1475s/iter; left time: 205.1041s
	iters: 300, epoch: 6 | loss: 0.1800061
	speed: 0.1360s/iter; left time: 175.6042s
Epoch: 6 cost time: 45.82537126541138
Epoch: 6, Steps: 318 | Train Loss: 0.1935618 Vali Loss: 0.2058331 Test Loss: 0.1888308
Validation loss decreased (0.208428 --> 0.205833).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2056435
	speed: 0.1313s/iter; left time: 153.9622s
	iters: 200, epoch: 7 | loss: 0.2117341
	speed: 0.1379s/iter; left time: 147.9950s
	iters: 300, epoch: 7 | loss: 0.2414290
	speed: 0.1343s/iter; left time: 130.6893s
Epoch: 7 cost time: 42.92649698257446
Epoch: 7, Steps: 318 | Train Loss: 0.1926925 Vali Loss: 0.2072908 Test Loss: 0.1879939
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1990036
	speed: 0.1497s/iter; left time: 127.9529s
	iters: 200, epoch: 8 | loss: 0.2227968
	speed: 0.1319s/iter; left time: 99.5978s
	iters: 300, epoch: 8 | loss: 0.1626400
	speed: 0.1392s/iter; left time: 91.1433s
Epoch: 8 cost time: 44.76901602745056
Epoch: 8, Steps: 318 | Train Loss: 0.1920616 Vali Loss: 0.2081494 Test Loss: 0.1880506
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1751633
	speed: 0.1453s/iter; left time: 78.0469s
	iters: 200, epoch: 9 | loss: 0.2590639
	speed: 0.1419s/iter; left time: 62.0114s
	iters: 300, epoch: 9 | loss: 0.1596794
	speed: 0.1308s/iter; left time: 44.0777s
Epoch: 9 cost time: 44.45872449874878
Epoch: 9, Steps: 318 | Train Loss: 0.1922508 Vali Loss: 0.2078001 Test Loss: 0.1879398
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.41438180208206177, mae:0.49982142448425293, rmse:0.6437249183654785, mape:0.017878476530313492, mspe:0.0005390652804635465, rse:0.45027488470077515, r2_score:0.7763983497491228, acc:0.9821215234696865
corr: [37.139984 37.05077  37.032246 37.152905 37.100674 36.99869  36.960228
 36.947952 37.015667 37.09062  37.113647 37.201263 37.256195 37.254894
 37.23847  37.246758 37.372658 37.303497 37.50549  37.541134 37.5137
 37.591396 37.708324 37.71084  37.865334]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2971319
	speed: 0.2280s/iter; left time: 700.1066s
	iters: 200, epoch: 1 | loss: 0.2205557
	speed: 0.2845s/iter; left time: 845.2779s
	iters: 300, epoch: 1 | loss: 0.2384819
	speed: 0.2909s/iter; left time: 835.2670s
Epoch: 1 cost time: 85.63955926895142
Epoch: 1, Steps: 317 | Train Loss: 0.3306979 Vali Loss: 0.2166817 Test Loss: 0.2069359
Validation loss decreased (inf --> 0.216682).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2221092
	speed: 0.3069s/iter; left time: 845.3122s
	iters: 200, epoch: 2 | loss: 0.1598845
	speed: 0.2856s/iter; left time: 758.0945s
	iters: 300, epoch: 2 | loss: 0.2064940
	speed: 0.2661s/iter; left time: 679.6797s
Epoch: 2 cost time: 91.09996151924133
Epoch: 2, Steps: 317 | Train Loss: 0.2154240 Vali Loss: 0.2068281 Test Loss: 0.1998858
Validation loss decreased (0.216682 --> 0.206828).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1935987
	speed: 0.2150s/iter; left time: 523.8937s
	iters: 200, epoch: 3 | loss: 0.1904355
	speed: 0.2193s/iter; left time: 512.4258s
	iters: 300, epoch: 3 | loss: 0.2310591
	speed: 0.2139s/iter; left time: 478.4593s
Epoch: 3 cost time: 68.82496953010559
Epoch: 3, Steps: 317 | Train Loss: 0.2008525 Vali Loss: 0.2049319 Test Loss: 0.1961561
Validation loss decreased (0.206828 --> 0.204932).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1804049
	speed: 0.2185s/iter; left time: 463.1577s
	iters: 200, epoch: 4 | loss: 0.2029670
	speed: 0.2251s/iter; left time: 454.6370s
	iters: 300, epoch: 4 | loss: 0.1857778
	speed: 0.2257s/iter; left time: 433.2613s
Epoch: 4 cost time: 70.57273626327515
Epoch: 4, Steps: 317 | Train Loss: 0.1947100 Vali Loss: 0.2020375 Test Loss: 0.1937531
Validation loss decreased (0.204932 --> 0.202038).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1713410
	speed: 0.2300s/iter; left time: 414.7333s
	iters: 200, epoch: 5 | loss: 0.1789456
	speed: 0.2100s/iter; left time: 357.6743s
	iters: 300, epoch: 5 | loss: 0.1757671
	speed: 0.2308s/iter; left time: 370.0381s
Epoch: 5 cost time: 70.6173779964447
Epoch: 5, Steps: 317 | Train Loss: 0.1916912 Vali Loss: 0.2029984 Test Loss: 0.1944713
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1684747
	speed: 0.2375s/iter; left time: 352.9115s
	iters: 200, epoch: 6 | loss: 0.1529036
	speed: 0.2077s/iter; left time: 287.9325s
	iters: 300, epoch: 6 | loss: 0.1853566
	speed: 0.2241s/iter; left time: 288.2504s
Epoch: 6 cost time: 70.66474795341492
Epoch: 6, Steps: 317 | Train Loss: 0.1900688 Vali Loss: 0.2016124 Test Loss: 0.1949007
Validation loss decreased (0.202038 --> 0.201612).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1836500
	speed: 0.2393s/iter; left time: 279.7665s
	iters: 200, epoch: 7 | loss: 0.2029891
	speed: 0.2194s/iter; left time: 234.5757s
	iters: 300, epoch: 7 | loss: 0.1596485
	speed: 0.2227s/iter; left time: 215.7773s
Epoch: 7 cost time: 71.79146885871887
Epoch: 7, Steps: 317 | Train Loss: 0.1900852 Vali Loss: 0.2025292 Test Loss: 0.1963332
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1787140
	speed: 0.2363s/iter; left time: 201.3129s
	iters: 200, epoch: 8 | loss: 0.1949367
	speed: 0.2178s/iter; left time: 163.8036s
	iters: 300, epoch: 8 | loss: 0.2231990
	speed: 0.2122s/iter; left time: 138.3694s
Epoch: 8 cost time: 70.10740971565247
Epoch: 8, Steps: 317 | Train Loss: 0.1892124 Vali Loss: 0.2028491 Test Loss: 0.1964116
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2319148
	speed: 0.2287s/iter; left time: 122.3513s
	iters: 200, epoch: 9 | loss: 0.1792367
	speed: 0.2305s/iter; left time: 100.2625s
	iters: 300, epoch: 9 | loss: 0.1743315
	speed: 0.2301s/iter; left time: 77.0863s
Epoch: 9 cost time: 72.84576630592346
Epoch: 9, Steps: 317 | Train Loss: 0.1894774 Vali Loss: 0.2030283 Test Loss: 0.1968727
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.4277018904685974, mae:0.5037010312080383, rmse:0.6539891958236694, mape:0.018012400716543198, mspe:0.0005559907876886427, rse:0.45712608098983765, r2_score:0.760241847554648, acc:0.9819875992834568
corr: [36.82635  36.85263  36.87725  36.75888  36.76328  36.64572  36.72161
 36.715347 36.67703  36.705856 36.800716 36.802227 36.719696 36.687725
 36.809338 36.691257 36.735287 36.876232 36.850384 37.042206 37.12535
 37.074368 36.91095  36.92566  36.86026  37.07402  37.122902 37.196575
 37.239025 37.060608]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3030851
	speed: 0.1914s/iter; left time: 589.7259s
	iters: 200, epoch: 1 | loss: 0.2264358
	speed: 0.1665s/iter; left time: 496.2154s
	iters: 300, epoch: 1 | loss: 0.2191960
	speed: 0.1797s/iter; left time: 517.7654s
Epoch: 1 cost time: 56.78348684310913
Epoch: 1, Steps: 318 | Train Loss: 0.3367873 Vali Loss: 0.1963358 Test Loss: 0.1682312
Validation loss decreased (inf --> 0.196336).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1804077
	speed: 0.1614s/iter; left time: 445.8987s
	iters: 200, epoch: 2 | loss: 0.1625027
	speed: 0.1655s/iter; left time: 440.7456s
	iters: 300, epoch: 2 | loss: 0.1296611
	speed: 0.1795s/iter; left time: 460.0265s
Epoch: 2 cost time: 53.723936319351196
Epoch: 2, Steps: 318 | Train Loss: 0.1820891 Vali Loss: 0.1832670 Test Loss: 0.1608558
Validation loss decreased (0.196336 --> 0.183267).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1797778
	speed: 0.1756s/iter; left time: 429.4204s
	iters: 200, epoch: 3 | loss: 0.1692508
	speed: 0.1766s/iter; left time: 414.0299s
	iters: 300, epoch: 3 | loss: 0.1221938
	speed: 0.1787s/iter; left time: 401.0908s
Epoch: 3 cost time: 56.26390504837036
Epoch: 3, Steps: 318 | Train Loss: 0.1628998 Vali Loss: 0.1757596 Test Loss: 0.1597590
Validation loss decreased (0.183267 --> 0.175760).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1379685
	speed: 0.1847s/iter; left time: 392.9061s
	iters: 200, epoch: 4 | loss: 0.1348610
	speed: 0.1792s/iter; left time: 363.1505s
	iters: 300, epoch: 4 | loss: 0.1626000
	speed: 0.1659s/iter; left time: 319.6219s
Epoch: 4 cost time: 56.348106384277344
Epoch: 4, Steps: 318 | Train Loss: 0.1556384 Vali Loss: 0.1705014 Test Loss: 0.1550672
Validation loss decreased (0.175760 --> 0.170501).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1957058
	speed: 0.1958s/iter; left time: 354.2073s
	iters: 200, epoch: 5 | loss: 0.1512651
	speed: 0.1760s/iter; left time: 300.7947s
	iters: 300, epoch: 5 | loss: 0.1345307
	speed: 0.1657s/iter; left time: 266.6309s
Epoch: 5 cost time: 56.450907468795776
Epoch: 5, Steps: 318 | Train Loss: 0.1498991 Vali Loss: 0.1714107 Test Loss: 0.1554535
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1380848
	speed: 0.1768s/iter; left time: 263.6166s
	iters: 200, epoch: 6 | loss: 0.1174279
	speed: 0.1733s/iter; left time: 241.0147s
	iters: 300, epoch: 6 | loss: 0.1400822
	speed: 0.1673s/iter; left time: 215.9319s
Epoch: 6 cost time: 54.57846212387085
Epoch: 6, Steps: 318 | Train Loss: 0.1489870 Vali Loss: 0.1724288 Test Loss: 0.1565459
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1389310
	speed: 0.1790s/iter; left time: 209.9116s
	iters: 200, epoch: 7 | loss: 0.1474211
	speed: 0.1727s/iter; left time: 185.2677s
	iters: 300, epoch: 7 | loss: 0.1772573
	speed: 0.1701s/iter; left time: 165.5423s
Epoch: 7 cost time: 55.10391712188721
Epoch: 7, Steps: 318 | Train Loss: 0.1476010 Vali Loss: 0.1757148 Test Loss: 0.1570542
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.34028884768486023, mae:0.4503091871738434, rmse:0.5833428502082825, mape:0.016102321445941925, mspe:0.00044101360253989697, rse:0.40874040126800537, r2_score:0.8202887516713089, acc:0.9838976785540581
corr: [37.515396 37.30562  37.405113 37.368668 37.274075 37.346798 37.363728
 37.357433 37.51853  37.30907 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3837703
	speed: 0.2323s/iter; left time: 715.6980s
	iters: 200, epoch: 1 | loss: 0.1910274
	speed: 0.2188s/iter; left time: 652.2925s
	iters: 300, epoch: 1 | loss: 0.2771377
	speed: 0.2394s/iter; left time: 689.5762s
Epoch: 1 cost time: 73.55120658874512
Epoch: 1, Steps: 318 | Train Loss: 0.3409034 Vali Loss: 0.1945812 Test Loss: 0.1865031
Validation loss decreased (inf --> 0.194581).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2137118
	speed: 0.2635s/iter; left time: 727.9560s
	iters: 200, epoch: 2 | loss: 0.1164549
	speed: 0.2607s/iter; left time: 694.3474s
	iters: 300, epoch: 2 | loss: 0.1489347
	speed: 0.2548s/iter; left time: 652.9541s
Epoch: 2 cost time: 82.24694418907166
Epoch: 2, Steps: 318 | Train Loss: 0.1925896 Vali Loss: 0.1829751 Test Loss: 0.1670730
Validation loss decreased (0.194581 --> 0.182975).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1702286
	speed: 0.2574s/iter; left time: 629.2938s
	iters: 200, epoch: 3 | loss: 0.2100894
	speed: 0.2547s/iter; left time: 597.3000s
	iters: 300, epoch: 3 | loss: 0.1841756
	speed: 0.2613s/iter; left time: 586.5210s
Epoch: 3 cost time: 82.35296583175659
Epoch: 3, Steps: 318 | Train Loss: 0.1787742 Vali Loss: 0.1794785 Test Loss: 0.1630106
Validation loss decreased (0.182975 --> 0.179478).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1609861
	speed: 0.2706s/iter; left time: 575.5432s
	iters: 200, epoch: 4 | loss: 0.1647305
	speed: 0.2568s/iter; left time: 520.4803s
	iters: 300, epoch: 4 | loss: 0.1784272
	speed: 0.2525s/iter; left time: 486.4761s
Epoch: 4 cost time: 82.68837404251099
Epoch: 4, Steps: 318 | Train Loss: 0.1742628 Vali Loss: 0.1777453 Test Loss: 0.1614979
Validation loss decreased (0.179478 --> 0.177745).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1426453
	speed: 0.2579s/iter; left time: 466.5639s
	iters: 200, epoch: 5 | loss: 0.1263454
	speed: 0.2531s/iter; left time: 432.5545s
	iters: 300, epoch: 5 | loss: 0.1867725
	speed: 0.2626s/iter; left time: 422.5195s
Epoch: 5 cost time: 82.28819823265076
Epoch: 5, Steps: 318 | Train Loss: 0.1716401 Vali Loss: 0.1763737 Test Loss: 0.1597575
Validation loss decreased (0.177745 --> 0.176374).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1794535
	speed: 0.2749s/iter; left time: 409.9429s
	iters: 200, epoch: 6 | loss: 0.2050060
	speed: 0.2554s/iter; left time: 355.2242s
	iters: 300, epoch: 6 | loss: 0.1934872
	speed: 0.2642s/iter; left time: 341.0399s
Epoch: 6 cost time: 83.81098890304565
Epoch: 6, Steps: 318 | Train Loss: 0.1702223 Vali Loss: 0.1744812 Test Loss: 0.1589726
Validation loss decreased (0.176374 --> 0.174481).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1570172
	speed: 0.2543s/iter; left time: 298.2569s
	iters: 200, epoch: 7 | loss: 0.1856760
	speed: 0.2569s/iter; left time: 275.6570s
	iters: 300, epoch: 7 | loss: 0.1550877
	speed: 0.2652s/iter; left time: 258.0544s
Epoch: 7 cost time: 82.0527114868164
Epoch: 7, Steps: 318 | Train Loss: 0.1697543 Vali Loss: 0.1746239 Test Loss: 0.1586622
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1535409
	speed: 0.3241s/iter; left time: 277.0749s
	iters: 200, epoch: 8 | loss: 0.1551850
	speed: 0.3173s/iter; left time: 239.5594s
	iters: 300, epoch: 8 | loss: 0.2002473
	speed: 0.3490s/iter; left time: 228.6011s
Epoch: 8 cost time: 105.56687378883362
Epoch: 8, Steps: 318 | Train Loss: 0.1698036 Vali Loss: 0.1731489 Test Loss: 0.1578242
Validation loss decreased (0.174481 --> 0.173149).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2068356
	speed: 0.2676s/iter; left time: 143.7009s
	iters: 200, epoch: 9 | loss: 0.1488525
	speed: 0.2721s/iter; left time: 118.9280s
	iters: 300, epoch: 9 | loss: 0.1755351
	speed: 0.2542s/iter; left time: 85.6791s
Epoch: 9 cost time: 83.70988655090332
Epoch: 9, Steps: 318 | Train Loss: 0.1696842 Vali Loss: 0.1739934 Test Loss: 0.1579458
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1807052
	speed: 0.2542s/iter; left time: 55.6656s
	iters: 200, epoch: 10 | loss: 0.1704663
	speed: 0.2626s/iter; left time: 31.2462s
	iters: 300, epoch: 10 | loss: 0.1346727
	speed: 0.2549s/iter; left time: 4.8426s
Epoch: 10 cost time: 81.97380208969116
Epoch: 10, Steps: 318 | Train Loss: 0.1693800 Vali Loss: 0.1736823 Test Loss: 0.1579176
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.3463389277458191, mae:0.45751094818115234, rmse:0.5885056853294373, mape:0.016365554183721542, mspe:0.0004497154732234776, rse:0.41216376423835754, r2_score:0.8107247667313092, acc:0.9836344458162785
corr: [36.90798  36.925312 36.964275 37.031925 36.88051  36.91023  36.887657
 36.99847  37.091064 37.161102 37.166615 37.222385 37.346886 37.46827
 37.566875]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2774680
	speed: 0.2106s/iter; left time: 648.7460s
	iters: 200, epoch: 1 | loss: 0.2034737
	speed: 0.2440s/iter; left time: 727.3867s
	iters: 300, epoch: 1 | loss: 0.1672734
	speed: 0.2762s/iter; left time: 795.6717s
Epoch: 1 cost time: 78.1517813205719
Epoch: 1, Steps: 318 | Train Loss: 0.3313966 Vali Loss: 0.2016570 Test Loss: 0.1896981
Validation loss decreased (inf --> 0.201657).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1882960
	speed: 0.2770s/iter; left time: 765.4757s
	iters: 200, epoch: 2 | loss: 0.1788003
	speed: 0.2738s/iter; left time: 729.1234s
	iters: 300, epoch: 2 | loss: 0.3210303
	speed: 0.2732s/iter; left time: 700.1238s
Epoch: 2 cost time: 87.1203932762146
Epoch: 2, Steps: 318 | Train Loss: 0.2081841 Vali Loss: 0.1972386 Test Loss: 0.1822126
Validation loss decreased (0.201657 --> 0.197239).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1722850
	speed: 0.2857s/iter; left time: 698.5501s
	iters: 200, epoch: 3 | loss: 0.2578225
	speed: 0.2644s/iter; left time: 620.0149s
	iters: 300, epoch: 3 | loss: 0.1899318
	speed: 0.2557s/iter; left time: 574.0789s
Epoch: 3 cost time: 85.39896702766418
Epoch: 3, Steps: 318 | Train Loss: 0.1948279 Vali Loss: 0.1889408 Test Loss: 0.1761144
Validation loss decreased (0.197239 --> 0.188941).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1911251
	speed: 0.2656s/iter; left time: 564.9009s
	iters: 200, epoch: 4 | loss: 0.2048766
	speed: 0.2735s/iter; left time: 554.4709s
	iters: 300, epoch: 4 | loss: 0.1567338
	speed: 0.2607s/iter; left time: 502.3976s
Epoch: 4 cost time: 85.07226800918579
Epoch: 4, Steps: 318 | Train Loss: 0.1901764 Vali Loss: 0.1898092 Test Loss: 0.1756413
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1835226
	speed: 0.2743s/iter; left time: 496.1464s
	iters: 200, epoch: 5 | loss: 0.1835113
	speed: 0.2641s/iter; left time: 451.3680s
	iters: 300, epoch: 5 | loss: 0.2734825
	speed: 0.2706s/iter; left time: 435.4273s
Epoch: 5 cost time: 86.06617951393127
Epoch: 5, Steps: 318 | Train Loss: 0.1871599 Vali Loss: 0.1890492 Test Loss: 0.1740151
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1820809
	speed: 0.2686s/iter; left time: 400.4679s
	iters: 200, epoch: 6 | loss: 0.2100481
	speed: 0.2867s/iter; left time: 398.7651s
	iters: 300, epoch: 6 | loss: 0.1839336
	speed: 0.2659s/iter; left time: 343.2641s
Epoch: 6 cost time: 87.08879971504211
Epoch: 6, Steps: 318 | Train Loss: 0.1862229 Vali Loss: 0.1889748 Test Loss: 0.1742632
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.3864760398864746, mae:0.48458993434906006, rmse:0.6216719746589661, mape:0.01733345352113247, mspe:0.0005022591212764382, rse:0.43513986468315125, r2_score:0.7734115858627549, acc:0.9826665464788675
corr: [37.045662 36.963406 36.97771  37.057896 36.893124 36.82831  36.91694
 36.901306 36.913303 36.969627 36.95635  37.11907  37.173164 37.16988
 37.159054 37.11055  37.250603 37.32715  37.58754  37.629402]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3464044
	speed: 0.1715s/iter; left time: 528.3136s
	iters: 200, epoch: 1 | loss: 0.2609273
	speed: 0.1620s/iter; left time: 482.8404s
	iters: 300, epoch: 1 | loss: 0.2390813
	speed: 0.1563s/iter; left time: 450.2769s
Epoch: 1 cost time: 52.468421459198
Epoch: 1, Steps: 318 | Train Loss: 0.3439268 Vali Loss: 0.2386552 Test Loss: 0.2237110
Validation loss decreased (inf --> 0.238655).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2844250
	speed: 0.1735s/iter; left time: 479.3773s
	iters: 200, epoch: 2 | loss: 0.2306035
	speed: 0.1831s/iter; left time: 487.6717s
	iters: 300, epoch: 2 | loss: 0.1775655
	speed: 0.1858s/iter; left time: 476.1040s
Epoch: 2 cost time: 57.54449820518494
Epoch: 2, Steps: 318 | Train Loss: 0.2172282 Vali Loss: 0.2152780 Test Loss: 0.2022735
Validation loss decreased (0.238655 --> 0.215278).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1678036
	speed: 0.1833s/iter; left time: 448.1367s
	iters: 200, epoch: 3 | loss: 0.2238782
	speed: 0.1715s/iter; left time: 402.1769s
	iters: 300, epoch: 3 | loss: 0.1982563
	speed: 0.1753s/iter; left time: 393.4727s
Epoch: 3 cost time: 56.65938448905945
Epoch: 3, Steps: 318 | Train Loss: 0.1970591 Vali Loss: 0.2165903 Test Loss: 0.2005261
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1925773
	speed: 0.1915s/iter; left time: 407.2742s
	iters: 200, epoch: 4 | loss: 0.2140049
	speed: 0.1836s/iter; left time: 372.0738s
	iters: 300, epoch: 4 | loss: 0.1846427
	speed: 0.1738s/iter; left time: 334.8753s
Epoch: 4 cost time: 58.32058382034302
Epoch: 4, Steps: 318 | Train Loss: 0.1889008 Vali Loss: 0.2171702 Test Loss: 0.1976249
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1851608
	speed: 0.1759s/iter; left time: 318.1300s
	iters: 200, epoch: 5 | loss: 0.2169468
	speed: 0.1856s/iter; left time: 317.1895s
	iters: 300, epoch: 5 | loss: 0.1929882
	speed: 0.1796s/iter; left time: 289.0420s
Epoch: 5 cost time: 57.327857971191406
Epoch: 5, Steps: 318 | Train Loss: 0.1854276 Vali Loss: 0.2174721 Test Loss: 0.1987452
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000031
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.44388115406036377, mae:0.5255903005599976, rmse:0.6662440896034241, mape:0.018785741180181503, mspe:0.0005747165414504707, rse:0.46602663397789, r2_score:0.7543986078317018, acc:0.9812142588198185
corr: [36.976635 36.940235 36.455822 36.914715 36.97091  36.64758  36.747646
 36.812134 36.72918  36.96941  36.837532 37.012016 37.2327   37.29683
 37.35655  37.325916 37.481598 37.23204  37.479206 37.37896  37.554638
 37.544    37.803303 37.50113  37.80378 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2971939
	speed: 0.2992s/iter; left time: 918.9666s
	iters: 200, epoch: 1 | loss: 0.2183342
	speed: 0.3484s/iter; left time: 1034.9505s
	iters: 300, epoch: 1 | loss: 0.2397777
	speed: 0.2912s/iter; left time: 835.9475s
Epoch: 1 cost time: 98.75170302391052
Epoch: 1, Steps: 317 | Train Loss: 0.3282956 Vali Loss: 0.2188941 Test Loss: 0.2099471
Validation loss decreased (inf --> 0.218894).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2227581
	speed: 0.2911s/iter; left time: 801.7965s
	iters: 200, epoch: 2 | loss: 0.1570639
	speed: 0.2742s/iter; left time: 727.8462s
	iters: 300, epoch: 2 | loss: 0.2031032
	speed: 0.2746s/iter; left time: 701.2210s
Epoch: 2 cost time: 88.39200210571289
Epoch: 2, Steps: 317 | Train Loss: 0.2144810 Vali Loss: 0.2073271 Test Loss: 0.2021350
Validation loss decreased (0.218894 --> 0.207327).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1934572
	speed: 0.2709s/iter; left time: 660.2034s
	iters: 200, epoch: 3 | loss: 0.1864430
	speed: 0.2656s/iter; left time: 620.6685s
	iters: 300, epoch: 3 | loss: 0.2306517
	speed: 0.2972s/iter; left time: 664.8618s
Epoch: 3 cost time: 88.67246961593628
Epoch: 3, Steps: 317 | Train Loss: 0.1997685 Vali Loss: 0.2064558 Test Loss: 0.1974320
Validation loss decreased (0.207327 --> 0.206456).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1794406
	speed: 0.2972s/iter; left time: 630.0637s
	iters: 200, epoch: 4 | loss: 0.2026663
	speed: 0.2841s/iter; left time: 573.8242s
	iters: 300, epoch: 4 | loss: 0.1866970
	speed: 0.2854s/iter; left time: 547.8853s
Epoch: 4 cost time: 91.88141179084778
Epoch: 4, Steps: 317 | Train Loss: 0.1936537 Vali Loss: 0.2042840 Test Loss: 0.1947812
Validation loss decreased (0.206456 --> 0.204284).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1690816
	speed: 0.2941s/iter; left time: 530.2971s
	iters: 200, epoch: 5 | loss: 0.1803602
	speed: 0.2759s/iter; left time: 469.8120s
	iters: 300, epoch: 5 | loss: 0.1721039
	speed: 0.2789s/iter; left time: 447.0491s
Epoch: 5 cost time: 89.89956593513489
Epoch: 5, Steps: 317 | Train Loss: 0.1906594 Vali Loss: 0.2050743 Test Loss: 0.1955937
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1650464
	speed: 0.2749s/iter; left time: 408.5224s
	iters: 200, epoch: 6 | loss: 0.1515419
	speed: 0.2722s/iter; left time: 377.2894s
	iters: 300, epoch: 6 | loss: 0.1851427
	speed: 0.2709s/iter; left time: 348.3628s
Epoch: 6 cost time: 88.1654109954834
Epoch: 6, Steps: 317 | Train Loss: 0.1890194 Vali Loss: 0.2037492 Test Loss: 0.1960295
Validation loss decreased (0.204284 --> 0.203749).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1813065
	speed: 0.3434s/iter; left time: 401.4611s
	iters: 200, epoch: 7 | loss: 0.2006932
	speed: 0.3452s/iter; left time: 369.0384s
	iters: 300, epoch: 7 | loss: 0.1604946
	speed: 0.3245s/iter; left time: 314.4394s
Epoch: 7 cost time: 107.19779753684998
Epoch: 7, Steps: 317 | Train Loss: 0.1888899 Vali Loss: 0.2047681 Test Loss: 0.1974668
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1767901
	speed: 0.2600s/iter; left time: 221.5172s
	iters: 200, epoch: 8 | loss: 0.1931485
	speed: 0.2570s/iter; left time: 193.2932s
	iters: 300, epoch: 8 | loss: 0.2165096
	speed: 0.2532s/iter; left time: 165.1144s
Epoch: 8 cost time: 81.315758228302
Epoch: 8, Steps: 317 | Train Loss: 0.1880896 Vali Loss: 0.2050232 Test Loss: 0.1974227
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2317271
	speed: 0.2643s/iter; left time: 141.4267s
	iters: 200, epoch: 9 | loss: 0.1753141
	speed: 0.2752s/iter; left time: 119.7308s
	iters: 300, epoch: 9 | loss: 0.1740097
	speed: 0.2508s/iter; left time: 84.0081s
Epoch: 9 cost time: 83.88191938400269
Epoch: 9, Steps: 317 | Train Loss: 0.1883018 Vali Loss: 0.2052292 Test Loss: 0.1979029
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.43017905950546265, mae:0.5044595003128052, rmse:0.6558803915977478, mape:0.018040407449007034, mspe:0.0005594720714725554, rse:0.45844799280166626, r2_score:0.7591355143183558, acc:0.981959592550993
corr: [36.835102 36.835796 36.877148 36.80413  36.82413  36.68527  36.734352
 36.734486 36.64844  36.63556  36.764008 36.828777 36.75523  36.68382
 36.782776 36.67976  36.676247 36.790836 36.769676 37.014538 37.141674
 37.0483   36.89564  36.86376  36.773933 36.972927 37.01663  37.203846
 37.2393   37.04552 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2943501
	speed: 0.2270s/iter; left time: 699.3854s
	iters: 200, epoch: 1 | loss: 0.2196051
	speed: 0.2191s/iter; left time: 653.2141s
	iters: 300, epoch: 1 | loss: 0.1860676
	speed: 0.2334s/iter; left time: 672.4124s
Epoch: 1 cost time: 72.33969807624817
Epoch: 1, Steps: 318 | Train Loss: 0.3078845 Vali Loss: 0.1760018 Test Loss: 0.1593593
Validation loss decreased (inf --> 0.176002).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1782759
	speed: 0.2218s/iter; left time: 612.7889s
	iters: 200, epoch: 2 | loss: 0.1465031
	speed: 0.2310s/iter; left time: 615.0717s
	iters: 300, epoch: 2 | loss: 0.1233592
	speed: 0.2194s/iter; left time: 562.3967s
Epoch: 2 cost time: 71.2277147769928
Epoch: 2, Steps: 318 | Train Loss: 0.1706979 Vali Loss: 0.1616893 Test Loss: 0.1468913
Validation loss decreased (0.176002 --> 0.161689).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1624444
	speed: 0.2361s/iter; left time: 577.2171s
	iters: 200, epoch: 3 | loss: 0.1838978
	speed: 0.2496s/iter; left time: 585.4231s
	iters: 300, epoch: 3 | loss: 0.1235951
	speed: 0.2329s/iter; left time: 522.9450s
Epoch: 3 cost time: 75.97817087173462
Epoch: 3, Steps: 318 | Train Loss: 0.1555739 Vali Loss: 0.1594655 Test Loss: 0.1458293
Validation loss decreased (0.161689 --> 0.159466).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1329428
	speed: 0.2458s/iter; left time: 522.8571s
	iters: 200, epoch: 4 | loss: 0.1330818
	speed: 0.2335s/iter; left time: 473.3895s
	iters: 300, epoch: 4 | loss: 0.1803852
	speed: 0.2456s/iter; left time: 473.2343s
Epoch: 4 cost time: 76.81125211715698
Epoch: 4, Steps: 318 | Train Loss: 0.1503220 Vali Loss: 0.1538515 Test Loss: 0.1385584
Validation loss decreased (0.159466 --> 0.153852).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1643902
	speed: 0.2409s/iter; left time: 435.7039s
	iters: 200, epoch: 5 | loss: 0.1510047
	speed: 0.2473s/iter; left time: 422.6333s
	iters: 300, epoch: 5 | loss: 0.1388520
	speed: 0.2349s/iter; left time: 377.8817s
Epoch: 5 cost time: 76.90411233901978
Epoch: 5, Steps: 318 | Train Loss: 0.1466407 Vali Loss: 0.1533387 Test Loss: 0.1388223
Validation loss decreased (0.153852 --> 0.153339).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1202136
	speed: 0.2437s/iter; left time: 363.3923s
	iters: 200, epoch: 6 | loss: 0.1239417
	speed: 0.2372s/iter; left time: 329.8802s
	iters: 300, epoch: 6 | loss: 0.1563054
	speed: 0.2324s/iter; left time: 299.9707s
Epoch: 6 cost time: 75.47245478630066
Epoch: 6, Steps: 318 | Train Loss: 0.1449943 Vali Loss: 0.1518737 Test Loss: 0.1381322
Validation loss decreased (0.153339 --> 0.151874).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1266336
	speed: 0.2527s/iter; left time: 296.3887s
	iters: 200, epoch: 7 | loss: 0.1420694
	speed: 0.2452s/iter; left time: 263.1105s
	iters: 300, epoch: 7 | loss: 0.1767813
	speed: 0.2366s/iter; left time: 230.2518s
Epoch: 7 cost time: 78.22886657714844
Epoch: 7, Steps: 318 | Train Loss: 0.1448373 Vali Loss: 0.1531001 Test Loss: 0.1383059
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1215279
	speed: 0.2466s/iter; left time: 210.8641s
	iters: 200, epoch: 8 | loss: 0.1070342
	speed: 0.2307s/iter; left time: 174.2158s
	iters: 300, epoch: 8 | loss: 0.1517033
	speed: 0.2407s/iter; left time: 157.6391s
Epoch: 8 cost time: 76.12327551841736
Epoch: 8, Steps: 318 | Train Loss: 0.1434472 Vali Loss: 0.1512030 Test Loss: 0.1376505
Validation loss decreased (0.151874 --> 0.151203).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1665695
	speed: 0.2342s/iter; left time: 125.7388s
	iters: 200, epoch: 9 | loss: 0.1319128
	speed: 0.2329s/iter; left time: 101.7923s
	iters: 300, epoch: 9 | loss: 0.1468469
	speed: 0.2335s/iter; left time: 78.6824s
Epoch: 9 cost time: 74.16896891593933
Epoch: 9, Steps: 318 | Train Loss: 0.1435748 Vali Loss: 0.1514896 Test Loss: 0.1379098
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1141669
	speed: 0.2338s/iter; left time: 51.1969s
	iters: 200, epoch: 10 | loss: 0.2136888
	speed: 0.2184s/iter; left time: 25.9866s
	iters: 300, epoch: 10 | loss: 0.1351501
	speed: 0.2183s/iter; left time: 4.1475s
Epoch: 10 cost time: 71.1155424118042
Epoch: 10, Steps: 318 | Train Loss: 0.1440795 Vali Loss: 0.1505955 Test Loss: 0.1379371
Validation loss decreased (0.151203 --> 0.150595).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.30269765853881836, mae:0.4285154342651367, rmse:0.550179660320282, mape:0.015310079790651798, mspe:0.0003911342064384371, rse:0.3855034112930298, r2_score:0.8404722909487221, acc:0.9846899202093482
corr: [36.786568 36.898594 36.90628  36.900837 36.826817 36.717434 36.94613
 36.903385 37.050323 36.96352 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3290510
	speed: 0.2563s/iter; left time: 789.7891s
	iters: 200, epoch: 1 | loss: 0.1825718
	speed: 0.2324s/iter; left time: 692.7831s
	iters: 300, epoch: 1 | loss: 0.2758500
	speed: 0.2764s/iter; left time: 796.2889s
Epoch: 1 cost time: 81.42146492004395
Epoch: 1, Steps: 318 | Train Loss: 0.3147025 Vali Loss: 0.1974276 Test Loss: 0.1927204
Validation loss decreased (inf --> 0.197428).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2008263
	speed: 0.2725s/iter; left time: 752.9206s
	iters: 200, epoch: 2 | loss: 0.1305764
	speed: 0.2772s/iter; left time: 738.0762s
	iters: 300, epoch: 2 | loss: 0.1508374
	speed: 0.2717s/iter; left time: 696.4262s
Epoch: 2 cost time: 87.37249231338501
Epoch: 2, Steps: 318 | Train Loss: 0.1832332 Vali Loss: 0.1781299 Test Loss: 0.1623727
Validation loss decreased (0.197428 --> 0.178130).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1716470
	speed: 0.2699s/iter; left time: 659.8495s
	iters: 200, epoch: 3 | loss: 0.1975567
	speed: 0.2704s/iter; left time: 634.1925s
	iters: 300, epoch: 3 | loss: 0.1525192
	speed: 0.2651s/iter; left time: 595.0457s
Epoch: 3 cost time: 85.35561394691467
Epoch: 3, Steps: 318 | Train Loss: 0.1688361 Vali Loss: 0.1763570 Test Loss: 0.1637383
Validation loss decreased (0.178130 --> 0.176357).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1553350
	speed: 0.2927s/iter; left time: 622.4992s
	iters: 200, epoch: 4 | loss: 0.1635049
	speed: 0.2630s/iter; left time: 533.1637s
	iters: 300, epoch: 4 | loss: 0.1925867
	speed: 0.2779s/iter; left time: 535.4594s
Epoch: 4 cost time: 88.47839188575745
Epoch: 4, Steps: 318 | Train Loss: 0.1631241 Vali Loss: 0.1742941 Test Loss: 0.1598158
Validation loss decreased (0.176357 --> 0.174294).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1375668
	speed: 0.2865s/iter; left time: 518.2206s
	iters: 200, epoch: 5 | loss: 0.1224748
	speed: 0.2803s/iter; left time: 479.0505s
	iters: 300, epoch: 5 | loss: 0.1714292
	speed: 0.2654s/iter; left time: 426.9777s
Epoch: 5 cost time: 88.04668760299683
Epoch: 5, Steps: 318 | Train Loss: 0.1601253 Vali Loss: 0.1734330 Test Loss: 0.1607926
Validation loss decreased (0.174294 --> 0.173433).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1691985
	speed: 0.3007s/iter; left time: 448.3832s
	iters: 200, epoch: 6 | loss: 0.1900872
	speed: 0.3091s/iter; left time: 430.0063s
	iters: 300, epoch: 6 | loss: 0.1911658
	speed: 0.3185s/iter; left time: 411.1848s
Epoch: 6 cost time: 97.96300792694092
Epoch: 6, Steps: 318 | Train Loss: 0.1584064 Vali Loss: 0.1720662 Test Loss: 0.1595948
Validation loss decreased (0.173433 --> 0.172066).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1518653
	speed: 0.2831s/iter; left time: 332.0212s
	iters: 200, epoch: 7 | loss: 0.1835115
	speed: 0.2696s/iter; left time: 289.3130s
	iters: 300, epoch: 7 | loss: 0.1387380
	speed: 0.2637s/iter; left time: 256.5832s
Epoch: 7 cost time: 86.9656445980072
Epoch: 7, Steps: 318 | Train Loss: 0.1575961 Vali Loss: 0.1724473 Test Loss: 0.1598847
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1390035
	speed: 0.2987s/iter; left time: 255.3965s
	iters: 200, epoch: 8 | loss: 0.1568041
	speed: 0.2766s/iter; left time: 208.7973s
	iters: 300, epoch: 8 | loss: 0.1763793
	speed: 0.2630s/iter; left time: 172.2336s
Epoch: 8 cost time: 89.099196434021
Epoch: 8, Steps: 318 | Train Loss: 0.1574138 Vali Loss: 0.1712787 Test Loss: 0.1587621
Validation loss decreased (0.172066 --> 0.171279).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1733840
	speed: 0.2843s/iter; left time: 152.6725s
	iters: 200, epoch: 9 | loss: 0.1518644
	speed: 0.2677s/iter; left time: 116.9699s
	iters: 300, epoch: 9 | loss: 0.1508148
	speed: 0.2775s/iter; left time: 93.5107s
Epoch: 9 cost time: 88.1138665676117
Epoch: 9, Steps: 318 | Train Loss: 0.1571763 Vali Loss: 0.1718755 Test Loss: 0.1585722
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1660057
	speed: 0.2397s/iter; left time: 52.4920s
	iters: 200, epoch: 10 | loss: 0.1449606
	speed: 0.2375s/iter; left time: 28.2645s
	iters: 300, epoch: 10 | loss: 0.1188029
	speed: 0.2337s/iter; left time: 4.4397s
Epoch: 10 cost time: 75.6211724281311
Epoch: 10, Steps: 318 | Train Loss: 0.1572272 Vali Loss: 0.1714800 Test Loss: 0.1585863
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.3483971953392029, mae:0.4603002667427063, rmse:0.5902518033981323, mape:0.016453145071864128, mspe:0.0004507340781856328, rse:0.413386732339859, r2_score:0.8114527604128985, acc:0.9835468549281359
corr: [36.842915 37.028214 37.035553 37.11375  36.871525 36.787746 36.657288
 36.759125 36.64036  36.750767 36.808838 37.075237 37.12349  37.0788
 36.99188 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2971231
	speed: 0.2234s/iter; left time: 688.2600s
	iters: 200, epoch: 1 | loss: 0.2064526
	speed: 0.2147s/iter; left time: 640.0062s
	iters: 300, epoch: 1 | loss: 0.1669557
	speed: 0.2373s/iter; left time: 683.6258s
Epoch: 1 cost time: 71.76548671722412
Epoch: 1, Steps: 318 | Train Loss: 0.3283544 Vali Loss: 0.2064896 Test Loss: 0.1981517
Validation loss decreased (inf --> 0.206490).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1854416
	speed: 0.2453s/iter; left time: 677.7290s
	iters: 200, epoch: 2 | loss: 0.1669929
	speed: 0.2388s/iter; left time: 635.8425s
	iters: 300, epoch: 2 | loss: 0.3041740
	speed: 0.2554s/iter; left time: 654.6431s
Epoch: 2 cost time: 78.408376455307
Epoch: 2, Steps: 318 | Train Loss: 0.2039328 Vali Loss: 0.2027808 Test Loss: 0.1942196
Validation loss decreased (0.206490 --> 0.202781).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1869841
	speed: 0.2387s/iter; left time: 583.6453s
	iters: 200, epoch: 3 | loss: 0.2465706
	speed: 0.2327s/iter; left time: 545.6304s
	iters: 300, epoch: 3 | loss: 0.1853591
	speed: 0.2464s/iter; left time: 553.2377s
Epoch: 3 cost time: 76.2287049293518
Epoch: 3, Steps: 318 | Train Loss: 0.1869237 Vali Loss: 0.1900307 Test Loss: 0.1842603
Validation loss decreased (0.202781 --> 0.190031).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1822601
	speed: 0.2434s/iter; left time: 517.7667s
	iters: 200, epoch: 4 | loss: 0.1855341
	speed: 0.2431s/iter; left time: 492.7879s
	iters: 300, epoch: 4 | loss: 0.1623749
	speed: 0.2406s/iter; left time: 463.7045s
Epoch: 4 cost time: 77.05757093429565
Epoch: 4, Steps: 318 | Train Loss: 0.1798099 Vali Loss: 0.1924790 Test Loss: 0.1876387
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1771745
	speed: 0.3047s/iter; left time: 551.1876s
	iters: 200, epoch: 5 | loss: 0.1701217
	speed: 0.2799s/iter; left time: 478.3049s
	iters: 300, epoch: 5 | loss: 0.2413939
	speed: 0.2878s/iter; left time: 463.1405s
Epoch: 5 cost time: 92.71041202545166
Epoch: 5, Steps: 318 | Train Loss: 0.1763083 Vali Loss: 0.1938144 Test Loss: 0.1869573
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1637472
	speed: 0.2418s/iter; left time: 360.5752s
	iters: 200, epoch: 6 | loss: 0.2098486
	speed: 0.2411s/iter; left time: 335.3755s
	iters: 300, epoch: 6 | loss: 0.1697759
	speed: 0.2447s/iter; left time: 315.8754s
Epoch: 6 cost time: 77.03585720062256
Epoch: 6, Steps: 318 | Train Loss: 0.1744246 Vali Loss: 0.1945477 Test Loss: 0.1889005
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.40435194969177246, mae:0.4954342246055603, rmse:0.6358867287635803, mape:0.01771887205541134, mspe:0.000524477509316057, rse:0.44508954882621765, r2_score:0.773397487060849, acc:0.9822811279445887
corr: [36.894733 37.060356 36.80991  36.98247  36.764767 36.45792  36.593853
 36.819252 36.66603  36.861866 36.8631   36.859303 37.072468 37.123592
 37.007996 37.13972  37.26048  37.198055 37.43747  37.628086]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3397903
	speed: 0.1903s/iter; left time: 586.1798s
	iters: 200, epoch: 1 | loss: 0.2890175
	speed: 0.1792s/iter; left time: 534.0535s
	iters: 300, epoch: 1 | loss: 0.2552230
	speed: 0.1759s/iter; left time: 506.7450s
Epoch: 1 cost time: 57.920679330825806
Epoch: 1, Steps: 318 | Train Loss: 0.3427899 Vali Loss: 0.2288786 Test Loss: 0.2194747
Validation loss decreased (inf --> 0.228879).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2794680
	speed: 0.1728s/iter; left time: 477.3782s
	iters: 200, epoch: 2 | loss: 0.2270233
	speed: 0.1821s/iter; left time: 484.8038s
	iters: 300, epoch: 2 | loss: 0.1684293
	speed: 0.1714s/iter; left time: 439.1888s
Epoch: 2 cost time: 55.793073415756226
Epoch: 2, Steps: 318 | Train Loss: 0.2213344 Vali Loss: 0.1974415 Test Loss: 0.2045093
Validation loss decreased (0.228879 --> 0.197442).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1814523
	speed: 0.1718s/iter; left time: 420.0079s
	iters: 200, epoch: 3 | loss: 0.2295960
	speed: 0.1816s/iter; left time: 425.8395s
	iters: 300, epoch: 3 | loss: 0.2008691
	speed: 0.1732s/iter; left time: 388.7653s
Epoch: 3 cost time: 55.84172034263611
Epoch: 3, Steps: 318 | Train Loss: 0.2006814 Vali Loss: 0.2006881 Test Loss: 0.1952738
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1967853
	speed: 0.1786s/iter; left time: 379.8162s
	iters: 200, epoch: 4 | loss: 0.2030739
	speed: 0.1748s/iter; left time: 354.2718s
	iters: 300, epoch: 4 | loss: 0.1964083
	speed: 0.1792s/iter; left time: 345.3505s
Epoch: 4 cost time: 56.865827560424805
Epoch: 4, Steps: 318 | Train Loss: 0.1923211 Vali Loss: 0.2001538 Test Loss: 0.1918557
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1957045
	speed: 0.1761s/iter; left time: 318.5229s
	iters: 200, epoch: 5 | loss: 0.2238218
	speed: 0.1728s/iter; left time: 295.2336s
	iters: 300, epoch: 5 | loss: 0.2044491
	speed: 0.1774s/iter; left time: 285.3685s
Epoch: 5 cost time: 56.0453155040741
Epoch: 5, Steps: 318 | Train Loss: 0.1880503 Vali Loss: 0.2009410 Test Loss: 0.1916166
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000031
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.448787659406662, mae:0.5287455916404724, rmse:0.6699161529541016, mape:0.0188900176435709, mspe:0.0005786858382634819, rse:0.4685952067375183, r2_score:0.7598699177803107, acc:0.9811099823564291
corr: [37.58698  37.557617 37.167694 37.47925  37.364304 37.233833 37.307476
 37.348293 37.19737  37.413353 37.24222  37.38974  37.458786 37.52434
 37.53352  37.66265  37.64426  37.586998 37.78438  37.72429  37.791172
 37.8092   37.953144 37.827484 37.87195 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2985877
	speed: 0.3053s/iter; left time: 937.4279s
	iters: 200, epoch: 1 | loss: 0.2182470
	speed: 0.3413s/iter; left time: 1013.9895s
	iters: 300, epoch: 1 | loss: 0.2473506
	speed: 0.3186s/iter; left time: 914.7010s
Epoch: 1 cost time: 102.42309427261353
Epoch: 1, Steps: 317 | Train Loss: 0.3279483 Vali Loss: 0.2294392 Test Loss: 0.2096504
Validation loss decreased (inf --> 0.229439).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2164243
	speed: 0.3386s/iter; left time: 932.5219s
	iters: 200, epoch: 2 | loss: 0.1614081
	speed: 0.3180s/iter; left time: 843.8572s
	iters: 300, epoch: 2 | loss: 0.2172541
	speed: 0.3295s/iter; left time: 841.6363s
Epoch: 2 cost time: 104.02353596687317
Epoch: 2, Steps: 317 | Train Loss: 0.2162926 Vali Loss: 0.2112721 Test Loss: 0.2021272
Validation loss decreased (0.229439 --> 0.211272).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1979756
	speed: 0.4121s/iter; left time: 1004.3068s
	iters: 200, epoch: 3 | loss: 0.1869219
	speed: 0.4153s/iter; left time: 970.5929s
	iters: 300, epoch: 3 | loss: 0.2280433
	speed: 0.3782s/iter; left time: 846.0471s
Epoch: 3 cost time: 127.49406433105469
Epoch: 3, Steps: 317 | Train Loss: 0.2001296 Vali Loss: 0.2134487 Test Loss: 0.1982569
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1847577
	speed: 0.3431s/iter; left time: 727.3572s
	iters: 200, epoch: 4 | loss: 0.2027693
	speed: 0.3280s/iter; left time: 662.6503s
	iters: 300, epoch: 4 | loss: 0.1869761
	speed: 0.3316s/iter; left time: 636.7371s
Epoch: 4 cost time: 105.58784937858582
Epoch: 4, Steps: 317 | Train Loss: 0.1929564 Vali Loss: 0.2087606 Test Loss: 0.1947898
Validation loss decreased (0.211272 --> 0.208761).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1673675
	speed: 0.3326s/iter; left time: 599.6628s
	iters: 200, epoch: 5 | loss: 0.1803977
	speed: 0.3342s/iter; left time: 569.1043s
	iters: 300, epoch: 5 | loss: 0.1720271
	speed: 0.3334s/iter; left time: 534.3652s
Epoch: 5 cost time: 105.62031698226929
Epoch: 5, Steps: 317 | Train Loss: 0.1896304 Vali Loss: 0.2103659 Test Loss: 0.1956469
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1648289
	speed: 0.3291s/iter; left time: 489.0470s
	iters: 200, epoch: 6 | loss: 0.1444650
	speed: 0.3254s/iter; left time: 450.9618s
	iters: 300, epoch: 6 | loss: 0.1787538
	speed: 0.3299s/iter; left time: 424.2812s
Epoch: 6 cost time: 104.32051920890808
Epoch: 6, Steps: 317 | Train Loss: 0.1876239 Vali Loss: 0.2094871 Test Loss: 0.1961202
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1896899
	speed: 0.3328s/iter; left time: 389.0904s
	iters: 200, epoch: 7 | loss: 0.2067423
	speed: 0.3243s/iter; left time: 346.6819s
	iters: 300, epoch: 7 | loss: 0.1586390
	speed: 0.3195s/iter; left time: 309.5626s
Epoch: 7 cost time: 103.04699349403381
Epoch: 7, Steps: 317 | Train Loss: 0.1874301 Vali Loss: 0.2111658 Test Loss: 0.1977769
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.42745864391326904, mae:0.502771258354187, rmse:0.6538032293319702, mape:0.01799297146499157, mspe:0.0005576057592406869, rse:0.4569960832595825, r2_score:0.7560160391200009, acc:0.9820070285350084
corr: [36.835907 36.860954 36.941696 36.70495  36.80999  36.70549  36.805843
 36.84881  36.733498 36.703094 36.817604 36.807743 36.80387  36.79177
 36.87707  36.7628   36.795586 36.9055   36.812637 37.051678 37.204857
 37.141293 36.97457  36.973083 36.8503   37.04342  37.059326 37.289623
 37.34646  37.110077]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3131411
	speed: 0.2462s/iter; left time: 758.5624s
	iters: 200, epoch: 1 | loss: 0.2160134
	speed: 0.2399s/iter; left time: 715.2905s
	iters: 300, epoch: 1 | loss: 0.1917204
	speed: 0.2563s/iter; left time: 738.4435s
Epoch: 1 cost time: 78.70006990432739
Epoch: 1, Steps: 318 | Train Loss: 0.3135800 Vali Loss: 0.1804177 Test Loss: 0.1624516
Validation loss decreased (inf --> 0.180418).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1800086
	speed: 0.2528s/iter; left time: 698.3663s
	iters: 200, epoch: 2 | loss: 0.1545257
	speed: 0.2594s/iter; left time: 690.9004s
	iters: 300, epoch: 2 | loss: 0.1244093
	speed: 0.2404s/iter; left time: 616.0994s
Epoch: 2 cost time: 80.34488368034363
Epoch: 2, Steps: 318 | Train Loss: 0.1728283 Vali Loss: 0.1693021 Test Loss: 0.1524932
Validation loss decreased (0.180418 --> 0.169302).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1809284
	speed: 0.2500s/iter; left time: 611.2001s
	iters: 200, epoch: 3 | loss: 0.1897997
	speed: 0.2460s/iter; left time: 576.7665s
	iters: 300, epoch: 3 | loss: 0.1259929
	speed: 0.2378s/iter; left time: 533.9519s
Epoch: 3 cost time: 77.70500063896179
Epoch: 3, Steps: 318 | Train Loss: 0.1556212 Vali Loss: 0.1681520 Test Loss: 0.1513243
Validation loss decreased (0.169302 --> 0.168152).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1359134
	speed: 0.2680s/iter; left time: 570.1181s
	iters: 200, epoch: 4 | loss: 0.1472939
	speed: 0.2510s/iter; left time: 508.7431s
	iters: 300, epoch: 4 | loss: 0.1745596
	speed: 0.2666s/iter; left time: 513.7835s
Epoch: 4 cost time: 83.0525872707367
Epoch: 4, Steps: 318 | Train Loss: 0.1494453 Vali Loss: 0.1624442 Test Loss: 0.1460530
Validation loss decreased (0.168152 --> 0.162444).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1700838
	speed: 0.2489s/iter; left time: 450.3088s
	iters: 200, epoch: 5 | loss: 0.1498535
	speed: 0.2517s/iter; left time: 430.1170s
	iters: 300, epoch: 5 | loss: 0.1364540
	speed: 0.2383s/iter; left time: 383.4958s
Epoch: 5 cost time: 78.24486541748047
Epoch: 5, Steps: 318 | Train Loss: 0.1450652 Vali Loss: 0.1620854 Test Loss: 0.1462980
Validation loss decreased (0.162444 --> 0.162085).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1216157
	speed: 0.2505s/iter; left time: 373.5404s
	iters: 200, epoch: 6 | loss: 0.1247603
	speed: 0.2364s/iter; left time: 328.7711s
	iters: 300, epoch: 6 | loss: 0.1405027
	speed: 0.2390s/iter; left time: 308.5188s
Epoch: 6 cost time: 77.04819583892822
Epoch: 6, Steps: 318 | Train Loss: 0.1434900 Vali Loss: 0.1610176 Test Loss: 0.1462048
Validation loss decreased (0.162085 --> 0.161018).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1318623
	speed: 0.2474s/iter; left time: 290.1592s
	iters: 200, epoch: 7 | loss: 0.1436011
	speed: 0.2508s/iter; left time: 269.1170s
	iters: 300, epoch: 7 | loss: 0.1741052
	speed: 0.2445s/iter; left time: 237.8534s
Epoch: 7 cost time: 79.27314233779907
Epoch: 7, Steps: 318 | Train Loss: 0.1429101 Vali Loss: 0.1626446 Test Loss: 0.1468784
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1262957
	speed: 0.2491s/iter; left time: 213.0023s
	iters: 200, epoch: 8 | loss: 0.1024048
	speed: 0.2472s/iter; left time: 186.6003s
	iters: 300, epoch: 8 | loss: 0.1466734
	speed: 0.2451s/iter; left time: 160.5360s
Epoch: 8 cost time: 79.25208044052124
Epoch: 8, Steps: 318 | Train Loss: 0.1415143 Vali Loss: 0.1601743 Test Loss: 0.1459686
Validation loss decreased (0.161018 --> 0.160174).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1619184
	speed: 0.2404s/iter; left time: 129.0995s
	iters: 200, epoch: 9 | loss: 0.1294680
	speed: 0.2458s/iter; left time: 107.4264s
	iters: 300, epoch: 9 | loss: 0.1424655
	speed: 0.2439s/iter; left time: 82.1848s
Epoch: 9 cost time: 77.58529710769653
Epoch: 9, Steps: 318 | Train Loss: 0.1417523 Vali Loss: 0.1611577 Test Loss: 0.1463348
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1191238
	speed: 0.2215s/iter; left time: 48.5036s
	iters: 200, epoch: 10 | loss: 0.1978935
	speed: 0.2154s/iter; left time: 25.6271s
	iters: 300, epoch: 10 | loss: 0.1438857
	speed: 0.2166s/iter; left time: 4.1156s
Epoch: 10 cost time: 69.41845273971558
Epoch: 10, Steps: 318 | Train Loss: 0.1421321 Vali Loss: 0.1599709 Test Loss: 0.1463643
Validation loss decreased (0.160174 --> 0.159971).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.32119065523147583, mae:0.44056668877601624, rmse:0.5667368769645691, mape:0.015745900571346283, mspe:0.0004155447822995484, rse:0.39710482954978943, r2_score:0.8331096175571769, acc:0.9842540994286537
corr: [36.83943  36.862423 36.88311  36.96638  36.64616  36.71806  36.89547
 36.82518  36.916344 36.83684 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3261133
	speed: 0.2196s/iter; left time: 676.6696s
	iters: 200, epoch: 1 | loss: 0.1846730
	speed: 0.2179s/iter; left time: 649.4693s
	iters: 300, epoch: 1 | loss: 0.2758075
	speed: 0.2253s/iter; left time: 649.0794s
Epoch: 1 cost time: 70.39762830734253
Epoch: 1, Steps: 318 | Train Loss: 0.3137686 Vali Loss: 0.1980302 Test Loss: 0.1924240
Validation loss decreased (inf --> 0.198030).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1999362
	speed: 0.2321s/iter; left time: 641.3509s
	iters: 200, epoch: 2 | loss: 0.1307588
	speed: 0.2335s/iter; left time: 621.7552s
	iters: 300, epoch: 2 | loss: 0.1507519
	speed: 0.2309s/iter; left time: 591.7373s
Epoch: 2 cost time: 73.92696952819824
Epoch: 2, Steps: 318 | Train Loss: 0.1835181 Vali Loss: 0.1792736 Test Loss: 0.1619946
Validation loss decreased (0.198030 --> 0.179274).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1717181
	speed: 0.2818s/iter; left time: 689.0714s
	iters: 200, epoch: 3 | loss: 0.1973766
	speed: 0.2727s/iter; left time: 639.5431s
	iters: 300, epoch: 3 | loss: 0.1536420
	speed: 0.2323s/iter; left time: 521.4762s
Epoch: 3 cost time: 82.75760388374329
Epoch: 3, Steps: 318 | Train Loss: 0.1689265 Vali Loss: 0.1780059 Test Loss: 0.1635914
Validation loss decreased (0.179274 --> 0.178006).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1565994
	speed: 0.2376s/iter; left time: 505.3420s
	iters: 200, epoch: 4 | loss: 0.1617168
	speed: 0.2336s/iter; left time: 473.5988s
	iters: 300, epoch: 4 | loss: 0.1936325
	speed: 0.2345s/iter; left time: 451.9680s
Epoch: 4 cost time: 74.66992092132568
Epoch: 4, Steps: 318 | Train Loss: 0.1631523 Vali Loss: 0.1755383 Test Loss: 0.1594718
Validation loss decreased (0.178006 --> 0.175538).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1384401
	speed: 0.2405s/iter; left time: 435.0180s
	iters: 200, epoch: 5 | loss: 0.1217243
	speed: 0.2345s/iter; left time: 400.8112s
	iters: 300, epoch: 5 | loss: 0.1706820
	speed: 0.2350s/iter; left time: 378.0381s
Epoch: 5 cost time: 75.27162599563599
Epoch: 5, Steps: 318 | Train Loss: 0.1601480 Vali Loss: 0.1747711 Test Loss: 0.1604565
Validation loss decreased (0.175538 --> 0.174771).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1698713
	speed: 0.3133s/iter; left time: 467.1801s
	iters: 200, epoch: 6 | loss: 0.1887835
	speed: 0.2924s/iter; left time: 406.7495s
	iters: 300, epoch: 6 | loss: 0.1912747
	speed: 0.2775s/iter; left time: 358.2593s
Epoch: 6 cost time: 92.90979599952698
Epoch: 6, Steps: 318 | Train Loss: 0.1584060 Vali Loss: 0.1734588 Test Loss: 0.1592976
Validation loss decreased (0.174771 --> 0.173459).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1522492
	speed: 0.2482s/iter; left time: 291.1808s
	iters: 200, epoch: 7 | loss: 0.1848136
	speed: 0.2709s/iter; left time: 290.6434s
	iters: 300, epoch: 7 | loss: 0.1388362
	speed: 0.2556s/iter; left time: 248.7217s
Epoch: 7 cost time: 82.78553056716919
Epoch: 7, Steps: 318 | Train Loss: 0.1576358 Vali Loss: 0.1739001 Test Loss: 0.1595911
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1381107
	speed: 0.2637s/iter; left time: 225.4655s
	iters: 200, epoch: 8 | loss: 0.1576689
	speed: 0.2873s/iter; left time: 216.9339s
	iters: 300, epoch: 8 | loss: 0.1757699
	speed: 0.2688s/iter; left time: 176.0803s
Epoch: 8 cost time: 86.95116257667542
Epoch: 8, Steps: 318 | Train Loss: 0.1574312 Vali Loss: 0.1728016 Test Loss: 0.1585200
Validation loss decreased (0.173459 --> 0.172802).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1725353
	speed: 0.2876s/iter; left time: 154.4615s
	iters: 200, epoch: 9 | loss: 0.1518877
	speed: 0.2666s/iter; left time: 116.5218s
	iters: 300, epoch: 9 | loss: 0.1511672
	speed: 0.2837s/iter; left time: 95.6079s
Epoch: 9 cost time: 88.79814052581787
Epoch: 9, Steps: 318 | Train Loss: 0.1571593 Vali Loss: 0.1733472 Test Loss: 0.1582949
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1653305
	speed: 0.2833s/iter; left time: 62.0396s
	iters: 200, epoch: 10 | loss: 0.1448564
	speed: 0.2771s/iter; left time: 32.9807s
	iters: 300, epoch: 10 | loss: 0.1190156
	speed: 0.2764s/iter; left time: 5.2516s
Epoch: 10 cost time: 88.17813396453857
Epoch: 10, Steps: 318 | Train Loss: 0.1571772 Vali Loss: 0.1730106 Test Loss: 0.1583151
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.3478659689426422, mae:0.45998415350914, rmse:0.5898016095161438, mape:0.016441944986581802, mspe:0.00044999917736276984, rse:0.4130714237689972, r2_score:0.8117969464363404, acc:0.9835580550134182
corr: [36.868587 37.049007 37.04881  37.131344 36.89148  36.822704 36.684956
 36.773773 36.634037 36.767067 36.827    37.07292  37.132557 37.061504
 36.985092]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2626909
	speed: 0.2359s/iter; left time: 726.7861s
	iters: 200, epoch: 1 | loss: 0.2090391
	speed: 0.2388s/iter; left time: 712.0110s
	iters: 300, epoch: 1 | loss: 0.1675131
	speed: 0.2976s/iter; left time: 857.3859s
Epoch: 1 cost time: 84.57801651954651
Epoch: 1, Steps: 318 | Train Loss: 0.3252027 Vali Loss: 0.2102060 Test Loss: 0.1963900
Validation loss decreased (inf --> 0.210206).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1762976
	speed: 0.3106s/iter; left time: 858.2066s
	iters: 200, epoch: 2 | loss: 0.1715784
	speed: 0.3253s/iter; left time: 866.3422s
	iters: 300, epoch: 2 | loss: 0.2975273
	speed: 0.3015s/iter; left time: 772.7494s
Epoch: 2 cost time: 99.070317029953
Epoch: 2, Steps: 318 | Train Loss: 0.2027714 Vali Loss: 0.2041649 Test Loss: 0.1885287
Validation loss decreased (0.210206 --> 0.204165).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1785941
	speed: 0.2927s/iter; left time: 715.5494s
	iters: 200, epoch: 3 | loss: 0.2515380
	speed: 0.3113s/iter; left time: 729.9533s
	iters: 300, epoch: 3 | loss: 0.1821954
	speed: 0.2950s/iter; left time: 662.1857s
Epoch: 3 cost time: 96.1754629611969
Epoch: 3, Steps: 318 | Train Loss: 0.1865765 Vali Loss: 0.1944633 Test Loss: 0.1793029
Validation loss decreased (0.204165 --> 0.194463).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1734419
	speed: 0.3146s/iter; left time: 669.2568s
	iters: 200, epoch: 4 | loss: 0.1953008
	speed: 0.2997s/iter; left time: 607.4259s
	iters: 300, epoch: 4 | loss: 0.1620825
	speed: 0.2965s/iter; left time: 571.3590s
Epoch: 4 cost time: 96.81026482582092
Epoch: 4, Steps: 318 | Train Loss: 0.1802988 Vali Loss: 0.1958446 Test Loss: 0.1812642
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1783327
	speed: 0.3076s/iter; left time: 556.3836s
	iters: 200, epoch: 5 | loss: 0.1736909
	speed: 0.2983s/iter; left time: 509.8073s
	iters: 300, epoch: 5 | loss: 0.2426193
	speed: 0.3193s/iter; left time: 513.8153s
Epoch: 5 cost time: 98.21474051475525
Epoch: 5, Steps: 318 | Train Loss: 0.1765768 Vali Loss: 0.1994794 Test Loss: 0.1814028
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1753533
	speed: 0.2976s/iter; left time: 443.7768s
	iters: 200, epoch: 6 | loss: 0.1923219
	speed: 0.3148s/iter; left time: 437.9525s
	iters: 300, epoch: 6 | loss: 0.1659581
	speed: 0.3117s/iter; left time: 402.4497s
Epoch: 6 cost time: 98.8799295425415
Epoch: 6, Steps: 318 | Train Loss: 0.1750458 Vali Loss: 0.1996418 Test Loss: 0.1818438
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.3934732675552368, mae:0.4850201904773712, rmse:0.6272744536399841, mape:0.017369171604514122, mspe:0.0005132672376930714, rse:0.43906137347221375, r2_score:0.7755243528739689, acc:0.9826308283954859
corr: [36.932434 37.101    36.898438 37.008625 36.92762  36.711525 36.651405
 36.800053 36.674587 36.910263 36.789967 36.98032  36.8803   36.812225
 36.85705  36.92338  36.83723  36.910313 37.223152 37.29321 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3508685
	speed: 0.2305s/iter; left time: 710.3086s
	iters: 200, epoch: 1 | loss: 0.2602715
	speed: 0.2360s/iter; left time: 703.4506s
	iters: 300, epoch: 1 | loss: 0.2585243
	speed: 0.2234s/iter; left time: 643.6389s
Epoch: 1 cost time: 73.17543601989746
Epoch: 1, Steps: 318 | Train Loss: 0.3430772 Vali Loss: 0.2400956 Test Loss: 0.2262156
Validation loss decreased (inf --> 0.240096).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2763667
	speed: 0.1684s/iter; left time: 465.2449s
	iters: 200, epoch: 2 | loss: 0.2161345
	speed: 0.1506s/iter; left time: 401.0387s
	iters: 300, epoch: 2 | loss: 0.1561238
	speed: 0.1592s/iter; left time: 407.9508s
Epoch: 2 cost time: 50.4554660320282
Epoch: 2, Steps: 318 | Train Loss: 0.2233033 Vali Loss: 0.2090854 Test Loss: 0.2009320
Validation loss decreased (0.240096 --> 0.209085).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1820589
	speed: 0.2235s/iter; left time: 546.4290s
	iters: 200, epoch: 3 | loss: 0.2275114
	speed: 0.2130s/iter; left time: 499.4018s
	iters: 300, epoch: 3 | loss: 0.2018072
	speed: 0.2114s/iter; left time: 474.5713s
Epoch: 3 cost time: 68.63736128807068
Epoch: 3, Steps: 318 | Train Loss: 0.2029272 Vali Loss: 0.2082914 Test Loss: 0.2016317
Validation loss decreased (0.209085 --> 0.208291).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1933069
	speed: 0.2267s/iter; left time: 482.2263s
	iters: 200, epoch: 4 | loss: 0.2059855
	speed: 0.2171s/iter; left time: 440.1151s
	iters: 300, epoch: 4 | loss: 0.2039588
	speed: 0.2233s/iter; left time: 430.2496s
Epoch: 4 cost time: 70.79649305343628
Epoch: 4, Steps: 318 | Train Loss: 0.1942408 Vali Loss: 0.2038341 Test Loss: 0.1940428
Validation loss decreased (0.208291 --> 0.203834).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2021711
	speed: 0.2265s/iter; left time: 409.8172s
	iters: 200, epoch: 5 | loss: 0.2138515
	speed: 0.2208s/iter; left time: 377.3577s
	iters: 300, epoch: 5 | loss: 0.2199758
	speed: 0.2241s/iter; left time: 360.6321s
Epoch: 5 cost time: 71.16785860061646
Epoch: 5, Steps: 318 | Train Loss: 0.1892824 Vali Loss: 0.2044254 Test Loss: 0.1926414
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1599278
	speed: 0.2247s/iter; left time: 335.0568s
	iters: 200, epoch: 6 | loss: 0.1596899
	speed: 0.2204s/iter; left time: 306.6321s
	iters: 300, epoch: 6 | loss: 0.1846783
	speed: 0.2124s/iter; left time: 274.2122s
Epoch: 6 cost time: 69.68712258338928
Epoch: 6, Steps: 318 | Train Loss: 0.1872003 Vali Loss: 0.2025280 Test Loss: 0.1942433
Validation loss decreased (0.203834 --> 0.202528).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2126319
	speed: 0.2173s/iter; left time: 254.9190s
	iters: 200, epoch: 7 | loss: 0.2054889
	speed: 0.2062s/iter; left time: 221.3029s
	iters: 300, epoch: 7 | loss: 0.2287015
	speed: 0.2181s/iter; left time: 212.1866s
Epoch: 7 cost time: 68.15166568756104
Epoch: 7, Steps: 318 | Train Loss: 0.1853569 Vali Loss: 0.2019056 Test Loss: 0.1927238
Validation loss decreased (0.202528 --> 0.201906).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1842301
	speed: 0.2187s/iter; left time: 186.9639s
	iters: 200, epoch: 8 | loss: 0.1998555
	speed: 0.2150s/iter; left time: 162.3334s
	iters: 300, epoch: 8 | loss: 0.1498204
	speed: 0.2221s/iter; left time: 145.4762s
Epoch: 8 cost time: 69.98649096488953
Epoch: 8, Steps: 318 | Train Loss: 0.1852809 Vali Loss: 0.2034703 Test Loss: 0.1926801
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1785110
	speed: 0.2223s/iter; left time: 119.3874s
	iters: 200, epoch: 9 | loss: 0.2758704
	speed: 0.2177s/iter; left time: 95.1279s
	iters: 300, epoch: 9 | loss: 0.1543690
	speed: 0.2028s/iter; left time: 68.3297s
Epoch: 9 cost time: 68.21381902694702
Epoch: 9, Steps: 318 | Train Loss: 0.1850828 Vali Loss: 0.2026219 Test Loss: 0.1928204
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2066317
	speed: 0.2284s/iter; left time: 50.0207s
	iters: 200, epoch: 10 | loss: 0.1870216
	speed: 0.2059s/iter; left time: 24.5064s
	iters: 300, epoch: 10 | loss: 0.1405005
	speed: 0.2137s/iter; left time: 4.0599s
Epoch: 10 cost time: 68.80148935317993
Epoch: 10, Steps: 318 | Train Loss: 0.1846487 Vali Loss: 0.2035420 Test Loss: 0.1927468
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.4229249358177185, mae:0.5096739530563354, rmse:0.6503267884254456, mape:0.018228311091661453, mspe:0.000548400217667222, rse:0.4548927843570709, r2_score:0.7743236583648547, acc:0.9817716889083385
corr: [37.264614 37.288918 37.01657  37.232933 37.070763 36.909447 36.97116
 36.95092  36.852066 37.16807  37.005795 37.143085 37.171497 37.15461
 37.229473 37.368816 37.258083 37.27151  37.41112  37.449165 37.490364
 37.49951  37.62983  37.530796 37.634003]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2965135
	speed: 0.4049s/iter; left time: 1243.3184s
	iters: 200, epoch: 1 | loss: 0.2208954
	speed: 0.3835s/iter; left time: 1139.4264s
	iters: 300, epoch: 1 | loss: 0.2418196
	speed: 0.3855s/iter; left time: 1106.7347s
Epoch: 1 cost time: 122.5366358757019
Epoch: 1, Steps: 317 | Train Loss: 0.3316150 Vali Loss: 0.2179272 Test Loss: 0.2093674
Validation loss decreased (inf --> 0.217927).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2213431
	speed: 0.3340s/iter; left time: 919.8551s
	iters: 200, epoch: 2 | loss: 0.1561009
	speed: 0.3403s/iter; left time: 903.1170s
	iters: 300, epoch: 2 | loss: 0.1997128
	speed: 0.3245s/iter; left time: 828.7899s
Epoch: 2 cost time: 104.96492910385132
Epoch: 2, Steps: 317 | Train Loss: 0.2142866 Vali Loss: 0.2078307 Test Loss: 0.2029107
Validation loss decreased (0.217927 --> 0.207831).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1949685
	speed: 0.3474s/iter; left time: 846.6340s
	iters: 200, epoch: 3 | loss: 0.1857522
	speed: 0.3146s/iter; left time: 735.2073s
	iters: 300, epoch: 3 | loss: 0.2279166
	speed: 0.3216s/iter; left time: 719.3911s
Epoch: 3 cost time: 103.99073433876038
Epoch: 3, Steps: 317 | Train Loss: 0.1993572 Vali Loss: 0.2078698 Test Loss: 0.1983817
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1766393
	speed: 0.3440s/iter; left time: 729.2071s
	iters: 200, epoch: 4 | loss: 0.2007910
	speed: 0.3477s/iter; left time: 702.2802s
	iters: 300, epoch: 4 | loss: 0.1853327
	speed: 0.3458s/iter; left time: 663.9200s
Epoch: 4 cost time: 109.96582436561584
Epoch: 4, Steps: 317 | Train Loss: 0.1932137 Vali Loss: 0.2064046 Test Loss: 0.1957418
Validation loss decreased (0.207831 --> 0.206405).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1709972
	speed: 0.3331s/iter; left time: 600.4910s
	iters: 200, epoch: 5 | loss: 0.1790051
	speed: 0.3191s/iter; left time: 543.4271s
	iters: 300, epoch: 5 | loss: 0.1711854
	speed: 0.3512s/iter; left time: 563.0017s
Epoch: 5 cost time: 106.65565419197083
Epoch: 5, Steps: 317 | Train Loss: 0.1903206 Vali Loss: 0.2072463 Test Loss: 0.1966388
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1662406
	speed: 0.3483s/iter; left time: 517.5423s
	iters: 200, epoch: 6 | loss: 0.1517916
	speed: 0.3389s/iter; left time: 469.6601s
	iters: 300, epoch: 6 | loss: 0.1851209
	speed: 0.3554s/iter; left time: 457.0780s
Epoch: 6 cost time: 110.0200023651123
Epoch: 6, Steps: 317 | Train Loss: 0.1886290 Vali Loss: 0.2058134 Test Loss: 0.1972519
Validation loss decreased (0.206405 --> 0.205813).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1828691
	speed: 0.3533s/iter; left time: 413.0069s
	iters: 200, epoch: 7 | loss: 0.1994801
	speed: 0.3524s/iter; left time: 376.7196s
	iters: 300, epoch: 7 | loss: 0.1637272
	speed: 0.3446s/iter; left time: 333.9199s
Epoch: 7 cost time: 110.97622394561768
Epoch: 7, Steps: 317 | Train Loss: 0.1884637 Vali Loss: 0.2069209 Test Loss: 0.1986685
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1758622
	speed: 0.3520s/iter; left time: 299.9258s
	iters: 200, epoch: 8 | loss: 0.1939797
	speed: 0.3594s/iter; left time: 270.2338s
	iters: 300, epoch: 8 | loss: 0.2175232
	speed: 0.3535s/iter; left time: 230.4568s
Epoch: 8 cost time: 113.08201146125793
Epoch: 8, Steps: 317 | Train Loss: 0.1876233 Vali Loss: 0.2070707 Test Loss: 0.1985830
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2265407
	speed: 0.3553s/iter; left time: 190.0788s
	iters: 200, epoch: 9 | loss: 0.1742084
	speed: 0.3964s/iter; left time: 172.4166s
	iters: 300, epoch: 9 | loss: 0.1775313
	speed: 0.3307s/iter; left time: 110.7705s
Epoch: 9 cost time: 113.98502135276794
Epoch: 9, Steps: 317 | Train Loss: 0.1877693 Vali Loss: 0.2072120 Test Loss: 0.1991012
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.4328616261482239, mae:0.5073050856590271, rmse:0.6579222083091736, mape:0.01813982054591179, mspe:0.0005625780904665589, rse:0.4598751962184906, r2_score:0.7569162982225079, acc:0.9818601794540882
corr: [36.833336 36.797268 36.802467 36.80345  36.84851  36.6934   36.72424
 36.72236  36.60961  36.57188  36.64697  36.81752  36.764103 36.692738
 36.77759  36.6882   36.655704 36.714245 36.666954 37.03244  37.130127
 37.021748 36.893005 36.866936 36.773426 36.93959  36.97647  37.2076
 37.231976 37.083374]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2999049
	speed: 0.1728s/iter; left time: 532.2616s
	iters: 200, epoch: 1 | loss: 0.2366131
	speed: 0.1249s/iter; left time: 372.4471s
	iters: 300, epoch: 1 | loss: 0.2574269
	speed: 0.1868s/iter; left time: 538.1454s
Epoch: 1 cost time: 52.09219551086426
Epoch: 1, Steps: 318 | Train Loss: 0.3237533 Vali Loss: 0.1843658 Test Loss: 0.2337999
Validation loss decreased (inf --> 0.184366).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1910233
	speed: 0.2711s/iter; left time: 748.9508s
	iters: 200, epoch: 2 | loss: 0.2211971
	speed: 0.2640s/iter; left time: 702.9147s
	iters: 300, epoch: 2 | loss: 0.1963276
	speed: 0.2606s/iter; left time: 668.0005s
Epoch: 2 cost time: 84.4574601650238
Epoch: 2, Steps: 318 | Train Loss: 0.1796996 Vali Loss: 0.1462066 Test Loss: 0.2032702
Validation loss decreased (0.184366 --> 0.146207).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1528734
	speed: 0.2738s/iter; left time: 669.3354s
	iters: 200, epoch: 3 | loss: 0.1279067
	speed: 0.2639s/iter; left time: 618.8637s
	iters: 300, epoch: 3 | loss: 0.1301135
	speed: 0.2622s/iter; left time: 588.5950s
Epoch: 3 cost time: 85.15616631507874
Epoch: 3, Steps: 318 | Train Loss: 0.1632712 Vali Loss: 0.1418667 Test Loss: 0.2012335
Validation loss decreased (0.146207 --> 0.141867).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1932786
	speed: 0.2232s/iter; left time: 474.7710s
	iters: 200, epoch: 4 | loss: 0.1423115
	speed: 0.2100s/iter; left time: 425.6215s
	iters: 300, epoch: 4 | loss: 0.1497395
	speed: 0.2713s/iter; left time: 522.8393s
Epoch: 4 cost time: 75.70644855499268
Epoch: 4, Steps: 318 | Train Loss: 0.1558988 Vali Loss: 0.1411920 Test Loss: 0.2073502
Validation loss decreased (0.141867 --> 0.141192).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0998551
	speed: 0.2200s/iter; left time: 397.9712s
	iters: 200, epoch: 5 | loss: 0.1596471
	speed: 0.2177s/iter; left time: 371.9858s
	iters: 300, epoch: 5 | loss: 0.1696369
	speed: 0.2046s/iter; left time: 329.1506s
Epoch: 5 cost time: 67.92026686668396
Epoch: 5, Steps: 318 | Train Loss: 0.1536292 Vali Loss: 0.1410454 Test Loss: 0.2085752
Validation loss decreased (0.141192 --> 0.141045).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0994749
	speed: 0.2263s/iter; left time: 337.3906s
	iters: 200, epoch: 6 | loss: 0.1517671
	speed: 0.2289s/iter; left time: 318.3850s
	iters: 300, epoch: 6 | loss: 0.1664030
	speed: 0.2112s/iter; left time: 272.6920s
Epoch: 6 cost time: 70.40158700942993
Epoch: 6, Steps: 318 | Train Loss: 0.1520798 Vali Loss: 0.1396422 Test Loss: 0.2083746
Validation loss decreased (0.141045 --> 0.139642).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1574902
	speed: 0.2188s/iter; left time: 256.7028s
	iters: 200, epoch: 7 | loss: 0.1408353
	speed: 0.2089s/iter; left time: 224.1935s
	iters: 300, epoch: 7 | loss: 0.1764554
	speed: 0.2126s/iter; left time: 206.8190s
Epoch: 7 cost time: 68.09279990196228
Epoch: 7, Steps: 318 | Train Loss: 0.1509931 Vali Loss: 0.1401352 Test Loss: 0.2089441
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1735263
	speed: 0.2422s/iter; left time: 207.0862s
	iters: 200, epoch: 8 | loss: 0.2179326
	speed: 0.2173s/iter; left time: 164.0483s
	iters: 300, epoch: 8 | loss: 0.1045242
	speed: 0.2125s/iter; left time: 139.1840s
Epoch: 8 cost time: 71.27139139175415
Epoch: 8, Steps: 318 | Train Loss: 0.1511025 Vali Loss: 0.1398165 Test Loss: 0.2085241
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1629253
	speed: 0.2288s/iter; left time: 122.8556s
	iters: 200, epoch: 9 | loss: 0.1402688
	speed: 0.2047s/iter; left time: 89.4432s
	iters: 300, epoch: 9 | loss: 0.1290272
	speed: 0.2105s/iter; left time: 70.9492s
Epoch: 9 cost time: 68.39221167564392
Epoch: 9, Steps: 318 | Train Loss: 0.1506013 Vali Loss: 0.1391048 Test Loss: 0.2085704
Validation loss decreased (0.139642 --> 0.139105).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1842141
	speed: 0.2204s/iter; left time: 48.2772s
	iters: 200, epoch: 10 | loss: 0.1330680
	speed: 0.2167s/iter; left time: 25.7878s
	iters: 300, epoch: 10 | loss: 0.1422531
	speed: 0.2186s/iter; left time: 4.1539s
Epoch: 10 cost time: 69.77403664588928
Epoch: 10, Steps: 318 | Train Loss: 0.1506871 Vali Loss: 0.1399586 Test Loss: 0.2084896
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.4576995074748993, mae:0.5090056657791138, rmse:0.6765349507331848, mape:0.018272584304213524, mspe:0.0006049965741112828, rse:0.47403883934020996, r2_score:0.7370291122558551, acc:0.9817274156957865
corr: [35.709106 36.062042 36.079002 35.972157 36.063046 35.986023 34.77146
 35.586563 35.62051  35.085472]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2441608
	speed: 0.1268s/iter; left time: 390.7985s
	iters: 200, epoch: 1 | loss: 0.2178119
	speed: 0.1141s/iter; left time: 340.2041s
	iters: 300, epoch: 1 | loss: 0.2404357
	speed: 0.1102s/iter; left time: 317.6028s
Epoch: 1 cost time: 37.22786545753479
Epoch: 1, Steps: 318 | Train Loss: 0.3008427 Vali Loss: 0.2023629 Test Loss: 0.3617461
Validation loss decreased (inf --> 0.202363).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1820651
	speed: 0.1071s/iter; left time: 295.9816s
	iters: 200, epoch: 2 | loss: 0.2021156
	speed: 0.1062s/iter; left time: 282.6986s
	iters: 300, epoch: 2 | loss: 0.2244719
	speed: 0.1099s/iter; left time: 281.6264s
Epoch: 2 cost time: 34.30645442008972
Epoch: 2, Steps: 318 | Train Loss: 0.1925401 Vali Loss: 0.1858138 Test Loss: 0.3038602
Validation loss decreased (0.202363 --> 0.185814).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1863842
	speed: 0.1070s/iter; left time: 261.6257s
	iters: 200, epoch: 3 | loss: 0.1693527
	speed: 0.1017s/iter; left time: 238.4341s
	iters: 300, epoch: 3 | loss: 0.1902151
	speed: 0.1099s/iter; left time: 246.7695s
Epoch: 3 cost time: 33.78106641769409
Epoch: 3, Steps: 318 | Train Loss: 0.1699622 Vali Loss: 0.1846733 Test Loss: 0.3105655
Validation loss decreased (0.185814 --> 0.184673).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1672512
	speed: 0.1188s/iter; left time: 252.7633s
	iters: 200, epoch: 4 | loss: 0.1784664
	speed: 0.1139s/iter; left time: 230.9485s
	iters: 300, epoch: 4 | loss: 0.1514753
	speed: 0.1024s/iter; left time: 197.2301s
Epoch: 4 cost time: 35.50103712081909
Epoch: 4, Steps: 318 | Train Loss: 0.1635212 Vali Loss: 0.1755977 Test Loss: 0.3251865
Validation loss decreased (0.184673 --> 0.175598).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1101830
	speed: 0.1180s/iter; left time: 213.5219s
	iters: 200, epoch: 5 | loss: 0.1650745
	speed: 0.1124s/iter; left time: 192.0782s
	iters: 300, epoch: 5 | loss: 0.1702062
	speed: 0.1079s/iter; left time: 173.5343s
Epoch: 5 cost time: 36.16636943817139
Epoch: 5, Steps: 318 | Train Loss: 0.1606806 Vali Loss: 0.1776010 Test Loss: 0.3263080
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1514713
	speed: 0.1143s/iter; left time: 170.3666s
	iters: 200, epoch: 6 | loss: 0.1719127
	speed: 0.1082s/iter; left time: 150.4621s
	iters: 300, epoch: 6 | loss: 0.1465859
	speed: 0.1030s/iter; left time: 132.9425s
Epoch: 6 cost time: 34.44520330429077
Epoch: 6, Steps: 318 | Train Loss: 0.1584704 Vali Loss: 0.1801537 Test Loss: 0.3348623
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1317801
	speed: 0.1185s/iter; left time: 138.9608s
	iters: 200, epoch: 7 | loss: 0.1479889
	speed: 0.1104s/iter; left time: 118.4942s
	iters: 300, epoch: 7 | loss: 0.1605808
	speed: 0.1064s/iter; left time: 103.5065s
Epoch: 7 cost time: 35.667874813079834
Epoch: 7, Steps: 318 | Train Loss: 0.1579627 Vali Loss: 0.1786172 Test Loss: 0.3298378
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.7136090993881226, mae:0.6486051082611084, rmse:0.8447538614273071, mape:0.023281564936041832, mspe:0.0009387579630129039, rse:0.591628909111023, r2_score:0.5880514545260725, acc:0.9767184350639582
corr: [34.617527 34.928932 34.581722 34.054543 34.434277 35.113525 35.56097
 34.998955 35.248505 35.44958  35.57916  35.30241  35.112473 33.28714
 33.536762]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2712373
	speed: 0.1843s/iter; left time: 567.8086s
	iters: 200, epoch: 1 | loss: 0.2714491
	speed: 0.1418s/iter; left time: 422.6709s
	iters: 300, epoch: 1 | loss: 0.2384208
	speed: 0.1475s/iter; left time: 424.9352s
Epoch: 1 cost time: 49.81485915184021
Epoch: 1, Steps: 318 | Train Loss: 0.3378631 Vali Loss: 0.2289252 Test Loss: 0.2481709
Validation loss decreased (inf --> 0.228925).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2046482
	speed: 0.1606s/iter; left time: 443.7188s
	iters: 200, epoch: 2 | loss: 0.2279894
	speed: 0.1417s/iter; left time: 377.2796s
	iters: 300, epoch: 2 | loss: 0.1901106
	speed: 0.1424s/iter; left time: 365.0482s
Epoch: 2 cost time: 47.05443596839905
Epoch: 2, Steps: 318 | Train Loss: 0.2103766 Vali Loss: 0.2292376 Test Loss: 0.2207038
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2005474
	speed: 0.1447s/iter; left time: 353.7190s
	iters: 200, epoch: 3 | loss: 0.2086568
	speed: 0.1576s/iter; left time: 369.5882s
	iters: 300, epoch: 3 | loss: 0.1473159
	speed: 0.1560s/iter; left time: 350.1600s
Epoch: 3 cost time: 48.248751640319824
Epoch: 3, Steps: 318 | Train Loss: 0.1959800 Vali Loss: 0.2166607 Test Loss: 0.2084959
Validation loss decreased (0.228925 --> 0.216661).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1409189
	speed: 0.1516s/iter; left time: 322.5084s
	iters: 200, epoch: 4 | loss: 0.1748249
	speed: 0.1469s/iter; left time: 297.8375s
	iters: 300, epoch: 4 | loss: 0.1472468
	speed: 0.1484s/iter; left time: 285.9233s
Epoch: 4 cost time: 47.43005323410034
Epoch: 4, Steps: 318 | Train Loss: 0.1898131 Vali Loss: 0.2030906 Test Loss: 0.2059296
Validation loss decreased (0.216661 --> 0.203091).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1863848
	speed: 0.1427s/iter; left time: 258.1856s
	iters: 200, epoch: 5 | loss: 0.1675172
	speed: 0.1388s/iter; left time: 237.2869s
	iters: 300, epoch: 5 | loss: 0.2797420
	speed: 0.1470s/iter; left time: 236.5149s
Epoch: 5 cost time: 45.405330419540405
Epoch: 5, Steps: 318 | Train Loss: 0.1937654 Vali Loss: 0.2164671 Test Loss: 0.2064085
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1544507
	speed: 0.1511s/iter; left time: 225.2880s
	iters: 200, epoch: 6 | loss: 0.1749592
	speed: 0.1490s/iter; left time: 207.2433s
	iters: 300, epoch: 6 | loss: 0.1755364
	speed: 0.1421s/iter; left time: 183.4809s
Epoch: 6 cost time: 47.109673261642456
Epoch: 6, Steps: 318 | Train Loss: 0.1918030 Vali Loss: 0.2199499 Test Loss: 0.2080746
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2441072
	speed: 0.1504s/iter; left time: 176.4164s
	iters: 200, epoch: 7 | loss: 0.1707487
	speed: 0.1386s/iter; left time: 148.7219s
	iters: 300, epoch: 7 | loss: 0.1692756
	speed: 0.1299s/iter; left time: 126.3623s
Epoch: 7 cost time: 44.351035833358765
Epoch: 7, Steps: 318 | Train Loss: 0.1949569 Vali Loss: 0.2195019 Test Loss: 0.2059520
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.4519045948982239, mae:0.5147724151611328, rmse:0.6722384691238403, mape:0.018456894904375076, mspe:0.0005914949579164386, rse:0.47053396701812744, r2_score:0.7410114498315342, acc:0.9815431050956249
corr: [35.751797 35.798367 35.78539  35.86124  35.92421  36.095802 36.235962
 36.26071  36.277477 36.365387 36.434715 36.355515 36.273285 36.447044
 36.313637 36.18168  36.109154 35.92601  35.89223  35.948437]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3584222
	speed: 0.1800s/iter; left time: 554.6405s
	iters: 200, epoch: 1 | loss: 0.2201133
	speed: 0.1780s/iter; left time: 530.7519s
	iters: 300, epoch: 1 | loss: 0.2115846
	speed: 0.1995s/iter; left time: 574.6804s
Epoch: 1 cost time: 59.79418158531189
Epoch: 1, Steps: 318 | Train Loss: 0.3339240 Vali Loss: 0.2473757 Test Loss: 0.2265262
Validation loss decreased (inf --> 0.247376).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2475386
	speed: 0.2115s/iter; left time: 584.4827s
	iters: 200, epoch: 2 | loss: 0.1969451
	speed: 0.1951s/iter; left time: 519.5984s
	iters: 300, epoch: 2 | loss: 0.1942231
	speed: 0.1969s/iter; left time: 504.6957s
Epoch: 2 cost time: 64.0399317741394
Epoch: 2, Steps: 318 | Train Loss: 0.2082300 Vali Loss: 0.2318389 Test Loss: 0.1960487
Validation loss decreased (0.247376 --> 0.231839).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1968718
	speed: 0.1928s/iter; left time: 471.4445s
	iters: 200, epoch: 3 | loss: 0.2395036
	speed: 0.1734s/iter; left time: 406.6186s
	iters: 300, epoch: 3 | loss: 0.1998162
	speed: 0.1888s/iter; left time: 423.8705s
Epoch: 3 cost time: 58.89693260192871
Epoch: 3, Steps: 318 | Train Loss: 0.1877270 Vali Loss: 0.2202871 Test Loss: 0.1948351
Validation loss decreased (0.231839 --> 0.220287).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1628988
	speed: 0.1940s/iter; left time: 412.6778s
	iters: 200, epoch: 4 | loss: 0.1985567
	speed: 0.1907s/iter; left time: 386.6449s
	iters: 300, epoch: 4 | loss: 0.1584730
	speed: 0.1872s/iter; left time: 360.8146s
Epoch: 4 cost time: 60.92283058166504
Epoch: 4, Steps: 318 | Train Loss: 0.1781253 Vali Loss: 0.2146989 Test Loss: 0.1906924
Validation loss decreased (0.220287 --> 0.214699).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1773464
	speed: 0.1856s/iter; left time: 335.7130s
	iters: 200, epoch: 5 | loss: 0.1789640
	speed: 0.1911s/iter; left time: 326.5851s
	iters: 300, epoch: 5 | loss: 0.1714008
	speed: 0.1935s/iter; left time: 311.3748s
Epoch: 5 cost time: 60.5707643032074
Epoch: 5, Steps: 318 | Train Loss: 0.1735253 Vali Loss: 0.2161693 Test Loss: 0.1894185
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1526943
	speed: 0.1911s/iter; left time: 284.9046s
	iters: 200, epoch: 6 | loss: 0.1667739
	speed: 0.1886s/iter; left time: 262.3701s
	iters: 300, epoch: 6 | loss: 0.1818697
	speed: 0.1884s/iter; left time: 243.1771s
Epoch: 6 cost time: 60.13264775276184
Epoch: 6, Steps: 318 | Train Loss: 0.1709545 Vali Loss: 0.2160582 Test Loss: 0.1890726
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1252573
	speed: 0.1936s/iter; left time: 227.0614s
	iters: 200, epoch: 7 | loss: 0.1849570
	speed: 0.1923s/iter; left time: 206.3121s
	iters: 300, epoch: 7 | loss: 0.1555955
	speed: 0.1894s/iter; left time: 184.2701s
Epoch: 7 cost time: 60.81429886817932
Epoch: 7, Steps: 318 | Train Loss: 0.1698191 Vali Loss: 0.2169145 Test Loss: 0.1889271
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.41846713423728943, mae:0.5016549825668335, rmse:0.6468903422355652, mape:0.017963742837309837, mspe:0.0005464772111736238, rse:0.45248904824256897, r2_score:0.780163698760521, acc:0.9820362571626902
corr: [36.37203  36.34235  36.409534 36.42526  36.37723  36.397827 36.24698
 36.10144  36.160526 36.216995 36.255943 36.360126 36.456493 36.575687
 36.694122 36.846275 37.005436 37.142796 37.23186  37.204605 37.211426
 37.19747  37.13846  36.826225 36.873383]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3744474
	speed: 0.1503s/iter; left time: 461.6991s
	iters: 200, epoch: 1 | loss: 0.2521783
	speed: 0.1249s/iter; left time: 371.2038s
	iters: 300, epoch: 1 | loss: 0.2365571
	speed: 0.1200s/iter; left time: 344.4833s
Epoch: 1 cost time: 41.76217603683472
Epoch: 1, Steps: 317 | Train Loss: 0.3693871 Vali Loss: 0.2467623 Test Loss: 0.2243657
Validation loss decreased (inf --> 0.246762).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2558863
	speed: 0.1293s/iter; left time: 356.0283s
	iters: 200, epoch: 2 | loss: 0.2452499
	speed: 0.1185s/iter; left time: 314.5494s
	iters: 300, epoch: 2 | loss: 0.2236310
	speed: 0.1175s/iter; left time: 300.1947s
Epoch: 2 cost time: 38.68537449836731
Epoch: 2, Steps: 317 | Train Loss: 0.2404020 Vali Loss: 0.2236116 Test Loss: 0.2059395
Validation loss decreased (0.246762 --> 0.223612).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.2375126
	speed: 0.1245s/iter; left time: 303.3186s
	iters: 200, epoch: 3 | loss: 0.2566753
	speed: 0.1162s/iter; left time: 271.6139s
	iters: 300, epoch: 3 | loss: 0.1920531
	speed: 0.1158s/iter; left time: 258.9682s
Epoch: 3 cost time: 38.05897831916809
Epoch: 3, Steps: 317 | Train Loss: 0.2281672 Vali Loss: 0.2266822 Test Loss: 0.2097542
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.2208028
	speed: 0.1244s/iter; left time: 263.8221s
	iters: 200, epoch: 4 | loss: 0.2111736
	speed: 0.1257s/iter; left time: 253.9106s
	iters: 300, epoch: 4 | loss: 0.1874348
	speed: 0.1232s/iter; left time: 236.4928s
Epoch: 4 cost time: 39.82189083099365
Epoch: 4, Steps: 317 | Train Loss: 0.2231845 Vali Loss: 0.2208580 Test Loss: 0.2076882
Validation loss decreased (0.223612 --> 0.220858).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.2676902
	speed: 0.1261s/iter; left time: 227.3461s
	iters: 200, epoch: 5 | loss: 0.3231223
	speed: 0.1183s/iter; left time: 201.5298s
	iters: 300, epoch: 5 | loss: 0.1873461
	speed: 0.1229s/iter; left time: 197.0472s
Epoch: 5 cost time: 38.786274909973145
Epoch: 5, Steps: 317 | Train Loss: 0.2204613 Vali Loss: 0.2210943 Test Loss: 0.2079430
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1932446
	speed: 0.1331s/iter; left time: 197.7790s
	iters: 200, epoch: 6 | loss: 0.1474131
	speed: 0.1233s/iter; left time: 170.8388s
	iters: 300, epoch: 6 | loss: 0.1916891
	speed: 0.1172s/iter; left time: 150.7475s
Epoch: 6 cost time: 39.37516760826111
Epoch: 6, Steps: 317 | Train Loss: 0.2187004 Vali Loss: 0.2192057 Test Loss: 0.2069701
Validation loss decreased (0.220858 --> 0.219206).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.2240306
	speed: 0.1259s/iter; left time: 147.2117s
	iters: 200, epoch: 7 | loss: 0.2064909
	speed: 0.1165s/iter; left time: 124.4937s
	iters: 300, epoch: 7 | loss: 0.2961232
	speed: 0.1193s/iter; left time: 115.5677s
Epoch: 7 cost time: 38.37241196632385
Epoch: 7, Steps: 317 | Train Loss: 0.2183416 Vali Loss: 0.2190252 Test Loss: 0.2072347
Validation loss decreased (0.219206 --> 0.219025).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.2538330
	speed: 0.1268s/iter; left time: 108.0155s
	iters: 200, epoch: 8 | loss: 0.1928367
	speed: 0.1265s/iter; left time: 95.0955s
	iters: 300, epoch: 8 | loss: 0.1929376
	speed: 0.1286s/iter; left time: 83.8568s
Epoch: 8 cost time: 40.25970435142517
Epoch: 8, Steps: 317 | Train Loss: 0.2174794 Vali Loss: 0.2185284 Test Loss: 0.2070134
Validation loss decreased (0.219025 --> 0.218528).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.2152468
	speed: 0.1233s/iter; left time: 65.9455s
	iters: 200, epoch: 9 | loss: 0.2164766
	speed: 0.1236s/iter; left time: 53.7486s
	iters: 300, epoch: 9 | loss: 0.2331190
	speed: 0.1192s/iter; left time: 39.9438s
Epoch: 9 cost time: 38.55072999000549
Epoch: 9, Steps: 317 | Train Loss: 0.2175208 Vali Loss: 0.2186124 Test Loss: 0.2069436
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.2325835
	speed: 0.1271s/iter; left time: 27.7028s
	iters: 200, epoch: 10 | loss: 0.2105492
	speed: 0.1245s/iter; left time: 14.6916s
	iters: 300, epoch: 10 | loss: 0.1953001
	speed: 0.1234s/iter; left time: 2.2218s
Epoch: 10 cost time: 40.00716948509216
Epoch: 10, Steps: 317 | Train Loss: 0.2175869 Vali Loss: 0.2190828 Test Loss: 0.2069974
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata1_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.4542827308177948, mae:0.5325525999069214, rmse:0.6740049719810486, mape:0.01903715170919895, mspe:0.0005886467406526208, rse:0.4711167514324188, r2_score:0.7506046608282309, acc:0.980962848290801
corr: [36.89932  37.078705 36.99491  37.1166   37.182266 36.981667 37.023697
 37.228573 37.14172  36.937912 37.240227 37.394535 37.33952  37.333336
 37.46776  37.38691  37.472973 37.293217 37.203724 37.528263 37.373688
 37.31243  37.625347 37.598465 37.850567 37.827194 37.659298 37.86946
 38.035    38.05033 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2405996
	speed: 0.2135s/iter; left time: 657.9042s
	iters: 200, epoch: 1 | loss: 0.1354882
	speed: 0.1767s/iter; left time: 526.7894s
	iters: 300, epoch: 1 | loss: 0.2394149
	speed: 0.1581s/iter; left time: 455.3834s
Epoch: 1 cost time: 57.74620223045349
Epoch: 1, Steps: 318 | Train Loss: 0.2577036 Vali Loss: 0.1669988 Test Loss: 0.1477183
Validation loss decreased (inf --> 0.166999).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1842985
	speed: 0.1525s/iter; left time: 421.4432s
	iters: 200, epoch: 2 | loss: 0.1799564
	speed: 0.1584s/iter; left time: 421.9169s
	iters: 300, epoch: 2 | loss: 0.1901440
	speed: 0.1642s/iter; left time: 420.8867s
Epoch: 2 cost time: 50.8113694190979
Epoch: 2, Steps: 318 | Train Loss: 0.1576105 Vali Loss: 0.1474990 Test Loss: 0.1313417
Validation loss decreased (0.166999 --> 0.147499).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1478882
	speed: 0.1662s/iter; left time: 406.3488s
	iters: 200, epoch: 3 | loss: 0.1241414
	speed: 0.1554s/iter; left time: 364.5143s
	iters: 300, epoch: 3 | loss: 0.1196068
	speed: 0.1549s/iter; left time: 347.7977s
Epoch: 3 cost time: 50.66647934913635
Epoch: 3, Steps: 318 | Train Loss: 0.1428478 Vali Loss: 0.1436682 Test Loss: 0.1322829
Validation loss decreased (0.147499 --> 0.143668).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1621876
	speed: 0.1784s/iter; left time: 379.4765s
	iters: 200, epoch: 4 | loss: 0.1179847
	speed: 0.1673s/iter; left time: 339.0359s
	iters: 300, epoch: 4 | loss: 0.1432521
	speed: 0.1682s/iter; left time: 324.0321s
Epoch: 4 cost time: 54.47310185432434
Epoch: 4, Steps: 318 | Train Loss: 0.1356713 Vali Loss: 0.1462448 Test Loss: 0.1294069
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0899061
	speed: 0.1787s/iter; left time: 323.3531s
	iters: 200, epoch: 5 | loss: 0.1385238
	speed: 0.1596s/iter; left time: 272.7678s
	iters: 300, epoch: 5 | loss: 0.1286745
	speed: 0.1579s/iter; left time: 254.0984s
Epoch: 5 cost time: 52.836446046829224
Epoch: 5, Steps: 318 | Train Loss: 0.1330272 Vali Loss: 0.1469925 Test Loss: 0.1314294
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0939679
	speed: 0.1754s/iter; left time: 261.4807s
	iters: 200, epoch: 6 | loss: 0.1292257
	speed: 0.1627s/iter; left time: 226.3734s
	iters: 300, epoch: 6 | loss: 0.1419335
	speed: 0.1660s/iter; left time: 214.3360s
Epoch: 6 cost time: 53.28521227836609
Epoch: 6, Steps: 318 | Train Loss: 0.1303524 Vali Loss: 0.1475647 Test Loss: 0.1301521
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.2902897298336029, mae:0.4173281192779541, rmse:0.5387853980064392, mape:0.014917614869773388, mspe:0.000375070987502113, rse:0.3775196373462677, r2_score:0.8460173827888495, acc:0.9850823851302266
corr: [36.445957 36.470184 36.49384  36.622704 36.56134  36.656    36.776955
 37.00288  36.896427 36.91649 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2529858
	speed: 0.2437s/iter; left time: 750.7000s
	iters: 200, epoch: 1 | loss: 0.1810794
	speed: 0.2273s/iter; left time: 677.5022s
	iters: 300, epoch: 1 | loss: 0.2223169
	speed: 0.2315s/iter; left time: 666.8492s
Epoch: 1 cost time: 74.59384298324585
Epoch: 1, Steps: 318 | Train Loss: 0.2598753 Vali Loss: 0.2003222 Test Loss: 0.1703090
Validation loss decreased (inf --> 0.200322).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1593310
	speed: 0.2315s/iter; left time: 639.7055s
	iters: 200, epoch: 2 | loss: 0.2043230
	speed: 0.2389s/iter; left time: 636.2753s
	iters: 300, epoch: 2 | loss: 0.1888579
	speed: 0.2361s/iter; left time: 605.0171s
Epoch: 2 cost time: 74.74682307243347
Epoch: 2, Steps: 318 | Train Loss: 0.1660957 Vali Loss: 0.1773945 Test Loss: 0.1619921
Validation loss decreased (0.200322 --> 0.177394).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1667496
	speed: 0.2402s/iter; left time: 587.2885s
	iters: 200, epoch: 3 | loss: 0.1568911
	speed: 0.2339s/iter; left time: 548.5222s
	iters: 300, epoch: 3 | loss: 0.1626891
	speed: 0.2338s/iter; left time: 524.7920s
Epoch: 3 cost time: 75.00345182418823
Epoch: 3, Steps: 318 | Train Loss: 0.1494282 Vali Loss: 0.1775188 Test Loss: 0.1579593
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1492765
	speed: 0.2469s/iter; left time: 525.2041s
	iters: 200, epoch: 4 | loss: 0.1524813
	speed: 0.2131s/iter; left time: 431.9098s
	iters: 300, epoch: 4 | loss: 0.1372983
	speed: 0.2261s/iter; left time: 435.7656s
Epoch: 4 cost time: 72.9992983341217
Epoch: 4, Steps: 318 | Train Loss: 0.1408904 Vali Loss: 0.1755267 Test Loss: 0.1579405
Validation loss decreased (0.177394 --> 0.175527).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0987599
	speed: 0.2563s/iter; left time: 463.6125s
	iters: 200, epoch: 5 | loss: 0.1440811
	speed: 0.2375s/iter; left time: 405.8153s
	iters: 300, epoch: 5 | loss: 0.1429368
	speed: 0.2383s/iter; left time: 383.3564s
Epoch: 5 cost time: 77.31920194625854
Epoch: 5, Steps: 318 | Train Loss: 0.1356067 Vali Loss: 0.1774968 Test Loss: 0.1598391
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1352655
	speed: 0.2165s/iter; left time: 322.8152s
	iters: 200, epoch: 6 | loss: 0.1457903
	speed: 0.2212s/iter; left time: 307.7562s
	iters: 300, epoch: 6 | loss: 0.1325890
	speed: 0.2163s/iter; left time: 279.2228s
Epoch: 6 cost time: 69.86071944236755
Epoch: 6, Steps: 318 | Train Loss: 0.1333211 Vali Loss: 0.1804191 Test Loss: 0.1613112
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1138215
	speed: 0.2233s/iter; left time: 261.8744s
	iters: 200, epoch: 7 | loss: 0.1075970
	speed: 0.2207s/iter; left time: 236.8305s
	iters: 300, epoch: 7 | loss: 0.1383240
	speed: 0.2308s/iter; left time: 224.5597s
Epoch: 7 cost time: 71.91377973556519
Epoch: 7, Steps: 318 | Train Loss: 0.1317814 Vali Loss: 0.1789623 Test Loss: 0.1595046
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.3465941250324249, mae:0.4561455249786377, rmse:0.5887224674224854, mape:0.016318805515766144, mspe:0.0004500730137806386, rse:0.41231560707092285, r2_score:0.8152831019877856, acc:0.9836811944842339
corr: [36.1246   36.151806 36.03228  36.135063 35.983517 36.16491  36.031773
 36.138054 36.23821  36.14041  36.24976  36.333385 36.726048 37.022907
 36.99719 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2441831
	speed: 0.2207s/iter; left time: 680.0788s
	iters: 200, epoch: 1 | loss: 0.2658257
	speed: 0.2578s/iter; left time: 768.4867s
	iters: 300, epoch: 1 | loss: 0.2082838
	speed: 0.2579s/iter; left time: 742.8854s
Epoch: 1 cost time: 78.19806480407715
Epoch: 1, Steps: 318 | Train Loss: 0.2845540 Vali Loss: 0.1952747 Test Loss: 0.1757529
Validation loss decreased (inf --> 0.195275).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1695219
	speed: 0.2542s/iter; left time: 702.3299s
	iters: 200, epoch: 2 | loss: 0.1828826
	speed: 0.2479s/iter; left time: 660.0952s
	iters: 300, epoch: 2 | loss: 0.1745002
	speed: 0.2725s/iter; left time: 698.4794s
Epoch: 2 cost time: 82.42686796188354
Epoch: 2, Steps: 318 | Train Loss: 0.1823839 Vali Loss: 0.2027874 Test Loss: 0.1781844
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1824559
	speed: 0.2378s/iter; left time: 581.3926s
	iters: 200, epoch: 3 | loss: 0.1837704
	speed: 0.2620s/iter; left time: 614.3388s
	iters: 300, epoch: 3 | loss: 0.1309295
	speed: 0.2493s/iter; left time: 559.7328s
Epoch: 3 cost time: 79.80091142654419
Epoch: 3, Steps: 318 | Train Loss: 0.1670962 Vali Loss: 0.1985035 Test Loss: 0.1726218
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1221730
	speed: 0.2511s/iter; left time: 534.1106s
	iters: 200, epoch: 4 | loss: 0.1620450
	speed: 0.2605s/iter; left time: 528.1277s
	iters: 300, epoch: 4 | loss: 0.1197589
	speed: 0.2660s/iter; left time: 512.5467s
Epoch: 4 cost time: 82.2104160785675
Epoch: 4, Steps: 318 | Train Loss: 0.1585831 Vali Loss: 0.1974511 Test Loss: 0.1715547
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000063
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.3856827914714813, mae:0.48242297768592834, rmse:0.6210336685180664, mape:0.017316635698080063, mspe:0.0005077401292510331, rse:0.4346930682659149, r2_score:0.79032784852148, acc:0.9826833643019199
corr: [37.1249   37.016876 36.953556 37.041485 37.04675  37.157803 37.25972
 37.27274  37.322525 37.273254 37.332718 37.2951   37.3065   37.135025
 37.286522 37.348984 37.311047 37.31459  37.33648  37.315166]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3117887
	speed: 0.2102s/iter; left time: 647.5537s
	iters: 200, epoch: 1 | loss: 0.2108978
	speed: 0.1504s/iter; left time: 448.4595s
	iters: 300, epoch: 1 | loss: 0.1931292
	speed: 0.1591s/iter; left time: 458.4997s
Epoch: 1 cost time: 55.36731171607971
Epoch: 1, Steps: 318 | Train Loss: 0.2904956 Vali Loss: 0.1988464 Test Loss: 0.1919423
Validation loss decreased (inf --> 0.198846).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2104020
	speed: 0.1872s/iter; left time: 517.2048s
	iters: 200, epoch: 2 | loss: 0.1646801
	speed: 0.1648s/iter; left time: 438.9787s
	iters: 300, epoch: 2 | loss: 0.1815350
	speed: 0.1519s/iter; left time: 389.3931s
Epoch: 2 cost time: 53.525991916656494
Epoch: 2, Steps: 318 | Train Loss: 0.1899390 Vali Loss: 0.2041990 Test Loss: 0.1889603
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1736118
	speed: 0.1617s/iter; left time: 395.2707s
	iters: 200, epoch: 3 | loss: 0.2184184
	speed: 0.1642s/iter; left time: 385.1228s
	iters: 300, epoch: 3 | loss: 0.1716309
	speed: 0.1717s/iter; left time: 385.4874s
Epoch: 3 cost time: 52.67862367630005
Epoch: 3, Steps: 318 | Train Loss: 0.1715643 Vali Loss: 0.2101381 Test Loss: 0.1929959
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1646829
	speed: 0.1758s/iter; left time: 373.9809s
	iters: 200, epoch: 4 | loss: 0.1717334
	speed: 0.1613s/iter; left time: 326.9714s
	iters: 300, epoch: 4 | loss: 0.1621528
	speed: 0.1470s/iter; left time: 283.2649s
Epoch: 4 cost time: 51.32352948188782
Epoch: 4, Steps: 318 | Train Loss: 0.1598698 Vali Loss: 0.2108043 Test Loss: 0.1971205
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000063
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.4212099611759186, mae:0.5038081407546997, rmse:0.6490069031715393, mape:0.018068114295601845, mspe:0.0005525096785277128, rse:0.4539695382118225, r2_score:0.7470886708531285, acc:0.9819318857043982
corr: [36.252934 36.536926 36.531742 36.222946 36.17293  36.518738 36.63121
 36.3938   36.3304   36.67575  36.65306  36.55914  36.270695 36.36771
 36.46439  36.387287 36.469814 36.659653 36.956696 36.958553 36.75891
 37.217262 37.0296   36.92075  36.840706]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3239863
	speed: 0.2033s/iter; left time: 624.1913s
	iters: 200, epoch: 1 | loss: 0.2687632
	speed: 0.1756s/iter; left time: 521.8050s
	iters: 300, epoch: 1 | loss: 0.2067805
	speed: 0.1812s/iter; left time: 520.1704s
Epoch: 1 cost time: 59.0970242023468
Epoch: 1, Steps: 317 | Train Loss: 0.2956393 Vali Loss: 0.2168529 Test Loss: 0.2124796
Validation loss decreased (inf --> 0.216853).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2026729
	speed: 0.1539s/iter; left time: 423.7556s
	iters: 200, epoch: 2 | loss: 0.2481551
	speed: 0.1268s/iter; left time: 336.6024s
	iters: 300, epoch: 2 | loss: 0.1969193
	speed: 0.1229s/iter; left time: 313.7856s
Epoch: 2 cost time: 42.47853922843933
Epoch: 2, Steps: 317 | Train Loss: 0.2005672 Vali Loss: 0.2165959 Test Loss: 0.1921638
Validation loss decreased (0.216853 --> 0.216596).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1816398
	speed: 0.1238s/iter; left time: 301.6015s
	iters: 200, epoch: 3 | loss: 0.2004546
	speed: 0.1192s/iter; left time: 278.5977s
	iters: 300, epoch: 3 | loss: 0.1456050
	speed: 0.1536s/iter; left time: 343.4959s
Epoch: 3 cost time: 42.748332262039185
Epoch: 3, Steps: 317 | Train Loss: 0.1835450 Vali Loss: 0.2389475 Test Loss: 0.2041795
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1748758
	speed: 0.1956s/iter; left time: 414.6305s
	iters: 200, epoch: 4 | loss: 0.1434519
	speed: 0.1793s/iter; left time: 362.1630s
	iters: 300, epoch: 4 | loss: 0.1900186
	speed: 0.1809s/iter; left time: 347.2700s
Epoch: 4 cost time: 58.64511823654175
Epoch: 4, Steps: 317 | Train Loss: 0.1753289 Vali Loss: 0.2340300 Test Loss: 0.1977236
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1895137
	speed: 0.1805s/iter; left time: 325.4341s
	iters: 200, epoch: 5 | loss: 0.2368449
	speed: 0.1809s/iter; left time: 308.1121s
	iters: 300, epoch: 5 | loss: 0.1584696
	speed: 0.1819s/iter; left time: 291.6229s
Epoch: 5 cost time: 57.6034619808197
Epoch: 5, Steps: 317 | Train Loss: 0.1706574 Vali Loss: 0.2371249 Test Loss: 0.1984123
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000031
Early stopping
>>>>>>>testing : omdata1_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.42169591784477234, mae:0.5015140175819397, rmse:0.649381160736084, mape:0.017960630357265472, mspe:0.0005508463946171105, rse:0.4539051949977875, r2_score:0.7571716596727858, acc:0.9820393696427345
corr: [36.524967 36.48385  36.424587 36.47765  36.441586 36.58137  36.57204
 36.72694  36.852345 36.514645 36.570164 36.48977  36.48084  36.55798
 36.515163 36.47443  36.422718 36.33189  36.30283  36.482132 36.51798
 36.474667 36.755737 36.693657 36.88174  37.05893  36.966484 36.91162
 36.940228 36.886707]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2416458
	speed: 0.2720s/iter; left time: 838.1703s
	iters: 200, epoch: 1 | loss: 0.1367175
	speed: 0.2837s/iter; left time: 845.8479s
	iters: 300, epoch: 1 | loss: 0.2383195
	speed: 0.2385s/iter; left time: 687.0964s
Epoch: 1 cost time: 84.02443337440491
Epoch: 1, Steps: 318 | Train Loss: 0.2591160 Vali Loss: 0.1657357 Test Loss: 0.1467898
Validation loss decreased (inf --> 0.165736).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1857901
	speed: 0.2562s/iter; left time: 707.8756s
	iters: 200, epoch: 2 | loss: 0.1796327
	speed: 0.2634s/iter; left time: 701.4008s
	iters: 300, epoch: 2 | loss: 0.1892079
	speed: 0.2745s/iter; left time: 703.5498s
Epoch: 2 cost time: 85.07142281532288
Epoch: 2, Steps: 318 | Train Loss: 0.1578074 Vali Loss: 0.1464419 Test Loss: 0.1307059
Validation loss decreased (0.165736 --> 0.146442).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1461381
	speed: 0.2741s/iter; left time: 670.2282s
	iters: 200, epoch: 3 | loss: 0.1236213
	speed: 0.2704s/iter; left time: 634.1463s
	iters: 300, epoch: 3 | loss: 0.1196206
	speed: 0.2819s/iter; left time: 632.7746s
Epoch: 3 cost time: 88.45423436164856
Epoch: 3, Steps: 318 | Train Loss: 0.1432245 Vali Loss: 0.1427030 Test Loss: 0.1316749
Validation loss decreased (0.146442 --> 0.142703).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1618951
	speed: 0.2895s/iter; left time: 615.7013s
	iters: 200, epoch: 4 | loss: 0.1183387
	speed: 0.2691s/iter; left time: 545.4666s
	iters: 300, epoch: 4 | loss: 0.1417333
	speed: 0.2718s/iter; left time: 523.8430s
Epoch: 4 cost time: 88.54903411865234
Epoch: 4, Steps: 318 | Train Loss: 0.1360759 Vali Loss: 0.1449145 Test Loss: 0.1286889
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0889645
	speed: 0.2803s/iter; left time: 507.0144s
	iters: 200, epoch: 5 | loss: 0.1391018
	speed: 0.2768s/iter; left time: 473.0365s
	iters: 300, epoch: 5 | loss: 0.1300540
	speed: 0.2806s/iter; left time: 451.4408s
Epoch: 5 cost time: 88.48831462860107
Epoch: 5, Steps: 318 | Train Loss: 0.1334790 Vali Loss: 0.1455379 Test Loss: 0.1305549
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0931873
	speed: 0.2780s/iter; left time: 414.5653s
	iters: 200, epoch: 6 | loss: 0.1297140
	speed: 0.2624s/iter; left time: 364.9309s
	iters: 300, epoch: 6 | loss: 0.1415779
	speed: 0.2753s/iter; left time: 355.4428s
Epoch: 6 cost time: 86.91374850273132
Epoch: 6, Steps: 318 | Train Loss: 0.1308016 Vali Loss: 0.1461052 Test Loss: 0.1292651
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.288955420255661, mae:0.4166218042373657, rmse:0.5375457406044006, mape:0.01489254366606474, mspe:0.00037329940823838115, rse:0.37665101885795593, r2_score:0.8471585844116103, acc:0.9851074563339353
corr: [36.449635 36.47482  36.497257 36.609005 36.554783 36.655357 36.7768
 37.005592 36.903084 36.91639 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2519018
	speed: 0.2705s/iter; left time: 833.4893s
	iters: 200, epoch: 1 | loss: 0.1799898
	speed: 0.2836s/iter; left time: 845.5222s
	iters: 300, epoch: 1 | loss: 0.2221846
	speed: 0.2876s/iter; left time: 828.7063s
Epoch: 1 cost time: 89.6302649974823
Epoch: 1, Steps: 318 | Train Loss: 0.2609033 Vali Loss: 0.2005556 Test Loss: 0.1708438
Validation loss decreased (inf --> 0.200556).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1586411
	speed: 0.3391s/iter; left time: 937.0387s
	iters: 200, epoch: 2 | loss: 0.2044498
	speed: 0.3426s/iter; left time: 912.4118s
	iters: 300, epoch: 2 | loss: 0.1900519
	speed: 0.3235s/iter; left time: 829.2281s
Epoch: 2 cost time: 106.4356746673584
Epoch: 2, Steps: 318 | Train Loss: 0.1659938 Vali Loss: 0.1772038 Test Loss: 0.1620652
Validation loss decreased (0.200556 --> 0.177204).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1654529
	speed: 0.3415s/iter; left time: 834.8527s
	iters: 200, epoch: 3 | loss: 0.1572824
	speed: 0.3346s/iter; left time: 784.6820s
	iters: 300, epoch: 3 | loss: 0.1627256
	speed: 0.3259s/iter; left time: 731.5565s
Epoch: 3 cost time: 106.22690081596375
Epoch: 3, Steps: 318 | Train Loss: 0.1490799 Vali Loss: 0.1766584 Test Loss: 0.1580336
Validation loss decreased (0.177204 --> 0.176658).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1490985
	speed: 0.3577s/iter; left time: 760.7630s
	iters: 200, epoch: 4 | loss: 0.1529723
	speed: 0.3589s/iter; left time: 727.4518s
	iters: 300, epoch: 4 | loss: 0.1378663
	speed: 0.3398s/iter; left time: 654.7176s
Epoch: 4 cost time: 111.48972487449646
Epoch: 4, Steps: 318 | Train Loss: 0.1405512 Vali Loss: 0.1749832 Test Loss: 0.1582010
Validation loss decreased (0.176658 --> 0.174983).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0995053
	speed: 0.3497s/iter; left time: 632.5839s
	iters: 200, epoch: 5 | loss: 0.1435282
	speed: 0.3098s/iter; left time: 529.4162s
	iters: 300, epoch: 5 | loss: 0.1433604
	speed: 0.3233s/iter; left time: 520.2244s
Epoch: 5 cost time: 104.67460799217224
Epoch: 5, Steps: 318 | Train Loss: 0.1352120 Vali Loss: 0.1772283 Test Loss: 0.1599968
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1336394
	speed: 0.3500s/iter; left time: 521.7960s
	iters: 200, epoch: 6 | loss: 0.1422601
	speed: 0.3859s/iter; left time: 536.7651s
	iters: 300, epoch: 6 | loss: 0.1325788
	speed: 0.3295s/iter; left time: 425.3459s
Epoch: 6 cost time: 112.7766261100769
Epoch: 6, Steps: 318 | Train Loss: 0.1329858 Vali Loss: 0.1801729 Test Loss: 0.1614363
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1135989
	speed: 0.3275s/iter; left time: 384.1824s
	iters: 200, epoch: 7 | loss: 0.1073824
	speed: 0.3278s/iter; left time: 351.6892s
	iters: 300, epoch: 7 | loss: 0.1390061
	speed: 0.3292s/iter; left time: 320.2688s
Epoch: 7 cost time: 104.56944489479065
Epoch: 7, Steps: 318 | Train Loss: 0.1314019 Vali Loss: 0.1786808 Test Loss: 0.1597191
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.34716591238975525, mae:0.4571191668510437, rmse:0.5892078876495361, mape:0.01635332964360714, mspe:0.0004507045086938888, rse:0.4126555621623993, r2_score:0.815586072021658, acc:0.9836466703563929
corr: [36.133396 36.145103 36.03089  36.120937 35.93951  36.126198 35.990356
 36.0891   36.20445  36.113243 36.214653 36.29759  36.679802 37.00898
 36.98548 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2658099
	speed: 0.2568s/iter; left time: 791.3143s
	iters: 200, epoch: 1 | loss: 0.2625666
	speed: 0.2800s/iter; left time: 834.7996s
	iters: 300, epoch: 1 | loss: 0.2012579
	speed: 0.2830s/iter; left time: 815.2529s
Epoch: 1 cost time: 86.5803062915802
Epoch: 1, Steps: 318 | Train Loss: 0.2912194 Vali Loss: 0.1973146 Test Loss: 0.1774502
Validation loss decreased (inf --> 0.197315).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1703349
	speed: 0.2835s/iter; left time: 783.4416s
	iters: 200, epoch: 2 | loss: 0.1707619
	speed: 0.2829s/iter; left time: 753.3921s
	iters: 300, epoch: 2 | loss: 0.1743812
	speed: 0.3001s/iter; left time: 769.2613s
Epoch: 2 cost time: 92.14662170410156
Epoch: 2, Steps: 318 | Train Loss: 0.1834871 Vali Loss: 0.1981164 Test Loss: 0.1798469
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1770266
	speed: 0.2986s/iter; left time: 730.1915s
	iters: 200, epoch: 3 | loss: 0.1942611
	speed: 0.2849s/iter; left time: 668.0016s
	iters: 300, epoch: 3 | loss: 0.1282824
	speed: 0.2725s/iter; left time: 611.7149s
Epoch: 3 cost time: 90.3965470790863
Epoch: 3, Steps: 318 | Train Loss: 0.1673648 Vali Loss: 0.1943967 Test Loss: 0.1746987
Validation loss decreased (0.197315 --> 0.194397).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1183596
	speed: 0.2185s/iter; left time: 464.8157s
	iters: 200, epoch: 4 | loss: 0.1559429
	speed: 0.2026s/iter; left time: 410.6345s
	iters: 300, epoch: 4 | loss: 0.1233622
	speed: 0.1951s/iter; left time: 375.9923s
Epoch: 4 cost time: 65.31857752799988
Epoch: 4, Steps: 318 | Train Loss: 0.1578935 Vali Loss: 0.1979833 Test Loss: 0.1755236
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1536443
	speed: 0.2644s/iter; left time: 478.3108s
	iters: 200, epoch: 5 | loss: 0.1562261
	speed: 0.2513s/iter; left time: 429.4420s
	iters: 300, epoch: 5 | loss: 0.1732617
	speed: 0.2524s/iter; left time: 406.1372s
Epoch: 5 cost time: 81.8158450126648
Epoch: 5, Steps: 318 | Train Loss: 0.1537457 Vali Loss: 0.1979128 Test Loss: 0.1785798
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1401606
	speed: 0.2543s/iter; left time: 379.1018s
	iters: 200, epoch: 6 | loss: 0.1544576
	speed: 0.2548s/iter; left time: 354.4679s
	iters: 300, epoch: 6 | loss: 0.1499786
	speed: 0.2488s/iter; left time: 321.2184s
Epoch: 6 cost time: 80.3958945274353
Epoch: 6, Steps: 318 | Train Loss: 0.1511028 Vali Loss: 0.1980581 Test Loss: 0.1788018
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.3833693861961365, mae:0.4821583926677704, rmse:0.6191682815551758, mape:0.017272481694817543, mspe:0.0005000630626454949, rse:0.4333874583244324, r2_score:0.7785365181158276, acc:0.9827275183051825
corr: [36.43085  36.3722   36.285088 36.27719  36.25132  36.364918 36.378433
 36.450478 36.50393  36.591167 36.591427 36.596756 36.574768 36.654118
 36.698826 36.740917 36.707928 36.843185 36.93641  37.06706 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2907749
	speed: 0.1927s/iter; left time: 593.7066s
	iters: 200, epoch: 1 | loss: 0.2044685
	speed: 0.1577s/iter; left time: 469.9950s
	iters: 300, epoch: 1 | loss: 0.1884534
	speed: 0.1556s/iter; left time: 448.3094s
Epoch: 1 cost time: 53.73991680145264
Epoch: 1, Steps: 318 | Train Loss: 0.2841868 Vali Loss: 0.2027068 Test Loss: 0.2004807
Validation loss decreased (inf --> 0.202707).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2138102
	speed: 0.1642s/iter; left time: 453.5525s
	iters: 200, epoch: 2 | loss: 0.1681153
	speed: 0.1747s/iter; left time: 465.2653s
	iters: 300, epoch: 2 | loss: 0.1783222
	speed: 0.1765s/iter; left time: 452.2881s
Epoch: 2 cost time: 54.14496040344238
Epoch: 2, Steps: 318 | Train Loss: 0.1885416 Vali Loss: 0.2173141 Test Loss: 0.1880404
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1709283
	speed: 0.1694s/iter; left time: 414.1686s
	iters: 200, epoch: 3 | loss: 0.2214202
	speed: 0.1585s/iter; left time: 371.7263s
	iters: 300, epoch: 3 | loss: 0.1773018
	speed: 0.1561s/iter; left time: 350.4368s
Epoch: 3 cost time: 51.289793491363525
Epoch: 3, Steps: 318 | Train Loss: 0.1690672 Vali Loss: 0.2109634 Test Loss: 0.1933365
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1610533
	speed: 0.1662s/iter; left time: 353.4700s
	iters: 200, epoch: 4 | loss: 0.1719824
	speed: 0.1604s/iter; left time: 325.1744s
	iters: 300, epoch: 4 | loss: 0.1573159
	speed: 0.1668s/iter; left time: 321.3353s
Epoch: 4 cost time: 52.17993998527527
Epoch: 4, Steps: 318 | Train Loss: 0.1582253 Vali Loss: 0.2160588 Test Loss: 0.1988722
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000063
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.4399470388889313, mae:0.5134692788124084, rmse:0.6632850170135498, mape:0.018389353528618813, mspe:0.0005739533226005733, rse:0.4639568626880646, r2_score:0.7183910522400868, acc:0.9816106464713812
corr: [36.15752  36.31079  36.246143 36.108784 36.059628 36.206383 36.280132
 36.356636 36.481594 36.711357 36.59109  36.33387  36.161865 36.254288
 36.257458 36.14877  36.271545 36.361607 36.37978  36.38529  36.54688
 36.69027  36.812927 36.770145 36.755524]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3233650
	speed: 0.1777s/iter; left time: 545.5929s
	iters: 200, epoch: 1 | loss: 0.2735242
	speed: 0.1855s/iter; left time: 551.0362s
	iters: 300, epoch: 1 | loss: 0.2060858
	speed: 0.1966s/iter; left time: 564.3106s
Epoch: 1 cost time: 59.881937980651855
Epoch: 1, Steps: 317 | Train Loss: 0.2975172 Vali Loss: 0.2169188 Test Loss: 0.2128278
Validation loss decreased (inf --> 0.216919).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2034288
	speed: 0.3131s/iter; left time: 862.3851s
	iters: 200, epoch: 2 | loss: 0.2480015
	speed: 0.2968s/iter; left time: 787.6648s
	iters: 300, epoch: 2 | loss: 0.1966701
	speed: 0.2908s/iter; left time: 742.6557s
Epoch: 2 cost time: 94.98320436477661
Epoch: 2, Steps: 317 | Train Loss: 0.2008397 Vali Loss: 0.2170024 Test Loss: 0.1919466
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1824162
	speed: 0.2962s/iter; left time: 721.7867s
	iters: 200, epoch: 3 | loss: 0.2017421
	speed: 0.2950s/iter; left time: 689.3225s
	iters: 300, epoch: 3 | loss: 0.1463611
	speed: 0.3086s/iter; left time: 690.3425s
Epoch: 3 cost time: 95.12408447265625
Epoch: 3, Steps: 317 | Train Loss: 0.1837111 Vali Loss: 0.2396003 Test Loss: 0.2043145
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1756714
	speed: 0.3089s/iter; left time: 654.8024s
	iters: 200, epoch: 4 | loss: 0.1431813
	speed: 0.2995s/iter; left time: 605.0560s
	iters: 300, epoch: 4 | loss: 0.1893412
	speed: 0.3003s/iter; left time: 576.6585s
Epoch: 4 cost time: 96.13223099708557
Epoch: 4, Steps: 317 | Train Loss: 0.1755317 Vali Loss: 0.2338287 Test Loss: 0.1972155
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000063
Early stopping
>>>>>>>testing : omdata1_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.4670421779155731, mae:0.536131739616394, rmse:0.6834048628807068, mape:0.019181521609425545, mspe:0.0006060994928702712, rse:0.4776870608329773, r2_score:0.7756666398571763, acc:0.9808184783905745
corr: [37.338093 37.295933 37.318428 37.33666  37.28417  37.35158  37.190533
 37.363396 37.424366 37.158466 37.389427 37.429653 37.435905 37.692772
 37.63313  37.661606 37.550133 37.5622   37.526325 37.631313 37.600227
 37.442436 37.64228  37.527435 37.59466  37.732285 37.699036 37.831387
 37.900204 37.852665]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2422412
	speed: 0.3115s/iter; left time: 959.7170s
	iters: 200, epoch: 1 | loss: 0.1367257
	speed: 0.3097s/iter; left time: 923.1070s
	iters: 300, epoch: 1 | loss: 0.2370095
	speed: 0.3066s/iter; left time: 883.4175s
Epoch: 1 cost time: 98.57918095588684
Epoch: 1, Steps: 318 | Train Loss: 0.2604840 Vali Loss: 0.1649865 Test Loss: 0.1465959
Validation loss decreased (inf --> 0.164987).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1860558
	speed: 0.3063s/iter; left time: 846.3491s
	iters: 200, epoch: 2 | loss: 0.1804161
	speed: 0.3135s/iter; left time: 834.9285s
	iters: 300, epoch: 2 | loss: 0.1902604
	speed: 0.3022s/iter; left time: 774.4525s
Epoch: 2 cost time: 97.6087920665741
Epoch: 2, Steps: 318 | Train Loss: 0.1580611 Vali Loss: 0.1456505 Test Loss: 0.1305997
Validation loss decreased (0.164987 --> 0.145651).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1455765
	speed: 0.2894s/iter; left time: 707.5385s
	iters: 200, epoch: 3 | loss: 0.1231397
	speed: 0.2594s/iter; left time: 608.2485s
	iters: 300, epoch: 3 | loss: 0.1194152
	speed: 0.2805s/iter; left time: 629.7543s
Epoch: 3 cost time: 87.96725487709045
Epoch: 3, Steps: 318 | Train Loss: 0.1434763 Vali Loss: 0.1419713 Test Loss: 0.1316261
Validation loss decreased (0.145651 --> 0.141971).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1629302
	speed: 0.2814s/iter; left time: 598.5282s
	iters: 200, epoch: 4 | loss: 0.1186385
	speed: 0.2812s/iter; left time: 570.0398s
	iters: 300, epoch: 4 | loss: 0.1260252
	speed: 0.2712s/iter; left time: 522.5651s
Epoch: 4 cost time: 88.23318099975586
Epoch: 4, Steps: 318 | Train Loss: 0.1363612 Vali Loss: 0.1442774 Test Loss: 0.1285998
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0889639
	speed: 0.2807s/iter; left time: 507.7966s
	iters: 200, epoch: 5 | loss: 0.1325833
	speed: 0.2702s/iter; left time: 461.7713s
	iters: 300, epoch: 5 | loss: 0.1305537
	speed: 0.2815s/iter; left time: 452.9044s
Epoch: 5 cost time: 88.52285313606262
Epoch: 5, Steps: 318 | Train Loss: 0.1336854 Vali Loss: 0.1445998 Test Loss: 0.1301878
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0933182
	speed: 0.1906s/iter; left time: 284.2410s
	iters: 200, epoch: 6 | loss: 0.1302679
	speed: 0.1677s/iter; left time: 233.2691s
	iters: 300, epoch: 6 | loss: 0.1406443
	speed: 0.1750s/iter; left time: 225.8643s
Epoch: 6 cost time: 56.832512617111206
Epoch: 6, Steps: 318 | Train Loss: 0.1309409 Vali Loss: 0.1453541 Test Loss: 0.1292835
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.28884825110435486, mae:0.41673529148101807, rmse:0.5374460220336914, mape:0.014894885942339897, mspe:0.0003729352611117065, rse:0.3765811324119568, r2_score:0.8473602671648244, acc:0.9851051140576601
corr: [36.46049  36.485542 36.500984 36.598167 36.547825 36.651253 36.771767
 37.000587 36.907516 36.91415 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2525355
	speed: 0.3829s/iter; left time: 1179.6224s
	iters: 200, epoch: 1 | loss: 0.1805403
	speed: 0.3951s/iter; left time: 1177.7283s
	iters: 300, epoch: 1 | loss: 0.2214465
	speed: 0.3863s/iter; left time: 1112.8359s
Epoch: 1 cost time: 123.328298330307
Epoch: 1, Steps: 318 | Train Loss: 0.2623991 Vali Loss: 0.2004334 Test Loss: 0.1706463
Validation loss decreased (inf --> 0.200433).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1586485
	speed: 0.3818s/iter; left time: 1054.9435s
	iters: 200, epoch: 2 | loss: 0.2052843
	speed: 0.3942s/iter; left time: 1049.6370s
	iters: 300, epoch: 2 | loss: 0.1901271
	speed: 0.3912s/iter; left time: 1002.6855s
Epoch: 2 cost time: 123.83495593070984
Epoch: 2, Steps: 318 | Train Loss: 0.1661496 Vali Loss: 0.1773582 Test Loss: 0.1623856
Validation loss decreased (0.200433 --> 0.177358).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1659015
	speed: 0.3959s/iter; left time: 968.0059s
	iters: 200, epoch: 3 | loss: 0.1577972
	speed: 0.4033s/iter; left time: 945.6650s
	iters: 300, epoch: 3 | loss: 0.1627405
	speed: 0.4021s/iter; left time: 902.7645s
Epoch: 3 cost time: 127.62356519699097
Epoch: 3, Steps: 318 | Train Loss: 0.1492740 Vali Loss: 0.1768226 Test Loss: 0.1581891
Validation loss decreased (0.177358 --> 0.176823).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1491738
	speed: 0.4127s/iter; left time: 877.8608s
	iters: 200, epoch: 4 | loss: 0.1521644
	speed: 0.3978s/iter; left time: 806.4393s
	iters: 300, epoch: 4 | loss: 0.1378233
	speed: 0.4014s/iter; left time: 773.5069s
Epoch: 4 cost time: 129.14953517913818
Epoch: 4, Steps: 318 | Train Loss: 0.1407006 Vali Loss: 0.1749611 Test Loss: 0.1582738
Validation loss decreased (0.176823 --> 0.174961).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0999454
	speed: 0.4016s/iter; left time: 726.5622s
	iters: 200, epoch: 5 | loss: 0.1434678
	speed: 0.4048s/iter; left time: 691.7480s
	iters: 300, epoch: 5 | loss: 0.1437775
	speed: 0.4113s/iter; left time: 661.7040s
Epoch: 5 cost time: 129.20863819122314
Epoch: 5, Steps: 318 | Train Loss: 0.1353276 Vali Loss: 0.1772218 Test Loss: 0.1600490
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1331892
	speed: 0.4116s/iter; left time: 613.6348s
	iters: 200, epoch: 6 | loss: 0.1420483
	speed: 0.3958s/iter; left time: 550.5213s
	iters: 300, epoch: 6 | loss: 0.1333706
	speed: 0.4006s/iter; left time: 517.1369s
Epoch: 6 cost time: 128.1929714679718
Epoch: 6, Steps: 318 | Train Loss: 0.1331167 Vali Loss: 0.1801874 Test Loss: 0.1615491
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1140001
	speed: 0.4211s/iter; left time: 493.9981s
	iters: 200, epoch: 7 | loss: 0.1073055
	speed: 0.4283s/iter; left time: 459.5729s
	iters: 300, epoch: 7 | loss: 0.1388916
	speed: 0.4116s/iter; left time: 400.5177s
Epoch: 7 cost time: 133.29806518554688
Epoch: 7, Steps: 318 | Train Loss: 0.1315445 Vali Loss: 0.1786221 Test Loss: 0.1597898
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.3473256826400757, mae:0.45734986662864685, rmse:0.5893434286117554, mape:0.016360923647880554, mspe:0.00045082453289069235, rse:0.4127505123615265, r2_score:0.8155302315230438, acc:0.9836390763521194
corr: [36.1176   36.130314 36.012268 36.09803  35.914467 36.097435 35.976425
 36.077625 36.197033 36.105232 36.214046 36.297672 36.675297 37.017277
 36.99475 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2573521
	speed: 0.2903s/iter; left time: 894.2644s
	iters: 200, epoch: 1 | loss: 0.2745160
	speed: 0.2792s/iter; left time: 832.2582s
	iters: 300, epoch: 1 | loss: 0.2109984
	speed: 0.2689s/iter; left time: 774.7748s
Epoch: 1 cost time: 89.0142674446106
Epoch: 1, Steps: 318 | Train Loss: 0.2866308 Vali Loss: 0.1965641 Test Loss: 0.1751183
Validation loss decreased (inf --> 0.196564).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1708815
	speed: 0.2678s/iter; left time: 739.8001s
	iters: 200, epoch: 2 | loss: 0.1896705
	speed: 0.2568s/iter; left time: 683.8188s
	iters: 300, epoch: 2 | loss: 0.1756615
	speed: 0.2845s/iter; left time: 729.0748s
Epoch: 2 cost time: 86.4310028553009
Epoch: 2, Steps: 318 | Train Loss: 0.1833844 Vali Loss: 0.2011320 Test Loss: 0.1771754
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1838575
	speed: 0.2560s/iter; left time: 625.9355s
	iters: 200, epoch: 3 | loss: 0.1809656
	speed: 0.2611s/iter; left time: 612.2121s
	iters: 300, epoch: 3 | loss: 0.1324804
	speed: 0.2629s/iter; left time: 590.2208s
Epoch: 3 cost time: 82.88236904144287
Epoch: 3, Steps: 318 | Train Loss: 0.1674039 Vali Loss: 0.1973290 Test Loss: 0.1721590
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1123245
	speed: 0.2654s/iter; left time: 564.4665s
	iters: 200, epoch: 4 | loss: 0.1575532
	speed: 0.2630s/iter; left time: 533.1495s
	iters: 300, epoch: 4 | loss: 0.1221974
	speed: 0.2467s/iter; left time: 475.3833s
Epoch: 4 cost time: 82.6661970615387
Epoch: 4, Steps: 318 | Train Loss: 0.1581920 Vali Loss: 0.1986238 Test Loss: 0.1722914
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000063
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.38429027795791626, mae:0.4816912114620209, rmse:0.6199114918708801, mape:0.017289066687226295, mspe:0.000505962350871414, rse:0.4339076280593872, r2_score:0.7864544762559567, acc:0.9827109333127737
corr: [37.23139  37.15827  37.04818  37.194977 37.150173 37.246235 37.301777
 37.30082  37.374504 37.297943 37.34053  37.310642 37.28022  37.13608
 37.27106  37.334873 37.254047 37.288216 37.23439  37.200077]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2933331
	speed: 0.3711s/iter; left time: 1143.4848s
	iters: 200, epoch: 1 | loss: 0.2061657
	speed: 0.3427s/iter; left time: 1021.6951s
	iters: 300, epoch: 1 | loss: 0.1903108
	speed: 0.3511s/iter; left time: 1011.3898s
Epoch: 1 cost time: 113.08055973052979
Epoch: 1, Steps: 318 | Train Loss: 0.2883728 Vali Loss: 0.1996091 Test Loss: 0.1894477
Validation loss decreased (inf --> 0.199609).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2117448
	speed: 0.3704s/iter; left time: 1023.4940s
	iters: 200, epoch: 2 | loss: 0.1656496
	speed: 0.3557s/iter; left time: 947.2320s
	iters: 300, epoch: 2 | loss: 0.1797386
	speed: 0.3596s/iter; left time: 921.7752s
Epoch: 2 cost time: 115.10447859764099
Epoch: 2, Steps: 318 | Train Loss: 0.1886732 Vali Loss: 0.2070343 Test Loss: 0.1854662
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1689416
	speed: 0.3677s/iter; left time: 899.1328s
	iters: 200, epoch: 3 | loss: 0.2228471
	speed: 0.3513s/iter; left time: 823.8651s
	iters: 300, epoch: 3 | loss: 0.1684280
	speed: 0.3568s/iter; left time: 800.9412s
Epoch: 3 cost time: 114.23545837402344
Epoch: 3, Steps: 318 | Train Loss: 0.1695399 Vali Loss: 0.2141938 Test Loss: 0.1916540
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1641498
	speed: 0.3679s/iter; left time: 782.4406s
	iters: 200, epoch: 4 | loss: 0.1604538
	speed: 0.3577s/iter; left time: 725.1194s
	iters: 300, epoch: 4 | loss: 0.1650197
	speed: 0.3634s/iter; left time: 700.3218s
Epoch: 4 cost time: 115.74709010124207
Epoch: 4, Steps: 318 | Train Loss: 0.1575832 Vali Loss: 0.2103607 Test Loss: 0.1931210
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000063
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.4157356917858124, mae:0.5006949305534363, rmse:0.6447756886482239, mape:0.017959149554371834, mspe:0.0005454819183796644, rse:0.4510098695755005, r2_score:0.770079467255783, acc:0.9820408504456282
corr: [36.30589  36.61645  36.62586  36.357513 36.310932 36.641563 36.76387
 36.565628 36.479122 36.81364  36.793873 36.69706  36.43426  36.56821
 36.669815 36.63144  36.664513 36.870274 37.146885 37.12341  36.86912
 37.314262 37.10724  37.054596 36.978592]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3221644
	speed: 0.2445s/iter; left time: 750.9201s
	iters: 200, epoch: 1 | loss: 0.2752337
	speed: 0.2960s/iter; left time: 879.3157s
	iters: 300, epoch: 1 | loss: 0.2061183
	speed: 0.3069s/iter; left time: 881.0777s
Epoch: 1 cost time: 90.55074787139893
Epoch: 1, Steps: 317 | Train Loss: 0.2991579 Vali Loss: 0.2161129 Test Loss: 0.2130301
Validation loss decreased (inf --> 0.216113).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2032814
	speed: 0.4115s/iter; left time: 1133.3427s
	iters: 200, epoch: 2 | loss: 0.2487763
	speed: 0.4154s/iter; left time: 1102.3919s
	iters: 300, epoch: 2 | loss: 0.1959115
	speed: 0.3975s/iter; left time: 1015.1049s
Epoch: 2 cost time: 128.81506276130676
Epoch: 2, Steps: 317 | Train Loss: 0.2013275 Vali Loss: 0.2153141 Test Loss: 0.1911941
Validation loss decreased (0.216113 --> 0.215314).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1840930
	speed: 0.4074s/iter; left time: 992.8537s
	iters: 200, epoch: 3 | loss: 0.2034790
	speed: 0.4099s/iter; left time: 957.8449s
	iters: 300, epoch: 3 | loss: 0.1481169
	speed: 0.4054s/iter; left time: 906.8295s
Epoch: 3 cost time: 129.04107546806335
Epoch: 3, Steps: 317 | Train Loss: 0.1843251 Vali Loss: 0.2382467 Test Loss: 0.2042486
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1761411
	speed: 0.4233s/iter; left time: 897.4852s
	iters: 200, epoch: 4 | loss: 0.1439905
	speed: 0.4018s/iter; left time: 811.6838s
	iters: 300, epoch: 4 | loss: 0.1886506
	speed: 0.4012s/iter; left time: 770.2884s
Epoch: 4 cost time: 129.72603344917297
Epoch: 4, Steps: 317 | Train Loss: 0.1763873 Vali Loss: 0.2315982 Test Loss: 0.1965042
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1880422
	speed: 0.4220s/iter; left time: 760.9372s
	iters: 200, epoch: 5 | loss: 0.2369307
	speed: 0.3986s/iter; left time: 678.8125s
	iters: 300, epoch: 5 | loss: 0.1605682
	speed: 0.4152s/iter; left time: 665.5123s
Epoch: 5 cost time: 130.45984315872192
Epoch: 5, Steps: 317 | Train Loss: 0.1717262 Vali Loss: 0.2350308 Test Loss: 0.1975097
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000031
Early stopping
>>>>>>>testing : omdata1_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.41956788301467896, mae:0.4999234974384308, rmse:0.6477406024932861, mape:0.017905281856656075, mspe:0.0005479831015691161, rse:0.45275843143463135, r2_score:0.7573600176173649, acc:0.9820947181433439
corr: [36.49488  36.486958 36.443836 36.490322 36.45306  36.621597 36.61035
 36.777126 36.917408 36.55333  36.633686 36.56326  36.57253  36.669445
 36.596706 36.581818 36.513588 36.436195 36.411045 36.57955  36.618565
 36.586033 36.849392 36.778236 36.962627 37.125744 37.007595 36.97987
 37.019325 36.964848]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2426894
	speed: 0.3543s/iter; left time: 1091.5282s
	iters: 200, epoch: 1 | loss: 0.1363050
	speed: 0.3411s/iter; left time: 1016.6773s
	iters: 300, epoch: 1 | loss: 0.2378737
	speed: 0.3426s/iter; left time: 987.0477s
Epoch: 1 cost time: 110.66078686714172
Epoch: 1, Steps: 318 | Train Loss: 0.2615411 Vali Loss: 0.1646202 Test Loss: 0.1465305
Validation loss decreased (inf --> 0.164620).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1861518
	speed: 0.3510s/iter; left time: 969.6958s
	iters: 200, epoch: 2 | loss: 0.1808718
	speed: 0.3278s/iter; left time: 872.9338s
	iters: 300, epoch: 2 | loss: 0.1911324
	speed: 0.3388s/iter; left time: 868.2690s
Epoch: 2 cost time: 107.9775447845459
Epoch: 2, Steps: 318 | Train Loss: 0.1583032 Vali Loss: 0.1453561 Test Loss: 0.1305740
Validation loss decreased (0.164620 --> 0.145356).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1455881
	speed: 0.3415s/iter; left time: 834.9799s
	iters: 200, epoch: 3 | loss: 0.1229725
	speed: 0.3242s/iter; left time: 760.3282s
	iters: 300, epoch: 3 | loss: 0.1190668
	speed: 0.3374s/iter; left time: 757.4663s
Epoch: 3 cost time: 107.09115099906921
Epoch: 3, Steps: 318 | Train Loss: 0.1437223 Vali Loss: 0.1416069 Test Loss: 0.1314772
Validation loss decreased (0.145356 --> 0.141607).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1642644
	speed: 0.2209s/iter; left time: 469.8627s
	iters: 200, epoch: 4 | loss: 0.1182833
	speed: 0.2219s/iter; left time: 449.7151s
	iters: 300, epoch: 4 | loss: 0.1417373
	speed: 0.2164s/iter; left time: 416.9611s
Epoch: 4 cost time: 69.63152313232422
Epoch: 4, Steps: 318 | Train Loss: 0.1366212 Vali Loss: 0.1435647 Test Loss: 0.1284252
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0887852
	speed: 0.3324s/iter; left time: 601.3885s
	iters: 200, epoch: 5 | loss: 0.1407921
	speed: 0.3406s/iter; left time: 582.0628s
	iters: 300, epoch: 5 | loss: 0.1311342
	speed: 0.3194s/iter; left time: 513.9643s
Epoch: 5 cost time: 105.17955446243286
Epoch: 5, Steps: 318 | Train Loss: 0.1340516 Vali Loss: 0.1440566 Test Loss: 0.1302334
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0933803
	speed: 0.3362s/iter; left time: 501.3182s
	iters: 200, epoch: 6 | loss: 0.1304997
	speed: 0.3387s/iter; left time: 471.1905s
	iters: 300, epoch: 6 | loss: 0.1406956
	speed: 0.3230s/iter; left time: 417.0289s
Epoch: 6 cost time: 105.86805844306946
Epoch: 6, Steps: 318 | Train Loss: 0.1313246 Vali Loss: 0.1445244 Test Loss: 0.1288519
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.2885216474533081, mae:0.41666093468666077, rmse:0.5371420979499817, mape:0.014891891740262508, mspe:0.0003724575799424201, rse:0.3763681948184967, r2_score:0.8476480613094763, acc:0.9851081082597375
corr: [36.464966 36.490986 36.498734 36.585377 36.54188  36.64327  36.765923
 36.987564 36.901386 36.905445]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2541053
	speed: 0.4374s/iter; left time: 1347.5709s
	iters: 200, epoch: 1 | loss: 0.1811995
	speed: 0.4546s/iter; left time: 1355.1332s
	iters: 300, epoch: 1 | loss: 0.2221456
	speed: 0.4461s/iter; left time: 1285.1742s
Epoch: 1 cost time: 141.26712036132812
Epoch: 1, Steps: 318 | Train Loss: 0.2635048 Vali Loss: 0.2012601 Test Loss: 0.1709254
Validation loss decreased (inf --> 0.201260).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1593718
	speed: 0.4722s/iter; left time: 1304.7561s
	iters: 200, epoch: 2 | loss: 0.2052548
	speed: 0.4733s/iter; left time: 1260.3472s
	iters: 300, epoch: 2 | loss: 0.1897478
	speed: 0.4610s/iter; left time: 1181.6004s
Epoch: 2 cost time: 149.40785765647888
Epoch: 2, Steps: 318 | Train Loss: 0.1663199 Vali Loss: 0.1779490 Test Loss: 0.1626946
Validation loss decreased (0.201260 --> 0.177949).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1675662
	speed: 0.4770s/iter; left time: 1166.3401s
	iters: 200, epoch: 3 | loss: 0.1575400
	speed: 0.4701s/iter; left time: 1102.4670s
	iters: 300, epoch: 3 | loss: 0.1626793
	speed: 0.4631s/iter; left time: 1039.6698s
Epoch: 3 cost time: 149.97744131088257
Epoch: 3, Steps: 318 | Train Loss: 0.1492737 Vali Loss: 0.1775890 Test Loss: 0.1583297
Validation loss decreased (0.177949 --> 0.177589).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1486855
	speed: 0.4790s/iter; left time: 1018.7917s
	iters: 200, epoch: 4 | loss: 0.1528750
	speed: 0.4780s/iter; left time: 968.9175s
	iters: 300, epoch: 4 | loss: 0.1381980
	speed: 0.4780s/iter; left time: 921.1838s
Epoch: 4 cost time: 152.2718484401703
Epoch: 4, Steps: 318 | Train Loss: 0.1407147 Vali Loss: 0.1756200 Test Loss: 0.1585178
Validation loss decreased (0.177589 --> 0.175620).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0998938
	speed: 0.3967s/iter; left time: 717.6092s
	iters: 200, epoch: 5 | loss: 0.1436766
	speed: 0.3782s/iter; left time: 646.3246s
	iters: 300, epoch: 5 | loss: 0.1439112
	speed: 0.3852s/iter; left time: 619.8018s
Epoch: 5 cost time: 123.40333843231201
Epoch: 5, Steps: 318 | Train Loss: 0.1353329 Vali Loss: 0.1779001 Test Loss: 0.1603715
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1328498
	speed: 0.4454s/iter; left time: 664.0870s
	iters: 200, epoch: 6 | loss: 0.1419415
	speed: 0.4522s/iter; left time: 628.9636s
	iters: 300, epoch: 6 | loss: 0.1330889
	speed: 0.4952s/iter; left time: 639.3111s
Epoch: 6 cost time: 147.95166158676147
Epoch: 6, Steps: 318 | Train Loss: 0.1331006 Vali Loss: 0.1809135 Test Loss: 0.1617506
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1141581
	speed: 0.4881s/iter; left time: 572.5505s
	iters: 200, epoch: 7 | loss: 0.1070267
	speed: 0.4770s/iter; left time: 511.8055s
	iters: 300, epoch: 7 | loss: 0.1391571
	speed: 0.4759s/iter; left time: 463.0144s
Epoch: 7 cost time: 151.8064296245575
Epoch: 7, Steps: 318 | Train Loss: 0.1315342 Vali Loss: 0.1793048 Test Loss: 0.1600232
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.3478611707687378, mae:0.45764514803886414, rmse:0.5897975564002991, mape:0.01637159287929535, mspe:0.0004515688051469624, rse:0.413068562746048, r2_score:0.8150645334941647, acc:0.9836284071207047
corr: [36.12429  36.139618 36.018143 36.10089  35.92041  36.104237 35.981556
 36.078304 36.197296 36.106064 36.21909  36.300163 36.671005 37.0387
 37.00837 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2676713
	speed: 0.3516s/iter; left time: 1083.1839s
	iters: 200, epoch: 1 | loss: 0.2710317
	speed: 0.3601s/iter; left time: 1073.3930s
	iters: 300, epoch: 1 | loss: 0.2019144
	speed: 0.3527s/iter; left time: 1016.1941s
Epoch: 1 cost time: 113.32317757606506
Epoch: 1, Steps: 318 | Train Loss: 0.2856956 Vali Loss: 0.1972225 Test Loss: 0.1788811
Validation loss decreased (inf --> 0.197222).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1694015
	speed: 0.3694s/iter; left time: 1020.6602s
	iters: 200, epoch: 2 | loss: 0.1935565
	speed: 0.3767s/iter; left time: 1003.1064s
	iters: 300, epoch: 2 | loss: 0.1758412
	speed: 0.3827s/iter; left time: 980.8082s
Epoch: 2 cost time: 119.2248466014862
Epoch: 2, Steps: 318 | Train Loss: 0.1816578 Vali Loss: 0.2059249 Test Loss: 0.1854737
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1735464
	speed: 0.4201s/iter; left time: 1027.2599s
	iters: 200, epoch: 3 | loss: 0.1821227
	speed: 0.4135s/iter; left time: 969.6331s
	iters: 300, epoch: 3 | loss: 0.1361362
	speed: 0.4045s/iter; left time: 908.1319s
Epoch: 3 cost time: 131.18849515914917
Epoch: 3, Steps: 318 | Train Loss: 0.1641469 Vali Loss: 0.1997452 Test Loss: 0.1781301
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1151667
	speed: 0.3979s/iter; left time: 846.2286s
	iters: 200, epoch: 4 | loss: 0.1462257
	speed: 0.3823s/iter; left time: 774.9914s
	iters: 300, epoch: 4 | loss: 0.1209402
	speed: 0.3847s/iter; left time: 741.3395s
Epoch: 4 cost time: 124.27065777778625
Epoch: 4, Steps: 318 | Train Loss: 0.1551880 Vali Loss: 0.2023489 Test Loss: 0.1808616
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000063
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.3925475776195526, mae:0.48713418841362, rmse:0.6265361905097961, mape:0.017506010830402374, mspe:0.0005194093682803214, rse:0.4385445713996887, r2_score:0.7826393069745822, acc:0.9824939891695976
corr: [37.349316 37.22286  37.10859  37.25957  37.22856  37.325203 37.33849
 37.27651  37.33626  37.260216 37.28549  37.241917 37.199093 37.02772
 37.136173 37.18139  37.11235  37.094196 37.043533 37.05488 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2963252
	speed: 0.2862s/iter; left time: 881.8181s
	iters: 200, epoch: 1 | loss: 0.2058101
	speed: 0.2892s/iter; left time: 861.9796s
	iters: 300, epoch: 1 | loss: 0.1749121
	speed: 0.2736s/iter; left time: 788.1283s
Epoch: 1 cost time: 90.02121806144714
Epoch: 1, Steps: 318 | Train Loss: 0.2834943 Vali Loss: 0.2046992 Test Loss: 0.1996015
Validation loss decreased (inf --> 0.204699).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2118551
	speed: 0.3823s/iter; left time: 1056.1730s
	iters: 200, epoch: 2 | loss: 0.1674964
	speed: 0.4001s/iter; left time: 1065.3632s
	iters: 300, epoch: 2 | loss: 0.1825117
	speed: 0.3777s/iter; left time: 968.0444s
Epoch: 2 cost time: 123.94601154327393
Epoch: 2, Steps: 318 | Train Loss: 0.1898401 Vali Loss: 0.2049052 Test Loss: 0.1889090
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1651774
	speed: 0.4102s/iter; left time: 1002.8557s
	iters: 200, epoch: 3 | loss: 0.2209328
	speed: 0.3649s/iter; left time: 855.6199s
	iters: 300, epoch: 3 | loss: 0.1734298
	speed: 0.3734s/iter; left time: 838.3152s
Epoch: 3 cost time: 121.88952422142029
Epoch: 3, Steps: 318 | Train Loss: 0.1720289 Vali Loss: 0.2095945 Test Loss: 0.1945202
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1595251
	speed: 0.3985s/iter; left time: 847.5465s
	iters: 200, epoch: 4 | loss: 0.1832100
	speed: 0.3783s/iter; left time: 766.8339s
	iters: 300, epoch: 4 | loss: 0.1642532
	speed: 0.3710s/iter; left time: 714.8340s
Epoch: 4 cost time: 121.98223853111267
Epoch: 4, Steps: 318 | Train Loss: 0.1612256 Vali Loss: 0.2125131 Test Loss: 0.2010460
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000063
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.4380177855491638, mae:0.5139639377593994, rmse:0.6618291139602661, mape:0.018436351791024208, mspe:0.000575121957808733, rse:0.46293845772743225, r2_score:0.7113127870733981, acc:0.9815636482089758
corr: [35.957108 36.15444  36.072247 36.011578 36.046604 36.158436 36.3079
 36.26955  36.456142 36.613007 36.602695 36.30509  36.25116  36.33998
 36.37342  36.28747  36.38327  36.504414 36.407707 36.408627 36.533867
 36.734314 36.853985 36.86009  36.800945]
Args in experiment:
Namespace(is_training=1, model_id='omdata1_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3228894
	speed: 0.4196s/iter; left time: 1288.5282s
	iters: 200, epoch: 1 | loss: 0.2760534
	speed: 0.4758s/iter; left time: 1413.5410s
	iters: 300, epoch: 1 | loss: 0.2067721
	speed: 0.5021s/iter; left time: 1441.5495s
Epoch: 1 cost time: 148.4552526473999
Epoch: 1, Steps: 317 | Train Loss: 0.3001686 Vali Loss: 0.2160691 Test Loss: 0.2131766
Validation loss decreased (inf --> 0.216069).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.2029915
	speed: 0.5207s/iter; left time: 1433.8977s
	iters: 200, epoch: 2 | loss: 0.2485037
	speed: 0.5016s/iter; left time: 1331.3374s
	iters: 300, epoch: 2 | loss: 0.1960733
	speed: 0.5181s/iter; left time: 1323.1571s
Epoch: 2 cost time: 162.84208846092224
Epoch: 2, Steps: 317 | Train Loss: 0.2014644 Vali Loss: 0.2147540 Test Loss: 0.1912444
Validation loss decreased (0.216069 --> 0.214754).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1844696
	speed: 0.5307s/iter; left time: 1293.2234s
	iters: 200, epoch: 3 | loss: 0.2039781
	speed: 0.5000s/iter; left time: 1168.5892s
	iters: 300, epoch: 3 | loss: 0.1485599
	speed: 0.4990s/iter; left time: 1116.3120s
Epoch: 3 cost time: 161.59060215950012
Epoch: 3, Steps: 317 | Train Loss: 0.1843518 Vali Loss: 0.2377802 Test Loss: 0.2046119
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1760634
	speed: 0.5283s/iter; left time: 1119.8974s
	iters: 200, epoch: 4 | loss: 0.1437842
	speed: 0.5114s/iter; left time: 1032.9543s
	iters: 300, epoch: 4 | loss: 0.1878501
	speed: 0.4641s/iter; left time: 891.0643s
Epoch: 4 cost time: 157.1747374534607
Epoch: 4, Steps: 317 | Train Loss: 0.1763785 Vali Loss: 0.2312739 Test Loss: 0.1968480
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1875147
	speed: 0.4107s/iter; left time: 740.4674s
	iters: 200, epoch: 5 | loss: 0.2369766
	speed: 0.3873s/iter; left time: 659.5426s
	iters: 300, epoch: 5 | loss: 0.1602235
	speed: 0.4591s/iter; left time: 735.8743s
Epoch: 5 cost time: 134.42527794837952
Epoch: 5, Steps: 317 | Train Loss: 0.1716600 Vali Loss: 0.2346745 Test Loss: 0.1980183
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000031
Early stopping
>>>>>>>testing : omdata1_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.41967830061912537, mae:0.4998803436756134, rmse:0.6478258371353149, mape:0.017902350053191185, mspe:0.0005479211104102433, rse:0.45281800627708435, r2_score:0.7564654457036781, acc:0.9820976499468088
corr: [36.488857 36.481354 36.441555 36.48925  36.45413  36.628506 36.6172
 36.794186 36.927658 36.5609   36.637745 36.557404 36.575905 36.67418
 36.58708  36.580738 36.52181  36.445137 36.42751  36.594868 36.63252
 36.60552  36.869717 36.789764 36.988575 37.142433 37.012375 36.98416
 37.026085 36.9621  ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2385700
	speed: 0.1324s/iter; left time: 408.0421s
	iters: 200, epoch: 1 | loss: 0.2147719
	speed: 0.1105s/iter; left time: 329.5011s
	iters: 300, epoch: 1 | loss: 0.1803906
	speed: 0.1126s/iter; left time: 324.5016s
Epoch: 1 cost time: 37.34250569343567
Epoch: 1, Steps: 318 | Train Loss: 0.3324088 Vali Loss: 0.1713655 Test Loss: 0.1527176
Validation loss decreased (inf --> 0.171366).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1590060
	speed: 0.1361s/iter; left time: 376.1594s
	iters: 200, epoch: 2 | loss: 0.1265089
	speed: 0.1075s/iter; left time: 286.3698s
	iters: 300, epoch: 2 | loss: 0.1655959
	speed: 0.1186s/iter; left time: 303.9728s
Epoch: 2 cost time: 38.106834173202515
Epoch: 2, Steps: 318 | Train Loss: 0.1454792 Vali Loss: 0.1572252 Test Loss: 0.1439304
Validation loss decreased (0.171366 --> 0.157225).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1277889
	speed: 0.1307s/iter; left time: 319.5145s
	iters: 200, epoch: 3 | loss: 0.2124104
	speed: 0.1092s/iter; left time: 256.0618s
	iters: 300, epoch: 3 | loss: 0.1222805
	speed: 0.1038s/iter; left time: 233.0134s
Epoch: 3 cost time: 35.85328125953674
Epoch: 3, Steps: 318 | Train Loss: 0.1343729 Vali Loss: 0.1500840 Test Loss: 0.1450816
Validation loss decreased (0.157225 --> 0.150084).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1373247
	speed: 0.1126s/iter; left time: 239.4316s
	iters: 200, epoch: 4 | loss: 0.1716503
	speed: 0.1136s/iter; left time: 230.3538s
	iters: 300, epoch: 4 | loss: 0.1333367
	speed: 0.1022s/iter; left time: 197.0357s
Epoch: 4 cost time: 34.81606388092041
Epoch: 4, Steps: 318 | Train Loss: 0.1304415 Vali Loss: 0.1491990 Test Loss: 0.1452759
Validation loss decreased (0.150084 --> 0.149199).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1381820
	speed: 0.1036s/iter; left time: 187.3636s
	iters: 200, epoch: 5 | loss: 0.1276767
	speed: 0.1064s/iter; left time: 181.8927s
	iters: 300, epoch: 5 | loss: 0.1092018
	speed: 0.0933s/iter; left time: 150.1916s
Epoch: 5 cost time: 32.185346841812134
Epoch: 5, Steps: 318 | Train Loss: 0.1286518 Vali Loss: 0.1460076 Test Loss: 0.1454295
Validation loss decreased (0.149199 --> 0.146008).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1627927
	speed: 0.1064s/iter; left time: 158.6680s
	iters: 200, epoch: 6 | loss: 0.1007742
	speed: 0.1105s/iter; left time: 153.7418s
	iters: 300, epoch: 6 | loss: 0.1205670
	speed: 0.1097s/iter; left time: 141.6786s
Epoch: 6 cost time: 34.65193796157837
Epoch: 6, Steps: 318 | Train Loss: 0.1275231 Vali Loss: 0.1474093 Test Loss: 0.1455405
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1025452
	speed: 0.1104s/iter; left time: 129.4642s
	iters: 200, epoch: 7 | loss: 0.1131359
	speed: 0.0960s/iter; left time: 102.9586s
	iters: 300, epoch: 7 | loss: 0.1310735
	speed: 0.1085s/iter; left time: 105.5624s
Epoch: 7 cost time: 33.57317018508911
Epoch: 7, Steps: 318 | Train Loss: 0.1271229 Vali Loss: 0.1470796 Test Loss: 0.1456350
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1156569
	speed: 0.1081s/iter; left time: 92.3972s
	iters: 200, epoch: 8 | loss: 0.1142939
	speed: 0.0987s/iter; left time: 74.5093s
	iters: 300, epoch: 8 | loss: 0.1362524
	speed: 0.1185s/iter; left time: 77.6133s
Epoch: 8 cost time: 34.80787372589111
Epoch: 8, Steps: 318 | Train Loss: 0.1268055 Vali Loss: 0.1459071 Test Loss: 0.1456057
Validation loss decreased (0.146008 --> 0.145907).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0970523
	speed: 0.1193s/iter; left time: 64.0464s
	iters: 200, epoch: 9 | loss: 0.1247918
	speed: 0.1181s/iter; left time: 51.6007s
	iters: 300, epoch: 9 | loss: 0.1476755
	speed: 0.1117s/iter; left time: 37.6319s
Epoch: 9 cost time: 36.79467225074768
Epoch: 9, Steps: 318 | Train Loss: 0.1265374 Vali Loss: 0.1454771 Test Loss: 0.1457629
Validation loss decreased (0.145907 --> 0.145477).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1642538
	speed: 0.1219s/iter; left time: 26.6931s
	iters: 200, epoch: 10 | loss: 0.1135195
	speed: 0.1185s/iter; left time: 14.1001s
	iters: 300, epoch: 10 | loss: 0.1515640
	speed: 0.1173s/iter; left time: 2.2288s
Epoch: 10 cost time: 38.26505494117737
Epoch: 10, Steps: 318 | Train Loss: 0.1264968 Vali Loss: 0.1463587 Test Loss: 0.1457450
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.7445244193077087, mae:0.6480185985565186, rmse:0.8628582954406738, mape:0.024029873311519623, mspe:0.0010292305378243327, rse:0.3842191994190216, r2_score:0.8466417823016481, acc:0.9759701266884804
corr: [41.09692  41.30786  41.394295 41.69641  41.426815 41.4575   41.37282
 41.610413 41.592453 41.225613]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1733348
	speed: 0.1097s/iter; left time: 337.9180s
	iters: 200, epoch: 1 | loss: 0.1644010
	speed: 0.0855s/iter; left time: 254.8859s
	iters: 300, epoch: 1 | loss: 0.1323561
	speed: 0.0914s/iter; left time: 263.3399s
Epoch: 1 cost time: 30.101812601089478
Epoch: 1, Steps: 318 | Train Loss: 0.3204237 Vali Loss: 0.1686904 Test Loss: 0.1368851
Validation loss decreased (inf --> 0.168690).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1771197
	speed: 0.0885s/iter; left time: 244.4847s
	iters: 200, epoch: 2 | loss: 0.1413475
	speed: 0.0929s/iter; left time: 247.5064s
	iters: 300, epoch: 2 | loss: 0.1257707
	speed: 0.0887s/iter; left time: 227.3969s
Epoch: 2 cost time: 28.77238130569458
Epoch: 2, Steps: 318 | Train Loss: 0.1418200 Vali Loss: 0.1572469 Test Loss: 0.1294570
Validation loss decreased (0.168690 --> 0.157247).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1470559
	speed: 0.0868s/iter; left time: 212.2994s
	iters: 200, epoch: 3 | loss: 0.1362249
	speed: 0.0844s/iter; left time: 197.9047s
	iters: 300, epoch: 3 | loss: 0.1570323
	speed: 0.0908s/iter; left time: 203.9517s
Epoch: 3 cost time: 28.010128259658813
Epoch: 3, Steps: 318 | Train Loss: 0.1352670 Vali Loss: 0.1542838 Test Loss: 0.1271468
Validation loss decreased (0.157247 --> 0.154284).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1835497
	speed: 0.0936s/iter; left time: 199.0931s
	iters: 200, epoch: 4 | loss: 0.1154179
	speed: 0.0847s/iter; left time: 171.6913s
	iters: 300, epoch: 4 | loss: 0.1588154
	speed: 0.0900s/iter; left time: 173.3739s
Epoch: 4 cost time: 28.66438579559326
Epoch: 4, Steps: 318 | Train Loss: 0.1330774 Vali Loss: 0.1538481 Test Loss: 0.1260840
Validation loss decreased (0.154284 --> 0.153848).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1310481
	speed: 0.0930s/iter; left time: 168.1523s
	iters: 200, epoch: 5 | loss: 0.1169414
	speed: 0.0865s/iter; left time: 147.8528s
	iters: 300, epoch: 5 | loss: 0.1367504
	speed: 0.0934s/iter; left time: 150.2187s
Epoch: 5 cost time: 28.98823380470276
Epoch: 5, Steps: 318 | Train Loss: 0.1316279 Vali Loss: 0.1521597 Test Loss: 0.1256722
Validation loss decreased (0.153848 --> 0.152160).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1883978
	speed: 0.0943s/iter; left time: 140.5436s
	iters: 200, epoch: 6 | loss: 0.0950057
	speed: 0.0863s/iter; left time: 120.0393s
	iters: 300, epoch: 6 | loss: 0.1340270
	speed: 0.0852s/iter; left time: 110.0006s
Epoch: 6 cost time: 28.417917490005493
Epoch: 6, Steps: 318 | Train Loss: 0.1312039 Vali Loss: 0.1532921 Test Loss: 0.1251966
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1131135
	speed: 0.0945s/iter; left time: 110.8790s
	iters: 200, epoch: 7 | loss: 0.1597795
	speed: 0.0810s/iter; left time: 86.8722s
	iters: 300, epoch: 7 | loss: 0.1392215
	speed: 0.0909s/iter; left time: 88.4214s
Epoch: 7 cost time: 28.297925233840942
Epoch: 7, Steps: 318 | Train Loss: 0.1308223 Vali Loss: 0.1516098 Test Loss: 0.1255729
Validation loss decreased (0.152160 --> 0.151610).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0992893
	speed: 0.0916s/iter; left time: 78.3101s
	iters: 200, epoch: 8 | loss: 0.1517295
	speed: 0.0806s/iter; left time: 60.8531s
	iters: 300, epoch: 8 | loss: 0.0980992
	speed: 0.0835s/iter; left time: 54.6668s
Epoch: 8 cost time: 27.047810077667236
Epoch: 8, Steps: 318 | Train Loss: 0.1307656 Vali Loss: 0.1512931 Test Loss: 0.1255696
Validation loss decreased (0.151610 --> 0.151293).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1052586
	speed: 0.1024s/iter; left time: 54.9621s
	iters: 200, epoch: 9 | loss: 0.1541535
	speed: 0.0874s/iter; left time: 38.1968s
	iters: 300, epoch: 9 | loss: 0.1222589
	speed: 0.0807s/iter; left time: 27.1814s
Epoch: 9 cost time: 28.484670877456665
Epoch: 9, Steps: 318 | Train Loss: 0.1309124 Vali Loss: 0.1521866 Test Loss: 0.1252107
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1364578
	speed: 0.0959s/iter; left time: 20.9938s
	iters: 200, epoch: 10 | loss: 0.1506259
	speed: 0.0916s/iter; left time: 10.9046s
	iters: 300, epoch: 10 | loss: 0.1292914
	speed: 0.0725s/iter; left time: 1.3781s
Epoch: 10 cost time: 27.48237943649292
Epoch: 10, Steps: 318 | Train Loss: 0.1306071 Vali Loss: 0.1514394 Test Loss: 0.1251990
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.641381561756134, mae:0.6128507852554321, rmse:0.8008630275726318, mape:0.022618241608142853, mspe:0.0008673029951751232, rse:0.35642731189727783, r2_score:0.8595188853114659, acc:0.9773817583918571
corr: [41.6628   41.14684  41.04956  40.88276  41.614716 40.695633 40.562912
 40.513336 40.50533  40.381844 40.548653 40.891773 40.54341  40.186913
 40.001038]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2453225
	speed: 0.1328s/iter; left time: 409.1920s
	iters: 200, epoch: 1 | loss: 0.1823533
	speed: 0.1252s/iter; left time: 373.2568s
	iters: 300, epoch: 1 | loss: 0.1602023
	speed: 0.1162s/iter; left time: 334.7900s
Epoch: 1 cost time: 39.36547303199768
Epoch: 1, Steps: 318 | Train Loss: 0.3268562 Vali Loss: 0.1638656 Test Loss: 0.1430262
Validation loss decreased (inf --> 0.163866).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1452977
	speed: 0.1292s/iter; left time: 357.0836s
	iters: 200, epoch: 2 | loss: 0.1732400
	speed: 0.1125s/iter; left time: 299.5726s
	iters: 300, epoch: 2 | loss: 0.1159146
	speed: 0.1089s/iter; left time: 279.1089s
Epoch: 2 cost time: 37.070884704589844
Epoch: 2, Steps: 318 | Train Loss: 0.1399933 Vali Loss: 0.1562527 Test Loss: 0.1530303
Validation loss decreased (0.163866 --> 0.156253).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1245811
	speed: 0.1257s/iter; left time: 307.2225s
	iters: 200, epoch: 3 | loss: 0.1274204
	speed: 0.1119s/iter; left time: 262.4947s
	iters: 300, epoch: 3 | loss: 0.1594993
	speed: 0.1026s/iter; left time: 230.4078s
Epoch: 3 cost time: 35.60040903091431
Epoch: 3, Steps: 318 | Train Loss: 0.1344374 Vali Loss: 0.1543341 Test Loss: 0.1624197
Validation loss decreased (0.156253 --> 0.154334).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1467264
	speed: 0.0969s/iter; left time: 206.0834s
	iters: 200, epoch: 4 | loss: 0.1363047
	speed: 0.0836s/iter; left time: 169.4988s
	iters: 300, epoch: 4 | loss: 0.1395588
	speed: 0.1023s/iter; left time: 197.1543s
Epoch: 4 cost time: 30.16578483581543
Epoch: 4, Steps: 318 | Train Loss: 0.1325104 Vali Loss: 0.1525951 Test Loss: 0.1687616
Validation loss decreased (0.154334 --> 0.152595).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1313799
	speed: 0.1229s/iter; left time: 222.2966s
	iters: 200, epoch: 5 | loss: 0.0993093
	speed: 0.1130s/iter; left time: 193.1728s
	iters: 300, epoch: 5 | loss: 0.1253152
	speed: 0.1125s/iter; left time: 180.9540s
Epoch: 5 cost time: 36.704668045043945
Epoch: 5, Steps: 318 | Train Loss: 0.1316403 Vali Loss: 0.1526233 Test Loss: 0.1737259
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1193167
	speed: 0.1205s/iter; left time: 179.6817s
	iters: 200, epoch: 6 | loss: 0.1015499
	speed: 0.1172s/iter; left time: 162.9991s
	iters: 300, epoch: 6 | loss: 0.1275497
	speed: 0.0955s/iter; left time: 123.2342s
Epoch: 6 cost time: 35.29252052307129
Epoch: 6, Steps: 318 | Train Loss: 0.1311229 Vali Loss: 0.1515843 Test Loss: 0.1757700
Validation loss decreased (0.152595 --> 0.151584).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1344915
	speed: 0.1115s/iter; left time: 130.7411s
	iters: 200, epoch: 7 | loss: 0.1295526
	speed: 0.1143s/iter; left time: 122.6670s
	iters: 300, epoch: 7 | loss: 0.1285896
	speed: 0.1176s/iter; left time: 114.3866s
Epoch: 7 cost time: 36.61990427970886
Epoch: 7, Steps: 318 | Train Loss: 0.1310353 Vali Loss: 0.1511506 Test Loss: 0.1778091
Validation loss decreased (0.151584 --> 0.151151).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0937074
	speed: 0.1269s/iter; left time: 108.5363s
	iters: 200, epoch: 8 | loss: 0.1011616
	speed: 0.1151s/iter; left time: 86.9136s
	iters: 300, epoch: 8 | loss: 0.1569037
	speed: 0.1089s/iter; left time: 71.3287s
Epoch: 8 cost time: 36.957712173461914
Epoch: 8, Steps: 318 | Train Loss: 0.1307515 Vali Loss: 0.1503014 Test Loss: 0.1780700
Validation loss decreased (0.151151 --> 0.150301).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1351397
	speed: 0.1286s/iter; left time: 69.0700s
	iters: 200, epoch: 9 | loss: 0.1495130
	speed: 0.1008s/iter; left time: 44.0523s
	iters: 300, epoch: 9 | loss: 0.1091017
	speed: 0.1089s/iter; left time: 36.6957s
Epoch: 9 cost time: 35.90784192085266
Epoch: 9, Steps: 318 | Train Loss: 0.1307879 Vali Loss: 0.1518024 Test Loss: 0.1783019
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1350612
	speed: 0.1057s/iter; left time: 23.1495s
	iters: 200, epoch: 10 | loss: 0.1224653
	speed: 0.1104s/iter; left time: 13.1353s
	iters: 300, epoch: 10 | loss: 0.1118819
	speed: 0.0898s/iter; left time: 1.7059s
Epoch: 10 cost time: 31.649163484573364
Epoch: 10, Steps: 318 | Train Loss: 0.1307397 Vali Loss: 0.1513772 Test Loss: 0.1789136
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.9095418453216553, mae:0.7068613171577454, rmse:0.9536990523338318, mape:0.02600104920566082, mspe:0.0012056983541697264, rse:0.42418259382247925, r2_score:0.8121034198701811, acc:0.9739989507943392
corr: [41.576443 41.312687 41.277355 41.0437   41.027943 40.82634  40.69699
 40.600975 40.745853 40.588787 40.663567 40.517944 40.454205 40.29746
 40.182827 40.070236 40.18952  40.152847 40.25261  40.569443]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1881598
	speed: 0.1121s/iter; left time: 345.4992s
	iters: 200, epoch: 1 | loss: 0.1539676
	speed: 0.0866s/iter; left time: 258.2061s
	iters: 300, epoch: 1 | loss: 0.1901238
	speed: 0.0671s/iter; left time: 193.3826s
Epoch: 1 cost time: 27.951745986938477
Epoch: 1, Steps: 318 | Train Loss: 0.2876921 Vali Loss: 0.1719446 Test Loss: 0.1388321
Validation loss decreased (inf --> 0.171945).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1313749
	speed: 0.0796s/iter; left time: 219.9644s
	iters: 200, epoch: 2 | loss: 0.1559582
	speed: 0.0740s/iter; left time: 196.9635s
	iters: 300, epoch: 2 | loss: 0.1283012
	speed: 0.0720s/iter; left time: 184.5115s
Epoch: 2 cost time: 24.095317363739014
Epoch: 2, Steps: 318 | Train Loss: 0.1458489 Vali Loss: 0.1606323 Test Loss: 0.1316306
Validation loss decreased (0.171945 --> 0.160632).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1698220
	speed: 0.0775s/iter; left time: 189.5648s
	iters: 200, epoch: 3 | loss: 0.1134211
	speed: 0.0786s/iter; left time: 184.2185s
	iters: 300, epoch: 3 | loss: 0.1525229
	speed: 0.0936s/iter; left time: 210.1312s
Epoch: 3 cost time: 26.8385169506073
Epoch: 3, Steps: 318 | Train Loss: 0.1373656 Vali Loss: 0.1567750 Test Loss: 0.1323177
Validation loss decreased (0.160632 --> 0.156775).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1322278
	speed: 0.0849s/iter; left time: 180.6203s
	iters: 200, epoch: 4 | loss: 0.1080782
	speed: 0.0849s/iter; left time: 172.1516s
	iters: 300, epoch: 4 | loss: 0.1234134
	speed: 0.0841s/iter; left time: 162.0435s
Epoch: 4 cost time: 27.133352041244507
Epoch: 4, Steps: 318 | Train Loss: 0.1349489 Vali Loss: 0.1567065 Test Loss: 0.1324677
Validation loss decreased (0.156775 --> 0.156706).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1292449
	speed: 0.0935s/iter; left time: 169.2120s
	iters: 200, epoch: 5 | loss: 0.1511828
	speed: 0.0912s/iter; left time: 155.8272s
	iters: 300, epoch: 5 | loss: 0.1875482
	speed: 0.0899s/iter; left time: 144.5705s
Epoch: 5 cost time: 28.85762882232666
Epoch: 5, Steps: 318 | Train Loss: 0.1335842 Vali Loss: 0.1544209 Test Loss: 0.1336601
Validation loss decreased (0.156706 --> 0.154421).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1435458
	speed: 0.1014s/iter; left time: 151.1277s
	iters: 200, epoch: 6 | loss: 0.0980117
	speed: 0.0943s/iter; left time: 131.1957s
	iters: 300, epoch: 6 | loss: 0.1292924
	speed: 0.0916s/iter; left time: 118.2011s
Epoch: 6 cost time: 30.320903301239014
Epoch: 6, Steps: 318 | Train Loss: 0.1329481 Vali Loss: 0.1536865 Test Loss: 0.1347896
Validation loss decreased (0.154421 --> 0.153686).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1431057
	speed: 0.0902s/iter; left time: 105.8275s
	iters: 200, epoch: 7 | loss: 0.1794426
	speed: 0.0836s/iter; left time: 89.7168s
	iters: 300, epoch: 7 | loss: 0.1350791
	speed: 0.0739s/iter; left time: 71.9044s
Epoch: 7 cost time: 26.287370920181274
Epoch: 7, Steps: 318 | Train Loss: 0.1328281 Vali Loss: 0.1535316 Test Loss: 0.1349725
Validation loss decreased (0.153686 --> 0.153532).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1171790
	speed: 0.0854s/iter; left time: 73.0039s
	iters: 200, epoch: 8 | loss: 0.1051438
	speed: 0.0806s/iter; left time: 60.8476s
	iters: 300, epoch: 8 | loss: 0.1376045
	speed: 0.0787s/iter; left time: 51.5454s
Epoch: 8 cost time: 25.881157636642456
Epoch: 8, Steps: 318 | Train Loss: 0.1327277 Vali Loss: 0.1535560 Test Loss: 0.1350715
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1232532
	speed: 0.0829s/iter; left time: 44.5378s
	iters: 200, epoch: 9 | loss: 0.1524369
	speed: 0.0976s/iter; left time: 42.6587s
	iters: 300, epoch: 9 | loss: 0.1024752
	speed: 0.0923s/iter; left time: 31.1170s
Epoch: 9 cost time: 29.140156507492065
Epoch: 9, Steps: 318 | Train Loss: 0.1326005 Vali Loss: 0.1542610 Test Loss: 0.1350584
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1384473
	speed: 0.0928s/iter; left time: 20.3275s
	iters: 200, epoch: 10 | loss: 0.1331379
	speed: 0.0879s/iter; left time: 10.4595s
	iters: 300, epoch: 10 | loss: 0.1230863
	speed: 0.0948s/iter; left time: 1.8016s
Epoch: 10 cost time: 29.323131561279297
Epoch: 10, Steps: 318 | Train Loss: 0.1325221 Vali Loss: 0.1541468 Test Loss: 0.1351833
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.6894098520278931, mae:0.6347681879997253, rmse:0.8303070664405823, mape:0.023447884246706963, mspe:0.0009356072987429798, rse:0.3690430521965027, r2_score:0.8542512448691438, acc:0.976552115753293
corr: [41.210373 41.416378 41.482838 41.299763 41.24276  41.23611  41.110107
 41.17895  41.219574 41.207687 41.2342   41.234837 41.293133 41.163147
 41.085346 41.126698 41.23143  41.364178 41.174004 41.506557 41.262085
 41.522255 41.582825 41.465622 41.606987]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2532038
	speed: 0.1042s/iter; left time: 320.0906s
	iters: 200, epoch: 1 | loss: 0.1864029
	speed: 0.0784s/iter; left time: 233.0468s
	iters: 300, epoch: 1 | loss: 0.1669138
	speed: 0.0712s/iter; left time: 204.3784s
Epoch: 1 cost time: 26.566876649856567
Epoch: 1, Steps: 317 | Train Loss: 0.2739430 Vali Loss: 0.1689749 Test Loss: 0.1931903
Validation loss decreased (inf --> 0.168975).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1574120
	speed: 0.0819s/iter; left time: 225.6215s
	iters: 200, epoch: 2 | loss: 0.1596057
	speed: 0.0684s/iter; left time: 181.4705s
	iters: 300, epoch: 2 | loss: 0.2025204
	speed: 0.0937s/iter; left time: 239.4179s
Epoch: 2 cost time: 25.91405487060547
Epoch: 2, Steps: 317 | Train Loss: 0.1627052 Vali Loss: 0.1816675 Test Loss: 0.1604228
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1435608
	speed: 0.0947s/iter; left time: 230.7271s
	iters: 200, epoch: 3 | loss: 0.1633729
	speed: 0.0875s/iter; left time: 204.5307s
	iters: 300, epoch: 3 | loss: 0.1832308
	speed: 0.0866s/iter; left time: 193.6944s
Epoch: 3 cost time: 28.38402271270752
Epoch: 3, Steps: 317 | Train Loss: 0.1676144 Vali Loss: 0.1755726 Test Loss: 0.1411528
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1017678
	speed: 0.0969s/iter; left time: 205.4563s
	iters: 200, epoch: 4 | loss: 0.1614519
	speed: 0.0838s/iter; left time: 169.3256s
	iters: 300, epoch: 4 | loss: 0.1482907
	speed: 0.0944s/iter; left time: 181.1644s
Epoch: 4 cost time: 29.244922399520874
Epoch: 4, Steps: 317 | Train Loss: 0.1565701 Vali Loss: 0.1698358 Test Loss: 0.1355195
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000063
Early stopping
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.9867731928825378, mae:0.7426252365112305, rmse:0.9933645725250244, mape:0.027498776093125343, mspe:0.0013379072770476341, rse:0.4411889314651489, r2_score:0.7526505648315699, acc:0.9725012239068747
corr: [38.882103 39.605732 38.8005   40.761383 39.4681   41.060688 40.498596
 41.084732 40.923847 41.399284 41.29074  40.24089  40.629047 41.10526
 41.130276 41.106617 40.198322 38.88579  39.714245 37.24898  40.69911
 39.712837 39.628593 40.11457  39.705853 39.97869  40.165207 40.211723
 39.908325 40.008526]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1869501
	speed: 0.1329s/iter; left time: 409.3377s
	iters: 200, epoch: 1 | loss: 0.1948208
	speed: 0.0936s/iter; left time: 279.1282s
	iters: 300, epoch: 1 | loss: 0.1607446
	speed: 0.0943s/iter; left time: 271.7035s
Epoch: 1 cost time: 33.800002098083496
Epoch: 1, Steps: 318 | Train Loss: 0.2856531 Vali Loss: 0.1607089 Test Loss: 0.1209752
Validation loss decreased (inf --> 0.160709).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1486391
	speed: 0.1030s/iter; left time: 284.5989s
	iters: 200, epoch: 2 | loss: 0.1162445
	speed: 0.0907s/iter; left time: 241.4821s
	iters: 300, epoch: 2 | loss: 0.1498190
	speed: 0.0853s/iter; left time: 218.6001s
Epoch: 2 cost time: 29.218989849090576
Epoch: 2, Steps: 318 | Train Loss: 0.1339401 Vali Loss: 0.1474123 Test Loss: 0.1068064
Validation loss decreased (0.160709 --> 0.147412).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1137618
	speed: 0.1171s/iter; left time: 286.3950s
	iters: 200, epoch: 3 | loss: 0.2043426
	speed: 0.1199s/iter; left time: 281.2505s
	iters: 300, epoch: 3 | loss: 0.1145970
	speed: 0.1093s/iter; left time: 245.4847s
Epoch: 3 cost time: 36.72576713562012
Epoch: 3, Steps: 318 | Train Loss: 0.1236063 Vali Loss: 0.1406162 Test Loss: 0.1037371
Validation loss decreased (0.147412 --> 0.140616).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1190806
	speed: 0.1245s/iter; left time: 264.7361s
	iters: 200, epoch: 4 | loss: 0.1580877
	speed: 0.1273s/iter; left time: 258.1106s
	iters: 300, epoch: 4 | loss: 0.1133619
	speed: 0.1334s/iter; left time: 257.1463s
Epoch: 4 cost time: 40.87455868721008
Epoch: 4, Steps: 318 | Train Loss: 0.1192396 Vali Loss: 0.1380196 Test Loss: 0.1003368
Validation loss decreased (0.140616 --> 0.138020).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1202285
	speed: 0.1299s/iter; left time: 234.9938s
	iters: 200, epoch: 5 | loss: 0.1083257
	speed: 0.1244s/iter; left time: 212.6475s
	iters: 300, epoch: 5 | loss: 0.0968224
	speed: 0.1216s/iter; left time: 195.5951s
Epoch: 5 cost time: 40.257306814193726
Epoch: 5, Steps: 318 | Train Loss: 0.1171696 Vali Loss: 0.1348380 Test Loss: 0.0990930
Validation loss decreased (0.138020 --> 0.134838).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1493449
	speed: 0.1250s/iter; left time: 186.3927s
	iters: 200, epoch: 6 | loss: 0.0962217
	speed: 0.1144s/iter; left time: 159.1068s
	iters: 300, epoch: 6 | loss: 0.1104746
	speed: 0.1203s/iter; left time: 155.3186s
Epoch: 6 cost time: 38.07180571556091
Epoch: 6, Steps: 318 | Train Loss: 0.1158916 Vali Loss: 0.1357813 Test Loss: 0.0986581
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0859043
	speed: 0.1034s/iter; left time: 121.2991s
	iters: 200, epoch: 7 | loss: 0.1046956
	speed: 0.0884s/iter; left time: 94.8482s
	iters: 300, epoch: 7 | loss: 0.1209941
	speed: 0.0885s/iter; left time: 86.0807s
Epoch: 7 cost time: 29.00416851043701
Epoch: 7, Steps: 318 | Train Loss: 0.1153310 Vali Loss: 0.1353305 Test Loss: 0.0982886
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1038157
	speed: 0.0858s/iter; left time: 73.3302s
	iters: 200, epoch: 8 | loss: 0.1053884
	speed: 0.0793s/iter; left time: 59.9045s
	iters: 300, epoch: 8 | loss: 0.1153831
	speed: 0.0767s/iter; left time: 50.2557s
Epoch: 8 cost time: 25.893078327178955
Epoch: 8, Steps: 318 | Train Loss: 0.1149421 Vali Loss: 0.1340510 Test Loss: 0.0981560
Validation loss decreased (0.134838 --> 0.134051).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0882222
	speed: 0.0836s/iter; left time: 44.8846s
	iters: 200, epoch: 9 | loss: 0.1127763
	speed: 0.0867s/iter; left time: 37.8964s
	iters: 300, epoch: 9 | loss: 0.1355225
	speed: 0.0733s/iter; left time: 24.7045s
Epoch: 9 cost time: 25.674621105194092
Epoch: 9, Steps: 318 | Train Loss: 0.1145901 Vali Loss: 0.1339722 Test Loss: 0.0980903
Validation loss decreased (0.134051 --> 0.133972).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1452326
	speed: 0.1007s/iter; left time: 22.0627s
	iters: 200, epoch: 10 | loss: 0.1081770
	speed: 0.0760s/iter; left time: 9.0437s
	iters: 300, epoch: 10 | loss: 0.1407915
	speed: 0.0795s/iter; left time: 1.5105s
Epoch: 10 cost time: 26.66509771347046
Epoch: 10, Steps: 318 | Train Loss: 0.1145665 Vali Loss: 0.1346156 Test Loss: 0.0980454
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.501023530960083, mae:0.546707034111023, rmse:0.7078301310539246, mape:0.020282210782170296, mspe:0.0006938233855180442, rse:0.3151872754096985, r2_score:0.8904143320536388, acc:0.9797177892178297
corr: [40.933445 41.075336 41.09703  41.327972 41.07648  41.22557  41.106762
 41.216103 41.19875  41.136013]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1624060
	speed: 0.1097s/iter; left time: 337.9507s
	iters: 200, epoch: 1 | loss: 0.1626775
	speed: 0.0597s/iter; left time: 177.9867s
	iters: 300, epoch: 1 | loss: 0.1252949
	speed: 0.0620s/iter; left time: 178.5999s
Epoch: 1 cost time: 24.260390996932983
Epoch: 1, Steps: 318 | Train Loss: 0.2831703 Vali Loss: 0.1592603 Test Loss: 0.1197647
Validation loss decreased (inf --> 0.159260).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1686928
	speed: 0.0823s/iter; left time: 227.4119s
	iters: 200, epoch: 2 | loss: 0.1385631
	speed: 0.0741s/iter; left time: 197.2236s
	iters: 300, epoch: 2 | loss: 0.1222376
	speed: 0.0771s/iter; left time: 197.5171s
Epoch: 2 cost time: 24.40713143348694
Epoch: 2, Steps: 318 | Train Loss: 0.1356804 Vali Loss: 0.1505755 Test Loss: 0.1109420
Validation loss decreased (0.159260 --> 0.150575).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1386337
	speed: 0.0823s/iter; left time: 201.1536s
	iters: 200, epoch: 3 | loss: 0.1331512
	speed: 0.0856s/iter; left time: 200.6599s
	iters: 300, epoch: 3 | loss: 0.1557077
	speed: 0.0783s/iter; left time: 175.8150s
Epoch: 3 cost time: 26.0396671295166
Epoch: 3, Steps: 318 | Train Loss: 0.1292961 Vali Loss: 0.1475894 Test Loss: 0.1068558
Validation loss decreased (0.150575 --> 0.147589).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1707812
	speed: 0.0726s/iter; left time: 154.4832s
	iters: 200, epoch: 4 | loss: 0.1100400
	speed: 0.0731s/iter; left time: 148.1885s
	iters: 300, epoch: 4 | loss: 0.1498677
	speed: 0.0658s/iter; left time: 126.8237s
Epoch: 4 cost time: 22.52412223815918
Epoch: 4, Steps: 318 | Train Loss: 0.1267374 Vali Loss: 0.1469560 Test Loss: 0.1055245
Validation loss decreased (0.147589 --> 0.146956).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1254455
	speed: 0.0801s/iter; left time: 144.9565s
	iters: 200, epoch: 5 | loss: 0.1147655
	speed: 0.0728s/iter; left time: 124.4413s
	iters: 300, epoch: 5 | loss: 0.1332785
	speed: 0.0773s/iter; left time: 124.4355s
Epoch: 5 cost time: 24.488723278045654
Epoch: 5, Steps: 318 | Train Loss: 0.1252533 Vali Loss: 0.1451570 Test Loss: 0.1047800
Validation loss decreased (0.146956 --> 0.145157).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1739903
	speed: 0.0855s/iter; left time: 127.4093s
	iters: 200, epoch: 6 | loss: 0.0914454
	speed: 0.0769s/iter; left time: 107.0249s
	iters: 300, epoch: 6 | loss: 0.1323871
	speed: 0.0735s/iter; left time: 94.8759s
Epoch: 6 cost time: 25.104581117630005
Epoch: 6, Steps: 318 | Train Loss: 0.1246167 Vali Loss: 0.1460484 Test Loss: 0.1043632
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1097523
	speed: 0.0809s/iter; left time: 94.8877s
	iters: 200, epoch: 7 | loss: 0.1538006
	speed: 0.0748s/iter; left time: 80.2195s
	iters: 300, epoch: 7 | loss: 0.1267452
	speed: 0.0690s/iter; left time: 67.1061s
Epoch: 7 cost time: 23.957157373428345
Epoch: 7, Steps: 318 | Train Loss: 0.1241598 Vali Loss: 0.1443063 Test Loss: 0.1041959
Validation loss decreased (0.145157 --> 0.144306).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0978124
	speed: 0.0906s/iter; left time: 77.4394s
	iters: 200, epoch: 8 | loss: 0.1446357
	speed: 0.0825s/iter; left time: 62.2513s
	iters: 300, epoch: 8 | loss: 0.0919221
	speed: 0.0810s/iter; left time: 53.0280s
Epoch: 8 cost time: 26.740511178970337
Epoch: 8, Steps: 318 | Train Loss: 0.1241047 Vali Loss: 0.1441206 Test Loss: 0.1041109
Validation loss decreased (0.144306 --> 0.144121).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1036015
	speed: 0.0922s/iter; left time: 49.5182s
	iters: 200, epoch: 9 | loss: 0.1486941
	speed: 0.0818s/iter; left time: 35.7532s
	iters: 300, epoch: 9 | loss: 0.1158165
	speed: 0.0840s/iter; left time: 28.2955s
Epoch: 9 cost time: 27.412869215011597
Epoch: 9, Steps: 318 | Train Loss: 0.1240575 Vali Loss: 0.1449630 Test Loss: 0.1040505
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1275627
	speed: 0.0900s/iter; left time: 19.7173s
	iters: 200, epoch: 10 | loss: 0.1361585
	speed: 0.0889s/iter; left time: 10.5811s
	iters: 300, epoch: 10 | loss: 0.1208990
	speed: 0.0852s/iter; left time: 1.6188s
Epoch: 10 cost time: 27.97245454788208
Epoch: 10, Steps: 318 | Train Loss: 0.1238388 Vali Loss: 0.1441559 Test Loss: 0.1040222
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.5317754149436951, mae:0.5671185255050659, rmse:0.7292293310165405, mape:0.02098662592470646, mspe:0.0007309976499527693, rse:0.3245464861392975, r2_score:0.8837163343764014, acc:0.9790133740752935
corr: [40.99523  41.092487 41.403854 41.00487  41.178307 40.987858 41.53626
 41.017174 41.239532 41.112255 41.09803  41.45967  41.240604 40.92864
 41.310055]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2288862
	speed: 0.1120s/iter; left time: 345.1954s
	iters: 200, epoch: 1 | loss: 0.1880393
	speed: 0.0892s/iter; left time: 265.9258s
	iters: 300, epoch: 1 | loss: 0.1544250
	speed: 0.1033s/iter; left time: 297.6165s
Epoch: 1 cost time: 32.37181806564331
Epoch: 1, Steps: 318 | Train Loss: 0.3027751 Vali Loss: 0.1652051 Test Loss: 0.1264593
Validation loss decreased (inf --> 0.165205).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1426735
	speed: 0.1098s/iter; left time: 303.3609s
	iters: 200, epoch: 2 | loss: 0.1711712
	speed: 0.1040s/iter; left time: 277.0447s
	iters: 300, epoch: 2 | loss: 0.1101551
	speed: 0.1012s/iter; left time: 259.3878s
Epoch: 2 cost time: 32.40100407600403
Epoch: 2, Steps: 318 | Train Loss: 0.1403749 Vali Loss: 0.1556689 Test Loss: 0.1148250
Validation loss decreased (0.165205 --> 0.155669).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1225659
	speed: 0.1033s/iter; left time: 252.5152s
	iters: 200, epoch: 3 | loss: 0.1282484
	speed: 0.0985s/iter; left time: 231.0041s
	iters: 300, epoch: 3 | loss: 0.1613308
	speed: 0.1062s/iter; left time: 238.4759s
Epoch: 3 cost time: 32.949511766433716
Epoch: 3, Steps: 318 | Train Loss: 0.1335324 Vali Loss: 0.1520925 Test Loss: 0.1112563
Validation loss decreased (0.155669 --> 0.152093).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1466592
	speed: 0.1112s/iter; left time: 236.4193s
	iters: 200, epoch: 4 | loss: 0.1294315
	speed: 0.1002s/iter; left time: 203.0244s
	iters: 300, epoch: 4 | loss: 0.1417970
	speed: 0.1013s/iter; left time: 195.2101s
Epoch: 4 cost time: 33.17177677154541
Epoch: 4, Steps: 318 | Train Loss: 0.1311957 Vali Loss: 0.1505967 Test Loss: 0.1111635
Validation loss decreased (0.152093 --> 0.150597).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1221822
	speed: 0.0964s/iter; left time: 174.3901s
	iters: 200, epoch: 5 | loss: 0.0965636
	speed: 0.0982s/iter; left time: 167.9014s
	iters: 300, epoch: 5 | loss: 0.1262021
	speed: 0.1026s/iter; left time: 165.0293s
Epoch: 5 cost time: 31.88880491256714
Epoch: 5, Steps: 318 | Train Loss: 0.1300870 Vali Loss: 0.1505397 Test Loss: 0.1100648
Validation loss decreased (0.150597 --> 0.150540).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1187850
	speed: 0.1071s/iter; left time: 159.6892s
	iters: 200, epoch: 6 | loss: 0.1025618
	speed: 0.1083s/iter; left time: 150.6028s
	iters: 300, epoch: 6 | loss: 0.1275639
	speed: 0.0945s/iter; left time: 121.9743s
Epoch: 6 cost time: 32.8991973400116
Epoch: 6, Steps: 318 | Train Loss: 0.1296209 Vali Loss: 0.1490751 Test Loss: 0.1098533
Validation loss decreased (0.150540 --> 0.149075).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1332393
	speed: 0.0969s/iter; left time: 113.7173s
	iters: 200, epoch: 7 | loss: 0.1286038
	speed: 0.1025s/iter; left time: 109.9988s
	iters: 300, epoch: 7 | loss: 0.1328468
	speed: 0.1053s/iter; left time: 102.4446s
Epoch: 7 cost time: 31.08674120903015
Epoch: 7, Steps: 318 | Train Loss: 0.1293993 Vali Loss: 0.1484940 Test Loss: 0.1096633
Validation loss decreased (0.149075 --> 0.148494).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0930612
	speed: 0.1061s/iter; left time: 90.7226s
	iters: 200, epoch: 8 | loss: 0.1019404
	speed: 0.1051s/iter; left time: 79.3399s
	iters: 300, epoch: 8 | loss: 0.1520805
	speed: 0.0933s/iter; left time: 61.1324s
Epoch: 8 cost time: 32.186927795410156
Epoch: 8, Steps: 318 | Train Loss: 0.1291068 Vali Loss: 0.1477784 Test Loss: 0.1095463
Validation loss decreased (0.148494 --> 0.147778).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1344293
	speed: 0.0935s/iter; left time: 50.2168s
	iters: 200, epoch: 9 | loss: 0.1464951
	speed: 0.0991s/iter; left time: 43.3183s
	iters: 300, epoch: 9 | loss: 0.1092715
	speed: 0.0855s/iter; left time: 28.8125s
Epoch: 9 cost time: 29.334243297576904
Epoch: 9, Steps: 318 | Train Loss: 0.1291401 Vali Loss: 0.1492699 Test Loss: 0.1095039
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1410560
	speed: 0.0863s/iter; left time: 18.8992s
	iters: 200, epoch: 10 | loss: 0.1205105
	speed: 0.0899s/iter; left time: 10.6965s
	iters: 300, epoch: 10 | loss: 0.1075900
	speed: 0.0668s/iter; left time: 1.2692s
Epoch: 10 cost time: 25.314843893051147
Epoch: 10, Steps: 318 | Train Loss: 0.1290449 Vali Loss: 0.1488802 Test Loss: 0.1094805
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.5595383048057556, mae:0.5795481204986572, rmse:0.7480229139328003, mape:0.02144545502960682, mspe:0.000767419405747205, rse:0.33270275592803955, r2_score:0.8771628619356763, acc:0.9785545449703932
corr: [41.201965 40.94985  41.05596  40.84144  41.245926 40.951218 41.10596
 40.756252 40.893913 40.80937  40.82104  40.86905  40.731586 41.132057
 41.05782  41.008476 41.271107 41.25772  41.31911  41.534122]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1696776
	speed: 0.1120s/iter; left time: 345.0282s
	iters: 200, epoch: 1 | loss: 0.1231298
	speed: 0.0816s/iter; left time: 243.1080s
	iters: 300, epoch: 1 | loss: 0.1728984
	speed: 0.0728s/iter; left time: 209.6534s
Epoch: 1 cost time: 27.940048456192017
Epoch: 1, Steps: 318 | Train Loss: 0.2574569 Vali Loss: 0.1621971 Test Loss: 0.1202264
Validation loss decreased (inf --> 0.162197).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1263945
	speed: 0.0836s/iter; left time: 230.9662s
	iters: 200, epoch: 2 | loss: 0.1486472
	speed: 0.0789s/iter; left time: 209.9867s
	iters: 300, epoch: 2 | loss: 0.1184421
	speed: 0.0856s/iter; left time: 219.2952s
Epoch: 2 cost time: 26.49364972114563
Epoch: 2, Steps: 318 | Train Loss: 0.1371269 Vali Loss: 0.1552307 Test Loss: 0.1155367
Validation loss decreased (0.162197 --> 0.155231).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1625282
	speed: 0.1218s/iter; left time: 297.7275s
	iters: 200, epoch: 3 | loss: 0.1081730
	speed: 0.1218s/iter; left time: 285.7232s
	iters: 300, epoch: 3 | loss: 0.1444165
	speed: 0.1016s/iter; left time: 228.1599s
Epoch: 3 cost time: 36.4908332824707
Epoch: 3, Steps: 318 | Train Loss: 0.1324403 Vali Loss: 0.1539531 Test Loss: 0.1142882
Validation loss decreased (0.155231 --> 0.153953).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1274543
	speed: 0.1236s/iter; left time: 262.9065s
	iters: 200, epoch: 4 | loss: 0.1050682
	speed: 0.1066s/iter; left time: 216.1142s
	iters: 300, epoch: 4 | loss: 0.1276871
	speed: 0.1070s/iter; left time: 206.2779s
Epoch: 4 cost time: 36.038954973220825
Epoch: 4, Steps: 318 | Train Loss: 0.1308332 Vali Loss: 0.1550662 Test Loss: 0.1134722
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1289886
	speed: 0.1139s/iter; left time: 206.1047s
	iters: 200, epoch: 5 | loss: 0.1488569
	speed: 0.1068s/iter; left time: 182.4403s
	iters: 300, epoch: 5 | loss: 0.1807971
	speed: 0.1022s/iter; left time: 164.4862s
Epoch: 5 cost time: 33.75424838066101
Epoch: 5, Steps: 318 | Train Loss: 0.1299045 Vali Loss: 0.1524304 Test Loss: 0.1126068
Validation loss decreased (0.153953 --> 0.152430).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1422274
	speed: 0.0798s/iter; left time: 118.9284s
	iters: 200, epoch: 6 | loss: 0.0944609
	speed: 0.0761s/iter; left time: 105.8821s
	iters: 300, epoch: 6 | loss: 0.1248389
	speed: 0.0823s/iter; left time: 106.2492s
Epoch: 6 cost time: 25.392476320266724
Epoch: 6, Steps: 318 | Train Loss: 0.1294961 Vali Loss: 0.1518669 Test Loss: 0.1125360
Validation loss decreased (0.152430 --> 0.151867).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1480075
	speed: 0.0835s/iter; left time: 97.9899s
	iters: 200, epoch: 7 | loss: 0.1827663
	speed: 0.0745s/iter; left time: 79.9377s
	iters: 300, epoch: 7 | loss: 0.1336566
	speed: 0.0681s/iter; left time: 66.2378s
Epoch: 7 cost time: 23.641996383666992
Epoch: 7, Steps: 318 | Train Loss: 0.1294700 Vali Loss: 0.1516330 Test Loss: 0.1124874
Validation loss decreased (0.151867 --> 0.151633).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1123693
	speed: 0.0731s/iter; left time: 62.4935s
	iters: 200, epoch: 8 | loss: 0.1028857
	speed: 0.0657s/iter; left time: 49.5987s
	iters: 300, epoch: 8 | loss: 0.1354944
	speed: 0.0665s/iter; left time: 43.5476s
Epoch: 8 cost time: 21.745024919509888
Epoch: 8, Steps: 318 | Train Loss: 0.1293870 Vali Loss: 0.1518485 Test Loss: 0.1124025
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1167015
	speed: 0.0789s/iter; left time: 42.3493s
	iters: 200, epoch: 9 | loss: 0.1519709
	speed: 0.0734s/iter; left time: 32.0866s
	iters: 300, epoch: 9 | loss: 0.0986997
	speed: 0.0824s/iter; left time: 27.7715s
Epoch: 9 cost time: 24.951841115951538
Epoch: 9, Steps: 318 | Train Loss: 0.1293532 Vali Loss: 0.1525798 Test Loss: 0.1123801
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1361637
	speed: 0.0829s/iter; left time: 18.1610s
	iters: 200, epoch: 10 | loss: 0.1280126
	speed: 0.0797s/iter; left time: 9.4887s
	iters: 300, epoch: 10 | loss: 0.1211188
	speed: 0.0755s/iter; left time: 1.4343s
Epoch: 10 cost time: 25.07257652282715
Epoch: 10, Steps: 318 | Train Loss: 0.1292499 Vali Loss: 0.1525272 Test Loss: 0.1123754
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.574560821056366, mae:0.5871832966804504, rmse:0.7579979300498962, mape:0.02173299714922905, mspe:0.0007880546036176383, rse:0.3369041383266449, r2_score:0.8753103657007079, acc:0.978267002850771
corr: [40.8528   40.776054 41.118675 40.93628  40.932117 40.714333 40.92051
 40.903965 41.01111  41.042683 40.912323 41.02602  41.057854 41.1
 41.006325 41.04479  41.11776  41.16223  41.165623 41.558613 41.183064
 41.28621  41.621937 41.30596  41.15891 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2527713
	speed: 0.1056s/iter; left time: 324.1835s
	iters: 200, epoch: 1 | loss: 0.1676287
	speed: 0.0772s/iter; left time: 229.4250s
	iters: 300, epoch: 1 | loss: 0.1537795
	speed: 0.0790s/iter; left time: 226.7598s
Epoch: 1 cost time: 27.07368278503418
Epoch: 1, Steps: 317 | Train Loss: 0.2640157 Vali Loss: 0.1661992 Test Loss: 0.1215168
Validation loss decreased (inf --> 0.166199).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1536351
	speed: 0.0941s/iter; left time: 259.1623s
	iters: 200, epoch: 2 | loss: 0.1510427
	speed: 0.0744s/iter; left time: 197.5449s
	iters: 300, epoch: 2 | loss: 0.1309939
	speed: 0.0746s/iter; left time: 190.5060s
Epoch: 2 cost time: 25.401451587677002
Epoch: 2, Steps: 317 | Train Loss: 0.1369087 Vali Loss: 0.1571856 Test Loss: 0.1138064
Validation loss decreased (0.166199 --> 0.157186).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1420338
	speed: 0.0919s/iter; left time: 223.9028s
	iters: 200, epoch: 3 | loss: 0.1227420
	speed: 0.0865s/iter; left time: 202.2519s
	iters: 300, epoch: 3 | loss: 0.1472162
	speed: 0.0833s/iter; left time: 186.2905s
Epoch: 3 cost time: 27.728110551834106
Epoch: 3, Steps: 317 | Train Loss: 0.1323865 Vali Loss: 0.1562503 Test Loss: 0.1119710
Validation loss decreased (0.157186 --> 0.156250).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0976207
	speed: 0.0905s/iter; left time: 191.8608s
	iters: 200, epoch: 4 | loss: 0.1422593
	speed: 0.0829s/iter; left time: 167.5187s
	iters: 300, epoch: 4 | loss: 0.1469709
	speed: 0.0801s/iter; left time: 153.8653s
Epoch: 4 cost time: 26.79822587966919
Epoch: 4, Steps: 317 | Train Loss: 0.1305616 Vali Loss: 0.1541568 Test Loss: 0.1111326
Validation loss decreased (0.156250 --> 0.154157).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1127331
	speed: 0.0752s/iter; left time: 135.4982s
	iters: 200, epoch: 5 | loss: 0.1052860
	speed: 0.0685s/iter; left time: 116.6417s
	iters: 300, epoch: 5 | loss: 0.1345213
	speed: 0.0678s/iter; left time: 108.7497s
Epoch: 5 cost time: 22.59859013557434
Epoch: 5, Steps: 317 | Train Loss: 0.1298888 Vali Loss: 0.1542448 Test Loss: 0.1104587
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1075733
	speed: 0.0810s/iter; left time: 120.3484s
	iters: 200, epoch: 6 | loss: 0.1637271
	speed: 0.0747s/iter; left time: 103.5963s
	iters: 300, epoch: 6 | loss: 0.1321585
	speed: 0.0764s/iter; left time: 98.2727s
Epoch: 6 cost time: 24.709651231765747
Epoch: 6, Steps: 317 | Train Loss: 0.1294327 Vali Loss: 0.1534496 Test Loss: 0.1105345
Validation loss decreased (0.154157 --> 0.153450).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1513977
	speed: 0.0839s/iter; left time: 98.0984s
	iters: 200, epoch: 7 | loss: 0.1131730
	speed: 0.0788s/iter; left time: 84.2557s
	iters: 300, epoch: 7 | loss: 0.1407013
	speed: 0.0711s/iter; left time: 68.9097s
Epoch: 7 cost time: 24.439584255218506
Epoch: 7, Steps: 317 | Train Loss: 0.1291296 Vali Loss: 0.1534319 Test Loss: 0.1106511
Validation loss decreased (0.153450 --> 0.153432).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1474611
	speed: 0.0807s/iter; left time: 68.7462s
	iters: 200, epoch: 8 | loss: 0.1086962
	speed: 0.0756s/iter; left time: 56.8561s
	iters: 300, epoch: 8 | loss: 0.1054007
	speed: 0.0740s/iter; left time: 48.2404s
Epoch: 8 cost time: 24.42166233062744
Epoch: 8, Steps: 317 | Train Loss: 0.1289874 Vali Loss: 0.1529886 Test Loss: 0.1105388
Validation loss decreased (0.153432 --> 0.152989).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1149870
	speed: 0.0795s/iter; left time: 42.5113s
	iters: 200, epoch: 9 | loss: 0.1580379
	speed: 0.0800s/iter; left time: 34.8178s
	iters: 300, epoch: 9 | loss: 0.1617247
	speed: 0.0795s/iter; left time: 26.6470s
Epoch: 9 cost time: 25.461331844329834
Epoch: 9, Steps: 317 | Train Loss: 0.1291299 Vali Loss: 0.1532640 Test Loss: 0.1104941
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1245978
	speed: 0.0847s/iter; left time: 18.4681s
	iters: 200, epoch: 10 | loss: 0.1356738
	speed: 0.0788s/iter; left time: 9.3005s
	iters: 300, epoch: 10 | loss: 0.1354206
	speed: 0.0769s/iter; left time: 1.3845s
Epoch: 10 cost time: 25.533119916915894
Epoch: 10, Steps: 317 | Train Loss: 0.1289388 Vali Loss: 0.1530992 Test Loss: 0.1104789
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.5646077990531921, mae:0.5818727016448975, rmse:0.7514038681983948, mape:0.021534090861678123, mspe:0.0007750942022539675, rse:0.3337254822254181, r2_score:0.8755987961502364, acc:0.9784659091383219
corr: [40.79452  40.706505 40.788086 40.84113  40.849277 40.994183 40.98165
 40.866802 40.884037 41.044228 41.03516  40.893738 41.068825 41.247887
 41.119404 41.16682  41.151955 41.081554 41.080612 41.051403 41.166786
 41.38871  41.395878 41.214054 41.09741  41.141342 41.325684 41.20613
 41.234474 41.413605]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1881939
	speed: 0.1135s/iter; left time: 349.7814s
	iters: 200, epoch: 1 | loss: 0.1944835
	speed: 0.0942s/iter; left time: 280.9567s
	iters: 300, epoch: 1 | loss: 0.1645453
	speed: 0.0922s/iter; left time: 265.5094s
Epoch: 1 cost time: 31.658994674682617
Epoch: 1, Steps: 318 | Train Loss: 0.2872183 Vali Loss: 0.1617911 Test Loss: 0.1214726
Validation loss decreased (inf --> 0.161791).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1501907
	speed: 0.0906s/iter; left time: 250.1917s
	iters: 200, epoch: 2 | loss: 0.1177649
	speed: 0.0858s/iter; left time: 228.4805s
	iters: 300, epoch: 2 | loss: 0.1544454
	speed: 0.0888s/iter; left time: 227.5820s
Epoch: 2 cost time: 28.412709951400757
Epoch: 2, Steps: 318 | Train Loss: 0.1349771 Vali Loss: 0.1484188 Test Loss: 0.1069001
Validation loss decreased (0.161791 --> 0.148419).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1159750
	speed: 0.0997s/iter; left time: 243.6873s
	iters: 200, epoch: 3 | loss: 0.2032961
	speed: 0.0869s/iter; left time: 203.7782s
	iters: 300, epoch: 3 | loss: 0.1116441
	speed: 0.0909s/iter; left time: 203.9818s
Epoch: 3 cost time: 29.456417322158813
Epoch: 3, Steps: 318 | Train Loss: 0.1236813 Vali Loss: 0.1408059 Test Loss: 0.1032902
Validation loss decreased (0.148419 --> 0.140806).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1200275
	speed: 0.1006s/iter; left time: 213.9801s
	iters: 200, epoch: 4 | loss: 0.1586981
	speed: 0.0925s/iter; left time: 187.4966s
	iters: 300, epoch: 4 | loss: 0.1124269
	speed: 0.0972s/iter; left time: 187.2405s
Epoch: 4 cost time: 30.79177498817444
Epoch: 4, Steps: 318 | Train Loss: 0.1190320 Vali Loss: 0.1382318 Test Loss: 0.1000243
Validation loss decreased (0.140806 --> 0.138232).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1202644
	speed: 0.0973s/iter; left time: 176.0389s
	iters: 200, epoch: 5 | loss: 0.1099471
	speed: 0.0916s/iter; left time: 156.5777s
	iters: 300, epoch: 5 | loss: 0.0985015
	speed: 0.1023s/iter; left time: 164.5880s
Epoch: 5 cost time: 30.922199010849
Epoch: 5, Steps: 318 | Train Loss: 0.1169006 Vali Loss: 0.1348549 Test Loss: 0.0987483
Validation loss decreased (0.138232 --> 0.134855).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1525919
	speed: 0.0971s/iter; left time: 144.8394s
	iters: 200, epoch: 6 | loss: 0.0964423
	speed: 0.0974s/iter; left time: 135.4245s
	iters: 300, epoch: 6 | loss: 0.1128669
	speed: 0.1044s/iter; left time: 134.8347s
Epoch: 6 cost time: 31.530407190322876
Epoch: 6, Steps: 318 | Train Loss: 0.1156272 Vali Loss: 0.1357191 Test Loss: 0.0982611
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0838267
	speed: 0.0942s/iter; left time: 110.5279s
	iters: 200, epoch: 7 | loss: 0.1059174
	speed: 0.0974s/iter; left time: 104.5386s
	iters: 300, epoch: 7 | loss: 0.1196639
	speed: 0.0944s/iter; left time: 91.8456s
Epoch: 7 cost time: 30.33671760559082
Epoch: 7, Steps: 318 | Train Loss: 0.1150281 Vali Loss: 0.1352733 Test Loss: 0.0979245
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1030614
	speed: 0.0971s/iter; left time: 83.0365s
	iters: 200, epoch: 8 | loss: 0.1064000
	speed: 0.0965s/iter; left time: 72.8317s
	iters: 300, epoch: 8 | loss: 0.1160375
	speed: 0.1096s/iter; left time: 71.7875s
Epoch: 8 cost time: 32.216368675231934
Epoch: 8, Steps: 318 | Train Loss: 0.1146401 Vali Loss: 0.1340135 Test Loss: 0.0977887
Validation loss decreased (0.134855 --> 0.134013).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0888072
	speed: 0.0976s/iter; left time: 52.4190s
	iters: 200, epoch: 9 | loss: 0.1122794
	speed: 0.0933s/iter; left time: 40.7715s
	iters: 300, epoch: 9 | loss: 0.1355016
	speed: 0.1036s/iter; left time: 34.9264s
Epoch: 9 cost time: 31.03655219078064
Epoch: 9, Steps: 318 | Train Loss: 0.1142872 Vali Loss: 0.1338829 Test Loss: 0.0977288
Validation loss decreased (0.134013 --> 0.133883).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1463159
	speed: 0.0976s/iter; left time: 21.3812s
	iters: 200, epoch: 10 | loss: 0.1096909
	speed: 0.0956s/iter; left time: 11.3790s
	iters: 300, epoch: 10 | loss: 0.1401855
	speed: 0.0946s/iter; left time: 1.7965s
Epoch: 10 cost time: 30.54882836341858
Epoch: 10, Steps: 318 | Train Loss: 0.1142446 Vali Loss: 0.1345711 Test Loss: 0.0976880
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.4991770088672638, mae:0.5479046702384949, rmse:0.7065246105194092, mape:0.02032453380525112, mspe:0.000691150373313576, rse:0.3146059215068817, r2_score:0.890478566894527, acc:0.9796754661947489
corr: [40.956146 41.089638 41.05012  41.294407 41.004993 41.27539  41.061535
 41.285793 41.251637 41.232037]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1621661
	speed: 0.1171s/iter; left time: 360.8841s
	iters: 200, epoch: 1 | loss: 0.1568633
	speed: 0.1064s/iter; left time: 317.0888s
	iters: 300, epoch: 1 | loss: 0.1239317
	speed: 0.1380s/iter; left time: 397.6739s
Epoch: 1 cost time: 38.38511919975281
Epoch: 1, Steps: 318 | Train Loss: 0.2733120 Vali Loss: 0.1597091 Test Loss: 0.1181822
Validation loss decreased (inf --> 0.159709).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1673570
	speed: 0.1253s/iter; left time: 346.2379s
	iters: 200, epoch: 2 | loss: 0.1373594
	speed: 0.1190s/iter; left time: 316.7712s
	iters: 300, epoch: 2 | loss: 0.1202524
	speed: 0.1190s/iter; left time: 304.9388s
Epoch: 2 cost time: 38.64516735076904
Epoch: 2, Steps: 318 | Train Loss: 0.1346291 Vali Loss: 0.1505581 Test Loss: 0.1099647
Validation loss decreased (0.159709 --> 0.150558).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1394886
	speed: 0.1274s/iter; left time: 311.5452s
	iters: 200, epoch: 3 | loss: 0.1333862
	speed: 0.1241s/iter; left time: 291.1301s
	iters: 300, epoch: 3 | loss: 0.1549881
	speed: 0.1145s/iter; left time: 257.0587s
Epoch: 3 cost time: 38.790507793426514
Epoch: 3, Steps: 318 | Train Loss: 0.1285577 Vali Loss: 0.1478969 Test Loss: 0.1062230
Validation loss decreased (0.150558 --> 0.147897).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1693853
	speed: 0.0972s/iter; left time: 206.7481s
	iters: 200, epoch: 4 | loss: 0.1055459
	speed: 0.0963s/iter; left time: 195.1769s
	iters: 300, epoch: 4 | loss: 0.1469382
	speed: 0.0898s/iter; left time: 172.9683s
Epoch: 4 cost time: 29.703205108642578
Epoch: 4, Steps: 318 | Train Loss: 0.1262844 Vali Loss: 0.1476173 Test Loss: 0.1050656
Validation loss decreased (0.147897 --> 0.147617).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1213315
	speed: 0.0919s/iter; left time: 166.2163s
	iters: 200, epoch: 5 | loss: 0.1095924
	speed: 0.0784s/iter; left time: 134.0446s
	iters: 300, epoch: 5 | loss: 0.1347443
	speed: 0.0824s/iter; left time: 132.5027s
Epoch: 5 cost time: 26.77721881866455
Epoch: 5, Steps: 318 | Train Loss: 0.1249820 Vali Loss: 0.1457480 Test Loss: 0.1044960
Validation loss decreased (0.147617 --> 0.145748).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1728438
	speed: 0.0896s/iter; left time: 133.5289s
	iters: 200, epoch: 6 | loss: 0.0889747
	speed: 0.0978s/iter; left time: 136.0679s
	iters: 300, epoch: 6 | loss: 0.1312740
	speed: 0.0837s/iter; left time: 108.0137s
Epoch: 6 cost time: 28.488224983215332
Epoch: 6, Steps: 318 | Train Loss: 0.1245611 Vali Loss: 0.1468828 Test Loss: 0.1041024
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1082423
	speed: 0.0760s/iter; left time: 89.1535s
	iters: 200, epoch: 7 | loss: 0.1553232
	speed: 0.0777s/iter; left time: 83.3380s
	iters: 300, epoch: 7 | loss: 0.1315501
	speed: 0.0900s/iter; left time: 87.5948s
Epoch: 7 cost time: 26.00000023841858
Epoch: 7, Steps: 318 | Train Loss: 0.1241472 Vali Loss: 0.1450775 Test Loss: 0.1039710
Validation loss decreased (0.145748 --> 0.145077).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0947999
	speed: 0.0850s/iter; left time: 72.6476s
	iters: 200, epoch: 8 | loss: 0.1460071
	speed: 0.0824s/iter; left time: 62.2227s
	iters: 300, epoch: 8 | loss: 0.0950645
	speed: 0.0824s/iter; left time: 53.9561s
Epoch: 8 cost time: 26.742551565170288
Epoch: 8, Steps: 318 | Train Loss: 0.1240347 Vali Loss: 0.1448896 Test Loss: 0.1038978
Validation loss decreased (0.145077 --> 0.144890).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1045749
	speed: 0.0848s/iter; left time: 45.5555s
	iters: 200, epoch: 9 | loss: 0.1492050
	speed: 0.0799s/iter; left time: 34.9149s
	iters: 300, epoch: 9 | loss: 0.1124370
	speed: 0.0776s/iter; left time: 26.1471s
Epoch: 9 cost time: 25.752861499786377
Epoch: 9, Steps: 318 | Train Loss: 0.1241144 Vali Loss: 0.1457949 Test Loss: 0.1038435
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1267793
	speed: 0.0895s/iter; left time: 19.6061s
	iters: 200, epoch: 10 | loss: 0.1364868
	speed: 0.0865s/iter; left time: 10.2964s
	iters: 300, epoch: 10 | loss: 0.1228467
	speed: 0.0868s/iter; left time: 1.6496s
Epoch: 10 cost time: 27.63558316230774
Epoch: 10, Steps: 318 | Train Loss: 0.1237949 Vali Loss: 0.1448909 Test Loss: 0.1038187
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.5306868553161621, mae:0.5660784840583801, rmse:0.7284825444221497, mape:0.02094511315226555, mspe:0.0007292093359865248, rse:0.3242141008377075, r2_score:0.8836603273452128, acc:0.9790548868477345
corr: [40.880524 41.08694  41.362766 41.17011  41.203804 41.0964   41.631516
 41.14469  41.12463  41.119213 41.09002  41.198944 41.304195 41.19722
 41.24677 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2287249
	speed: 0.1287s/iter; left time: 396.4486s
	iters: 200, epoch: 1 | loss: 0.1881136
	speed: 0.1116s/iter; left time: 332.7960s
	iters: 300, epoch: 1 | loss: 0.1541719
	speed: 0.1112s/iter; left time: 320.4791s
Epoch: 1 cost time: 37.19730830192566
Epoch: 1, Steps: 318 | Train Loss: 0.3030737 Vali Loss: 0.1648618 Test Loss: 0.1262264
Validation loss decreased (inf --> 0.164862).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1427016
	speed: 0.1171s/iter; left time: 323.6416s
	iters: 200, epoch: 2 | loss: 0.1708506
	speed: 0.1068s/iter; left time: 284.3814s
	iters: 300, epoch: 2 | loss: 0.1099979
	speed: 0.1018s/iter; left time: 260.8847s
Epoch: 2 cost time: 34.64898633956909
Epoch: 2, Steps: 318 | Train Loss: 0.1401738 Vali Loss: 0.1555175 Test Loss: 0.1147715
Validation loss decreased (0.164862 --> 0.155517).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1224586
	speed: 0.1159s/iter; left time: 283.3697s
	iters: 200, epoch: 3 | loss: 0.1281730
	speed: 0.1172s/iter; left time: 274.9043s
	iters: 300, epoch: 3 | loss: 0.1615115
	speed: 0.1050s/iter; left time: 235.6132s
Epoch: 3 cost time: 35.844391107559204
Epoch: 3, Steps: 318 | Train Loss: 0.1333782 Vali Loss: 0.1520032 Test Loss: 0.1112521
Validation loss decreased (0.155517 --> 0.152003).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1467247
	speed: 0.1180s/iter; left time: 251.0456s
	iters: 200, epoch: 4 | loss: 0.1293898
	speed: 0.1130s/iter; left time: 228.9785s
	iters: 300, epoch: 4 | loss: 0.1415559
	speed: 0.1125s/iter; left time: 216.7805s
Epoch: 4 cost time: 36.44951415061951
Epoch: 4, Steps: 318 | Train Loss: 0.1310733 Vali Loss: 0.1505556 Test Loss: 0.1111818
Validation loss decreased (0.152003 --> 0.150556).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1221500
	speed: 0.1163s/iter; left time: 210.4178s
	iters: 200, epoch: 5 | loss: 0.0963721
	speed: 0.1132s/iter; left time: 193.4095s
	iters: 300, epoch: 5 | loss: 0.1263560
	speed: 0.1174s/iter; left time: 188.8872s
Epoch: 5 cost time: 36.730828046798706
Epoch: 5, Steps: 318 | Train Loss: 0.1299764 Vali Loss: 0.1504843 Test Loss: 0.1100977
Validation loss decreased (0.150556 --> 0.150484).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1184184
	speed: 0.1152s/iter; left time: 171.7605s
	iters: 200, epoch: 6 | loss: 0.1023303
	speed: 0.1173s/iter; left time: 163.1878s
	iters: 300, epoch: 6 | loss: 0.1273172
	speed: 0.1216s/iter; left time: 157.0473s
Epoch: 6 cost time: 37.7943217754364
Epoch: 6, Steps: 318 | Train Loss: 0.1295189 Vali Loss: 0.1490287 Test Loss: 0.1098894
Validation loss decreased (0.150484 --> 0.149029).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1332680
	speed: 0.1149s/iter; left time: 134.7701s
	iters: 200, epoch: 7 | loss: 0.1282120
	speed: 0.1118s/iter; left time: 119.9634s
	iters: 300, epoch: 7 | loss: 0.1327357
	speed: 0.1125s/iter; left time: 109.4423s
Epoch: 7 cost time: 36.03651428222656
Epoch: 7, Steps: 318 | Train Loss: 0.1293006 Vali Loss: 0.1484503 Test Loss: 0.1097044
Validation loss decreased (0.149029 --> 0.148450).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0929365
	speed: 0.1207s/iter; left time: 103.1595s
	iters: 200, epoch: 8 | loss: 0.1017169
	speed: 0.1134s/iter; left time: 85.5847s
	iters: 300, epoch: 8 | loss: 0.1519393
	speed: 0.1093s/iter; left time: 71.5959s
Epoch: 8 cost time: 36.37608790397644
Epoch: 8, Steps: 318 | Train Loss: 0.1290109 Vali Loss: 0.1477373 Test Loss: 0.1095908
Validation loss decreased (0.148450 --> 0.147737).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1343627
	speed: 0.1232s/iter; left time: 66.1702s
	iters: 200, epoch: 9 | loss: 0.1465631
	speed: 0.1045s/iter; left time: 45.6762s
	iters: 300, epoch: 9 | loss: 0.1094259
	speed: 0.1120s/iter; left time: 37.7302s
Epoch: 9 cost time: 36.13257336616516
Epoch: 9, Steps: 318 | Train Loss: 0.1290455 Vali Loss: 0.1492289 Test Loss: 0.1095490
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1413516
	speed: 0.1182s/iter; left time: 25.8760s
	iters: 200, epoch: 10 | loss: 0.1206670
	speed: 0.1155s/iter; left time: 13.7423s
	iters: 300, epoch: 10 | loss: 0.1073207
	speed: 0.1261s/iter; left time: 2.3953s
Epoch: 10 cost time: 37.96234750747681
Epoch: 10, Steps: 318 | Train Loss: 0.1289509 Vali Loss: 0.1488365 Test Loss: 0.1095273
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.5597655177116394, mae:0.5796575546264648, rmse:0.748174786567688, mape:0.021450523287057877, mspe:0.0007678427500650287, rse:0.33277028799057007, r2_score:0.8771447705641693, acc:0.9785494767129421
corr: [41.20599  40.952187 41.071747 40.85388  41.24464  40.95856  41.105347
 40.761723 40.894257 40.813778 40.825268 40.868946 40.728947 41.135014
 41.059227 41.014366 41.26764  41.262203 41.317097 41.533524]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1695155
	speed: 0.1132s/iter; left time: 348.6608s
	iters: 200, epoch: 1 | loss: 0.1216033
	speed: 0.0876s/iter; left time: 261.1150s
	iters: 300, epoch: 1 | loss: 0.1719946
	speed: 0.0842s/iter; left time: 242.5288s
Epoch: 1 cost time: 30.03200054168701
Epoch: 1, Steps: 318 | Train Loss: 0.2567136 Vali Loss: 0.1621649 Test Loss: 0.1197779
Validation loss decreased (inf --> 0.162165).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1258840
	speed: 0.0915s/iter; left time: 252.7449s
	iters: 200, epoch: 2 | loss: 0.1478590
	speed: 0.0905s/iter; left time: 241.0998s
	iters: 300, epoch: 2 | loss: 0.1176768
	speed: 0.0959s/iter; left time: 245.8279s
Epoch: 2 cost time: 29.463926076889038
Epoch: 2, Steps: 318 | Train Loss: 0.1367000 Vali Loss: 0.1550358 Test Loss: 0.1151027
Validation loss decreased (0.162165 --> 0.155036).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1627837
	speed: 0.0958s/iter; left time: 234.3187s
	iters: 200, epoch: 3 | loss: 0.1084304
	speed: 0.0846s/iter; left time: 198.3411s
	iters: 300, epoch: 3 | loss: 0.1446899
	speed: 0.0860s/iter; left time: 193.0709s
Epoch: 3 cost time: 28.406119346618652
Epoch: 3, Steps: 318 | Train Loss: 0.1322054 Vali Loss: 0.1538364 Test Loss: 0.1140392
Validation loss decreased (0.155036 --> 0.153836).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1268633
	speed: 0.0923s/iter; left time: 196.2630s
	iters: 200, epoch: 4 | loss: 0.1048214
	speed: 0.0906s/iter; left time: 183.6640s
	iters: 300, epoch: 4 | loss: 0.1270890
	speed: 0.0919s/iter; left time: 177.1426s
Epoch: 4 cost time: 29.152812957763672
Epoch: 4, Steps: 318 | Train Loss: 0.1306432 Vali Loss: 0.1549265 Test Loss: 0.1132374
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1281111
	speed: 0.0855s/iter; left time: 154.5953s
	iters: 200, epoch: 5 | loss: 0.1486017
	speed: 0.0831s/iter; left time: 141.9397s
	iters: 300, epoch: 5 | loss: 0.1813287
	speed: 0.0842s/iter; left time: 135.5529s
Epoch: 5 cost time: 26.820472478866577
Epoch: 5, Steps: 318 | Train Loss: 0.1296919 Vali Loss: 0.1523203 Test Loss: 0.1123902
Validation loss decreased (0.153836 --> 0.152320).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1425365
	speed: 0.0903s/iter; left time: 134.5832s
	iters: 200, epoch: 6 | loss: 0.0942130
	speed: 0.0853s/iter; left time: 118.7196s
	iters: 300, epoch: 6 | loss: 0.1240577
	speed: 0.0872s/iter; left time: 112.5255s
Epoch: 6 cost time: 27.900456190109253
Epoch: 6, Steps: 318 | Train Loss: 0.1292734 Vali Loss: 0.1518286 Test Loss: 0.1123597
Validation loss decreased (0.152320 --> 0.151829).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1484357
	speed: 0.0867s/iter; left time: 101.7028s
	iters: 200, epoch: 7 | loss: 0.1825196
	speed: 0.0814s/iter; left time: 87.3282s
	iters: 300, epoch: 7 | loss: 0.1336709
	speed: 0.0816s/iter; left time: 79.3713s
Epoch: 7 cost time: 26.114723920822144
Epoch: 7, Steps: 318 | Train Loss: 0.1292497 Vali Loss: 0.1515938 Test Loss: 0.1122941
Validation loss decreased (0.151829 --> 0.151594).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1121647
	speed: 0.1162s/iter; left time: 99.3112s
	iters: 200, epoch: 8 | loss: 0.1029867
	speed: 0.1132s/iter; left time: 85.4287s
	iters: 300, epoch: 8 | loss: 0.1351058
	speed: 0.1195s/iter; left time: 78.2861s
Epoch: 8 cost time: 37.14730668067932
Epoch: 8, Steps: 318 | Train Loss: 0.1291682 Vali Loss: 0.1517905 Test Loss: 0.1122141
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1158851
	speed: 0.1254s/iter; left time: 67.3468s
	iters: 200, epoch: 9 | loss: 0.1521644
	speed: 0.1214s/iter; left time: 53.0590s
	iters: 300, epoch: 9 | loss: 0.0988181
	speed: 0.1165s/iter; left time: 39.2448s
Epoch: 9 cost time: 38.40452027320862
Epoch: 9, Steps: 318 | Train Loss: 0.1291402 Vali Loss: 0.1525184 Test Loss: 0.1121846
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1358730
	speed: 0.1259s/iter; left time: 27.5702s
	iters: 200, epoch: 10 | loss: 0.1279146
	speed: 0.1162s/iter; left time: 13.8309s
	iters: 300, epoch: 10 | loss: 0.1212297
	speed: 0.1161s/iter; left time: 2.2054s
Epoch: 10 cost time: 38.18200087547302
Epoch: 10, Steps: 318 | Train Loss: 0.1290581 Vali Loss: 0.1524587 Test Loss: 0.1121825
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.5735731720924377, mae:0.586702287197113, rmse:0.7573461532592773, mape:0.02171497792005539, mspe:0.0007866168161854148, rse:0.3366144299507141, r2_score:0.8755726150662713, acc:0.9782850220799446
corr: [40.830536 40.76163  41.095623 40.916653 40.90593  40.698387 40.90718
 40.900303 41.01371  41.036747 40.912647 41.01805  41.054955 41.086895
 40.999203 41.03348  41.113182 41.1565   41.163136 41.54852  41.179653
 41.283913 41.60729  41.29927  41.16013 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2488125
	speed: 0.1438s/iter; left time: 441.6837s
	iters: 200, epoch: 1 | loss: 0.1674804
	speed: 0.1211s/iter; left time: 359.8145s
	iters: 300, epoch: 1 | loss: 0.1540900
	speed: 0.1156s/iter; left time: 332.0269s
Epoch: 1 cost time: 39.902862787246704
Epoch: 1, Steps: 317 | Train Loss: 0.2638074 Vali Loss: 0.1674129 Test Loss: 0.1217555
Validation loss decreased (inf --> 0.167413).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1533813
	speed: 0.1338s/iter; left time: 368.4651s
	iters: 200, epoch: 2 | loss: 0.1524085
	speed: 0.1341s/iter; left time: 356.0091s
	iters: 300, epoch: 2 | loss: 0.1318511
	speed: 0.1404s/iter; left time: 358.5581s
Epoch: 2 cost time: 42.803783893585205
Epoch: 2, Steps: 317 | Train Loss: 0.1367654 Vali Loss: 0.1583108 Test Loss: 0.1147059
Validation loss decreased (0.167413 --> 0.158311).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1422729
	speed: 0.1485s/iter; left time: 361.8179s
	iters: 200, epoch: 3 | loss: 0.1213311
	speed: 0.1480s/iter; left time: 345.8443s
	iters: 300, epoch: 3 | loss: 0.1481697
	speed: 0.1379s/iter; left time: 308.4163s
Epoch: 3 cost time: 45.89658975601196
Epoch: 3, Steps: 317 | Train Loss: 0.1325442 Vali Loss: 0.1573719 Test Loss: 0.1126585
Validation loss decreased (0.158311 --> 0.157372).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0971268
	speed: 0.1488s/iter; left time: 315.5594s
	iters: 200, epoch: 4 | loss: 0.1419214
	speed: 0.1353s/iter; left time: 273.3074s
	iters: 300, epoch: 4 | loss: 0.1489955
	speed: 0.1357s/iter; left time: 260.5479s
Epoch: 4 cost time: 44.203675985336304
Epoch: 4, Steps: 317 | Train Loss: 0.1309188 Vali Loss: 0.1553696 Test Loss: 0.1116615
Validation loss decreased (0.157372 --> 0.155370).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1137375
	speed: 0.1462s/iter; left time: 263.6608s
	iters: 200, epoch: 5 | loss: 0.1051436
	speed: 0.1343s/iter; left time: 228.6431s
	iters: 300, epoch: 5 | loss: 0.1334585
	speed: 0.1330s/iter; left time: 213.2174s
Epoch: 5 cost time: 43.39474129676819
Epoch: 5, Steps: 317 | Train Loss: 0.1302530 Vali Loss: 0.1555163 Test Loss: 0.1110763
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1079209
	speed: 0.1333s/iter; left time: 198.1509s
	iters: 200, epoch: 6 | loss: 0.1707678
	speed: 0.1382s/iter; left time: 191.5584s
	iters: 300, epoch: 6 | loss: 0.1326519
	speed: 0.1207s/iter; left time: 155.2594s
Epoch: 6 cost time: 41.747798919677734
Epoch: 6, Steps: 317 | Train Loss: 0.1299041 Vali Loss: 0.1547264 Test Loss: 0.1110268
Validation loss decreased (0.155370 --> 0.154726).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1525763
	speed: 0.1358s/iter; left time: 158.7111s
	iters: 200, epoch: 7 | loss: 0.1131612
	speed: 0.1280s/iter; left time: 136.8298s
	iters: 300, epoch: 7 | loss: 0.1413590
	speed: 0.1381s/iter; left time: 133.7878s
Epoch: 7 cost time: 42.65729236602783
Epoch: 7, Steps: 317 | Train Loss: 0.1295972 Vali Loss: 0.1547534 Test Loss: 0.1110713
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1484470
	speed: 0.1377s/iter; left time: 117.3463s
	iters: 200, epoch: 8 | loss: 0.1099591
	speed: 0.1438s/iter; left time: 108.1321s
	iters: 300, epoch: 8 | loss: 0.1057086
	speed: 0.1463s/iter; left time: 95.3662s
Epoch: 8 cost time: 45.15400147438049
Epoch: 8, Steps: 317 | Train Loss: 0.1294746 Vali Loss: 0.1542739 Test Loss: 0.1109490
Validation loss decreased (0.154726 --> 0.154274).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1147497
	speed: 0.1482s/iter; left time: 79.2968s
	iters: 200, epoch: 9 | loss: 0.1591363
	speed: 0.1459s/iter; left time: 63.4807s
	iters: 300, epoch: 9 | loss: 0.1619447
	speed: 0.1267s/iter; left time: 42.4344s
Epoch: 9 cost time: 44.446189641952515
Epoch: 9, Steps: 317 | Train Loss: 0.1296385 Vali Loss: 0.1545435 Test Loss: 0.1109069
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1261237
	speed: 0.1438s/iter; left time: 31.3536s
	iters: 200, epoch: 10 | loss: 0.1359378
	speed: 0.1398s/iter; left time: 16.5006s
	iters: 300, epoch: 10 | loss: 0.1361138
	speed: 0.1449s/iter; left time: 2.6082s
Epoch: 10 cost time: 45.273433685302734
Epoch: 10, Steps: 317 | Train Loss: 0.1294136 Vali Loss: 0.1544029 Test Loss: 0.1108888
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.5667028427124023, mae:0.5833662748336792, rmse:0.7527967095375061, mape:0.021584834903478622, mspe:0.0007776884012855589, rse:0.3343440890312195, r2_score:0.8753665646729695, acc:0.9784151650965214
corr: [40.928528 40.816555 40.905037 40.92861  40.893677 41.102146 40.991
 40.91024  40.909077 41.03754  41.040657 40.89902  41.09513  41.281693
 41.12485  41.22121  41.122936 41.062263 41.09507  41.006416 41.146378
 41.32725  41.347507 41.198235 41.038338 41.09828  41.293823 41.14425
 41.17323  41.338867]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2030100
	speed: 0.1477s/iter; left time: 455.0691s
	iters: 200, epoch: 1 | loss: 0.1999961
	speed: 0.1052s/iter; left time: 313.6413s
	iters: 300, epoch: 1 | loss: 0.1702415
	speed: 0.1056s/iter; left time: 304.1601s
Epoch: 1 cost time: 38.07767868041992
Epoch: 1, Steps: 318 | Train Loss: 0.2957880 Vali Loss: 0.1670751 Test Loss: 0.1262198
Validation loss decreased (inf --> 0.167075).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1491015
	speed: 0.1163s/iter; left time: 321.2498s
	iters: 200, epoch: 2 | loss: 0.1242822
	speed: 0.1126s/iter; left time: 299.9384s
	iters: 300, epoch: 2 | loss: 0.1593142
	speed: 0.1012s/iter; left time: 259.4140s
Epoch: 2 cost time: 34.99320411682129
Epoch: 2, Steps: 318 | Train Loss: 0.1392730 Vali Loss: 0.1545690 Test Loss: 0.1128215
Validation loss decreased (0.167075 --> 0.154569).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1221984
	speed: 0.1167s/iter; left time: 285.2812s
	iters: 200, epoch: 3 | loss: 0.2057441
	speed: 0.1067s/iter; left time: 250.1879s
	iters: 300, epoch: 3 | loss: 0.1192598
	speed: 0.0995s/iter; left time: 223.4305s
Epoch: 3 cost time: 34.24224305152893
Epoch: 3, Steps: 318 | Train Loss: 0.1294204 Vali Loss: 0.1477961 Test Loss: 0.1098102
Validation loss decreased (0.154569 --> 0.147796).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1312346
	speed: 0.1194s/iter; left time: 254.0696s
	iters: 200, epoch: 4 | loss: 0.1665415
	speed: 0.1067s/iter; left time: 216.3475s
	iters: 300, epoch: 4 | loss: 0.1160846
	speed: 0.1144s/iter; left time: 220.4976s
Epoch: 4 cost time: 36.30510473251343
Epoch: 4, Steps: 318 | Train Loss: 0.1260559 Vali Loss: 0.1473393 Test Loss: 0.1073048
Validation loss decreased (0.147796 --> 0.147339).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1317892
	speed: 0.1191s/iter; left time: 215.4256s
	iters: 200, epoch: 5 | loss: 0.1300709
	speed: 0.1152s/iter; left time: 196.8179s
	iters: 300, epoch: 5 | loss: 0.1043107
	speed: 0.1216s/iter; left time: 195.7329s
Epoch: 5 cost time: 37.65980529785156
Epoch: 5, Steps: 318 | Train Loss: 0.1246947 Vali Loss: 0.1443419 Test Loss: 0.1064004
Validation loss decreased (0.147339 --> 0.144342).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1577030
	speed: 0.1187s/iter; left time: 176.9229s
	iters: 200, epoch: 6 | loss: 0.1001458
	speed: 0.1118s/iter; left time: 155.4925s
	iters: 300, epoch: 6 | loss: 0.1118611
	speed: 0.1192s/iter; left time: 153.8685s
Epoch: 6 cost time: 36.848334312438965
Epoch: 6, Steps: 318 | Train Loss: 0.1236964 Vali Loss: 0.1456990 Test Loss: 0.1058607
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0948113
	speed: 0.1256s/iter; left time: 147.3654s
	iters: 200, epoch: 7 | loss: 0.1112360
	speed: 0.1108s/iter; left time: 118.8998s
	iters: 300, epoch: 7 | loss: 0.1268267
	speed: 0.1152s/iter; left time: 112.1173s
Epoch: 7 cost time: 37.27671146392822
Epoch: 7, Steps: 318 | Train Loss: 0.1233160 Vali Loss: 0.1454175 Test Loss: 0.1055916
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1153019
	speed: 0.1207s/iter; left time: 103.1929s
	iters: 200, epoch: 8 | loss: 0.1156878
	speed: 0.1102s/iter; left time: 83.1909s
	iters: 300, epoch: 8 | loss: 0.1257523
	speed: 0.1141s/iter; left time: 74.7263s
Epoch: 8 cost time: 36.72652196884155
Epoch: 8, Steps: 318 | Train Loss: 0.1230800 Vali Loss: 0.1442764 Test Loss: 0.1054919
Validation loss decreased (0.144342 --> 0.144276).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0953939
	speed: 0.1235s/iter; left time: 66.3101s
	iters: 200, epoch: 9 | loss: 0.1210592
	speed: 0.1140s/iter; left time: 49.8222s
	iters: 300, epoch: 9 | loss: 0.1373601
	speed: 0.1148s/iter; left time: 38.7007s
Epoch: 9 cost time: 37.305615186691284
Epoch: 9, Steps: 318 | Train Loss: 0.1226866 Vali Loss: 0.1439168 Test Loss: 0.1054374
Validation loss decreased (0.144276 --> 0.143917).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1495828
	speed: 0.1253s/iter; left time: 27.4402s
	iters: 200, epoch: 10 | loss: 0.1152461
	speed: 0.1209s/iter; left time: 14.3907s
	iters: 300, epoch: 10 | loss: 0.1433655
	speed: 0.1152s/iter; left time: 2.1888s
Epoch: 10 cost time: 38.477067947387695
Epoch: 10, Steps: 318 | Train Loss: 0.1226296 Vali Loss: 0.1449972 Test Loss: 0.1054082
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.5385509729385376, mae:0.5683885812759399, rmse:0.7338603138923645, mape:0.021040605381131172, mspe:0.0007404238567687571, rse:0.3267781436443329, r2_score:0.8819816504640924, acc:0.9789593946188688
corr: [41.141766 41.235935 41.15274  41.251156 41.124424 41.244232 40.96298
 41.221687 41.298836 41.258823]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1593907
	speed: 0.1546s/iter; left time: 476.3150s
	iters: 200, epoch: 1 | loss: 0.1551470
	speed: 0.1571s/iter; left time: 468.4357s
	iters: 300, epoch: 1 | loss: 0.1236349
	speed: 0.1618s/iter; left time: 466.2557s
Epoch: 1 cost time: 50.01411461830139
Epoch: 1, Steps: 318 | Train Loss: 0.2772718 Vali Loss: 0.1599110 Test Loss: 0.1164130
Validation loss decreased (inf --> 0.159911).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1676897
	speed: 0.1070s/iter; left time: 295.5128s
	iters: 200, epoch: 2 | loss: 0.1393437
	speed: 0.0914s/iter; left time: 243.3159s
	iters: 300, epoch: 2 | loss: 0.1200263
	speed: 0.1092s/iter; left time: 279.9057s
Epoch: 2 cost time: 33.14284920692444
Epoch: 2, Steps: 318 | Train Loss: 0.1339280 Vali Loss: 0.1536145 Test Loss: 0.1100553
Validation loss decreased (0.159911 --> 0.153614).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1362899
	speed: 0.0838s/iter; left time: 204.8387s
	iters: 200, epoch: 3 | loss: 0.1320837
	speed: 0.0774s/iter; left time: 181.5273s
	iters: 300, epoch: 3 | loss: 0.1477523
	speed: 0.0985s/iter; left time: 221.1203s
Epoch: 3 cost time: 28.118731021881104
Epoch: 3, Steps: 318 | Train Loss: 0.1276304 Vali Loss: 0.1502998 Test Loss: 0.1049217
Validation loss decreased (0.153614 --> 0.150300).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1701879
	speed: 0.1146s/iter; left time: 243.7152s
	iters: 200, epoch: 4 | loss: 0.1066729
	speed: 0.1039s/iter; left time: 210.5459s
	iters: 300, epoch: 4 | loss: 0.1373869
	speed: 0.0987s/iter; left time: 190.1901s
Epoch: 4 cost time: 33.40653896331787
Epoch: 4, Steps: 318 | Train Loss: 0.1251217 Vali Loss: 0.1505762 Test Loss: 0.1047156
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1221110
	speed: 0.0983s/iter; left time: 177.8109s
	iters: 200, epoch: 5 | loss: 0.1117404
	speed: 0.1000s/iter; left time: 170.8739s
	iters: 300, epoch: 5 | loss: 0.1370754
	speed: 0.0898s/iter; left time: 144.4610s
Epoch: 5 cost time: 30.556657314300537
Epoch: 5, Steps: 318 | Train Loss: 0.1234992 Vali Loss: 0.1485606 Test Loss: 0.1040077
Validation loss decreased (0.150300 --> 0.148561).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1647899
	speed: 0.0972s/iter; left time: 144.9536s
	iters: 200, epoch: 6 | loss: 0.0891166
	speed: 0.1028s/iter; left time: 142.9362s
	iters: 300, epoch: 6 | loss: 0.1314593
	speed: 0.0985s/iter; left time: 127.2172s
Epoch: 6 cost time: 31.796005725860596
Epoch: 6, Steps: 318 | Train Loss: 0.1229702 Vali Loss: 0.1499344 Test Loss: 0.1041613
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1068010
	speed: 0.0975s/iter; left time: 114.3334s
	iters: 200, epoch: 7 | loss: 0.1543708
	speed: 0.0913s/iter; left time: 97.9119s
	iters: 300, epoch: 7 | loss: 0.1305866
	speed: 0.0939s/iter; left time: 91.4014s
Epoch: 7 cost time: 29.950458526611328
Epoch: 7, Steps: 318 | Train Loss: 0.1225236 Vali Loss: 0.1480918 Test Loss: 0.1037038
Validation loss decreased (0.148561 --> 0.148092).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0944180
	speed: 0.1063s/iter; left time: 90.9018s
	iters: 200, epoch: 8 | loss: 0.1308924
	speed: 0.1006s/iter; left time: 75.9887s
	iters: 300, epoch: 8 | loss: 0.0934751
	speed: 0.0977s/iter; left time: 63.9839s
Epoch: 8 cost time: 32.12910866737366
Epoch: 8, Steps: 318 | Train Loss: 0.1223888 Vali Loss: 0.1477096 Test Loss: 0.1035173
Validation loss decreased (0.148092 --> 0.147710).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1009531
	speed: 0.1006s/iter; left time: 53.9972s
	iters: 200, epoch: 9 | loss: 0.1532976
	speed: 0.0985s/iter; left time: 43.0339s
	iters: 300, epoch: 9 | loss: 0.1102681
	speed: 0.0974s/iter; left time: 32.8116s
Epoch: 9 cost time: 31.44002056121826
Epoch: 9, Steps: 318 | Train Loss: 0.1223624 Vali Loss: 0.1486963 Test Loss: 0.1034041
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1270240
	speed: 0.1050s/iter; left time: 23.0021s
	iters: 200, epoch: 10 | loss: 0.1308155
	speed: 0.0990s/iter; left time: 11.7818s
	iters: 300, epoch: 10 | loss: 0.1211777
	speed: 0.1025s/iter; left time: 1.9468s
Epoch: 10 cost time: 32.55618166923523
Epoch: 10, Steps: 318 | Train Loss: 0.1221293 Vali Loss: 0.1478429 Test Loss: 0.1034015
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.5287433862686157, mae:0.5631767511367798, rmse:0.7271474599838257, mape:0.020858576521277428, mspe:0.0007289802306331694, rse:0.32361990213394165, r2_score:0.8843792328063864, acc:0.9791414234787226
corr: [40.972347 41.038586 41.248596 41.157158 41.20534  41.16622  41.427425
 41.19024  41.139843 41.210323 41.115658 41.415295 41.391617 41.217735
 41.32504 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2344265
	speed: 0.1355s/iter; left time: 417.5482s
	iters: 200, epoch: 1 | loss: 0.1982730
	speed: 0.1253s/iter; left time: 373.4240s
	iters: 300, epoch: 1 | loss: 0.1496871
	speed: 0.1283s/iter; left time: 369.7088s
Epoch: 1 cost time: 41.164833068847656
Epoch: 1, Steps: 318 | Train Loss: 0.3081941 Vali Loss: 0.1639944 Test Loss: 0.1262397
Validation loss decreased (inf --> 0.163994).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1439513
	speed: 0.1238s/iter; left time: 342.1116s
	iters: 200, epoch: 2 | loss: 0.1685107
	speed: 0.1265s/iter; left time: 336.8562s
	iters: 300, epoch: 2 | loss: 0.1109959
	speed: 0.1303s/iter; left time: 333.8808s
Epoch: 2 cost time: 40.47998023033142
Epoch: 2, Steps: 318 | Train Loss: 0.1392426 Vali Loss: 0.1551431 Test Loss: 0.1161897
Validation loss decreased (0.163994 --> 0.155143).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1264239
	speed: 0.1339s/iter; left time: 327.4465s
	iters: 200, epoch: 3 | loss: 0.1271896
	speed: 0.1291s/iter; left time: 302.6372s
	iters: 300, epoch: 3 | loss: 0.1544991
	speed: 0.1258s/iter; left time: 282.4205s
Epoch: 3 cost time: 41.08071780204773
Epoch: 3, Steps: 318 | Train Loss: 0.1320887 Vali Loss: 0.1534176 Test Loss: 0.1130508
Validation loss decreased (0.155143 --> 0.153418).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1493089
	speed: 0.1401s/iter; left time: 297.9577s
	iters: 200, epoch: 4 | loss: 0.1274023
	speed: 0.1273s/iter; left time: 257.9441s
	iters: 300, epoch: 4 | loss: 0.1377705
	speed: 0.1275s/iter; left time: 245.6941s
Epoch: 4 cost time: 41.93873429298401
Epoch: 4, Steps: 318 | Train Loss: 0.1293660 Vali Loss: 0.1521641 Test Loss: 0.1136734
Validation loss decreased (0.153418 --> 0.152164).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1247053
	speed: 0.1353s/iter; left time: 244.7974s
	iters: 200, epoch: 5 | loss: 0.0944355
	speed: 0.1303s/iter; left time: 222.6217s
	iters: 300, epoch: 5 | loss: 0.1273593
	speed: 0.1425s/iter; left time: 229.3001s
Epoch: 5 cost time: 43.156452894210815
Epoch: 5, Steps: 318 | Train Loss: 0.1280901 Vali Loss: 0.1518714 Test Loss: 0.1121698
Validation loss decreased (0.152164 --> 0.151871).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1164619
	speed: 0.1525s/iter; left time: 227.3323s
	iters: 200, epoch: 6 | loss: 0.0982243
	speed: 0.1417s/iter; left time: 197.1173s
	iters: 300, epoch: 6 | loss: 0.1280088
	speed: 0.1336s/iter; left time: 172.4216s
Epoch: 6 cost time: 45.39664173126221
Epoch: 6, Steps: 318 | Train Loss: 0.1275138 Vali Loss: 0.1510724 Test Loss: 0.1123248
Validation loss decreased (0.151871 --> 0.151072).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1232800
	speed: 0.1496s/iter; left time: 175.5014s
	iters: 200, epoch: 7 | loss: 0.1223469
	speed: 0.1452s/iter; left time: 155.8333s
	iters: 300, epoch: 7 | loss: 0.1311490
	speed: 0.1511s/iter; left time: 147.0683s
Epoch: 7 cost time: 47.247000217437744
Epoch: 7, Steps: 318 | Train Loss: 0.1272576 Vali Loss: 0.1509536 Test Loss: 0.1121521
Validation loss decreased (0.151072 --> 0.150954).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0929424
	speed: 0.1528s/iter; left time: 130.6023s
	iters: 200, epoch: 8 | loss: 0.0982081
	speed: 0.1410s/iter; left time: 106.4245s
	iters: 300, epoch: 8 | loss: 0.1489548
	speed: 0.1438s/iter; left time: 94.1803s
Epoch: 8 cost time: 46.384992361068726
Epoch: 8, Steps: 318 | Train Loss: 0.1268194 Vali Loss: 0.1504252 Test Loss: 0.1121142
Validation loss decreased (0.150954 --> 0.150425).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1356689
	speed: 0.1456s/iter; left time: 78.1605s
	iters: 200, epoch: 9 | loss: 0.1410246
	speed: 0.1452s/iter; left time: 63.4344s
	iters: 300, epoch: 9 | loss: 0.1076167
	speed: 0.1357s/iter; left time: 45.7271s
Epoch: 9 cost time: 45.49642777442932
Epoch: 9, Steps: 318 | Train Loss: 0.1269058 Vali Loss: 0.1517206 Test Loss: 0.1120529
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1377507
	speed: 0.1473s/iter; left time: 32.2622s
	iters: 200, epoch: 10 | loss: 0.1257190
	speed: 0.1379s/iter; left time: 16.4123s
	iters: 300, epoch: 10 | loss: 0.1044061
	speed: 0.1377s/iter; left time: 2.6156s
Epoch: 10 cost time: 44.924832344055176
Epoch: 10, Steps: 318 | Train Loss: 0.1269016 Vali Loss: 0.1513995 Test Loss: 0.1120494
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.5726542472839355, mae:0.5910249352455139, rmse:0.7567391991615295, mape:0.02185678668320179, mspe:0.0007851116824895144, rse:0.3365795314311981, r2_score:0.8766139737635108, acc:0.9781432133167982
corr: [41.11724  40.911522 41.153446 41.0235   41.258934 40.922764 40.985126
 40.650433 40.900185 40.863964 40.851032 41.030132 41.038116 41.09764
 41.10886  41.08865  41.317207 41.357533 41.22419  41.30644 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1701522
	speed: 0.1160s/iter; left time: 357.3160s
	iters: 200, epoch: 1 | loss: 0.1234945
	speed: 0.0938s/iter; left time: 279.5391s
	iters: 300, epoch: 1 | loss: 0.1737598
	speed: 0.0869s/iter; left time: 250.2342s
Epoch: 1 cost time: 31.17320418357849
Epoch: 1, Steps: 318 | Train Loss: 0.2584491 Vali Loss: 0.1623674 Test Loss: 0.1207570
Validation loss decreased (inf --> 0.162367).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1274204
	speed: 0.1082s/iter; left time: 299.0435s
	iters: 200, epoch: 2 | loss: 0.1488246
	speed: 0.1071s/iter; left time: 285.2812s
	iters: 300, epoch: 2 | loss: 0.1183427
	speed: 0.1218s/iter; left time: 312.1011s
Epoch: 2 cost time: 35.96670365333557
Epoch: 2, Steps: 318 | Train Loss: 0.1372230 Vali Loss: 0.1550606 Test Loss: 0.1157614
Validation loss decreased (0.162367 --> 0.155061).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1630329
	speed: 0.1244s/iter; left time: 304.2460s
	iters: 200, epoch: 3 | loss: 0.1088997
	speed: 0.1143s/iter; left time: 267.9843s
	iters: 300, epoch: 3 | loss: 0.1444406
	speed: 0.1104s/iter; left time: 247.8663s
Epoch: 3 cost time: 36.83204936981201
Epoch: 3, Steps: 318 | Train Loss: 0.1325418 Vali Loss: 0.1539551 Test Loss: 0.1146018
Validation loss decreased (0.155061 --> 0.153955).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1273419
	speed: 0.1226s/iter; left time: 260.8669s
	iters: 200, epoch: 4 | loss: 0.1045564
	speed: 0.1212s/iter; left time: 245.6473s
	iters: 300, epoch: 4 | loss: 0.1265449
	speed: 0.1031s/iter; left time: 198.6707s
Epoch: 4 cost time: 36.32704472541809
Epoch: 4, Steps: 318 | Train Loss: 0.1309662 Vali Loss: 0.1550566 Test Loss: 0.1137992
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1292083
	speed: 0.1065s/iter; left time: 192.7269s
	iters: 200, epoch: 5 | loss: 0.1490193
	speed: 0.0935s/iter; left time: 159.8135s
	iters: 300, epoch: 5 | loss: 0.1810596
	speed: 0.0931s/iter; left time: 149.8183s
Epoch: 5 cost time: 30.98340678215027
Epoch: 5, Steps: 318 | Train Loss: 0.1300353 Vali Loss: 0.1525251 Test Loss: 0.1129842
Validation loss decreased (0.153955 --> 0.152525).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1431878
	speed: 0.0936s/iter; left time: 139.5285s
	iters: 200, epoch: 6 | loss: 0.0944637
	speed: 0.0926s/iter; left time: 128.7435s
	iters: 300, epoch: 6 | loss: 0.1243189
	speed: 0.0916s/iter; left time: 118.2670s
Epoch: 6 cost time: 29.512322187423706
Epoch: 6, Steps: 318 | Train Loss: 0.1296440 Vali Loss: 0.1520285 Test Loss: 0.1129181
Validation loss decreased (0.152525 --> 0.152028).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1487720
	speed: 0.0899s/iter; left time: 105.4264s
	iters: 200, epoch: 7 | loss: 0.1819015
	speed: 0.0876s/iter; left time: 93.9414s
	iters: 300, epoch: 7 | loss: 0.1337862
	speed: 0.0908s/iter; left time: 88.3224s
Epoch: 7 cost time: 28.456790447235107
Epoch: 7, Steps: 318 | Train Loss: 0.1296077 Vali Loss: 0.1518050 Test Loss: 0.1128786
Validation loss decreased (0.152028 --> 0.151805).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1131199
	speed: 0.0911s/iter; left time: 77.8614s
	iters: 200, epoch: 8 | loss: 0.1028726
	speed: 0.0846s/iter; left time: 63.8388s
	iters: 300, epoch: 8 | loss: 0.1356638
	speed: 0.0866s/iter; left time: 56.7255s
Epoch: 8 cost time: 27.406620740890503
Epoch: 8, Steps: 318 | Train Loss: 0.1295293 Vali Loss: 0.1520062 Test Loss: 0.1127973
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1163738
	speed: 0.0930s/iter; left time: 49.9414s
	iters: 200, epoch: 9 | loss: 0.1532914
	speed: 0.0824s/iter; left time: 36.0252s
	iters: 300, epoch: 9 | loss: 0.0991626
	speed: 0.0860s/iter; left time: 28.9805s
Epoch: 9 cost time: 27.963857173919678
Epoch: 9, Steps: 318 | Train Loss: 0.1294939 Vali Loss: 0.1527420 Test Loss: 0.1127752
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1360909
	speed: 0.0931s/iter; left time: 20.3981s
	iters: 200, epoch: 10 | loss: 0.1288224
	speed: 0.0810s/iter; left time: 9.6390s
	iters: 300, epoch: 10 | loss: 0.1209894
	speed: 0.0924s/iter; left time: 1.7556s
Epoch: 10 cost time: 28.646178483963013
Epoch: 10, Steps: 318 | Train Loss: 0.1293969 Vali Loss: 0.1526662 Test Loss: 0.1127728
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.5765588879585266, mae:0.5881717205047607, rmse:0.759314775466919, mape:0.02177288383245468, mspe:0.0007910400745458901, rse:0.33748939633369446, r2_score:0.8748883691884148, acc:0.9782271161675453
corr: [40.856956 40.76314  41.110138 40.915806 40.929554 40.711567 40.919895
 40.918983 41.022266 41.03974  40.905132 41.01546  41.053474 41.10402
 41.01297  41.02495  41.119064 41.159584 41.159344 41.55499  41.170452
 41.28868  41.628525 41.299675 41.160213]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2378711
	speed: 0.1387s/iter; left time: 426.0816s
	iters: 200, epoch: 1 | loss: 0.1681027
	speed: 0.1297s/iter; left time: 385.4521s
	iters: 300, epoch: 1 | loss: 0.1604895
	speed: 0.1162s/iter; left time: 333.4796s
Epoch: 1 cost time: 40.81713318824768
Epoch: 1, Steps: 317 | Train Loss: 0.2611594 Vali Loss: 0.1669670 Test Loss: 0.1234173
Validation loss decreased (inf --> 0.166967).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1520322
	speed: 0.1436s/iter; left time: 395.6031s
	iters: 200, epoch: 2 | loss: 0.1545992
	speed: 0.1311s/iter; left time: 348.0107s
	iters: 300, epoch: 2 | loss: 0.1313855
	speed: 0.1338s/iter; left time: 341.8275s
Epoch: 2 cost time: 43.544400691986084
Epoch: 2, Steps: 317 | Train Loss: 0.1383022 Vali Loss: 0.1613639 Test Loss: 0.1180951
Validation loss decreased (0.166967 --> 0.161364).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1421989
	speed: 0.1560s/iter; left time: 380.2239s
	iters: 200, epoch: 3 | loss: 0.1239751
	speed: 0.1512s/iter; left time: 353.2935s
	iters: 300, epoch: 3 | loss: 0.1427144
	speed: 0.1482s/iter; left time: 331.6236s
Epoch: 3 cost time: 47.96384382247925
Epoch: 3, Steps: 317 | Train Loss: 0.1331390 Vali Loss: 0.1604373 Test Loss: 0.1151397
Validation loss decreased (0.161364 --> 0.160437).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0982291
	speed: 0.1480s/iter; left time: 313.8597s
	iters: 200, epoch: 4 | loss: 0.1421335
	speed: 0.1457s/iter; left time: 294.3112s
	iters: 300, epoch: 4 | loss: 0.1497092
	speed: 0.1417s/iter; left time: 272.1111s
Epoch: 4 cost time: 46.02079701423645
Epoch: 4, Steps: 317 | Train Loss: 0.1312063 Vali Loss: 0.1579002 Test Loss: 0.1138818
Validation loss decreased (0.160437 --> 0.157900).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1157880
	speed: 0.1530s/iter; left time: 275.8936s
	iters: 200, epoch: 5 | loss: 0.1059552
	speed: 0.1410s/iter; left time: 240.1253s
	iters: 300, epoch: 5 | loss: 0.1323349
	speed: 0.1280s/iter; left time: 205.1395s
Epoch: 5 cost time: 45.09765076637268
Epoch: 5, Steps: 317 | Train Loss: 0.1303679 Vali Loss: 0.1584584 Test Loss: 0.1134684
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1090284
	speed: 0.1479s/iter; left time: 219.8429s
	iters: 200, epoch: 6 | loss: 0.1722709
	speed: 0.1469s/iter; left time: 203.6303s
	iters: 300, epoch: 6 | loss: 0.1302694
	speed: 0.1550s/iter; left time: 199.3397s
Epoch: 6 cost time: 47.988564252853394
Epoch: 6, Steps: 317 | Train Loss: 0.1298991 Vali Loss: 0.1579299 Test Loss: 0.1136210
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1500892
	speed: 0.1484s/iter; left time: 173.4352s
	iters: 200, epoch: 7 | loss: 0.1126794
	speed: 0.1485s/iter; left time: 158.7297s
	iters: 300, epoch: 7 | loss: 0.1417037
	speed: 0.1522s/iter; left time: 147.4402s
Epoch: 7 cost time: 47.61349821090698
Epoch: 7, Steps: 317 | Train Loss: 0.1295589 Vali Loss: 0.1578461 Test Loss: 0.1137403
Validation loss decreased (0.157900 --> 0.157846).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1506631
	speed: 0.1528s/iter; left time: 130.1551s
	iters: 200, epoch: 8 | loss: 0.1114836
	speed: 0.1528s/iter; left time: 114.8943s
	iters: 300, epoch: 8 | loss: 0.1060279
	speed: 0.1441s/iter; left time: 93.9463s
Epoch: 8 cost time: 47.52246570587158
Epoch: 8, Steps: 317 | Train Loss: 0.1294598 Vali Loss: 0.1572057 Test Loss: 0.1137351
Validation loss decreased (0.157846 --> 0.157206).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1156059
	speed: 0.1545s/iter; left time: 82.6448s
	iters: 200, epoch: 9 | loss: 0.1572393
	speed: 0.1412s/iter; left time: 61.4166s
	iters: 300, epoch: 9 | loss: 0.1621519
	speed: 0.1448s/iter; left time: 48.4975s
Epoch: 9 cost time: 46.573333978652954
Epoch: 9, Steps: 317 | Train Loss: 0.1296543 Vali Loss: 0.1575276 Test Loss: 0.1137080
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1223168
	speed: 0.1510s/iter; left time: 32.9147s
	iters: 200, epoch: 10 | loss: 0.1287220
	speed: 0.1518s/iter; left time: 17.9135s
	iters: 300, epoch: 10 | loss: 0.1361151
	speed: 0.1457s/iter; left time: 2.6221s
Epoch: 10 cost time: 47.48724627494812
Epoch: 10, Steps: 317 | Train Loss: 0.1294101 Vali Loss: 0.1574408 Test Loss: 0.1137173
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.580933690071106, mae:0.5899785757064819, rmse:0.762190043926239, mape:0.02183648943901062, mspe:0.0007977757486514747, rse:0.3385160267353058, r2_score:0.8737453385992731, acc:0.9781635105609894
corr: [41.145226 40.98168  40.94654  40.97646  40.932495 41.219013 40.98055
 40.9805   40.959328 41.035282 41.05188  40.827736 40.950077 41.166756
 41.129948 41.11393  40.948963 41.057766 41.03852  40.96545  41.213375
 41.348167 41.190926 41.113503 41.05302  41.15239  41.328598 41.204453
 41.268463 41.380737]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1823213
	speed: 0.1505s/iter; left time: 463.6262s
	iters: 200, epoch: 1 | loss: 0.1949497
	speed: 0.1203s/iter; left time: 358.6810s
	iters: 300, epoch: 1 | loss: 0.1683283
	speed: 0.1162s/iter; left time: 334.7430s
Epoch: 1 cost time: 40.672974586486816
Epoch: 1, Steps: 318 | Train Loss: 0.2872725 Vali Loss: 0.1645785 Test Loss: 0.1206245
Validation loss decreased (inf --> 0.164578).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1446054
	speed: 0.1152s/iter; left time: 318.2045s
	iters: 200, epoch: 2 | loss: 0.1138918
	speed: 0.1086s/iter; left time: 289.1995s
	iters: 300, epoch: 2 | loss: 0.1499074
	speed: 0.1108s/iter; left time: 284.1002s
Epoch: 2 cost time: 35.398398876190186
Epoch: 2, Steps: 318 | Train Loss: 0.1336857 Vali Loss: 0.1513026 Test Loss: 0.1079569
Validation loss decreased (0.164578 --> 0.151303).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1149552
	speed: 0.1169s/iter; left time: 285.7630s
	iters: 200, epoch: 3 | loss: 0.2035411
	speed: 0.1052s/iter; left time: 246.6707s
	iters: 300, epoch: 3 | loss: 0.1124731
	speed: 0.1079s/iter; left time: 242.2943s
Epoch: 3 cost time: 35.0272376537323
Epoch: 3, Steps: 318 | Train Loss: 0.1230697 Vali Loss: 0.1452298 Test Loss: 0.1037651
Validation loss decreased (0.151303 --> 0.145230).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1277147
	speed: 0.1461s/iter; left time: 310.6886s
	iters: 200, epoch: 4 | loss: 0.1577305
	speed: 0.1361s/iter; left time: 275.7746s
	iters: 300, epoch: 4 | loss: 0.1087314
	speed: 0.1289s/iter; left time: 248.4294s
Epoch: 4 cost time: 43.49254059791565
Epoch: 4, Steps: 318 | Train Loss: 0.1181980 Vali Loss: 0.1444557 Test Loss: 0.1017442
Validation loss decreased (0.145230 --> 0.144456).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1206768
	speed: 0.1415s/iter; left time: 255.9046s
	iters: 200, epoch: 5 | loss: 0.1048839
	speed: 0.1443s/iter; left time: 246.5371s
	iters: 300, epoch: 5 | loss: 0.0916831
	speed: 0.1426s/iter; left time: 229.4754s
Epoch: 5 cost time: 45.331029176712036
Epoch: 5, Steps: 318 | Train Loss: 0.1159860 Vali Loss: 0.1419983 Test Loss: 0.1008833
Validation loss decreased (0.144456 --> 0.141998).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1545624
	speed: 0.1147s/iter; left time: 171.0519s
	iters: 200, epoch: 6 | loss: 0.0959988
	speed: 0.0987s/iter; left time: 137.3431s
	iters: 300, epoch: 6 | loss: 0.1103425
	speed: 0.1010s/iter; left time: 130.3439s
Epoch: 6 cost time: 33.688788175582886
Epoch: 6, Steps: 318 | Train Loss: 0.1146761 Vali Loss: 0.1423073 Test Loss: 0.1002257
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0886545
	speed: 0.1024s/iter; left time: 120.0756s
	iters: 200, epoch: 7 | loss: 0.0869467
	speed: 0.1051s/iter; left time: 112.7416s
	iters: 300, epoch: 7 | loss: 0.1198658
	speed: 0.1205s/iter; left time: 117.1990s
Epoch: 7 cost time: 34.99863314628601
Epoch: 7, Steps: 318 | Train Loss: 0.1141966 Vali Loss: 0.1416137 Test Loss: 0.1000064
Validation loss decreased (0.141998 --> 0.141614).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1067209
	speed: 0.1203s/iter; left time: 102.8975s
	iters: 200, epoch: 8 | loss: 0.1086802
	speed: 0.1221s/iter; left time: 92.2098s
	iters: 300, epoch: 8 | loss: 0.1160102
	speed: 0.1178s/iter; left time: 77.1699s
Epoch: 8 cost time: 38.07182765007019
Epoch: 8, Steps: 318 | Train Loss: 0.1137335 Vali Loss: 0.1402463 Test Loss: 0.0999036
Validation loss decreased (0.141614 --> 0.140246).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0820148
	speed: 0.1140s/iter; left time: 61.2009s
	iters: 200, epoch: 9 | loss: 0.1122790
	speed: 0.1057s/iter; left time: 46.1760s
	iters: 300, epoch: 9 | loss: 0.1185925
	speed: 0.1063s/iter; left time: 35.8240s
Epoch: 9 cost time: 34.67944884300232
Epoch: 9, Steps: 318 | Train Loss: 0.1134317 Vali Loss: 0.1404403 Test Loss: 0.0998787
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1498231
	speed: 0.1146s/iter; left time: 25.0963s
	iters: 200, epoch: 10 | loss: 0.1159415
	speed: 0.1082s/iter; left time: 12.8751s
	iters: 300, epoch: 10 | loss: 0.1362256
	speed: 0.1020s/iter; left time: 1.9386s
Epoch: 10 cost time: 34.37181115150452
Epoch: 10, Steps: 318 | Train Loss: 0.1132752 Vali Loss: 0.1414972 Test Loss: 0.0998761
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.510285496711731, mae:0.5562677383422852, rmse:0.7143427133560181, mape:0.020615536719560623, mspe:0.0007051326683722436, rse:0.31808722019195557, r2_score:0.8907865760808781, acc:0.9793844632804394
corr: [40.98546  41.01185  40.904144 41.066563 40.85837  41.130684 40.932552
 41.22002  41.21303  41.190395]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1602058
	speed: 0.1164s/iter; left time: 358.6667s
	iters: 200, epoch: 1 | loss: 0.1545073
	speed: 0.1014s/iter; left time: 302.1350s
	iters: 300, epoch: 1 | loss: 0.1243449
	speed: 0.0955s/iter; left time: 275.0176s
Epoch: 1 cost time: 32.95115542411804
Epoch: 1, Steps: 318 | Train Loss: 0.2752638 Vali Loss: 0.1583791 Test Loss: 0.1184000
Validation loss decreased (inf --> 0.158379).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1716090
	speed: 0.0946s/iter; left time: 261.3989s
	iters: 200, epoch: 2 | loss: 0.1384388
	speed: 0.1040s/iter; left time: 277.0840s
	iters: 300, epoch: 2 | loss: 0.1223450
	speed: 0.0964s/iter; left time: 247.1866s
Epoch: 2 cost time: 31.373752117156982
Epoch: 2, Steps: 318 | Train Loss: 0.1344868 Vali Loss: 0.1492837 Test Loss: 0.1099359
Validation loss decreased (0.158379 --> 0.149284).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1362571
	speed: 0.1066s/iter; left time: 260.7154s
	iters: 200, epoch: 3 | loss: 0.1307057
	speed: 0.1035s/iter; left time: 242.7933s
	iters: 300, epoch: 3 | loss: 0.1549303
	speed: 0.1015s/iter; left time: 227.9698s
Epoch: 3 cost time: 32.80039072036743
Epoch: 3, Steps: 318 | Train Loss: 0.1283283 Vali Loss: 0.1463422 Test Loss: 0.1061332
Validation loss decreased (0.149284 --> 0.146342).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1692857
	speed: 0.1096s/iter; left time: 233.1549s
	iters: 200, epoch: 4 | loss: 0.1054380
	speed: 0.1068s/iter; left time: 216.4372s
	iters: 300, epoch: 4 | loss: 0.1448576
	speed: 0.1038s/iter; left time: 199.9351s
Epoch: 4 cost time: 33.753026723861694
Epoch: 4, Steps: 318 | Train Loss: 0.1259654 Vali Loss: 0.1459553 Test Loss: 0.1047836
Validation loss decreased (0.146342 --> 0.145955).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1220479
	speed: 0.1100s/iter; left time: 198.9976s
	iters: 200, epoch: 5 | loss: 0.1064268
	speed: 0.1026s/iter; left time: 175.2781s
	iters: 300, epoch: 5 | loss: 0.1340467
	speed: 0.0985s/iter; left time: 158.4481s
Epoch: 5 cost time: 32.92247033119202
Epoch: 5, Steps: 318 | Train Loss: 0.1245324 Vali Loss: 0.1442083 Test Loss: 0.1041955
Validation loss decreased (0.145955 --> 0.144208).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1721241
	speed: 0.1010s/iter; left time: 150.5897s
	iters: 200, epoch: 6 | loss: 0.0913872
	speed: 0.0992s/iter; left time: 138.0380s
	iters: 300, epoch: 6 | loss: 0.1295484
	speed: 0.1009s/iter; left time: 130.2898s
Epoch: 6 cost time: 31.98983359336853
Epoch: 6, Steps: 318 | Train Loss: 0.1240160 Vali Loss: 0.1450884 Test Loss: 0.1038121
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1073584
	speed: 0.0988s/iter; left time: 115.8488s
	iters: 200, epoch: 7 | loss: 0.1528088
	speed: 0.1005s/iter; left time: 107.8328s
	iters: 300, epoch: 7 | loss: 0.1312311
	speed: 0.1078s/iter; left time: 104.8535s
Epoch: 7 cost time: 32.76814150810242
Epoch: 7, Steps: 318 | Train Loss: 0.1235503 Vali Loss: 0.1432872 Test Loss: 0.1036600
Validation loss decreased (0.144208 --> 0.143287).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0946431
	speed: 0.1023s/iter; left time: 87.4878s
	iters: 200, epoch: 8 | loss: 0.1452794
	speed: 0.1006s/iter; left time: 75.9552s
	iters: 300, epoch: 8 | loss: 0.0915229
	speed: 0.1037s/iter; left time: 67.9375s
Epoch: 8 cost time: 32.60972237586975
Epoch: 8, Steps: 318 | Train Loss: 0.1234841 Vali Loss: 0.1430713 Test Loss: 0.1035779
Validation loss decreased (0.143287 --> 0.143071).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1048165
	speed: 0.1086s/iter; left time: 58.3009s
	iters: 200, epoch: 9 | loss: 0.1544701
	speed: 0.1007s/iter; left time: 44.0037s
	iters: 300, epoch: 9 | loss: 0.1137282
	speed: 0.0987s/iter; left time: 33.2502s
Epoch: 9 cost time: 32.726561069488525
Epoch: 9, Steps: 318 | Train Loss: 0.1235116 Vali Loss: 0.1438989 Test Loss: 0.1035253
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1271155
	speed: 0.1075s/iter; left time: 23.5328s
	iters: 200, epoch: 10 | loss: 0.1346624
	speed: 0.0958s/iter; left time: 11.4044s
	iters: 300, epoch: 10 | loss: 0.1184002
	speed: 0.0961s/iter; left time: 1.8263s
Epoch: 10 cost time: 31.578635215759277
Epoch: 10, Steps: 318 | Train Loss: 0.1232520 Vali Loss: 0.1431428 Test Loss: 0.1034992
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.5290530323982239, mae:0.5658785104751587, rmse:0.7273603081703186, mape:0.02094119042158127, mspe:0.0007276195683516562, rse:0.32371464371681213, r2_score:0.8842770979951042, acc:0.9790588095784187
corr: [40.95935  41.06791  41.204247 40.945194 41.14142  41.073353 41.25432
 41.03202  41.103455 41.01812  41.065968 41.660305 41.36929  41.08305
 41.270454]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2412476
	speed: 0.1556s/iter; left time: 479.3602s
	iters: 200, epoch: 1 | loss: 0.2009209
	speed: 0.1471s/iter; left time: 438.6063s
	iters: 300, epoch: 1 | loss: 0.1533526
	speed: 0.1510s/iter; left time: 434.8911s
Epoch: 1 cost time: 48.264485120773315
Epoch: 1, Steps: 318 | Train Loss: 0.3120363 Vali Loss: 0.1684456 Test Loss: 0.1295763
Validation loss decreased (inf --> 0.168446).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1446307
	speed: 0.1595s/iter; left time: 440.6421s
	iters: 200, epoch: 2 | loss: 0.1699871
	speed: 0.1520s/iter; left time: 404.8974s
	iters: 300, epoch: 2 | loss: 0.1116562
	speed: 0.1501s/iter; left time: 384.6615s
Epoch: 2 cost time: 48.93726468086243
Epoch: 2, Steps: 318 | Train Loss: 0.1410977 Vali Loss: 0.1581354 Test Loss: 0.1169325
Validation loss decreased (0.168446 --> 0.158135).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1237616
	speed: 0.1609s/iter; left time: 393.3155s
	iters: 200, epoch: 3 | loss: 0.1269559
	speed: 0.1480s/iter; left time: 347.1737s
	iters: 300, epoch: 3 | loss: 0.1557081
	speed: 0.1532s/iter; left time: 343.8751s
Epoch: 3 cost time: 49.031232833862305
Epoch: 3, Steps: 318 | Train Loss: 0.1334530 Vali Loss: 0.1552389 Test Loss: 0.1135088
Validation loss decreased (0.158135 --> 0.155239).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1469282
	speed: 0.1505s/iter; left time: 320.1726s
	iters: 200, epoch: 4 | loss: 0.1299351
	speed: 0.2035s/iter; left time: 412.5104s
	iters: 300, epoch: 4 | loss: 0.1378726
	speed: 0.2072s/iter; left time: 399.1975s
Epoch: 4 cost time: 60.424845933914185
Epoch: 4, Steps: 318 | Train Loss: 0.1305172 Vali Loss: 0.1540164 Test Loss: 0.1138924
Validation loss decreased (0.155239 --> 0.154016).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1259398
	speed: 0.2077s/iter; left time: 375.6779s
	iters: 200, epoch: 5 | loss: 0.0983294
	speed: 0.2029s/iter; left time: 346.7547s
	iters: 300, epoch: 5 | loss: 0.1287806
	speed: 0.1952s/iter; left time: 314.0015s
Epoch: 5 cost time: 63.761207818984985
Epoch: 5, Steps: 318 | Train Loss: 0.1290701 Vali Loss: 0.1542623 Test Loss: 0.1130557
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1173623
	speed: 0.1346s/iter; left time: 200.6935s
	iters: 200, epoch: 6 | loss: 0.0976928
	speed: 0.1317s/iter; left time: 183.1733s
	iters: 300, epoch: 6 | loss: 0.1249887
	speed: 0.1282s/iter; left time: 165.5261s
Epoch: 6 cost time: 42.367963790893555
Epoch: 6, Steps: 318 | Train Loss: 0.1281472 Vali Loss: 0.1529088 Test Loss: 0.1129095
Validation loss decreased (0.154016 --> 0.152909).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1279056
	speed: 0.1592s/iter; left time: 186.7248s
	iters: 200, epoch: 7 | loss: 0.1229661
	speed: 0.1500s/iter; left time: 160.9143s
	iters: 300, epoch: 7 | loss: 0.1347135
	speed: 0.1621s/iter; left time: 157.7500s
Epoch: 7 cost time: 49.72813558578491
Epoch: 7, Steps: 318 | Train Loss: 0.1279561 Vali Loss: 0.1524203 Test Loss: 0.1127802
Validation loss decreased (0.152909 --> 0.152420).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0899406
	speed: 0.1519s/iter; left time: 129.9146s
	iters: 200, epoch: 8 | loss: 0.1005476
	speed: 0.1620s/iter; left time: 122.3144s
	iters: 300, epoch: 8 | loss: 0.1516538
	speed: 0.1627s/iter; left time: 106.5997s
Epoch: 8 cost time: 50.644593715667725
Epoch: 8, Steps: 318 | Train Loss: 0.1277011 Vali Loss: 0.1518617 Test Loss: 0.1126788
Validation loss decreased (0.152420 --> 0.151862).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1352610
	speed: 0.1572s/iter; left time: 84.3897s
	iters: 200, epoch: 9 | loss: 0.1423791
	speed: 0.1578s/iter; left time: 68.9429s
	iters: 300, epoch: 9 | loss: 0.1094674
	speed: 0.1432s/iter; left time: 48.2451s
Epoch: 9 cost time: 48.089653730392456
Epoch: 9, Steps: 318 | Train Loss: 0.1275435 Vali Loss: 0.1534005 Test Loss: 0.1126501
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1393520
	speed: 0.1371s/iter; left time: 30.0171s
	iters: 200, epoch: 10 | loss: 0.1167344
	speed: 0.1406s/iter; left time: 16.7280s
	iters: 300, epoch: 10 | loss: 0.1067356
	speed: 0.1497s/iter; left time: 2.8450s
Epoch: 10 cost time: 45.32145571708679
Epoch: 10, Steps: 318 | Train Loss: 0.1275350 Vali Loss: 0.1530593 Test Loss: 0.1126311
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.5755383968353271, mae:0.5925974249839783, rmse:0.7586424946784973, mape:0.021912140771746635, mspe:0.0007887013489380479, rse:0.3374260663986206, r2_score:0.8750939930009791, acc:0.9780878592282534
corr: [41.127514 40.88604  41.157894 41.040974 41.305923 40.879616 40.95563
 40.59748  40.90597  40.814846 40.859333 40.979843 41.01913  41.111515
 41.166435 41.106285 41.335415 41.344723 41.18761  41.280396]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1730018
	speed: 0.1272s/iter; left time: 391.8348s
	iters: 200, epoch: 1 | loss: 0.1321389
	speed: 0.1088s/iter; left time: 324.2953s
	iters: 300, epoch: 1 | loss: 0.1784051
	speed: 0.0963s/iter; left time: 277.3936s
Epoch: 1 cost time: 34.93093180656433
Epoch: 1, Steps: 318 | Train Loss: 0.2629469 Vali Loss: 0.1662597 Test Loss: 0.1232970
Validation loss decreased (inf --> 0.166260).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1306050
	speed: 0.1157s/iter; left time: 319.8070s
	iters: 200, epoch: 2 | loss: 0.1532547
	speed: 0.0984s/iter; left time: 261.9528s
	iters: 300, epoch: 2 | loss: 0.1190809
	speed: 0.0962s/iter; left time: 246.4983s
Epoch: 2 cost time: 32.89117670059204
Epoch: 2, Steps: 318 | Train Loss: 0.1398171 Vali Loss: 0.1592068 Test Loss: 0.1184375
Validation loss decreased (0.166260 --> 0.159207).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1587394
	speed: 0.1091s/iter; left time: 266.8330s
	iters: 200, epoch: 3 | loss: 0.1086434
	speed: 0.1037s/iter; left time: 243.1021s
	iters: 300, epoch: 3 | loss: 0.1360737
	speed: 0.0931s/iter; left time: 209.1004s
Epoch: 3 cost time: 32.3280975818634
Epoch: 3, Steps: 318 | Train Loss: 0.1336717 Vali Loss: 0.1574419 Test Loss: 0.1159270
Validation loss decreased (0.159207 --> 0.157442).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1263591
	speed: 0.1041s/iter; left time: 221.3568s
	iters: 200, epoch: 4 | loss: 0.1069792
	speed: 0.0950s/iter; left time: 192.6403s
	iters: 300, epoch: 4 | loss: 0.1357584
	speed: 0.0977s/iter; left time: 188.2332s
Epoch: 4 cost time: 31.426006317138672
Epoch: 4, Steps: 318 | Train Loss: 0.1315034 Vali Loss: 0.1594493 Test Loss: 0.1161776
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1282047
	speed: 0.1013s/iter; left time: 183.3238s
	iters: 200, epoch: 5 | loss: 0.1442980
	speed: 0.1128s/iter; left time: 192.6986s
	iters: 300, epoch: 5 | loss: 0.1816151
	speed: 0.0974s/iter; left time: 156.7123s
Epoch: 5 cost time: 32.98037004470825
Epoch: 5, Steps: 318 | Train Loss: 0.1298687 Vali Loss: 0.1564055 Test Loss: 0.1146831
Validation loss decreased (0.157442 --> 0.156405).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1366151
	speed: 0.1113s/iter; left time: 166.0059s
	iters: 200, epoch: 6 | loss: 0.0940082
	speed: 0.0988s/iter; left time: 137.4859s
	iters: 300, epoch: 6 | loss: 0.1297535
	speed: 0.1009s/iter; left time: 130.2666s
Epoch: 6 cost time: 33.165342807769775
Epoch: 6, Steps: 318 | Train Loss: 0.1292912 Vali Loss: 0.1557061 Test Loss: 0.1148091
Validation loss decreased (0.156405 --> 0.155706).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1393983
	speed: 0.1065s/iter; left time: 124.9029s
	iters: 200, epoch: 7 | loss: 0.1840550
	speed: 0.1022s/iter; left time: 109.6575s
	iters: 300, epoch: 7 | loss: 0.1412395
	speed: 0.1041s/iter; left time: 101.3126s
Epoch: 7 cost time: 33.38811683654785
Epoch: 7, Steps: 318 | Train Loss: 0.1292909 Vali Loss: 0.1556418 Test Loss: 0.1148349
Validation loss decreased (0.155706 --> 0.155642).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1143856
	speed: 0.1003s/iter; left time: 85.7391s
	iters: 200, epoch: 8 | loss: 0.0996175
	speed: 0.0999s/iter; left time: 75.4544s
	iters: 300, epoch: 8 | loss: 0.1349096
	speed: 0.0988s/iter; left time: 64.7356s
Epoch: 8 cost time: 31.551642894744873
Epoch: 8, Steps: 318 | Train Loss: 0.1291854 Vali Loss: 0.1559977 Test Loss: 0.1147888
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1242912
	speed: 0.1027s/iter; left time: 55.1760s
	iters: 200, epoch: 9 | loss: 0.1514527
	speed: 0.0957s/iter; left time: 41.8218s
	iters: 300, epoch: 9 | loss: 0.0958257
	speed: 0.0979s/iter; left time: 32.9883s
Epoch: 9 cost time: 31.528746604919434
Epoch: 9, Steps: 318 | Train Loss: 0.1290655 Vali Loss: 0.1566268 Test Loss: 0.1147324
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1347226
	speed: 0.1016s/iter; left time: 22.2496s
	iters: 200, epoch: 10 | loss: 0.1225286
	speed: 0.0937s/iter; left time: 11.1511s
	iters: 300, epoch: 10 | loss: 0.1271425
	speed: 0.0942s/iter; left time: 1.7907s
Epoch: 10 cost time: 30.83453941345215
Epoch: 10, Steps: 318 | Train Loss: 0.1288272 Vali Loss: 0.1565847 Test Loss: 0.1147186
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.5865511894226074, mae:0.5969081521034241, rmse:0.7658662796020508, mape:0.022097906097769737, mspe:0.0008058077073656023, rse:0.3404013514518738, r2_score:0.8731876829158023, acc:0.9779020939022303
corr: [40.95157  40.90922  41.16015  40.980717 41.104645 41.055534 41.111256
 40.93095  41.015415 40.80791  40.822857 41.031437 40.836845 41.033413
 40.88534  40.83087  41.346844 41.157047 41.119446 41.496994 41.09557
 41.084076 41.357384 41.342472 41.392376]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=1, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2481788
	speed: 0.1571s/iter; left time: 482.4626s
	iters: 200, epoch: 1 | loss: 0.1695203
	speed: 0.1399s/iter; left time: 415.5740s
	iters: 300, epoch: 1 | loss: 0.1545325
	speed: 0.1517s/iter; left time: 435.5149s
Epoch: 1 cost time: 47.49280381202698
Epoch: 1, Steps: 317 | Train Loss: 0.2649929 Vali Loss: 0.1671259 Test Loss: 0.1207656
Validation loss decreased (inf --> 0.167126).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1547054
	speed: 0.1543s/iter; left time: 425.0221s
	iters: 200, epoch: 2 | loss: 0.1536126
	speed: 0.1469s/iter; left time: 389.7610s
	iters: 300, epoch: 2 | loss: 0.1319356
	speed: 0.1439s/iter; left time: 367.5312s
Epoch: 2 cost time: 46.71164011955261
Epoch: 2, Steps: 317 | Train Loss: 0.1368977 Vali Loss: 0.1582527 Test Loss: 0.1151962
Validation loss decreased (0.167126 --> 0.158253).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1419654
	speed: 0.1459s/iter; left time: 355.5204s
	iters: 200, epoch: 3 | loss: 0.1216713
	speed: 0.1435s/iter; left time: 335.3057s
	iters: 300, epoch: 3 | loss: 0.1489917
	speed: 0.1418s/iter; left time: 317.1260s
Epoch: 3 cost time: 45.952234506607056
Epoch: 3, Steps: 317 | Train Loss: 0.1325592 Vali Loss: 0.1572178 Test Loss: 0.1129475
Validation loss decreased (0.158253 --> 0.157218).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0979278
	speed: 0.1463s/iter; left time: 310.0547s
	iters: 200, epoch: 4 | loss: 0.1426698
	speed: 0.1768s/iter; left time: 357.2122s
	iters: 300, epoch: 4 | loss: 0.1495892
	speed: 0.2015s/iter; left time: 386.9559s
Epoch: 4 cost time: 56.5403938293457
Epoch: 4, Steps: 317 | Train Loss: 0.1308798 Vali Loss: 0.1552393 Test Loss: 0.1119396
Validation loss decreased (0.157218 --> 0.155239).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1136731
	speed: 0.2169s/iter; left time: 391.0385s
	iters: 200, epoch: 5 | loss: 0.1052966
	speed: 0.2152s/iter; left time: 366.4053s
	iters: 300, epoch: 5 | loss: 0.1328553
	speed: 0.2133s/iter; left time: 341.8703s
Epoch: 5 cost time: 68.54616332054138
Epoch: 5, Steps: 317 | Train Loss: 0.1302414 Vali Loss: 0.1554296 Test Loss: 0.1113678
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1083015
	speed: 0.1447s/iter; left time: 215.0457s
	iters: 200, epoch: 6 | loss: 0.1620680
	speed: 0.1333s/iter; left time: 184.7759s
	iters: 300, epoch: 6 | loss: 0.1320693
	speed: 0.1414s/iter; left time: 181.8338s
Epoch: 6 cost time: 44.62812352180481
Epoch: 6, Steps: 317 | Train Loss: 0.1298455 Vali Loss: 0.1547198 Test Loss: 0.1113140
Validation loss decreased (0.155239 --> 0.154720).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1534224
	speed: 0.1588s/iter; left time: 185.6524s
	iters: 200, epoch: 7 | loss: 0.1132899
	speed: 0.1396s/iter; left time: 149.2694s
	iters: 300, epoch: 7 | loss: 0.1419983
	speed: 0.1540s/iter; left time: 149.2658s
Epoch: 7 cost time: 47.37502145767212
Epoch: 7, Steps: 317 | Train Loss: 0.1295233 Vali Loss: 0.1547217 Test Loss: 0.1113910
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1489522
	speed: 0.1500s/iter; left time: 127.7691s
	iters: 200, epoch: 8 | loss: 0.1098435
	speed: 0.1536s/iter; left time: 115.5096s
	iters: 300, epoch: 8 | loss: 0.1059455
	speed: 0.1629s/iter; left time: 106.1805s
Epoch: 8 cost time: 49.24550104141235
Epoch: 8, Steps: 317 | Train Loss: 0.1294180 Vali Loss: 0.1542096 Test Loss: 0.1112219
Validation loss decreased (0.154720 --> 0.154210).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1151527
	speed: 0.1477s/iter; left time: 79.0117s
	iters: 200, epoch: 9 | loss: 0.1599561
	speed: 0.1353s/iter; left time: 58.8664s
	iters: 300, epoch: 9 | loss: 0.1615702
	speed: 0.1376s/iter; left time: 46.0979s
Epoch: 9 cost time: 44.88696312904358
Epoch: 9, Steps: 317 | Train Loss: 0.1295522 Vali Loss: 0.1545091 Test Loss: 0.1111809
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1266817
	speed: 0.1456s/iter; left time: 31.7468s
	iters: 200, epoch: 10 | loss: 0.1366814
	speed: 0.1572s/iter; left time: 18.5532s
	iters: 300, epoch: 10 | loss: 0.1357936
	speed: 0.1472s/iter; left time: 2.6496s
Epoch: 10 cost time: 47.37917518615723
Epoch: 10, Steps: 317 | Train Loss: 0.1293481 Vali Loss: 0.1543521 Test Loss: 0.1111618
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el1_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.5680966377258301, mae:0.5848405361175537, rmse:0.7537218332290649, mape:0.021639516577124596, mspe:0.0007795533165335655, rse:0.33475497364997864, r2_score:0.8751587729800742, acc:0.9783604834228754
corr: [40.940613 40.824112 40.87913  40.911163 40.900917 41.07143  40.974617
 40.883324 40.866615 41.00003  41.004875 40.85535  41.0579   41.269215
 41.0844   41.199722 41.086166 41.034435 41.080036 40.98042  41.12582
 41.30162  41.31432  41.183323 40.991554 41.057835 41.248196 41.087513
 41.153    41.311367]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2521736
	speed: 0.1073s/iter; left time: 330.4800s
	iters: 200, epoch: 1 | loss: 0.1700799
	speed: 0.0736s/iter; left time: 219.3464s
	iters: 300, epoch: 1 | loss: 0.1537351
	speed: 0.0688s/iter; left time: 198.3340s
Epoch: 1 cost time: 26.367658138275146
Epoch: 1, Steps: 318 | Train Loss: 0.3376716 Vali Loss: 0.1668956 Test Loss: 0.1595850
Validation loss decreased (inf --> 0.166896).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1282450
	speed: 0.0830s/iter; left time: 229.2440s
	iters: 200, epoch: 2 | loss: 0.1239027
	speed: 0.0769s/iter; left time: 204.7749s
	iters: 300, epoch: 2 | loss: 0.1427293
	speed: 0.0793s/iter; left time: 203.1423s
Epoch: 2 cost time: 25.24740982055664
Epoch: 2, Steps: 318 | Train Loss: 0.1547342 Vali Loss: 0.1628334 Test Loss: 0.1324648
Validation loss decreased (0.166896 --> 0.162833).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1071551
	speed: 0.0921s/iter; left time: 225.2958s
	iters: 200, epoch: 3 | loss: 0.1368154
	speed: 0.0815s/iter; left time: 191.1847s
	iters: 300, epoch: 3 | loss: 0.0852239
	speed: 0.0889s/iter; left time: 199.6287s
Epoch: 3 cost time: 27.142788648605347
Epoch: 3, Steps: 318 | Train Loss: 0.1456047 Vali Loss: 0.1556703 Test Loss: 0.1138131
Validation loss decreased (0.162833 --> 0.155670).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1577919
	speed: 0.0877s/iter; left time: 186.5556s
	iters: 200, epoch: 4 | loss: 0.1114608
	speed: 0.0925s/iter; left time: 187.5748s
	iters: 300, epoch: 4 | loss: 0.1518250
	speed: 0.0905s/iter; left time: 174.3789s
Epoch: 4 cost time: 28.60046410560608
Epoch: 4, Steps: 318 | Train Loss: 0.1385618 Vali Loss: 0.1517616 Test Loss: 0.1112872
Validation loss decreased (0.155670 --> 0.151762).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1790741
	speed: 0.0962s/iter; left time: 174.0486s
	iters: 200, epoch: 5 | loss: 0.0876951
	speed: 0.0884s/iter; left time: 151.1250s
	iters: 300, epoch: 5 | loss: 0.1190353
	speed: 0.0887s/iter; left time: 142.6711s
Epoch: 5 cost time: 28.919567823410034
Epoch: 5, Steps: 318 | Train Loss: 0.1361707 Vali Loss: 0.1516330 Test Loss: 0.1110118
Validation loss decreased (0.151762 --> 0.151633).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1159114
	speed: 0.0944s/iter; left time: 140.8170s
	iters: 200, epoch: 6 | loss: 0.1305040
	speed: 0.0910s/iter; left time: 126.6246s
	iters: 300, epoch: 6 | loss: 0.1435596
	speed: 0.1018s/iter; left time: 131.4319s
Epoch: 6 cost time: 30.423211336135864
Epoch: 6, Steps: 318 | Train Loss: 0.1345618 Vali Loss: 0.1501026 Test Loss: 0.1102541
Validation loss decreased (0.151633 --> 0.150103).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1449064
	speed: 0.0980s/iter; left time: 114.9360s
	iters: 200, epoch: 7 | loss: 0.1459436
	speed: 0.0863s/iter; left time: 92.5755s
	iters: 300, epoch: 7 | loss: 0.1546806
	speed: 0.0849s/iter; left time: 82.6099s
Epoch: 7 cost time: 28.538649082183838
Epoch: 7, Steps: 318 | Train Loss: 0.1344396 Vali Loss: 0.1503498 Test Loss: 0.1102289
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1249522
	speed: 0.0943s/iter; left time: 80.6528s
	iters: 200, epoch: 8 | loss: 0.1666357
	speed: 0.0862s/iter; left time: 65.0677s
	iters: 300, epoch: 8 | loss: 0.1230914
	speed: 0.0869s/iter; left time: 56.9276s
Epoch: 8 cost time: 28.34412169456482
Epoch: 8, Steps: 318 | Train Loss: 0.1344650 Vali Loss: 0.1513606 Test Loss: 0.1099601
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1273016
	speed: 0.0944s/iter; left time: 50.7067s
	iters: 200, epoch: 9 | loss: 0.1588303
	speed: 0.0943s/iter; left time: 41.2119s
	iters: 300, epoch: 9 | loss: 0.1136229
	speed: 0.0855s/iter; left time: 28.8156s
Epoch: 9 cost time: 29.099870920181274
Epoch: 9, Steps: 318 | Train Loss: 0.1338004 Vali Loss: 0.1506694 Test Loss: 0.1097973
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.5631533265113831, mae:0.5808557868003845, rmse:0.7504354119300842, mape:0.02157074771821499, mspe:0.0007849401445128024, rse:0.3341588079929352, r2_score:0.8762661456774801, acc:0.978429252281785
corr: [41.043102 41.43925  41.50645  41.46319  41.299225 41.263813 41.294518
 41.696007 41.4928   41.48616 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2282600
	speed: 0.0898s/iter; left time: 276.5242s
	iters: 200, epoch: 1 | loss: 0.1774664
	speed: 0.0777s/iter; left time: 231.4992s
	iters: 300, epoch: 1 | loss: 0.2028479
	speed: 0.0730s/iter; left time: 210.2388s
Epoch: 1 cost time: 25.608623027801514
Epoch: 1, Steps: 318 | Train Loss: 0.3643114 Vali Loss: 0.1703836 Test Loss: 0.1317501
Validation loss decreased (inf --> 0.170384).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1780970
	speed: 0.0704s/iter; left time: 194.3940s
	iters: 200, epoch: 2 | loss: 0.1623645
	speed: 0.0722s/iter; left time: 192.1613s
	iters: 300, epoch: 2 | loss: 0.1448235
	speed: 0.0785s/iter; left time: 201.3020s
Epoch: 2 cost time: 23.691047430038452
Epoch: 2, Steps: 318 | Train Loss: 0.1563034 Vali Loss: 0.1518005 Test Loss: 0.1163319
Validation loss decreased (0.170384 --> 0.151801).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1694973
	speed: 0.0835s/iter; left time: 204.2677s
	iters: 200, epoch: 3 | loss: 0.1470769
	speed: 0.0775s/iter; left time: 181.7683s
	iters: 300, epoch: 3 | loss: 0.1196171
	speed: 0.0733s/iter; left time: 164.5119s
Epoch: 3 cost time: 24.62703013420105
Epoch: 3, Steps: 318 | Train Loss: 0.1432845 Vali Loss: 0.1453031 Test Loss: 0.1102527
Validation loss decreased (0.151801 --> 0.145303).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1371428
	speed: 0.0857s/iter; left time: 182.2874s
	iters: 200, epoch: 4 | loss: 0.1560350
	speed: 0.0787s/iter; left time: 159.4583s
	iters: 300, epoch: 4 | loss: 0.1693179
	speed: 0.0723s/iter; left time: 139.3448s
Epoch: 4 cost time: 25.041114807128906
Epoch: 4, Steps: 318 | Train Loss: 0.1377192 Vali Loss: 0.1410682 Test Loss: 0.1080022
Validation loss decreased (0.145303 --> 0.141068).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1450074
	speed: 0.0734s/iter; left time: 132.7128s
	iters: 200, epoch: 5 | loss: 0.1425280
	speed: 0.0707s/iter; left time: 120.9044s
	iters: 300, epoch: 5 | loss: 0.1434546
	speed: 0.0785s/iter; left time: 126.2846s
Epoch: 5 cost time: 23.69260859489441
Epoch: 5, Steps: 318 | Train Loss: 0.1354433 Vali Loss: 0.1403121 Test Loss: 0.1068960
Validation loss decreased (0.141068 --> 0.140312).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1198203
	speed: 0.0813s/iter; left time: 121.1526s
	iters: 200, epoch: 6 | loss: 0.1446356
	speed: 0.0802s/iter; left time: 111.5349s
	iters: 300, epoch: 6 | loss: 0.0825238
	speed: 0.0782s/iter; left time: 100.9849s
Epoch: 6 cost time: 25.44609785079956
Epoch: 6, Steps: 318 | Train Loss: 0.1335608 Vali Loss: 0.1389746 Test Loss: 0.1062941
Validation loss decreased (0.140312 --> 0.138975).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1203518
	speed: 0.0841s/iter; left time: 98.6860s
	iters: 200, epoch: 7 | loss: 0.1391783
	speed: 0.0806s/iter; left time: 86.5164s
	iters: 300, epoch: 7 | loss: 0.1033262
	speed: 0.0774s/iter; left time: 75.3499s
Epoch: 7 cost time: 25.63938045501709
Epoch: 7, Steps: 318 | Train Loss: 0.1334329 Vali Loss: 0.1388924 Test Loss: 0.1059975
Validation loss decreased (0.138975 --> 0.138892).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1454062
	speed: 0.0838s/iter; left time: 71.6714s
	iters: 200, epoch: 8 | loss: 0.1398174
	speed: 0.0761s/iter; left time: 57.4453s
	iters: 300, epoch: 8 | loss: 0.1383452
	speed: 0.0899s/iter; left time: 58.8960s
Epoch: 8 cost time: 26.358124017715454
Epoch: 8, Steps: 318 | Train Loss: 0.1333387 Vali Loss: 0.1392528 Test Loss: 0.1059003
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1495522
	speed: 0.0756s/iter; left time: 40.6149s
	iters: 200, epoch: 9 | loss: 0.1145027
	speed: 0.0767s/iter; left time: 33.5037s
	iters: 300, epoch: 9 | loss: 0.1294008
	speed: 0.0880s/iter; left time: 29.6570s
Epoch: 9 cost time: 25.93351936340332
Epoch: 9, Steps: 318 | Train Loss: 0.1333418 Vali Loss: 0.1387108 Test Loss: 0.1058455
Validation loss decreased (0.138892 --> 0.138711).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1158046
	speed: 0.0823s/iter; left time: 18.0312s
	iters: 200, epoch: 10 | loss: 0.1596946
	speed: 0.0792s/iter; left time: 9.4286s
	iters: 300, epoch: 10 | loss: 0.1191877
	speed: 0.0767s/iter; left time: 1.4571s
Epoch: 10 cost time: 25.250972270965576
Epoch: 10, Steps: 318 | Train Loss: 0.1326513 Vali Loss: 0.1377213 Test Loss: 0.1058070
Validation loss decreased (0.138711 --> 0.137721).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.5404385924339294, mae:0.568874716758728, rmse:0.7351452708244324, mape:0.021092940121889114, mspe:0.0007491401047445834, rse:0.3271794021129608, r2_score:0.8776449030483752, acc:0.9789070598781109
corr: [41.293262 41.00067  40.605568 40.93604  40.942097 41.067776 40.986973
 40.601486 40.830288 41.146133 41.017025 40.984104 41.4577   41.371254
 41.048676]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2524443
	speed: 0.0936s/iter; left time: 288.4503s
	iters: 200, epoch: 1 | loss: 0.2496875
	speed: 0.0757s/iter; left time: 225.7078s
	iters: 300, epoch: 1 | loss: 0.1386679
	speed: 0.0759s/iter; left time: 218.6730s
Epoch: 1 cost time: 25.864269256591797
Epoch: 1, Steps: 318 | Train Loss: 0.3681928 Vali Loss: 0.1832091 Test Loss: 0.2120888
Validation loss decreased (inf --> 0.183209).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1448747
	speed: 0.1099s/iter; left time: 303.6996s
	iters: 200, epoch: 2 | loss: 0.1930480
	speed: 0.0968s/iter; left time: 257.7020s
	iters: 300, epoch: 2 | loss: 0.1279632
	speed: 0.0873s/iter; left time: 223.7825s
Epoch: 2 cost time: 31.016650676727295
Epoch: 2, Steps: 318 | Train Loss: 0.1563456 Vali Loss: 0.1595154 Test Loss: 0.1892045
Validation loss decreased (0.183209 --> 0.159515).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1114413
	speed: 0.1051s/iter; left time: 256.9157s
	iters: 200, epoch: 3 | loss: 0.1288492
	speed: 0.0944s/iter; left time: 221.4540s
	iters: 300, epoch: 3 | loss: 0.1263304
	speed: 0.0954s/iter; left time: 214.1579s
Epoch: 3 cost time: 31.338743209838867
Epoch: 3, Steps: 318 | Train Loss: 0.1452128 Vali Loss: 0.1549910 Test Loss: 0.1618068
Validation loss decreased (0.159515 --> 0.154991).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1449677
	speed: 0.1103s/iter; left time: 234.6660s
	iters: 200, epoch: 4 | loss: 0.1483767
	speed: 0.0994s/iter; left time: 201.4422s
	iters: 300, epoch: 4 | loss: 0.1418775
	speed: 0.0928s/iter; left time: 178.8769s
Epoch: 4 cost time: 31.856778144836426
Epoch: 4, Steps: 318 | Train Loss: 0.1401933 Vali Loss: 0.1529002 Test Loss: 0.1590565
Validation loss decreased (0.154991 --> 0.152900).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1733474
	speed: 0.0827s/iter; left time: 149.6767s
	iters: 200, epoch: 5 | loss: 0.1202213
	speed: 0.0743s/iter; left time: 126.9908s
	iters: 300, epoch: 5 | loss: 0.1357545
	speed: 0.0764s/iter; left time: 122.9658s
Epoch: 5 cost time: 24.911853551864624
Epoch: 5, Steps: 318 | Train Loss: 0.1381965 Vali Loss: 0.1523911 Test Loss: 0.1576653
Validation loss decreased (0.152900 --> 0.152391).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1433600
	speed: 0.0746s/iter; left time: 111.1879s
	iters: 200, epoch: 6 | loss: 0.1359714
	speed: 0.0719s/iter; left time: 100.0506s
	iters: 300, epoch: 6 | loss: 0.2084053
	speed: 0.0711s/iter; left time: 91.7624s
Epoch: 6 cost time: 23.23578190803528
Epoch: 6, Steps: 318 | Train Loss: 0.1371859 Vali Loss: 0.1515616 Test Loss: 0.1578617
Validation loss decreased (0.152391 --> 0.151562).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1237499
	speed: 0.0700s/iter; left time: 82.0677s
	iters: 200, epoch: 7 | loss: 0.1527156
	speed: 0.0605s/iter; left time: 64.9061s
	iters: 300, epoch: 7 | loss: 0.1305650
	speed: 0.0674s/iter; left time: 65.6248s
Epoch: 7 cost time: 21.103328227996826
Epoch: 7, Steps: 318 | Train Loss: 0.1370417 Vali Loss: 0.1502642 Test Loss: 0.1574882
Validation loss decreased (0.151562 --> 0.150264).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1554715
	speed: 0.0845s/iter; left time: 72.2303s
	iters: 200, epoch: 8 | loss: 0.1717741
	speed: 0.0738s/iter; left time: 55.7545s
	iters: 300, epoch: 8 | loss: 0.1119184
	speed: 0.0682s/iter; left time: 44.6895s
Epoch: 8 cost time: 24.055590629577637
Epoch: 8, Steps: 318 | Train Loss: 0.1367330 Vali Loss: 0.1500422 Test Loss: 0.1573501
Validation loss decreased (0.150264 --> 0.150042).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1492056
	speed: 0.0732s/iter; left time: 39.3319s
	iters: 200, epoch: 9 | loss: 0.1466660
	speed: 0.0683s/iter; left time: 29.8423s
	iters: 300, epoch: 9 | loss: 0.1398908
	speed: 0.0733s/iter; left time: 24.6871s
Epoch: 9 cost time: 22.896455764770508
Epoch: 9, Steps: 318 | Train Loss: 0.1363332 Vali Loss: 0.1514317 Test Loss: 0.1572647
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1280527
	speed: 0.0776s/iter; left time: 16.9934s
	iters: 200, epoch: 10 | loss: 0.1259556
	speed: 0.0760s/iter; left time: 9.0437s
	iters: 300, epoch: 10 | loss: 0.1158789
	speed: 0.0793s/iter; left time: 1.5063s
Epoch: 10 cost time: 24.77176856994629
Epoch: 10, Steps: 318 | Train Loss: 0.1362948 Vali Loss: 0.1513458 Test Loss: 0.1572335
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.803709089756012, mae:0.6883067488670349, rmse:0.8964982628822327, mape:0.02550686150789261, mspe:0.0011186412302777171, rse:0.3987410366535187, r2_score:0.8428048221669553, acc:0.9744931384921074
corr: [42.237125 42.00615  42.11371  42.242943 42.2123   42.08762  42.354774
 42.23345  42.393917 42.04422  42.18702  42.027847 42.059196 42.267338
 42.00489  41.919968 41.991894 41.61827  41.60692  40.357014]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2268691
	speed: 0.1073s/iter; left time: 330.6355s
	iters: 200, epoch: 1 | loss: 0.2028856
	speed: 0.0847s/iter; left time: 252.4059s
	iters: 300, epoch: 1 | loss: 0.1422596
	speed: 0.0816s/iter; left time: 235.1343s
Epoch: 1 cost time: 28.842825174331665
Epoch: 1, Steps: 318 | Train Loss: 0.2800510 Vali Loss: 0.1717006 Test Loss: 0.1614300
Validation loss decreased (inf --> 0.171701).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1618823
	speed: 0.0976s/iter; left time: 269.7589s
	iters: 200, epoch: 2 | loss: 0.1404489
	speed: 0.0901s/iter; left time: 239.8400s
	iters: 300, epoch: 2 | loss: 0.1386218
	speed: 0.0883s/iter; left time: 226.2075s
Epoch: 2 cost time: 29.28024911880493
Epoch: 2, Steps: 318 | Train Loss: 0.1541277 Vali Loss: 0.1591408 Test Loss: 0.1297448
Validation loss decreased (0.171701 --> 0.159141).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1336024
	speed: 0.0877s/iter; left time: 214.4697s
	iters: 200, epoch: 3 | loss: 0.1924879
	speed: 0.0848s/iter; left time: 198.8343s
	iters: 300, epoch: 3 | loss: 0.1842366
	speed: 0.0825s/iter; left time: 185.2653s
Epoch: 3 cost time: 27.036259651184082
Epoch: 3, Steps: 318 | Train Loss: 0.1427413 Vali Loss: 0.1550744 Test Loss: 0.1228705
Validation loss decreased (0.159141 --> 0.155074).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1360994
	speed: 0.0893s/iter; left time: 189.8727s
	iters: 200, epoch: 4 | loss: 0.1823291
	speed: 0.0790s/iter; left time: 160.1081s
	iters: 300, epoch: 4 | loss: 0.1498483
	speed: 0.0810s/iter; left time: 156.0943s
Epoch: 4 cost time: 26.556675672531128
Epoch: 4, Steps: 318 | Train Loss: 0.1392290 Vali Loss: 0.1539671 Test Loss: 0.1197728
Validation loss decreased (0.155074 --> 0.153967).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1094195
	speed: 0.0850s/iter; left time: 153.7910s
	iters: 200, epoch: 5 | loss: 0.1221261
	speed: 0.0737s/iter; left time: 126.0062s
	iters: 300, epoch: 5 | loss: 0.1648650
	speed: 0.0801s/iter; left time: 128.9590s
Epoch: 5 cost time: 25.330236434936523
Epoch: 5, Steps: 318 | Train Loss: 0.1373294 Vali Loss: 0.1526162 Test Loss: 0.1185093
Validation loss decreased (0.153967 --> 0.152616).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1283485
	speed: 0.0860s/iter; left time: 128.2656s
	iters: 200, epoch: 6 | loss: 0.1401513
	speed: 0.0813s/iter; left time: 113.1130s
	iters: 300, epoch: 6 | loss: 0.1210804
	speed: 0.0793s/iter; left time: 102.4199s
Epoch: 6 cost time: 26.1470627784729
Epoch: 6, Steps: 318 | Train Loss: 0.1367216 Vali Loss: 0.1516438 Test Loss: 0.1178391
Validation loss decreased (0.152616 --> 0.151644).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1472645
	speed: 0.0844s/iter; left time: 99.0226s
	iters: 200, epoch: 7 | loss: 0.1370249
	speed: 0.0803s/iter; left time: 86.1345s
	iters: 300, epoch: 7 | loss: 0.1500044
	speed: 0.0786s/iter; left time: 76.4378s
Epoch: 7 cost time: 25.974549055099487
Epoch: 7, Steps: 318 | Train Loss: 0.1365039 Vali Loss: 0.1521556 Test Loss: 0.1177493
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1437337
	speed: 0.0797s/iter; left time: 68.1496s
	iters: 200, epoch: 8 | loss: 0.1476341
	speed: 0.0717s/iter; left time: 54.1545s
	iters: 300, epoch: 8 | loss: 0.1274198
	speed: 0.0790s/iter; left time: 51.7275s
Epoch: 8 cost time: 24.51900553703308
Epoch: 8, Steps: 318 | Train Loss: 0.1361912 Vali Loss: 0.1520778 Test Loss: 0.1176278
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0935493
	speed: 0.0851s/iter; left time: 45.7000s
	iters: 200, epoch: 9 | loss: 0.1493661
	speed: 0.0829s/iter; left time: 36.2257s
	iters: 300, epoch: 9 | loss: 0.1532902
	speed: 0.0827s/iter; left time: 27.8563s
Epoch: 9 cost time: 26.62094783782959
Epoch: 9, Steps: 318 | Train Loss: 0.1361032 Vali Loss: 0.1517543 Test Loss: 0.1175267
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.6018960475921631, mae:0.60537189245224, rmse:0.7758195996284485, mape:0.022441960871219635, mspe:0.0008328088442794979, rse:0.34482523798942566, r2_score:0.8710744653785233, acc:0.9775580391287804
corr: [41.489494 40.869015 40.714657 40.94615  41.018383 41.098362 41.20833
 40.933384 40.98216  41.202644 40.94692  40.93847  41.15744  41.015995
 40.620552 41.08954  41.30832  41.399937 40.945274 41.302586 41.321575
 41.256596 41.307156 41.28848  41.55577 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2495939
	speed: 0.0931s/iter; left time: 285.9749s
	iters: 200, epoch: 1 | loss: 0.1933912
	speed: 0.0813s/iter; left time: 241.6581s
	iters: 300, epoch: 1 | loss: 0.2104482
	speed: 0.0811s/iter; left time: 232.7755s
Epoch: 1 cost time: 26.977768898010254
Epoch: 1, Steps: 317 | Train Loss: 0.3569560 Vali Loss: 0.1774888 Test Loss: 0.1377743
Validation loss decreased (inf --> 0.177489).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1799926
	speed: 0.0808s/iter; left time: 222.4995s
	iters: 200, epoch: 2 | loss: 0.1855804
	speed: 0.0816s/iter; left time: 216.5156s
	iters: 300, epoch: 2 | loss: 0.1707464
	speed: 0.0653s/iter; left time: 166.7008s
Epoch: 2 cost time: 24.105979442596436
Epoch: 2, Steps: 317 | Train Loss: 0.1670424 Vali Loss: 0.1636799 Test Loss: 0.1248953
Validation loss decreased (0.177489 --> 0.163680).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1554138
	speed: 0.0856s/iter; left time: 208.6260s
	iters: 200, epoch: 3 | loss: 0.1241629
	speed: 0.0807s/iter; left time: 188.5127s
	iters: 300, epoch: 3 | loss: 0.1871667
	speed: 0.0799s/iter; left time: 178.7287s
Epoch: 3 cost time: 25.94877052307129
Epoch: 3, Steps: 317 | Train Loss: 0.1546039 Vali Loss: 0.1594162 Test Loss: 0.1196403
Validation loss decreased (0.163680 --> 0.159416).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1400098
	speed: 0.0879s/iter; left time: 186.3328s
	iters: 200, epoch: 4 | loss: 0.1100797
	speed: 0.0802s/iter; left time: 161.9549s
	iters: 300, epoch: 4 | loss: 0.1748742
	speed: 0.0824s/iter; left time: 158.2261s
Epoch: 4 cost time: 26.50165057182312
Epoch: 4, Steps: 317 | Train Loss: 0.1495283 Vali Loss: 0.1560532 Test Loss: 0.1179079
Validation loss decreased (0.159416 --> 0.156053).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1557868
	speed: 0.0788s/iter; left time: 142.0842s
	iters: 200, epoch: 5 | loss: 0.1647625
	speed: 0.0928s/iter; left time: 158.0990s
	iters: 300, epoch: 5 | loss: 0.1327378
	speed: 0.0820s/iter; left time: 131.4038s
Epoch: 5 cost time: 26.871211051940918
Epoch: 5, Steps: 317 | Train Loss: 0.1470587 Vali Loss: 0.1553977 Test Loss: 0.1171164
Validation loss decreased (0.156053 --> 0.155398).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1226059
	speed: 0.0843s/iter; left time: 125.2812s
	iters: 200, epoch: 6 | loss: 0.1392931
	speed: 0.0812s/iter; left time: 112.4746s
	iters: 300, epoch: 6 | loss: 0.1538330
	speed: 0.0789s/iter; left time: 101.4027s
Epoch: 6 cost time: 25.874621391296387
Epoch: 6, Steps: 317 | Train Loss: 0.1458352 Vali Loss: 0.1537572 Test Loss: 0.1166401
Validation loss decreased (0.155398 --> 0.153757).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1451183
	speed: 0.0884s/iter; left time: 103.2867s
	iters: 200, epoch: 7 | loss: 0.1321205
	speed: 0.0785s/iter; left time: 83.9292s
	iters: 300, epoch: 7 | loss: 0.1350610
	speed: 0.0772s/iter; left time: 74.8370s
Epoch: 7 cost time: 25.735686540603638
Epoch: 7, Steps: 317 | Train Loss: 0.1449853 Vali Loss: 0.1547224 Test Loss: 0.1167286
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1314058
	speed: 0.0824s/iter; left time: 70.2100s
	iters: 200, epoch: 8 | loss: 0.1549589
	speed: 0.0788s/iter; left time: 59.2875s
	iters: 300, epoch: 8 | loss: 0.1189569
	speed: 0.0793s/iter; left time: 51.6938s
Epoch: 8 cost time: 25.47326350212097
Epoch: 8, Steps: 317 | Train Loss: 0.1449227 Vali Loss: 0.1544118 Test Loss: 0.1165026
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1235596
	speed: 0.0825s/iter; left time: 44.1356s
	iters: 200, epoch: 9 | loss: 0.1180530
	speed: 0.0849s/iter; left time: 36.9434s
	iters: 300, epoch: 9 | loss: 0.1384572
	speed: 0.0786s/iter; left time: 26.3212s
Epoch: 9 cost time: 26.006211042404175
Epoch: 9, Steps: 317 | Train Loss: 0.1449772 Vali Loss: 0.1543157 Test Loss: 0.1164842
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.5957717299461365, mae:0.5998131632804871, rmse:0.7718625068664551, mape:0.022211337462067604, mspe:0.0008203363395296037, rse:0.3428119122982025, r2_score:0.8692694756808783, acc:0.9777886625379324
corr: [41.642635 40.83327  41.08353  40.621944 41.69591  41.768867 41.758106
 41.318714 41.522366 41.516838 41.25753  41.378956 41.52331  41.554867
 41.0833   41.124947 41.48337  41.564552 41.13982  41.564865 41.45627
 41.441006 41.16029  41.25746  41.43369  41.987    41.82666  41.823627
 42.08945  41.525837]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1819069
	speed: 0.1160s/iter; left time: 357.4256s
	iters: 200, epoch: 1 | loss: 0.1408188
	speed: 0.0941s/iter; left time: 280.5468s
	iters: 300, epoch: 1 | loss: 0.1334252
	speed: 0.1032s/iter; left time: 297.4588s
Epoch: 1 cost time: 33.12204074859619
Epoch: 1, Steps: 318 | Train Loss: 0.2313032 Vali Loss: 0.1477827 Test Loss: 0.1085005
Validation loss decreased (inf --> 0.147783).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1115632
	speed: 0.1100s/iter; left time: 304.0056s
	iters: 200, epoch: 2 | loss: 0.0855574
	speed: 0.0915s/iter; left time: 243.7005s
	iters: 300, epoch: 2 | loss: 0.1182695
	speed: 0.0853s/iter; left time: 218.6299s
Epoch: 2 cost time: 30.56135106086731
Epoch: 2, Steps: 318 | Train Loss: 0.1252804 Vali Loss: 0.1339856 Test Loss: 0.0968155
Validation loss decreased (0.147783 --> 0.133986).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0822655
	speed: 0.1071s/iter; left time: 261.7869s
	iters: 200, epoch: 3 | loss: 0.1024255
	speed: 0.0999s/iter; left time: 234.3602s
	iters: 300, epoch: 3 | loss: 0.0713893
	speed: 0.1006s/iter; left time: 225.8294s
Epoch: 3 cost time: 32.641735315322876
Epoch: 3, Steps: 318 | Train Loss: 0.1135299 Vali Loss: 0.1292144 Test Loss: 0.0914840
Validation loss decreased (0.133986 --> 0.129214).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1015176
	speed: 0.1109s/iter; left time: 235.8334s
	iters: 200, epoch: 4 | loss: 0.0886194
	speed: 0.0973s/iter; left time: 197.1723s
	iters: 300, epoch: 4 | loss: 0.1227393
	speed: 0.1053s/iter; left time: 202.9124s
Epoch: 4 cost time: 33.44661903381348
Epoch: 4, Steps: 318 | Train Loss: 0.1085791 Vali Loss: 0.1247727 Test Loss: 0.0888329
Validation loss decreased (0.129214 --> 0.124773).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1259011
	speed: 0.1084s/iter; left time: 196.1333s
	iters: 200, epoch: 5 | loss: 0.0818539
	speed: 0.1239s/iter; left time: 211.8048s
	iters: 300, epoch: 5 | loss: 0.0813528
	speed: 0.1412s/iter; left time: 227.1976s
Epoch: 5 cost time: 39.6114387512207
Epoch: 5, Steps: 318 | Train Loss: 0.1066401 Vali Loss: 0.1244871 Test Loss: 0.0885106
Validation loss decreased (0.124773 --> 0.124487).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0988689
	speed: 0.1510s/iter; left time: 225.1863s
	iters: 200, epoch: 6 | loss: 0.0767841
	speed: 0.1297s/iter; left time: 180.4037s
	iters: 300, epoch: 6 | loss: 0.1066680
	speed: 0.1403s/iter; left time: 181.0728s
Epoch: 6 cost time: 44.53777885437012
Epoch: 6, Steps: 318 | Train Loss: 0.1056146 Vali Loss: 0.1233752 Test Loss: 0.0877647
Validation loss decreased (0.124487 --> 0.123375).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1178322
	speed: 0.1449s/iter; left time: 169.9253s
	iters: 200, epoch: 7 | loss: 0.1069934
	speed: 0.1368s/iter; left time: 146.8151s
	iters: 300, epoch: 7 | loss: 0.1112615
	speed: 0.1368s/iter; left time: 133.1053s
Epoch: 7 cost time: 44.00302052497864
Epoch: 7, Steps: 318 | Train Loss: 0.1050325 Vali Loss: 0.1229813 Test Loss: 0.0879569
Validation loss decreased (0.123375 --> 0.122981).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1264865
	speed: 0.0949s/iter; left time: 81.1801s
	iters: 200, epoch: 8 | loss: 0.1033918
	speed: 0.0760s/iter; left time: 57.3711s
	iters: 300, epoch: 8 | loss: 0.1003156
	speed: 0.0773s/iter; left time: 50.6639s
Epoch: 8 cost time: 25.96407699584961
Epoch: 8, Steps: 318 | Train Loss: 0.1051119 Vali Loss: 0.1240614 Test Loss: 0.0877832
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1019702
	speed: 0.1001s/iter; left time: 53.7538s
	iters: 200, epoch: 9 | loss: 0.1285242
	speed: 0.0827s/iter; left time: 36.1441s
	iters: 300, epoch: 9 | loss: 0.0752617
	speed: 0.0690s/iter; left time: 23.2505s
Epoch: 9 cost time: 26.674904823303223
Epoch: 9, Steps: 318 | Train Loss: 0.1045000 Vali Loss: 0.1235119 Test Loss: 0.0877018
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0870857
	speed: 0.0673s/iter; left time: 14.7281s
	iters: 200, epoch: 10 | loss: 0.0908026
	speed: 0.0679s/iter; left time: 8.0837s
	iters: 300, epoch: 10 | loss: 0.1368843
	speed: 0.0897s/iter; left time: 1.7052s
Epoch: 10 cost time: 24.214091062545776
Epoch: 10, Steps: 318 | Train Loss: 0.1045891 Vali Loss: 0.1237803 Test Loss: 0.0876768
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.44926416873931885, mae:0.5201571583747864, rmse:0.6702716946601868, mape:0.019295549020171165, mspe:0.0006229582941159606, rse:0.29846298694610596, r2_score:0.9016622436213518, acc:0.9807044509798288
corr: [40.471676 40.73349  40.741768 40.778194 40.84542  40.94704  40.793716
 41.03315  40.99746  40.89566 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1991983
	speed: 0.1083s/iter; left time: 333.7606s
	iters: 200, epoch: 1 | loss: 0.1541138
	speed: 0.0832s/iter; left time: 247.9802s
	iters: 300, epoch: 1 | loss: 0.1604745
	speed: 0.0839s/iter; left time: 241.7002s
Epoch: 1 cost time: 29.20954704284668
Epoch: 1, Steps: 318 | Train Loss: 0.2626084 Vali Loss: 0.1433753 Test Loss: 0.1114370
Validation loss decreased (inf --> 0.143375).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1328594
	speed: 0.0938s/iter; left time: 259.0691s
	iters: 200, epoch: 2 | loss: 0.1164364
	speed: 0.0746s/iter; left time: 198.7850s
	iters: 300, epoch: 2 | loss: 0.1195066
	speed: 0.0821s/iter; left time: 210.3667s
Epoch: 2 cost time: 26.379621505737305
Epoch: 2, Steps: 318 | Train Loss: 0.1261009 Vali Loss: 0.1317326 Test Loss: 0.0965668
Validation loss decreased (0.143375 --> 0.131733).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1467777
	speed: 0.0892s/iter; left time: 218.1228s
	iters: 200, epoch: 3 | loss: 0.1203249
	speed: 0.0754s/iter; left time: 176.9208s
	iters: 300, epoch: 3 | loss: 0.1099727
	speed: 0.0752s/iter; left time: 168.9068s
Epoch: 3 cost time: 25.42329502105713
Epoch: 3, Steps: 318 | Train Loss: 0.1174769 Vali Loss: 0.1269178 Test Loss: 0.0922771
Validation loss decreased (0.131733 --> 0.126918).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1125911
	speed: 0.0832s/iter; left time: 176.9452s
	iters: 200, epoch: 4 | loss: 0.1231067
	speed: 0.0714s/iter; left time: 144.6294s
	iters: 300, epoch: 4 | loss: 0.1509597
	speed: 0.0828s/iter; left time: 159.6352s
Epoch: 4 cost time: 25.216306447982788
Epoch: 4, Steps: 318 | Train Loss: 0.1132430 Vali Loss: 0.1236743 Test Loss: 0.0910176
Validation loss decreased (0.126918 --> 0.123674).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1263847
	speed: 0.0921s/iter; left time: 166.5416s
	iters: 200, epoch: 5 | loss: 0.1319254
	speed: 0.0803s/iter; left time: 137.1879s
	iters: 300, epoch: 5 | loss: 0.1147061
	speed: 0.0779s/iter; left time: 125.2809s
Epoch: 5 cost time: 26.548068523406982
Epoch: 5, Steps: 318 | Train Loss: 0.1117960 Vali Loss: 0.1233654 Test Loss: 0.0902012
Validation loss decreased (0.123674 --> 0.123365).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0983759
	speed: 0.0911s/iter; left time: 135.7949s
	iters: 200, epoch: 6 | loss: 0.1206515
	speed: 0.0875s/iter; left time: 121.6864s
	iters: 300, epoch: 6 | loss: 0.0746331
	speed: 0.0889s/iter; left time: 114.7716s
Epoch: 6 cost time: 28.478890419006348
Epoch: 6, Steps: 318 | Train Loss: 0.1105871 Vali Loss: 0.1226213 Test Loss: 0.0900807
Validation loss decreased (0.123365 --> 0.122621).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1009690
	speed: 0.0949s/iter; left time: 111.3553s
	iters: 200, epoch: 7 | loss: 0.1184828
	speed: 0.0877s/iter; left time: 94.0518s
	iters: 300, epoch: 7 | loss: 0.0823531
	speed: 0.0920s/iter; left time: 89.4793s
Epoch: 7 cost time: 29.163312911987305
Epoch: 7, Steps: 318 | Train Loss: 0.1104196 Vali Loss: 0.1224777 Test Loss: 0.0897992
Validation loss decreased (0.122621 --> 0.122478).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1212228
	speed: 0.0727s/iter; left time: 62.1410s
	iters: 200, epoch: 8 | loss: 0.1152444
	speed: 0.0799s/iter; left time: 60.3059s
	iters: 300, epoch: 8 | loss: 0.1054453
	speed: 0.0783s/iter; left time: 51.2817s
Epoch: 8 cost time: 24.594274520874023
Epoch: 8, Steps: 318 | Train Loss: 0.1103735 Vali Loss: 0.1226276 Test Loss: 0.0898414
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1219413
	speed: 0.0857s/iter; left time: 46.0079s
	iters: 200, epoch: 9 | loss: 0.0966978
	speed: 0.0816s/iter; left time: 35.6788s
	iters: 300, epoch: 9 | loss: 0.0984004
	speed: 0.0807s/iter; left time: 27.1855s
Epoch: 9 cost time: 26.219094038009644
Epoch: 9, Steps: 318 | Train Loss: 0.1101589 Vali Loss: 0.1219785 Test Loss: 0.0897771
Validation loss decreased (0.122478 --> 0.121978).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1017525
	speed: 0.0798s/iter; left time: 17.4837s
	iters: 200, epoch: 10 | loss: 0.1230900
	speed: 0.0771s/iter; left time: 9.1784s
	iters: 300, epoch: 10 | loss: 0.0939944
	speed: 0.0821s/iter; left time: 1.5602s
Epoch: 10 cost time: 25.439212322235107
Epoch: 10, Steps: 318 | Train Loss: 0.1097911 Vali Loss: 0.1212710 Test Loss: 0.0896960
Validation loss decreased (0.121978 --> 0.121271).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.45814722776412964, mae:0.5246360301971436, rmse:0.6768657565116882, mape:0.01945958286523819, mspe:0.0006353135104291141, rse:0.30124184489250183, r2_score:0.8999297200132993, acc:0.9805404171347618
corr: [40.62593  40.613873 40.75199  40.81569  40.92628  40.73245  40.997894
 40.913948 41.00691  40.907898 41.162792 41.223774 41.222134 41.37086
 41.39326 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1636906
	speed: 0.1102s/iter; left time: 339.3765s
	iters: 200, epoch: 1 | loss: 0.1729746
	speed: 0.0930s/iter; left time: 277.2634s
	iters: 300, epoch: 1 | loss: 0.1161906
	speed: 0.0838s/iter; left time: 241.5505s
Epoch: 1 cost time: 30.305349111557007
Epoch: 1, Steps: 318 | Train Loss: 0.2491789 Vali Loss: 0.1475781 Test Loss: 0.1114330
Validation loss decreased (inf --> 0.147578).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1133537
	speed: 0.1042s/iter; left time: 287.9393s
	iters: 200, epoch: 2 | loss: 0.1457206
	speed: 0.0909s/iter; left time: 242.1285s
	iters: 300, epoch: 2 | loss: 0.1129531
	speed: 0.0963s/iter; left time: 246.7570s
Epoch: 2 cost time: 30.66495418548584
Epoch: 2, Steps: 318 | Train Loss: 0.1288286 Vali Loss: 0.1339095 Test Loss: 0.1032478
Validation loss decreased (0.147578 --> 0.133910).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0839995
	speed: 0.1020s/iter; left time: 249.3583s
	iters: 200, epoch: 3 | loss: 0.1085449
	speed: 0.0906s/iter; left time: 212.4934s
	iters: 300, epoch: 3 | loss: 0.1153395
	speed: 0.0995s/iter; left time: 223.3123s
Epoch: 3 cost time: 31.021070957183838
Epoch: 3, Steps: 318 | Train Loss: 0.1203443 Vali Loss: 0.1308991 Test Loss: 0.0997769
Validation loss decreased (0.133910 --> 0.130899).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1313244
	speed: 0.1008s/iter; left time: 214.3493s
	iters: 200, epoch: 4 | loss: 0.1331280
	speed: 0.0935s/iter; left time: 189.5923s
	iters: 300, epoch: 4 | loss: 0.1205080
	speed: 0.0937s/iter; left time: 180.5864s
Epoch: 4 cost time: 30.564793825149536
Epoch: 4, Steps: 318 | Train Loss: 0.1171964 Vali Loss: 0.1306558 Test Loss: 0.0998169
Validation loss decreased (0.130899 --> 0.130656).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1304885
	speed: 0.1052s/iter; left time: 190.3598s
	iters: 200, epoch: 5 | loss: 0.1030647
	speed: 0.0895s/iter; left time: 152.9506s
	iters: 300, epoch: 5 | loss: 0.1103647
	speed: 0.0941s/iter; left time: 151.3758s
Epoch: 5 cost time: 30.98591637611389
Epoch: 5, Steps: 318 | Train Loss: 0.1155676 Vali Loss: 0.1304627 Test Loss: 0.0982762
Validation loss decreased (0.130656 --> 0.130463).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1258758
	speed: 0.0853s/iter; left time: 127.2267s
	iters: 200, epoch: 6 | loss: 0.1087145
	speed: 0.0867s/iter; left time: 120.6515s
	iters: 300, epoch: 6 | loss: 0.1766116
	speed: 0.0948s/iter; left time: 122.3433s
Epoch: 6 cost time: 28.543301582336426
Epoch: 6, Steps: 318 | Train Loss: 0.1146257 Vali Loss: 0.1298738 Test Loss: 0.0979856
Validation loss decreased (0.130463 --> 0.129874).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0966028
	speed: 0.0960s/iter; left time: 112.6498s
	iters: 200, epoch: 7 | loss: 0.1382185
	speed: 0.0823s/iter; left time: 88.3493s
	iters: 300, epoch: 7 | loss: 0.1093623
	speed: 0.0912s/iter; left time: 88.7425s
Epoch: 7 cost time: 28.68342638015747
Epoch: 7, Steps: 318 | Train Loss: 0.1143016 Vali Loss: 0.1288434 Test Loss: 0.0981244
Validation loss decreased (0.129874 --> 0.128843).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1219307
	speed: 0.0964s/iter; left time: 82.4209s
	iters: 200, epoch: 8 | loss: 0.1537673
	speed: 0.0841s/iter; left time: 63.4581s
	iters: 300, epoch: 8 | loss: 0.0896007
	speed: 0.0704s/iter; left time: 46.0962s
Epoch: 8 cost time: 26.68973135948181
Epoch: 8, Steps: 318 | Train Loss: 0.1141259 Vali Loss: 0.1287548 Test Loss: 0.0980130
Validation loss decreased (0.128843 --> 0.128755).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1230957
	speed: 0.0948s/iter; left time: 50.9280s
	iters: 200, epoch: 9 | loss: 0.1328125
	speed: 0.0885s/iter; left time: 38.6851s
	iters: 300, epoch: 9 | loss: 0.1043809
	speed: 0.0878s/iter; left time: 29.5895s
Epoch: 9 cost time: 28.763145923614502
Epoch: 9, Steps: 318 | Train Loss: 0.1140240 Vali Loss: 0.1298811 Test Loss: 0.0980614
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0984031
	speed: 0.0910s/iter; left time: 19.9291s
	iters: 200, epoch: 10 | loss: 0.1082492
	speed: 0.0911s/iter; left time: 10.8397s
	iters: 300, epoch: 10 | loss: 0.1009038
	speed: 0.0893s/iter; left time: 1.6973s
Epoch: 10 cost time: 28.940746307373047
Epoch: 10, Steps: 318 | Train Loss: 0.1138410 Vali Loss: 0.1297136 Test Loss: 0.0980522
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.500628650188446, mae:0.5438097715377808, rmse:0.7075511813163757, mape:0.020159104838967323, mspe:0.0006925832713022828, rse:0.3147018849849701, r2_score:0.8887622768756035, acc:0.9798408951610327
corr: [40.604805 40.698093 40.67655  41.09337  40.964687 40.903046 40.881058
 40.796223 40.813324 40.563744 40.837955 40.696053 40.865444 40.97001
 40.88729  41.010868 41.039917 41.365547 41.207447 40.94754 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1867572
	speed: 0.1162s/iter; left time: 358.1601s
	iters: 200, epoch: 1 | loss: 0.1771365
	speed: 0.0917s/iter; left time: 273.2469s
	iters: 300, epoch: 1 | loss: 0.1162189
	speed: 0.0898s/iter; left time: 258.6708s
Epoch: 1 cost time: 31.296584367752075
Epoch: 1, Steps: 318 | Train Loss: 0.2397840 Vali Loss: 0.1469166 Test Loss: 0.1101520
Validation loss decreased (inf --> 0.146917).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1316137
	speed: 0.1080s/iter; left time: 298.4365s
	iters: 200, epoch: 2 | loss: 0.1133330
	speed: 0.0914s/iter; left time: 243.5056s
	iters: 300, epoch: 2 | loss: 0.1126348
	speed: 0.0959s/iter; left time: 245.6996s
Epoch: 2 cost time: 31.318172693252563
Epoch: 2, Steps: 318 | Train Loss: 0.1275452 Vali Loss: 0.1378559 Test Loss: 0.1030179
Validation loss decreased (0.146917 --> 0.137856).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1134199
	speed: 0.0870s/iter; left time: 212.7230s
	iters: 200, epoch: 3 | loss: 0.1493894
	speed: 0.0951s/iter; left time: 222.9737s
	iters: 300, epoch: 3 | loss: 0.1548391
	speed: 0.0933s/iter; left time: 209.4674s
Epoch: 3 cost time: 29.170870542526245
Epoch: 3, Steps: 318 | Train Loss: 0.1210147 Vali Loss: 0.1344016 Test Loss: 0.1004221
Validation loss decreased (0.137856 --> 0.134402).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1136036
	speed: 0.0997s/iter; left time: 212.1601s
	iters: 200, epoch: 4 | loss: 0.1314708
	speed: 0.0927s/iter; left time: 187.9233s
	iters: 300, epoch: 4 | loss: 0.1348480
	speed: 0.0809s/iter; left time: 155.8415s
Epoch: 4 cost time: 29.09538769721985
Epoch: 4, Steps: 318 | Train Loss: 0.1181098 Vali Loss: 0.1341532 Test Loss: 0.0996003
Validation loss decreased (0.134402 --> 0.134153).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0971490
	speed: 0.1006s/iter; left time: 181.9795s
	iters: 200, epoch: 5 | loss: 0.1024681
	speed: 0.0950s/iter; left time: 162.4014s
	iters: 300, epoch: 5 | loss: 0.1308751
	speed: 0.0930s/iter; left time: 149.6822s
Epoch: 5 cost time: 30.843119621276855
Epoch: 5, Steps: 318 | Train Loss: 0.1165177 Vali Loss: 0.1315368 Test Loss: 0.0986033
Validation loss decreased (0.134153 --> 0.131537).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1117630
	speed: 0.0999s/iter; left time: 148.9848s
	iters: 200, epoch: 6 | loss: 0.1223970
	speed: 0.0898s/iter; left time: 124.8771s
	iters: 300, epoch: 6 | loss: 0.1067243
	speed: 0.0867s/iter; left time: 111.9192s
Epoch: 6 cost time: 29.511091232299805
Epoch: 6, Steps: 318 | Train Loss: 0.1160321 Vali Loss: 0.1306714 Test Loss: 0.0982692
Validation loss decreased (0.131537 --> 0.130671).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1138935
	speed: 0.0959s/iter; left time: 112.4864s
	iters: 200, epoch: 7 | loss: 0.1391230
	speed: 0.0980s/iter; left time: 105.1811s
	iters: 300, epoch: 7 | loss: 0.1336007
	speed: 0.0980s/iter; left time: 95.3476s
Epoch: 7 cost time: 31.229655265808105
Epoch: 7, Steps: 318 | Train Loss: 0.1155988 Vali Loss: 0.1311831 Test Loss: 0.0983324
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1209908
	speed: 0.1002s/iter; left time: 85.6961s
	iters: 200, epoch: 8 | loss: 0.1219658
	speed: 0.0978s/iter; left time: 73.8512s
	iters: 300, epoch: 8 | loss: 0.1099183
	speed: 0.0914s/iter; left time: 59.8582s
Epoch: 8 cost time: 30.622879028320312
Epoch: 8, Steps: 318 | Train Loss: 0.1153845 Vali Loss: 0.1313131 Test Loss: 0.0981877
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0854819
	speed: 0.1050s/iter; left time: 56.4102s
	iters: 200, epoch: 9 | loss: 0.1216162
	speed: 0.0852s/iter; left time: 37.2471s
	iters: 300, epoch: 9 | loss: 0.1286841
	speed: 0.0859s/iter; left time: 28.9318s
Epoch: 9 cost time: 29.18312668800354
Epoch: 9, Steps: 318 | Train Loss: 0.1153359 Vali Loss: 0.1308400 Test Loss: 0.0982003
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.5019373893737793, mae:0.5444843769073486, rmse:0.7084754109382629, mape:0.02017838880419731, mspe:0.0006936869467608631, rse:0.31489306688308716, r2_score:0.889090224257, acc:0.9798216111958027
corr: [40.775494 40.646    40.571384 40.692535 40.81274  40.84385  40.877853
 40.781345 40.99059  40.644054 40.94805  41.00956  40.94145  40.943024
 40.95217  40.88227  41.14311  41.290985 41.239723 41.17677  41.21763
 41.334293 41.07281  41.289917 41.191513]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2268517
	speed: 0.1157s/iter; left time: 355.3666s
	iters: 200, epoch: 1 | loss: 0.1333390
	speed: 0.0829s/iter; left time: 246.1993s
	iters: 300, epoch: 1 | loss: 0.1770136
	speed: 0.0875s/iter; left time: 251.2623s
Epoch: 1 cost time: 30.504881143569946
Epoch: 1, Steps: 317 | Train Loss: 0.2653655 Vali Loss: 0.1504162 Test Loss: 0.1133840
Validation loss decreased (inf --> 0.150416).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1354515
	speed: 0.1049s/iter; left time: 288.9494s
	iters: 200, epoch: 2 | loss: 0.1352356
	speed: 0.0948s/iter; left time: 251.6003s
	iters: 300, epoch: 2 | loss: 0.1582158
	speed: 0.0892s/iter; left time: 227.8883s
Epoch: 2 cost time: 30.581611156463623
Epoch: 2, Steps: 317 | Train Loss: 0.1331362 Vali Loss: 0.1427845 Test Loss: 0.1066246
Validation loss decreased (0.150416 --> 0.142785).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1217367
	speed: 0.0896s/iter; left time: 218.3515s
	iters: 200, epoch: 3 | loss: 0.1015477
	speed: 0.0818s/iter; left time: 191.0695s
	iters: 300, epoch: 3 | loss: 0.1558501
	speed: 0.0997s/iter; left time: 222.9277s
Epoch: 3 cost time: 28.809678554534912
Epoch: 3, Steps: 317 | Train Loss: 0.1260443 Vali Loss: 0.1392197 Test Loss: 0.1023536
Validation loss decreased (0.142785 --> 0.139220).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1206348
	speed: 0.0982s/iter; left time: 208.2533s
	iters: 200, epoch: 4 | loss: 0.0886616
	speed: 0.0985s/iter; left time: 199.0162s
	iters: 300, epoch: 4 | loss: 0.1468683
	speed: 0.0936s/iter; left time: 179.7116s
Epoch: 4 cost time: 30.49952006340027
Epoch: 4, Steps: 317 | Train Loss: 0.1231699 Vali Loss: 0.1384643 Test Loss: 0.1040383
Validation loss decreased (0.139220 --> 0.138464).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1407146
	speed: 0.1094s/iter; left time: 197.1840s
	iters: 200, epoch: 5 | loss: 0.1385159
	speed: 0.0982s/iter; left time: 167.2329s
	iters: 300, epoch: 5 | loss: 0.1138938
	speed: 0.0956s/iter; left time: 153.1705s
Epoch: 5 cost time: 32.26781511306763
Epoch: 5, Steps: 317 | Train Loss: 0.1218404 Vali Loss: 0.1370894 Test Loss: 0.1018051
Validation loss decreased (0.138464 --> 0.137089).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1199886
	speed: 0.1092s/iter; left time: 162.2830s
	iters: 200, epoch: 6 | loss: 0.1195633
	speed: 0.0909s/iter; left time: 125.9909s
	iters: 300, epoch: 6 | loss: 0.1218571
	speed: 0.0909s/iter; left time: 116.9242s
Epoch: 6 cost time: 30.86875867843628
Epoch: 6, Steps: 317 | Train Loss: 0.1211800 Vali Loss: 0.1357164 Test Loss: 0.1010637
Validation loss decreased (0.137089 --> 0.135716).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1274720
	speed: 0.1002s/iter; left time: 117.1470s
	iters: 200, epoch: 7 | loss: 0.1164353
	speed: 0.0893s/iter; left time: 95.4942s
	iters: 300, epoch: 7 | loss: 0.1052929
	speed: 0.0937s/iter; left time: 90.7525s
Epoch: 7 cost time: 29.919044971466064
Epoch: 7, Steps: 317 | Train Loss: 0.1208620 Vali Loss: 0.1370084 Test Loss: 0.1018716
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1028659
	speed: 0.1046s/iter; left time: 89.1146s
	iters: 200, epoch: 8 | loss: 0.1188027
	speed: 0.0889s/iter; left time: 66.8311s
	iters: 300, epoch: 8 | loss: 0.0951257
	speed: 0.0977s/iter; left time: 63.6763s
Epoch: 8 cost time: 30.669163942337036
Epoch: 8, Steps: 317 | Train Loss: 0.1206199 Vali Loss: 0.1364644 Test Loss: 0.1011662
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1067971
	speed: 0.0988s/iter; left time: 52.8706s
	iters: 200, epoch: 9 | loss: 0.1011044
	speed: 0.0934s/iter; left time: 40.6474s
	iters: 300, epoch: 9 | loss: 0.1056084
	speed: 0.0924s/iter; left time: 30.9386s
Epoch: 9 cost time: 30.199641466140747
Epoch: 9, Steps: 317 | Train Loss: 0.1207526 Vali Loss: 0.1364681 Test Loss: 0.1013670
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.5162110924720764, mae:0.5541283488273621, rmse:0.7184783220291138, mape:0.020511513575911522, mspe:0.0007103504613041878, rse:0.31910207867622375, r2_score:0.8873463115666518, acc:0.9794884864240885
corr: [40.78932  40.61298  40.777233 40.567947 40.792324 40.74262  40.87692
 40.811016 40.76734  40.57075  40.99582  40.84855  40.87479  41.093395
 40.986057 41.00613  40.933434 41.34411  40.984547 41.203457 41.130653
 41.284073 41.024895 41.214855 41.157204 41.520287 41.025295 41.539833
 41.599594 41.45555 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1813038
	speed: 0.1481s/iter; left time: 456.3903s
	iters: 200, epoch: 1 | loss: 0.1396681
	speed: 0.1192s/iter; left time: 355.2090s
	iters: 300, epoch: 1 | loss: 0.1333308
	speed: 0.1277s/iter; left time: 367.9164s
Epoch: 1 cost time: 41.5716598033905
Epoch: 1, Steps: 318 | Train Loss: 0.2324235 Vali Loss: 0.1475832 Test Loss: 0.1080908
Validation loss decreased (inf --> 0.147583).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1107863
	speed: 0.1128s/iter; left time: 311.7978s
	iters: 200, epoch: 2 | loss: 0.0851180
	speed: 0.1241s/iter; left time: 330.5623s
	iters: 300, epoch: 2 | loss: 0.1178676
	speed: 0.1299s/iter; left time: 332.9929s
Epoch: 2 cost time: 39.182496309280396
Epoch: 2, Steps: 318 | Train Loss: 0.1248790 Vali Loss: 0.1338282 Test Loss: 0.0965887
Validation loss decreased (0.147583 --> 0.133828).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0827941
	speed: 0.1202s/iter; left time: 293.9382s
	iters: 200, epoch: 3 | loss: 0.1024181
	speed: 0.1137s/iter; left time: 266.7272s
	iters: 300, epoch: 3 | loss: 0.0709876
	speed: 0.1319s/iter; left time: 296.1590s
Epoch: 3 cost time: 38.13447856903076
Epoch: 3, Steps: 318 | Train Loss: 0.1133172 Vali Loss: 0.1290378 Test Loss: 0.0912895
Validation loss decreased (0.133828 --> 0.129038).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1013699
	speed: 0.1157s/iter; left time: 246.1830s
	iters: 200, epoch: 4 | loss: 0.0885717
	speed: 0.1168s/iter; left time: 236.8383s
	iters: 300, epoch: 4 | loss: 0.1222261
	speed: 0.1245s/iter; left time: 239.9449s
Epoch: 4 cost time: 37.942511320114136
Epoch: 4, Steps: 318 | Train Loss: 0.1084288 Vali Loss: 0.1246679 Test Loss: 0.0886710
Validation loss decreased (0.129038 --> 0.124668).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1257052
	speed: 0.1606s/iter; left time: 290.5731s
	iters: 200, epoch: 5 | loss: 0.0816176
	speed: 0.1466s/iter; left time: 250.5337s
	iters: 300, epoch: 5 | loss: 0.0818769
	speed: 0.1429s/iter; left time: 229.9797s
Epoch: 5 cost time: 47.7589054107666
Epoch: 5, Steps: 318 | Train Loss: 0.1065133 Vali Loss: 0.1243192 Test Loss: 0.0883177
Validation loss decreased (0.124668 --> 0.124319).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0982698
	speed: 0.1416s/iter; left time: 211.1807s
	iters: 200, epoch: 6 | loss: 0.0755692
	speed: 0.1334s/iter; left time: 185.4911s
	iters: 300, epoch: 6 | loss: 0.1063219
	speed: 0.1500s/iter; left time: 193.6954s
Epoch: 6 cost time: 45.66191649436951
Epoch: 6, Steps: 318 | Train Loss: 0.1055193 Vali Loss: 0.1232431 Test Loss: 0.0875791
Validation loss decreased (0.124319 --> 0.123243).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1175379
	speed: 0.1562s/iter; left time: 183.2290s
	iters: 200, epoch: 7 | loss: 0.1060415
	speed: 0.1410s/iter; left time: 151.3024s
	iters: 300, epoch: 7 | loss: 0.1119021
	speed: 0.1344s/iter; left time: 130.7363s
Epoch: 7 cost time: 45.56957459449768
Epoch: 7, Steps: 318 | Train Loss: 0.1049405 Vali Loss: 0.1227964 Test Loss: 0.0877544
Validation loss decreased (0.123243 --> 0.122796).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1257241
	speed: 0.1423s/iter; left time: 121.6318s
	iters: 200, epoch: 8 | loss: 0.1033793
	speed: 0.1294s/iter; left time: 97.6765s
	iters: 300, epoch: 8 | loss: 0.1013034
	speed: 0.1258s/iter; left time: 82.3762s
Epoch: 8 cost time: 41.51491355895996
Epoch: 8, Steps: 318 | Train Loss: 0.1050122 Vali Loss: 0.1238993 Test Loss: 0.0875795
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1010227
	speed: 0.1290s/iter; left time: 69.2669s
	iters: 200, epoch: 9 | loss: 0.1283295
	speed: 0.1322s/iter; left time: 57.7909s
	iters: 300, epoch: 9 | loss: 0.0750432
	speed: 0.1202s/iter; left time: 40.4997s
Epoch: 9 cost time: 40.18151926994324
Epoch: 9, Steps: 318 | Train Loss: 0.1043989 Vali Loss: 0.1233365 Test Loss: 0.0874985
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0874160
	speed: 0.1301s/iter; left time: 28.4948s
	iters: 200, epoch: 10 | loss: 0.0904980
	speed: 0.1161s/iter; left time: 13.8156s
	iters: 300, epoch: 10 | loss: 0.1362289
	speed: 0.1259s/iter; left time: 2.3918s
Epoch: 10 cost time: 39.21815633773804
Epoch: 10, Steps: 318 | Train Loss: 0.1044995 Vali Loss: 0.1235958 Test Loss: 0.0874728
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.4482300579547882, mae:0.5197382569313049, rmse:0.6694998741149902, mape:0.01928071677684784, mspe:0.0006216058973222971, rse:0.29811930656433105, r2_score:0.9019743697823316, acc:0.9807192832231522
corr: [40.48735  40.749977 40.76788  40.788296 40.85987  40.95115  40.785652
 41.038517 40.995754 40.898666]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2050358
	speed: 0.1356s/iter; left time: 417.9281s
	iters: 200, epoch: 1 | loss: 0.1494553
	speed: 0.1048s/iter; left time: 312.5386s
	iters: 300, epoch: 1 | loss: 0.1426306
	speed: 0.0986s/iter; left time: 284.1890s
Epoch: 1 cost time: 35.77151679992676
Epoch: 1, Steps: 318 | Train Loss: 0.2522815 Vali Loss: 0.1410294 Test Loss: 0.1089698
Validation loss decreased (inf --> 0.141029).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1259176
	speed: 0.1213s/iter; left time: 335.2041s
	iters: 200, epoch: 2 | loss: 0.1081825
	speed: 0.1037s/iter; left time: 276.0671s
	iters: 300, epoch: 2 | loss: 0.1138711
	speed: 0.1101s/iter; left time: 282.1030s
Epoch: 2 cost time: 35.473881244659424
Epoch: 2, Steps: 318 | Train Loss: 0.1231979 Vali Loss: 0.1312316 Test Loss: 0.0961594
Validation loss decreased (0.141029 --> 0.131232).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1531457
	speed: 0.1224s/iter; left time: 299.3590s
	iters: 200, epoch: 3 | loss: 0.1153633
	speed: 0.1027s/iter; left time: 240.7442s
	iters: 300, epoch: 3 | loss: 0.1095057
	speed: 0.1102s/iter; left time: 247.4794s
Epoch: 3 cost time: 35.72837567329407
Epoch: 3, Steps: 318 | Train Loss: 0.1145586 Vali Loss: 0.1260575 Test Loss: 0.0911349
Validation loss decreased (0.131232 --> 0.126057).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1126385
	speed: 0.1110s/iter; left time: 236.0762s
	iters: 200, epoch: 4 | loss: 0.1249229
	speed: 0.1018s/iter; left time: 206.3665s
	iters: 300, epoch: 4 | loss: 0.1435371
	speed: 0.1133s/iter; left time: 218.2932s
Epoch: 4 cost time: 33.847716093063354
Epoch: 4, Steps: 318 | Train Loss: 0.1104049 Vali Loss: 0.1229026 Test Loss: 0.0898822
Validation loss decreased (0.126057 --> 0.122903).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1281490
	speed: 0.1095s/iter; left time: 198.1488s
	iters: 200, epoch: 5 | loss: 0.1275413
	speed: 0.1011s/iter; left time: 172.7584s
	iters: 300, epoch: 5 | loss: 0.1139603
	speed: 0.0991s/iter; left time: 159.5275s
Epoch: 5 cost time: 32.982643127441406
Epoch: 5, Steps: 318 | Train Loss: 0.1088815 Vali Loss: 0.1222705 Test Loss: 0.0891607
Validation loss decreased (0.122903 --> 0.122271).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1010027
	speed: 0.1142s/iter; left time: 170.2761s
	iters: 200, epoch: 6 | loss: 0.1161023
	speed: 0.1003s/iter; left time: 139.5176s
	iters: 300, epoch: 6 | loss: 0.0687276
	speed: 0.1104s/iter; left time: 142.5580s
Epoch: 6 cost time: 34.40659523010254
Epoch: 6, Steps: 318 | Train Loss: 0.1078227 Vali Loss: 0.1217328 Test Loss: 0.0888510
Validation loss decreased (0.122271 --> 0.121733).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0988346
	speed: 0.1138s/iter; left time: 133.4815s
	iters: 200, epoch: 7 | loss: 0.1118511
	speed: 0.1095s/iter; left time: 117.4545s
	iters: 300, epoch: 7 | loss: 0.0826133
	speed: 0.1085s/iter; left time: 105.6145s
Epoch: 7 cost time: 35.10480260848999
Epoch: 7, Steps: 318 | Train Loss: 0.1074200 Vali Loss: 0.1214903 Test Loss: 0.0886071
Validation loss decreased (0.121733 --> 0.121490).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1094951
	speed: 0.1173s/iter; left time: 100.2992s
	iters: 200, epoch: 8 | loss: 0.1172053
	speed: 0.1015s/iter; left time: 76.6023s
	iters: 300, epoch: 8 | loss: 0.0996484
	speed: 0.0967s/iter; left time: 63.3560s
Epoch: 8 cost time: 33.680943965911865
Epoch: 8, Steps: 318 | Train Loss: 0.1072445 Vali Loss: 0.1215069 Test Loss: 0.0886611
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1253768
	speed: 0.1064s/iter; left time: 57.1310s
	iters: 200, epoch: 9 | loss: 0.0891502
	speed: 0.0989s/iter; left time: 43.2197s
	iters: 300, epoch: 9 | loss: 0.0971674
	speed: 0.1061s/iter; left time: 35.7430s
Epoch: 9 cost time: 33.42007756233215
Epoch: 9, Steps: 318 | Train Loss: 0.1071267 Vali Loss: 0.1209736 Test Loss: 0.0885889
Validation loss decreased (0.121490 --> 0.120974).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1031126
	speed: 0.1113s/iter; left time: 24.3831s
	iters: 200, epoch: 10 | loss: 0.1219798
	speed: 0.1089s/iter; left time: 12.9632s
	iters: 300, epoch: 10 | loss: 0.0928113
	speed: 0.1079s/iter; left time: 2.0493s
Epoch: 10 cost time: 34.58844614028931
Epoch: 10, Steps: 318 | Train Loss: 0.1069958 Vali Loss: 0.1205972 Test Loss: 0.0885362
Validation loss decreased (0.120974 --> 0.120597).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.4522230327129364, mae:0.5179908275604248, rmse:0.672475278377533, mape:0.019221371039748192, mspe:0.0006272290484048426, rse:0.2992878556251526, r2_score:0.9008636344678745, acc:0.9807786289602518
corr: [40.642784 40.649036 40.60198  40.597534 40.578884 40.60708  40.680523
 40.75381  40.842396 40.703312 41.29344  41.097824 41.068874 41.22049
 41.16801 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1637048
	speed: 0.1411s/iter; left time: 434.6319s
	iters: 200, epoch: 1 | loss: 0.1707096
	speed: 0.1039s/iter; left time: 309.6194s
	iters: 300, epoch: 1 | loss: 0.1156727
	speed: 0.0954s/iter; left time: 274.8983s
Epoch: 1 cost time: 35.81397247314453
Epoch: 1, Steps: 318 | Train Loss: 0.2500594 Vali Loss: 0.1476291 Test Loss: 0.1113141
Validation loss decreased (inf --> 0.147629).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1135588
	speed: 0.1093s/iter; left time: 301.8789s
	iters: 200, epoch: 2 | loss: 0.1450554
	speed: 0.0970s/iter; left time: 258.3626s
	iters: 300, epoch: 2 | loss: 0.1133870
	speed: 0.0910s/iter; left time: 233.1208s
Epoch: 2 cost time: 31.572887182235718
Epoch: 2, Steps: 318 | Train Loss: 0.1289959 Vali Loss: 0.1340109 Test Loss: 0.1031726
Validation loss decreased (0.147629 --> 0.134011).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0840972
	speed: 0.1029s/iter; left time: 251.5791s
	iters: 200, epoch: 3 | loss: 0.1087309
	speed: 0.0968s/iter; left time: 227.0419s
	iters: 300, epoch: 3 | loss: 0.1156133
	speed: 0.1018s/iter; left time: 228.4659s
Epoch: 3 cost time: 32.03031039237976
Epoch: 3, Steps: 318 | Train Loss: 0.1206100 Vali Loss: 0.1311022 Test Loss: 0.0997555
Validation loss decreased (0.134011 --> 0.131102).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1311515
	speed: 0.1015s/iter; left time: 215.9281s
	iters: 200, epoch: 4 | loss: 0.1339558
	speed: 0.0980s/iter; left time: 198.6394s
	iters: 300, epoch: 4 | loss: 0.1215085
	speed: 0.0996s/iter; left time: 191.8434s
Epoch: 4 cost time: 31.64837622642517
Epoch: 4, Steps: 318 | Train Loss: 0.1174763 Vali Loss: 0.1308056 Test Loss: 0.0997891
Validation loss decreased (0.131102 --> 0.130806).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1303882
	speed: 0.1167s/iter; left time: 211.0411s
	iters: 200, epoch: 5 | loss: 0.1037528
	speed: 0.0932s/iter; left time: 159.2694s
	iters: 300, epoch: 5 | loss: 0.1106599
	speed: 0.0986s/iter; left time: 158.6560s
Epoch: 5 cost time: 32.580629110336304
Epoch: 5, Steps: 318 | Train Loss: 0.1158389 Vali Loss: 0.1306545 Test Loss: 0.0982101
Validation loss decreased (0.130806 --> 0.130655).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1264299
	speed: 0.1162s/iter; left time: 173.2051s
	iters: 200, epoch: 6 | loss: 0.1094531
	speed: 0.0960s/iter; left time: 133.5204s
	iters: 300, epoch: 6 | loss: 0.1764129
	speed: 0.0996s/iter; left time: 128.5838s
Epoch: 6 cost time: 32.9205117225647
Epoch: 6, Steps: 318 | Train Loss: 0.1148951 Vali Loss: 0.1300436 Test Loss: 0.0979224
Validation loss decreased (0.130655 --> 0.130044).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0970662
	speed: 0.0975s/iter; left time: 114.3479s
	iters: 200, epoch: 7 | loss: 0.1386871
	speed: 0.1052s/iter; left time: 112.8679s
	iters: 300, epoch: 7 | loss: 0.1092018
	speed: 0.1013s/iter; left time: 98.5903s
Epoch: 7 cost time: 32.313661336898804
Epoch: 7, Steps: 318 | Train Loss: 0.1145717 Vali Loss: 0.1289821 Test Loss: 0.0980559
Validation loss decreased (0.130044 --> 0.128982).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1224938
	speed: 0.1030s/iter; left time: 88.0674s
	iters: 200, epoch: 8 | loss: 0.1540928
	speed: 0.0995s/iter; left time: 75.1585s
	iters: 300, epoch: 8 | loss: 0.0900706
	speed: 0.0966s/iter; left time: 63.2480s
Epoch: 8 cost time: 31.758161067962646
Epoch: 8, Steps: 318 | Train Loss: 0.1143935 Vali Loss: 0.1288731 Test Loss: 0.0979473
Validation loss decreased (0.128982 --> 0.128873).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1225965
	speed: 0.1049s/iter; left time: 56.3415s
	iters: 200, epoch: 9 | loss: 0.1329731
	speed: 0.0982s/iter; left time: 42.9115s
	iters: 300, epoch: 9 | loss: 0.1045423
	speed: 0.0990s/iter; left time: 33.3659s
Epoch: 9 cost time: 32.37106728553772
Epoch: 9, Steps: 318 | Train Loss: 0.1142902 Vali Loss: 0.1300205 Test Loss: 0.0979889
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0986899
	speed: 0.1052s/iter; left time: 23.0304s
	iters: 200, epoch: 10 | loss: 0.1077347
	speed: 0.0988s/iter; left time: 11.7618s
	iters: 300, epoch: 10 | loss: 0.1012216
	speed: 0.0973s/iter; left time: 1.8494s
Epoch: 10 cost time: 32.16035771369934
Epoch: 10, Steps: 318 | Train Loss: 0.1140916 Vali Loss: 0.1298493 Test Loss: 0.0979794
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.5002931356430054, mae:0.5437888503074646, rmse:0.7073140144348145, mape:0.020157208666205406, mspe:0.0006920196465216577, rse:0.31459638476371765, r2_score:0.8888113772684146, acc:0.9798427913337946
corr: [40.617268 40.69759  40.684616 41.100212 40.97274  40.91745  40.88722
 40.807205 40.831604 40.570526 40.8477   40.70795  40.880367 40.9785
 40.913208 41.017193 41.055386 41.3866   41.206974 40.960266]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1895500
	speed: 0.1252s/iter; left time: 385.8728s
	iters: 200, epoch: 1 | loss: 0.1928121
	speed: 0.1102s/iter; left time: 328.6249s
	iters: 300, epoch: 1 | loss: 0.1275372
	speed: 0.1011s/iter; left time: 291.3600s
Epoch: 1 cost time: 35.55037713050842
Epoch: 1, Steps: 318 | Train Loss: 0.2424433 Vali Loss: 0.1615332 Test Loss: 0.1180873
Validation loss decreased (inf --> 0.161533).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1392758
	speed: 0.1049s/iter; left time: 289.8374s
	iters: 200, epoch: 2 | loss: 0.1267383
	speed: 0.0870s/iter; left time: 231.6138s
	iters: 300, epoch: 2 | loss: 0.1189661
	speed: 0.1012s/iter; left time: 259.3583s
Epoch: 2 cost time: 31.307132244110107
Epoch: 2, Steps: 318 | Train Loss: 0.1345934 Vali Loss: 0.1485654 Test Loss: 0.1091883
Validation loss decreased (0.161533 --> 0.148565).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1233876
	speed: 0.0992s/iter; left time: 242.4383s
	iters: 200, epoch: 3 | loss: 0.1644008
	speed: 0.0932s/iter; left time: 218.6398s
	iters: 300, epoch: 3 | loss: 0.1655216
	speed: 0.0958s/iter; left time: 215.0865s
Epoch: 3 cost time: 30.58088994026184
Epoch: 3, Steps: 318 | Train Loss: 0.1272261 Vali Loss: 0.1459581 Test Loss: 0.1063476
Validation loss decreased (0.148565 --> 0.145958).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1255130
	speed: 0.1021s/iter; left time: 217.2623s
	iters: 200, epoch: 4 | loss: 0.1542777
	speed: 0.0972s/iter; left time: 197.0593s
	iters: 300, epoch: 4 | loss: 0.1399548
	speed: 0.0958s/iter; left time: 184.6995s
Epoch: 4 cost time: 31.11016869544983
Epoch: 4, Steps: 318 | Train Loss: 0.1239630 Vali Loss: 0.1440565 Test Loss: 0.1052170
Validation loss decreased (0.145958 --> 0.144057).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0983892
	speed: 0.1032s/iter; left time: 186.7747s
	iters: 200, epoch: 5 | loss: 0.1178527
	speed: 0.0974s/iter; left time: 166.4763s
	iters: 300, epoch: 5 | loss: 0.1490368
	speed: 0.0965s/iter; left time: 155.2094s
Epoch: 5 cost time: 31.602362394332886
Epoch: 5, Steps: 318 | Train Loss: 0.1219729 Vali Loss: 0.1421530 Test Loss: 0.1047673
Validation loss decreased (0.144057 --> 0.142153).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1150045
	speed: 0.1080s/iter; left time: 161.1001s
	iters: 200, epoch: 6 | loss: 0.1184811
	speed: 0.0955s/iter; left time: 132.8432s
	iters: 300, epoch: 6 | loss: 0.1090684
	speed: 0.0990s/iter; left time: 127.8283s
Epoch: 6 cost time: 32.036001205444336
Epoch: 6, Steps: 318 | Train Loss: 0.1212211 Vali Loss: 0.1411021 Test Loss: 0.1043911
Validation loss decreased (0.142153 --> 0.141102).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1251459
	speed: 0.0947s/iter; left time: 111.0288s
	iters: 200, epoch: 7 | loss: 0.1281586
	speed: 0.1070s/iter; left time: 114.7725s
	iters: 300, epoch: 7 | loss: 0.1278170
	speed: 0.0995s/iter; left time: 96.7970s
Epoch: 7 cost time: 32.044639110565186
Epoch: 7, Steps: 318 | Train Loss: 0.1206225 Vali Loss: 0.1416126 Test Loss: 0.1048328
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1232129
	speed: 0.1019s/iter; left time: 87.1326s
	iters: 200, epoch: 8 | loss: 0.1254128
	speed: 0.1007s/iter; left time: 76.0054s
	iters: 300, epoch: 8 | loss: 0.1188831
	speed: 0.0989s/iter; left time: 64.7806s
Epoch: 8 cost time: 32.115575075149536
Epoch: 8, Steps: 318 | Train Loss: 0.1203029 Vali Loss: 0.1416074 Test Loss: 0.1045529
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0894337
	speed: 0.1071s/iter; left time: 57.5005s
	iters: 200, epoch: 9 | loss: 0.1293840
	speed: 0.0975s/iter; left time: 42.5866s
	iters: 300, epoch: 9 | loss: 0.1294170
	speed: 0.0955s/iter; left time: 32.1890s
Epoch: 9 cost time: 32.03729581832886
Epoch: 9, Steps: 318 | Train Loss: 0.1203488 Vali Loss: 0.1412270 Test Loss: 0.1045887
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.5332065224647522, mae:0.5663790702819824, rmse:0.7302098870277405, mape:0.020959457382559776, mspe:0.0007329151267185807, rse:0.32455331087112427, r2_score:0.8831993261291562, acc:0.9790405426174402
corr: [41.003284 41.006805 41.00491  41.1052   41.16118  41.17835  41.09327
 40.981976 41.016434 40.81134  41.024136 41.03364  41.099674 41.146175
 41.1132   41.12715  41.367554 41.38934  41.19006  41.11426  41.289444
 41.37236  41.192276 41.405457 41.320427]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2201248
	speed: 0.1329s/iter; left time: 408.0012s
	iters: 200, epoch: 1 | loss: 0.1339604
	speed: 0.1006s/iter; left time: 298.9271s
	iters: 300, epoch: 1 | loss: 0.1798740
	speed: 0.1169s/iter; left time: 335.5613s
Epoch: 1 cost time: 37.07149624824524
Epoch: 1, Steps: 317 | Train Loss: 0.2692468 Vali Loss: 0.1527225 Test Loss: 0.1143502
Validation loss decreased (inf --> 0.152722).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1408750
	speed: 0.1198s/iter; left time: 329.8905s
	iters: 200, epoch: 2 | loss: 0.1406257
	speed: 0.1049s/iter; left time: 278.4234s
	iters: 300, epoch: 2 | loss: 0.1573825
	speed: 0.1222s/iter; left time: 312.0817s
Epoch: 2 cost time: 36.64222264289856
Epoch: 2, Steps: 317 | Train Loss: 0.1346897 Vali Loss: 0.1440223 Test Loss: 0.1054863
Validation loss decreased (0.152722 --> 0.144022).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1220665
	speed: 0.1200s/iter; left time: 292.4294s
	iters: 200, epoch: 3 | loss: 0.1008312
	speed: 0.1136s/iter; left time: 265.3726s
	iters: 300, epoch: 3 | loss: 0.1583711
	speed: 0.1189s/iter; left time: 266.0740s
Epoch: 3 cost time: 37.24220514297485
Epoch: 3, Steps: 317 | Train Loss: 0.1271872 Vali Loss: 0.1409842 Test Loss: 0.1023575
Validation loss decreased (0.144022 --> 0.140984).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1188093
	speed: 0.1177s/iter; left time: 249.6266s
	iters: 200, epoch: 4 | loss: 0.0894830
	speed: 0.1120s/iter; left time: 226.3152s
	iters: 300, epoch: 4 | loss: 0.1485164
	speed: 0.1022s/iter; left time: 196.3006s
Epoch: 4 cost time: 35.59585881233215
Epoch: 4, Steps: 317 | Train Loss: 0.1241657 Vali Loss: 0.1398338 Test Loss: 0.1038619
Validation loss decreased (0.140984 --> 0.139834).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1384750
	speed: 0.1209s/iter; left time: 217.9186s
	iters: 200, epoch: 5 | loss: 0.1382255
	speed: 0.1066s/iter; left time: 181.5358s
	iters: 300, epoch: 5 | loss: 0.1100585
	speed: 0.1118s/iter; left time: 179.2606s
Epoch: 5 cost time: 35.891420125961304
Epoch: 5, Steps: 317 | Train Loss: 0.1226307 Vali Loss: 0.1385556 Test Loss: 0.1018981
Validation loss decreased (0.139834 --> 0.138556).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1215430
	speed: 0.1224s/iter; left time: 181.8283s
	iters: 200, epoch: 6 | loss: 0.1191345
	speed: 0.1100s/iter; left time: 152.4218s
	iters: 300, epoch: 6 | loss: 0.1237750
	speed: 0.1115s/iter; left time: 143.3522s
Epoch: 6 cost time: 35.97038555145264
Epoch: 6, Steps: 317 | Train Loss: 0.1219150 Vali Loss: 0.1373128 Test Loss: 0.1010589
Validation loss decreased (0.138556 --> 0.137313).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1267475
	speed: 0.1155s/iter; left time: 135.0503s
	iters: 200, epoch: 7 | loss: 0.1169660
	speed: 0.1109s/iter; left time: 118.5656s
	iters: 300, epoch: 7 | loss: 0.1062642
	speed: 0.1181s/iter; left time: 114.4083s
Epoch: 7 cost time: 36.39239692687988
Epoch: 7, Steps: 317 | Train Loss: 0.1214848 Vali Loss: 0.1386315 Test Loss: 0.1020101
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1032342
	speed: 0.1369s/iter; left time: 116.6325s
	iters: 200, epoch: 8 | loss: 0.1264610
	speed: 0.1224s/iter; left time: 92.0708s
	iters: 300, epoch: 8 | loss: 0.0943680
	speed: 0.1374s/iter; left time: 89.6001s
Epoch: 8 cost time: 41.87116479873657
Epoch: 8, Steps: 317 | Train Loss: 0.1213300 Vali Loss: 0.1382061 Test Loss: 0.1013833
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1065682
	speed: 0.1349s/iter; left time: 72.1485s
	iters: 200, epoch: 9 | loss: 0.0994368
	speed: 0.1286s/iter; left time: 55.9624s
	iters: 300, epoch: 9 | loss: 0.1048444
	speed: 0.1305s/iter; left time: 43.7289s
Epoch: 9 cost time: 41.54484033584595
Epoch: 9, Steps: 317 | Train Loss: 0.1213554 Vali Loss: 0.1381249 Test Loss: 0.1015895
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.5161861181259155, mae:0.5538557171821594, rmse:0.7184609174728394, mape:0.020493479445576668, mspe:0.0007090198341757059, rse:0.31909435987472534, r2_score:0.8870717923766601, acc:0.9795065205544233
corr: [40.831516 40.67393  40.835144 40.753242 40.873447 40.757347 40.937252
 40.74821  40.931107 40.81946  40.957493 40.973488 40.823345 40.979866
 41.025623 41.0004   41.02736  41.32724  40.953545 41.066574 41.051792
 41.18698  40.936497 41.16036  40.909622 41.4888   41.149487 41.43888
 41.285313 41.32169 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1821927
	speed: 0.1597s/iter; left time: 491.9583s
	iters: 200, epoch: 1 | loss: 0.1411141
	speed: 0.1185s/iter; left time: 353.3605s
	iters: 300, epoch: 1 | loss: 0.1341202
	speed: 0.1321s/iter; left time: 380.6143s
Epoch: 1 cost time: 43.45203757286072
Epoch: 1, Steps: 318 | Train Loss: 0.2334924 Vali Loss: 0.1478522 Test Loss: 0.1086103
Validation loss decreased (inf --> 0.147852).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1116720
	speed: 0.1499s/iter; left time: 414.2710s
	iters: 200, epoch: 2 | loss: 0.0858483
	speed: 0.1248s/iter; left time: 332.2439s
	iters: 300, epoch: 2 | loss: 0.1177237
	speed: 0.1248s/iter; left time: 319.9407s
Epoch: 2 cost time: 42.21206068992615
Epoch: 2, Steps: 318 | Train Loss: 0.1255388 Vali Loss: 0.1342044 Test Loss: 0.0971528
Validation loss decreased (0.147852 --> 0.134204).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0821657
	speed: 0.1316s/iter; left time: 321.6454s
	iters: 200, epoch: 3 | loss: 0.1028362
	speed: 0.1258s/iter; left time: 295.0257s
	iters: 300, epoch: 3 | loss: 0.0713978
	speed: 0.1219s/iter; left time: 273.7534s
Epoch: 3 cost time: 40.28345227241516
Epoch: 3, Steps: 318 | Train Loss: 0.1138418 Vali Loss: 0.1293525 Test Loss: 0.0917548
Validation loss decreased (0.134204 --> 0.129352).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1015687
	speed: 0.1410s/iter; left time: 299.8732s
	iters: 200, epoch: 4 | loss: 0.0884271
	speed: 0.1283s/iter; left time: 260.0496s
	iters: 300, epoch: 4 | loss: 0.1227665
	speed: 0.1298s/iter; left time: 250.1722s
Epoch: 4 cost time: 42.12096118927002
Epoch: 4, Steps: 318 | Train Loss: 0.1088510 Vali Loss: 0.1249396 Test Loss: 0.0890925
Validation loss decreased (0.129352 --> 0.124940).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1265282
	speed: 0.1386s/iter; left time: 250.6494s
	iters: 200, epoch: 5 | loss: 0.0822135
	speed: 0.1270s/iter; left time: 217.0992s
	iters: 300, epoch: 5 | loss: 0.0819076
	speed: 0.1095s/iter; left time: 176.2176s
Epoch: 5 cost time: 39.47657012939453
Epoch: 5, Steps: 318 | Train Loss: 0.1069157 Vali Loss: 0.1246322 Test Loss: 0.0887603
Validation loss decreased (0.124940 --> 0.124632).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0985545
	speed: 0.1167s/iter; left time: 174.0299s
	iters: 200, epoch: 6 | loss: 0.0768132
	speed: 0.1137s/iter; left time: 158.2042s
	iters: 300, epoch: 6 | loss: 0.1060406
	speed: 0.1044s/iter; left time: 134.7380s
Epoch: 6 cost time: 35.45393466949463
Epoch: 6, Steps: 318 | Train Loss: 0.1058768 Vali Loss: 0.1235106 Test Loss: 0.0880010
Validation loss decreased (0.124632 --> 0.123511).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1175739
	speed: 0.1178s/iter; left time: 138.1727s
	iters: 200, epoch: 7 | loss: 0.1065552
	speed: 0.1025s/iter; left time: 110.0150s
	iters: 300, epoch: 7 | loss: 0.1119387
	speed: 0.1130s/iter; left time: 109.9294s
Epoch: 7 cost time: 35.39768671989441
Epoch: 7, Steps: 318 | Train Loss: 0.1052832 Vali Loss: 0.1231141 Test Loss: 0.0881925
Validation loss decreased (0.123511 --> 0.123114).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1263598
	speed: 0.1179s/iter; left time: 100.7853s
	iters: 200, epoch: 8 | loss: 0.1037016
	speed: 0.1107s/iter; left time: 83.5431s
	iters: 300, epoch: 8 | loss: 0.1006347
	speed: 0.1011s/iter; left time: 66.2152s
Epoch: 8 cost time: 34.920377016067505
Epoch: 8, Steps: 318 | Train Loss: 0.1053567 Vali Loss: 0.1242041 Test Loss: 0.0880174
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1017636
	speed: 0.1068s/iter; left time: 57.3560s
	iters: 200, epoch: 9 | loss: 0.1286251
	speed: 0.1104s/iter; left time: 48.2635s
	iters: 300, epoch: 9 | loss: 0.0753559
	speed: 0.1116s/iter; left time: 37.5983s
Epoch: 9 cost time: 34.96472215652466
Epoch: 9, Steps: 318 | Train Loss: 0.1047553 Vali Loss: 0.1236566 Test Loss: 0.0879329
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0875600
	speed: 0.1218s/iter; left time: 26.6636s
	iters: 200, epoch: 10 | loss: 0.0909714
	speed: 0.0936s/iter; left time: 11.1375s
	iters: 300, epoch: 10 | loss: 0.1377713
	speed: 0.1104s/iter; left time: 2.0971s
Epoch: 10 cost time: 34.56268310546875
Epoch: 10, Steps: 318 | Train Loss: 0.1048503 Vali Loss: 0.1239126 Test Loss: 0.0879085
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.4504677951335907, mae:0.5206407308578491, rmse:0.6711689829826355, mape:0.019311631098389626, mspe:0.0006243931129574776, rse:0.2988625466823578, r2_score:0.9013912722959354, acc:0.9806883689016104
corr: [40.462456 40.73349  40.76725  40.787785 40.866642 40.95672  40.780582
 41.05717  40.998337 40.905968]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2046456
	speed: 0.1163s/iter; left time: 358.4738s
	iters: 200, epoch: 1 | loss: 0.1496560
	speed: 0.1060s/iter; left time: 316.0139s
	iters: 300, epoch: 1 | loss: 0.1425959
	speed: 0.1082s/iter; left time: 311.7515s
Epoch: 1 cost time: 34.96364426612854
Epoch: 1, Steps: 318 | Train Loss: 0.2530837 Vali Loss: 0.1411628 Test Loss: 0.1091911
Validation loss decreased (inf --> 0.141163).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1255481
	speed: 0.1065s/iter; left time: 294.3129s
	iters: 200, epoch: 2 | loss: 0.1084672
	speed: 0.1036s/iter; left time: 275.8307s
	iters: 300, epoch: 2 | loss: 0.1140259
	speed: 0.0975s/iter; left time: 250.0144s
Epoch: 2 cost time: 32.58955693244934
Epoch: 2, Steps: 318 | Train Loss: 0.1232360 Vali Loss: 0.1313284 Test Loss: 0.0963662
Validation loss decreased (0.141163 --> 0.131328).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1529450
	speed: 0.0996s/iter; left time: 243.5041s
	iters: 200, epoch: 3 | loss: 0.1151163
	speed: 0.1014s/iter; left time: 237.7779s
	iters: 300, epoch: 3 | loss: 0.1090976
	speed: 0.1021s/iter; left time: 229.1286s
Epoch: 3 cost time: 31.92946720123291
Epoch: 3, Steps: 318 | Train Loss: 0.1146024 Vali Loss: 0.1262517 Test Loss: 0.0913130
Validation loss decreased (0.131328 --> 0.126252).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1122982
	speed: 0.1077s/iter; left time: 229.0237s
	iters: 200, epoch: 4 | loss: 0.1248915
	speed: 0.0971s/iter; left time: 196.8082s
	iters: 300, epoch: 4 | loss: 0.1438617
	speed: 0.0999s/iter; left time: 192.5285s
Epoch: 4 cost time: 32.41681623458862
Epoch: 4, Steps: 318 | Train Loss: 0.1104652 Vali Loss: 0.1230627 Test Loss: 0.0900272
Validation loss decreased (0.126252 --> 0.123063).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1278232
	speed: 0.1029s/iter; left time: 186.1662s
	iters: 200, epoch: 5 | loss: 0.1275796
	speed: 0.1016s/iter; left time: 173.5811s
	iters: 300, epoch: 5 | loss: 0.1141949
	speed: 0.0956s/iter; left time: 153.7679s
Epoch: 5 cost time: 31.78197693824768
Epoch: 5, Steps: 318 | Train Loss: 0.1089379 Vali Loss: 0.1223954 Test Loss: 0.0892865
Validation loss decreased (0.123063 --> 0.122395).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1009725
	speed: 0.1156s/iter; left time: 172.3748s
	iters: 200, epoch: 6 | loss: 0.1169169
	speed: 0.1238s/iter; left time: 172.2433s
	iters: 300, epoch: 6 | loss: 0.0689124
	speed: 0.1259s/iter; left time: 162.5854s
Epoch: 6 cost time: 38.57636857032776
Epoch: 6, Steps: 318 | Train Loss: 0.1078733 Vali Loss: 0.1218615 Test Loss: 0.0889880
Validation loss decreased (0.122395 --> 0.121861).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0984533
	speed: 0.1403s/iter; left time: 164.6117s
	iters: 200, epoch: 7 | loss: 0.1118822
	speed: 0.1365s/iter; left time: 146.4426s
	iters: 300, epoch: 7 | loss: 0.0830550
	speed: 0.1265s/iter; left time: 123.0653s
Epoch: 7 cost time: 42.78082513809204
Epoch: 7, Steps: 318 | Train Loss: 0.1074718 Vali Loss: 0.1216154 Test Loss: 0.0887373
Validation loss decreased (0.121861 --> 0.121615).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1099495
	speed: 0.1527s/iter; left time: 130.5686s
	iters: 200, epoch: 8 | loss: 0.1177492
	speed: 0.1358s/iter; left time: 102.5361s
	iters: 300, epoch: 8 | loss: 0.0998088
	speed: 0.1333s/iter; left time: 87.3432s
Epoch: 8 cost time: 44.685547828674316
Epoch: 8, Steps: 318 | Train Loss: 0.1073043 Vali Loss: 0.1216110 Test Loss: 0.0887915
Validation loss decreased (0.121615 --> 0.121611).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1259555
	speed: 0.1309s/iter; left time: 70.3130s
	iters: 200, epoch: 9 | loss: 0.0890840
	speed: 0.1331s/iter; left time: 58.1735s
	iters: 300, epoch: 9 | loss: 0.0971443
	speed: 0.1169s/iter; left time: 39.4018s
Epoch: 9 cost time: 39.97040319442749
Epoch: 9, Steps: 318 | Train Loss: 0.1071878 Vali Loss: 0.1210744 Test Loss: 0.0887188
Validation loss decreased (0.121611 --> 0.121074).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1032639
	speed: 0.0955s/iter; left time: 20.9037s
	iters: 200, epoch: 10 | loss: 0.1217250
	speed: 0.0986s/iter; left time: 11.7365s
	iters: 300, epoch: 10 | loss: 0.0929005
	speed: 0.1000s/iter; left time: 1.9008s
Epoch: 10 cost time: 31.39162850379944
Epoch: 10, Steps: 318 | Train Loss: 0.1070547 Vali Loss: 0.1207001 Test Loss: 0.0886661
Validation loss decreased (0.121074 --> 0.120700).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.45288652181625366, mae:0.5180891752243042, rmse:0.6729684472084045, mape:0.019225427880883217, mspe:0.0006280785310082138, rse:0.2995073199272156, r2_score:0.9006771754693293, acc:0.9807745721191168
corr: [40.62092  40.62812  40.571358 40.570377 40.554504 40.5878   40.656857
 40.741848 40.8273   40.701313 41.2996   41.10234  41.066635 41.212154
 41.15188 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1640796
	speed: 0.1241s/iter; left time: 382.2022s
	iters: 200, epoch: 1 | loss: 0.1704430
	speed: 0.1082s/iter; left time: 322.4296s
	iters: 300, epoch: 1 | loss: 0.1149506
	speed: 0.1003s/iter; left time: 289.0377s
Epoch: 1 cost time: 35.008159160614014
Epoch: 1, Steps: 318 | Train Loss: 0.2503326 Vali Loss: 0.1474111 Test Loss: 0.1110059
Validation loss decreased (inf --> 0.147411).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1134885
	speed: 0.1155s/iter; left time: 319.1513s
	iters: 200, epoch: 2 | loss: 0.1446439
	speed: 0.1061s/iter; left time: 282.4419s
	iters: 300, epoch: 2 | loss: 0.1130430
	speed: 0.1125s/iter; left time: 288.2725s
Epoch: 2 cost time: 35.38097286224365
Epoch: 2, Steps: 318 | Train Loss: 0.1287367 Vali Loss: 0.1338179 Test Loss: 0.1029065
Validation loss decreased (0.147411 --> 0.133818).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0838933
	speed: 0.1007s/iter; left time: 246.1302s
	iters: 200, epoch: 3 | loss: 0.1085711
	speed: 0.0994s/iter; left time: 233.1688s
	iters: 300, epoch: 3 | loss: 0.1152752
	speed: 0.1047s/iter; left time: 234.9502s
Epoch: 3 cost time: 32.291327476501465
Epoch: 3, Steps: 318 | Train Loss: 0.1205104 Vali Loss: 0.1310390 Test Loss: 0.0995501
Validation loss decreased (0.133818 --> 0.131039).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1295940
	speed: 0.1043s/iter; left time: 221.7667s
	iters: 200, epoch: 4 | loss: 0.1344077
	speed: 0.0983s/iter; left time: 199.3163s
	iters: 300, epoch: 4 | loss: 0.1209629
	speed: 0.0996s/iter; left time: 191.8355s
Epoch: 4 cost time: 31.949769258499146
Epoch: 4, Steps: 318 | Train Loss: 0.1174132 Vali Loss: 0.1307221 Test Loss: 0.0995729
Validation loss decreased (0.131039 --> 0.130722).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1302020
	speed: 0.0973s/iter; left time: 176.0038s
	iters: 200, epoch: 5 | loss: 0.1035198
	speed: 0.0965s/iter; left time: 164.9915s
	iters: 300, epoch: 5 | loss: 0.1109335
	speed: 0.1105s/iter; left time: 177.8092s
Epoch: 5 cost time: 32.455605268478394
Epoch: 5, Steps: 318 | Train Loss: 0.1157826 Vali Loss: 0.1306621 Test Loss: 0.0980015
Validation loss decreased (0.130722 --> 0.130662).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1261577
	speed: 0.1060s/iter; left time: 157.9875s
	iters: 200, epoch: 6 | loss: 0.1094331
	speed: 0.1015s/iter; left time: 141.1680s
	iters: 300, epoch: 6 | loss: 0.1753820
	speed: 0.1012s/iter; left time: 130.5953s
Epoch: 6 cost time: 32.87503790855408
Epoch: 6, Steps: 318 | Train Loss: 0.1148476 Vali Loss: 0.1300073 Test Loss: 0.0977169
Validation loss decreased (0.130662 --> 0.130007).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0974804
	speed: 0.1057s/iter; left time: 123.9654s
	iters: 200, epoch: 7 | loss: 0.1394047
	speed: 0.1006s/iter; left time: 107.9118s
	iters: 300, epoch: 7 | loss: 0.1094198
	speed: 0.0979s/iter; left time: 95.3034s
Epoch: 7 cost time: 32.19001913070679
Epoch: 7, Steps: 318 | Train Loss: 0.1145319 Vali Loss: 0.1289099 Test Loss: 0.0978350
Validation loss decreased (0.130007 --> 0.128910).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1220621
	speed: 0.0996s/iter; left time: 85.1918s
	iters: 200, epoch: 8 | loss: 0.1539824
	speed: 0.0973s/iter; left time: 73.4970s
	iters: 300, epoch: 8 | loss: 0.0900923
	speed: 0.1175s/iter; left time: 76.9404s
Epoch: 8 cost time: 33.191038370132446
Epoch: 8, Steps: 318 | Train Loss: 0.1143606 Vali Loss: 0.1287982 Test Loss: 0.0977284
Validation loss decreased (0.128910 --> 0.128798).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1215865
	speed: 0.1102s/iter; left time: 59.1775s
	iters: 200, epoch: 9 | loss: 0.1330389
	speed: 0.1068s/iter; left time: 46.6557s
	iters: 300, epoch: 9 | loss: 0.1046128
	speed: 0.1032s/iter; left time: 34.7793s
Epoch: 9 cost time: 33.978391885757446
Epoch: 9, Steps: 318 | Train Loss: 0.1142488 Vali Loss: 0.1299740 Test Loss: 0.0977652
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0985400
	speed: 0.1074s/iter; left time: 23.5159s
	iters: 200, epoch: 10 | loss: 0.1076896
	speed: 0.1019s/iter; left time: 12.1293s
	iters: 300, epoch: 10 | loss: 0.1010818
	speed: 0.1028s/iter; left time: 1.9534s
Epoch: 10 cost time: 33.004090309143066
Epoch: 10, Steps: 318 | Train Loss: 0.1140375 Vali Loss: 0.1297909 Test Loss: 0.0977557
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.49917471408843994, mae:0.5435749292373657, rmse:0.7065230011940002, mape:0.020151378586888313, mspe:0.0006906635244376957, rse:0.3142445385456085, r2_score:0.8890691742239962, acc:0.9798486214131117
corr: [40.5893   40.665024 40.672558 41.077534 40.96229  40.894924 40.879776
 40.7984   40.833004 40.578068 40.83965  40.71177  40.892128 40.990025
 40.923374 41.018448 41.064674 41.376057 41.190334 40.95558 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1843935
	speed: 0.1278s/iter; left time: 393.8342s
	iters: 200, epoch: 1 | loss: 0.1853156
	speed: 0.1111s/iter; left time: 331.2320s
	iters: 300, epoch: 1 | loss: 0.1198749
	speed: 0.1094s/iter; left time: 315.2375s
Epoch: 1 cost time: 36.907214641571045
Epoch: 1, Steps: 318 | Train Loss: 0.2414757 Vali Loss: 0.1482544 Test Loss: 0.1109145
Validation loss decreased (inf --> 0.148254).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1334856
	speed: 0.1114s/iter; left time: 307.6974s
	iters: 200, epoch: 2 | loss: 0.1108898
	speed: 0.1067s/iter; left time: 284.2060s
	iters: 300, epoch: 2 | loss: 0.1151614
	speed: 0.0995s/iter; left time: 255.1180s
Epoch: 2 cost time: 33.57229971885681
Epoch: 2, Steps: 318 | Train Loss: 0.1284642 Vali Loss: 0.1378184 Test Loss: 0.1022669
Validation loss decreased (0.148254 --> 0.137818).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1139611
	speed: 0.1042s/iter; left time: 254.6990s
	iters: 200, epoch: 3 | loss: 0.1505795
	speed: 0.1122s/iter; left time: 263.1864s
	iters: 300, epoch: 3 | loss: 0.1512560
	speed: 0.1045s/iter; left time: 234.6944s
Epoch: 3 cost time: 34.041656732559204
Epoch: 3, Steps: 318 | Train Loss: 0.1211931 Vali Loss: 0.1345581 Test Loss: 0.1000781
Validation loss decreased (0.137818 --> 0.134558).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1147015
	speed: 0.1098s/iter; left time: 233.5080s
	iters: 200, epoch: 4 | loss: 0.1325598
	speed: 0.1051s/iter; left time: 213.0040s
	iters: 300, epoch: 4 | loss: 0.1355124
	speed: 0.1068s/iter; left time: 205.8532s
Epoch: 4 cost time: 33.85445809364319
Epoch: 4, Steps: 318 | Train Loss: 0.1182554 Vali Loss: 0.1338289 Test Loss: 0.0990012
Validation loss decreased (0.134558 --> 0.133829).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0959389
	speed: 0.1126s/iter; left time: 203.6293s
	iters: 200, epoch: 5 | loss: 0.1043708
	speed: 0.1070s/iter; left time: 182.8503s
	iters: 300, epoch: 5 | loss: 0.1302852
	speed: 0.1011s/iter; left time: 162.6022s
Epoch: 5 cost time: 34.04095125198364
Epoch: 5, Steps: 318 | Train Loss: 0.1166506 Vali Loss: 0.1316618 Test Loss: 0.0980763
Validation loss decreased (0.133829 --> 0.131662).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1147999
	speed: 0.1118s/iter; left time: 166.7624s
	iters: 200, epoch: 6 | loss: 0.1219252
	speed: 0.1091s/iter; left time: 151.7127s
	iters: 300, epoch: 6 | loss: 0.1057605
	speed: 0.1019s/iter; left time: 131.5422s
Epoch: 6 cost time: 33.920658588409424
Epoch: 6, Steps: 318 | Train Loss: 0.1162109 Vali Loss: 0.1306868 Test Loss: 0.0978641
Validation loss decreased (0.131662 --> 0.130687).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1171899
	speed: 0.1327s/iter; left time: 155.6396s
	iters: 200, epoch: 7 | loss: 0.1342673
	speed: 0.1337s/iter; left time: 143.4946s
	iters: 300, epoch: 7 | loss: 0.1294703
	speed: 0.1260s/iter; left time: 122.5553s
Epoch: 7 cost time: 41.48268365859985
Epoch: 7, Steps: 318 | Train Loss: 0.1157535 Vali Loss: 0.1312898 Test Loss: 0.0980265
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1240653
	speed: 0.1368s/iter; left time: 116.9520s
	iters: 200, epoch: 8 | loss: 0.1227328
	speed: 0.1359s/iter; left time: 102.5792s
	iters: 300, epoch: 8 | loss: 0.1103466
	speed: 0.1244s/iter; left time: 81.4636s
Epoch: 8 cost time: 41.890869140625
Epoch: 8, Steps: 318 | Train Loss: 0.1155521 Vali Loss: 0.1313618 Test Loss: 0.0977867
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0858501
	speed: 0.1306s/iter; left time: 70.1455s
	iters: 200, epoch: 9 | loss: 0.1205768
	speed: 0.1192s/iter; left time: 52.0887s
	iters: 300, epoch: 9 | loss: 0.1283264
	speed: 0.1071s/iter; left time: 36.1062s
Epoch: 9 cost time: 37.537137031555176
Epoch: 9, Steps: 318 | Train Loss: 0.1155134 Vali Loss: 0.1309072 Test Loss: 0.0978089
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.4998682737350464, mae:0.544712483882904, rmse:0.7070136070251465, mape:0.020180854946374893, mspe:0.0006903005414642394, rse:0.314243346452713, r2_score:0.8903529933658096, acc:0.9798191450536251
corr: [40.673412 40.601078 40.58729  40.70093  40.848656 40.851982 40.896053
 40.89062  41.074802 40.55079  40.932865 40.9249   40.928146 40.938343
 40.96623  40.94158  41.17246  41.34491  41.287582 41.118492 41.080025
 41.20767  40.943214 41.1963   41.13288 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2322579
	speed: 0.1182s/iter; left time: 363.0059s
	iters: 200, epoch: 1 | loss: 0.1301157
	speed: 0.0941s/iter; left time: 279.4450s
	iters: 300, epoch: 1 | loss: 0.1771990
	speed: 0.1001s/iter; left time: 287.3575s
Epoch: 1 cost time: 33.60795259475708
Epoch: 1, Steps: 317 | Train Loss: 0.2678483 Vali Loss: 0.1538698 Test Loss: 0.1142485
Validation loss decreased (inf --> 0.153870).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1362753
	speed: 0.1468s/iter; left time: 404.3270s
	iters: 200, epoch: 2 | loss: 0.1398880
	speed: 0.1391s/iter; left time: 369.1981s
	iters: 300, epoch: 2 | loss: 0.1508638
	speed: 0.1377s/iter; left time: 351.7861s
Epoch: 2 cost time: 44.77573347091675
Epoch: 2, Steps: 317 | Train Loss: 0.1336679 Vali Loss: 0.1449256 Test Loss: 0.1058228
Validation loss decreased (0.153870 --> 0.144926).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1216694
	speed: 0.1459s/iter; left time: 355.4630s
	iters: 200, epoch: 3 | loss: 0.1010281
	speed: 0.1287s/iter; left time: 300.7985s
	iters: 300, epoch: 3 | loss: 0.1632124
	speed: 0.1344s/iter; left time: 300.6467s
Epoch: 3 cost time: 43.060906410217285
Epoch: 3, Steps: 317 | Train Loss: 0.1265639 Vali Loss: 0.1431218 Test Loss: 0.1030296
Validation loss decreased (0.144926 --> 0.143122).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1201495
	speed: 0.1314s/iter; left time: 278.4939s
	iters: 200, epoch: 4 | loss: 0.0897567
	speed: 0.1427s/iter; left time: 288.2423s
	iters: 300, epoch: 4 | loss: 0.1474346
	speed: 0.1377s/iter; left time: 264.3745s
Epoch: 4 cost time: 43.54025864601135
Epoch: 4, Steps: 317 | Train Loss: 0.1234054 Vali Loss: 0.1409647 Test Loss: 0.1041719
Validation loss decreased (0.143122 --> 0.140965).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1388446
	speed: 0.1465s/iter; left time: 264.0745s
	iters: 200, epoch: 5 | loss: 0.1364444
	speed: 0.1452s/iter; left time: 247.3293s
	iters: 300, epoch: 5 | loss: 0.1130995
	speed: 0.1342s/iter; left time: 215.1733s
Epoch: 5 cost time: 44.84977388381958
Epoch: 5, Steps: 317 | Train Loss: 0.1217763 Vali Loss: 0.1402120 Test Loss: 0.1024413
Validation loss decreased (0.140965 --> 0.140212).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1234763
	speed: 0.1322s/iter; left time: 196.4285s
	iters: 200, epoch: 6 | loss: 0.1187519
	speed: 0.1441s/iter; left time: 199.7908s
	iters: 300, epoch: 6 | loss: 0.1233966
	speed: 0.1394s/iter; left time: 179.2600s
Epoch: 6 cost time: 43.8372106552124
Epoch: 6, Steps: 317 | Train Loss: 0.1211196 Vali Loss: 0.1385594 Test Loss: 0.1017186
Validation loss decreased (0.140212 --> 0.138559).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1269791
	speed: 0.1474s/iter; left time: 172.3535s
	iters: 200, epoch: 7 | loss: 0.1142007
	speed: 0.1389s/iter; left time: 148.4941s
	iters: 300, epoch: 7 | loss: 0.1110203
	speed: 0.1398s/iter; left time: 135.4411s
Epoch: 7 cost time: 44.790316581726074
Epoch: 7, Steps: 317 | Train Loss: 0.1205367 Vali Loss: 0.1400640 Test Loss: 0.1026839
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1038657
	speed: 0.1428s/iter; left time: 121.6417s
	iters: 200, epoch: 8 | loss: 0.1251718
	speed: 0.1392s/iter; left time: 104.6513s
	iters: 300, epoch: 8 | loss: 0.0958197
	speed: 0.1367s/iter; left time: 89.1301s
Epoch: 8 cost time: 44.379777669906616
Epoch: 8, Steps: 317 | Train Loss: 0.1204630 Vali Loss: 0.1394453 Test Loss: 0.1020519
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1078563
	speed: 0.1472s/iter; left time: 78.7300s
	iters: 200, epoch: 9 | loss: 0.1008267
	speed: 0.1363s/iter; left time: 59.2829s
	iters: 300, epoch: 9 | loss: 0.1061028
	speed: 0.1348s/iter; left time: 45.1741s
Epoch: 9 cost time: 44.205655574798584
Epoch: 9, Steps: 317 | Train Loss: 0.1204024 Vali Loss: 0.1394775 Test Loss: 0.1022431
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.5195562243461609, mae:0.5587504506111145, rmse:0.7208024859428406, mape:0.020675761625170708, mspe:0.0007131333113647997, rse:0.32013431191444397, r2_score:0.8872144451922657, acc:0.9793242383748293
corr: [40.793846 40.84449  40.858627 40.738564 40.865425 40.725906 41.101192
 40.875107 40.914665 40.87203  41.002342 41.06076  40.989742 41.01022
 41.08157  41.069878 41.00349  41.285347 41.092003 41.073025 41.16138
 41.213802 41.032444 41.051186 41.046898 41.27518  41.120888 41.38636
 41.336823 41.216423]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1829752
	speed: 0.1545s/iter; left time: 476.0502s
	iters: 200, epoch: 1 | loss: 0.1411171
	speed: 0.1422s/iter; left time: 423.9056s
	iters: 300, epoch: 1 | loss: 0.1334119
	speed: 0.1494s/iter; left time: 430.4149s
Epoch: 1 cost time: 47.253113746643066
Epoch: 1, Steps: 318 | Train Loss: 0.2342876 Vali Loss: 0.1472017 Test Loss: 0.1082111
Validation loss decreased (inf --> 0.147202).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1113416
	speed: 0.1506s/iter; left time: 416.1387s
	iters: 200, epoch: 2 | loss: 0.0852618
	speed: 0.1500s/iter; left time: 399.5829s
	iters: 300, epoch: 2 | loss: 0.1177974
	speed: 0.1455s/iter; left time: 373.0281s
Epoch: 2 cost time: 47.42113184928894
Epoch: 2, Steps: 318 | Train Loss: 0.1246630 Vali Loss: 0.1334203 Test Loss: 0.0963568
Validation loss decreased (0.147202 --> 0.133420).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0827085
	speed: 0.1531s/iter; left time: 374.3404s
	iters: 200, epoch: 3 | loss: 0.1025227
	speed: 0.1510s/iter; left time: 354.1773s
	iters: 300, epoch: 3 | loss: 0.0710047
	speed: 0.1671s/iter; left time: 375.2205s
Epoch: 3 cost time: 50.10790014266968
Epoch: 3, Steps: 318 | Train Loss: 0.1128797 Vali Loss: 0.1286227 Test Loss: 0.0911302
Validation loss decreased (0.133420 --> 0.128623).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1012421
	speed: 0.1697s/iter; left time: 360.8680s
	iters: 200, epoch: 4 | loss: 0.0871847
	speed: 0.1632s/iter; left time: 330.8779s
	iters: 300, epoch: 4 | loss: 0.1227594
	speed: 0.1585s/iter; left time: 305.4713s
Epoch: 4 cost time: 52.09767532348633
Epoch: 4, Steps: 318 | Train Loss: 0.1080308 Vali Loss: 0.1243402 Test Loss: 0.0884732
Validation loss decreased (0.128623 --> 0.124340).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1259877
	speed: 0.1611s/iter; left time: 291.3442s
	iters: 200, epoch: 5 | loss: 0.0820379
	speed: 0.1589s/iter; left time: 271.5521s
	iters: 300, epoch: 5 | loss: 0.0801698
	speed: 0.1571s/iter; left time: 252.7626s
Epoch: 5 cost time: 50.57503128051758
Epoch: 5, Steps: 318 | Train Loss: 0.1061316 Vali Loss: 0.1241657 Test Loss: 0.0882243
Validation loss decreased (0.124340 --> 0.124166).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0973684
	speed: 0.2095s/iter; left time: 312.4205s
	iters: 200, epoch: 6 | loss: 0.0768870
	speed: 0.2034s/iter; left time: 282.8710s
	iters: 300, epoch: 6 | loss: 0.1060528
	speed: 0.1917s/iter; left time: 247.5130s
Epoch: 6 cost time: 64.55960083007812
Epoch: 6, Steps: 318 | Train Loss: 0.1051144 Vali Loss: 0.1230365 Test Loss: 0.0874405
Validation loss decreased (0.124166 --> 0.123036).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1177622
	speed: 0.2044s/iter; left time: 239.7318s
	iters: 200, epoch: 7 | loss: 0.1062604
	speed: 0.1642s/iter; left time: 176.2037s
	iters: 300, epoch: 7 | loss: 0.1095641
	speed: 0.2161s/iter; left time: 210.2551s
Epoch: 7 cost time: 61.21744680404663
Epoch: 7, Steps: 318 | Train Loss: 0.1045306 Vali Loss: 0.1227200 Test Loss: 0.0876679
Validation loss decreased (0.123036 --> 0.122720).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1259911
	speed: 0.1330s/iter; left time: 113.7466s
	iters: 200, epoch: 8 | loss: 0.1031245
	speed: 0.1058s/iter; left time: 79.8935s
	iters: 300, epoch: 8 | loss: 0.1002214
	speed: 0.1380s/iter; left time: 90.4130s
Epoch: 8 cost time: 39.82888436317444
Epoch: 8, Steps: 318 | Train Loss: 0.1046226 Vali Loss: 0.1237294 Test Loss: 0.0874864
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1012670
	speed: 0.1426s/iter; left time: 76.5730s
	iters: 200, epoch: 9 | loss: 0.1270771
	speed: 0.1648s/iter; left time: 72.0364s
	iters: 300, epoch: 9 | loss: 0.0743817
	speed: 0.1633s/iter; left time: 55.0444s
Epoch: 9 cost time: 49.95002746582031
Epoch: 9, Steps: 318 | Train Loss: 0.1040291 Vali Loss: 0.1232002 Test Loss: 0.0874042
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0869971
	speed: 0.1056s/iter; left time: 23.1223s
	iters: 200, epoch: 10 | loss: 0.0917038
	speed: 0.1521s/iter; left time: 18.0993s
	iters: 300, epoch: 10 | loss: 0.1365164
	speed: 0.1503s/iter; left time: 2.8553s
Epoch: 10 cost time: 43.878626585006714
Epoch: 10, Steps: 318 | Train Loss: 0.1041037 Vali Loss: 0.1234866 Test Loss: 0.0873809
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.4477880299091339, mae:0.5196632742881775, rmse:0.6691696643829346, mape:0.01927885040640831, mspe:0.000621156650595367, rse:0.29797226190567017, r2_score:0.9021250878469163, acc:0.9807211495935917
corr: [40.452034 40.721527 40.724163 40.81042  40.877327 40.93176  40.771698
 41.015747 41.02152  40.869156]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2052339
	speed: 0.1444s/iter; left time: 444.8017s
	iters: 200, epoch: 1 | loss: 0.1490238
	speed: 0.1297s/iter; left time: 386.7154s
	iters: 300, epoch: 1 | loss: 0.1423593
	speed: 0.1264s/iter; left time: 364.0606s
Epoch: 1 cost time: 42.1049382686615
Epoch: 1, Steps: 318 | Train Loss: 0.2528701 Vali Loss: 0.1410564 Test Loss: 0.1092681
Validation loss decreased (inf --> 0.141056).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1260217
	speed: 0.1322s/iter; left time: 365.2135s
	iters: 200, epoch: 2 | loss: 0.1081596
	speed: 0.1218s/iter; left time: 324.2645s
	iters: 300, epoch: 2 | loss: 0.1138675
	speed: 0.1162s/iter; left time: 297.9290s
Epoch: 2 cost time: 39.18893218040466
Epoch: 2, Steps: 318 | Train Loss: 0.1232253 Vali Loss: 0.1310802 Test Loss: 0.0962451
Validation loss decreased (0.141056 --> 0.131080).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1537470
	speed: 0.1116s/iter; left time: 272.8781s
	iters: 200, epoch: 3 | loss: 0.1153727
	speed: 0.1127s/iter; left time: 264.2375s
	iters: 300, epoch: 3 | loss: 0.1099888
	speed: 0.1145s/iter; left time: 257.1393s
Epoch: 3 cost time: 36.21482825279236
Epoch: 3, Steps: 318 | Train Loss: 0.1146974 Vali Loss: 0.1260898 Test Loss: 0.0912946
Validation loss decreased (0.131080 --> 0.126090).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1128440
	speed: 0.1234s/iter; left time: 262.4529s
	iters: 200, epoch: 4 | loss: 0.1254956
	speed: 0.1187s/iter; left time: 240.6433s
	iters: 300, epoch: 4 | loss: 0.1434455
	speed: 0.1244s/iter; left time: 239.6722s
Epoch: 4 cost time: 38.90783190727234
Epoch: 4, Steps: 318 | Train Loss: 0.1105920 Vali Loss: 0.1229797 Test Loss: 0.0900341
Validation loss decreased (0.126090 --> 0.122980).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1279125
	speed: 0.1141s/iter; left time: 206.4011s
	iters: 200, epoch: 5 | loss: 0.1279536
	speed: 0.1166s/iter; left time: 199.2023s
	iters: 300, epoch: 5 | loss: 0.1150713
	speed: 0.1160s/iter; left time: 186.5899s
Epoch: 5 cost time: 36.67287564277649
Epoch: 5, Steps: 318 | Train Loss: 0.1090884 Vali Loss: 0.1223552 Test Loss: 0.0893294
Validation loss decreased (0.122980 --> 0.122355).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1011830
	speed: 0.1248s/iter; left time: 186.1364s
	iters: 200, epoch: 6 | loss: 0.1161531
	speed: 0.1261s/iter; left time: 175.3684s
	iters: 300, epoch: 6 | loss: 0.0690879
	speed: 0.1181s/iter; left time: 152.5315s
Epoch: 6 cost time: 39.09896755218506
Epoch: 6, Steps: 318 | Train Loss: 0.1080336 Vali Loss: 0.1218093 Test Loss: 0.0890224
Validation loss decreased (0.122355 --> 0.121809).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0993280
	speed: 0.1184s/iter; left time: 138.8992s
	iters: 200, epoch: 7 | loss: 0.1117029
	speed: 0.1143s/iter; left time: 122.6825s
	iters: 300, epoch: 7 | loss: 0.0824048
	speed: 0.1154s/iter; left time: 112.2915s
Epoch: 7 cost time: 36.940163373947144
Epoch: 7, Steps: 318 | Train Loss: 0.1076375 Vali Loss: 0.1215627 Test Loss: 0.0887692
Validation loss decreased (0.121809 --> 0.121563).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1101911
	speed: 0.1226s/iter; left time: 104.8314s
	iters: 200, epoch: 8 | loss: 0.1171490
	speed: 0.1175s/iter; left time: 88.7465s
	iters: 300, epoch: 8 | loss: 0.1004190
	speed: 0.1164s/iter; left time: 76.2511s
Epoch: 8 cost time: 37.72222542762756
Epoch: 8, Steps: 318 | Train Loss: 0.1074647 Vali Loss: 0.1215898 Test Loss: 0.0888287
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1249822
	speed: 0.1089s/iter; left time: 58.4944s
	iters: 200, epoch: 9 | loss: 0.0895322
	speed: 0.1185s/iter; left time: 51.8033s
	iters: 300, epoch: 9 | loss: 0.0969189
	speed: 0.1144s/iter; left time: 38.5667s
Epoch: 9 cost time: 36.41042613983154
Epoch: 9, Steps: 318 | Train Loss: 0.1073410 Vali Loss: 0.1210628 Test Loss: 0.0887576
Validation loss decreased (0.121563 --> 0.121063).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1032625
	speed: 0.1263s/iter; left time: 27.6619s
	iters: 200, epoch: 10 | loss: 0.1230017
	speed: 0.1145s/iter; left time: 13.6314s
	iters: 300, epoch: 10 | loss: 0.0933377
	speed: 0.1176s/iter; left time: 2.2337s
Epoch: 10 cost time: 37.81428003311157
Epoch: 10, Steps: 318 | Train Loss: 0.1072191 Vali Loss: 0.1206680 Test Loss: 0.0887013
Validation loss decreased (0.121063 --> 0.120668).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.45306649804115295, mae:0.5183282494544983, rmse:0.6731021404266357, mape:0.01923157088458538, mspe:0.0006280031520873308, rse:0.2995668351650238, r2_score:0.9008125534681043, acc:0.9807684291154146
corr: [40.640823 40.651    40.607006 40.597046 40.583153 40.609028 40.683228
 40.761364 40.848328 40.704773 41.295082 41.097168 41.068554 41.222836
 41.165245]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1638780
	speed: 0.1347s/iter; left time: 414.9323s
	iters: 200, epoch: 1 | loss: 0.1707149
	speed: 0.1116s/iter; left time: 332.5932s
	iters: 300, epoch: 1 | loss: 0.1148872
	speed: 0.1201s/iter; left time: 345.8842s
Epoch: 1 cost time: 38.575634479522705
Epoch: 1, Steps: 318 | Train Loss: 0.2507829 Vali Loss: 0.1474318 Test Loss: 0.1110218
Validation loss decreased (inf --> 0.147432).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1136041
	speed: 0.1229s/iter; left time: 339.5768s
	iters: 200, epoch: 2 | loss: 0.1449212
	speed: 0.1221s/iter; left time: 325.2686s
	iters: 300, epoch: 2 | loss: 0.1132103
	speed: 0.1121s/iter; left time: 287.2838s
Epoch: 2 cost time: 38.013824701309204
Epoch: 2, Steps: 318 | Train Loss: 0.1287549 Vali Loss: 0.1339160 Test Loss: 0.1028839
Validation loss decreased (0.147432 --> 0.133916).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0838717
	speed: 0.1482s/iter; left time: 362.2552s
	iters: 200, epoch: 3 | loss: 0.1086639
	speed: 0.1542s/iter; left time: 361.5057s
	iters: 300, epoch: 3 | loss: 0.1148893
	speed: 0.1537s/iter; left time: 345.1062s
Epoch: 3 cost time: 48.567184925079346
Epoch: 3, Steps: 318 | Train Loss: 0.1204421 Vali Loss: 0.1311392 Test Loss: 0.0995124
Validation loss decreased (0.133916 --> 0.131139).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1294999
	speed: 0.1593s/iter; left time: 338.8063s
	iters: 200, epoch: 4 | loss: 0.1344996
	speed: 0.1495s/iter; left time: 303.1298s
	iters: 300, epoch: 4 | loss: 0.1210080
	speed: 0.1518s/iter; left time: 292.4884s
Epoch: 4 cost time: 49.40385365486145
Epoch: 4, Steps: 318 | Train Loss: 0.1173396 Vali Loss: 0.1308441 Test Loss: 0.0995753
Validation loss decreased (0.131139 --> 0.130844).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1299433
	speed: 0.1130s/iter; left time: 204.3823s
	iters: 200, epoch: 5 | loss: 0.1036614
	speed: 0.1185s/iter; left time: 202.4713s
	iters: 300, epoch: 5 | loss: 0.1110685
	speed: 0.1212s/iter; left time: 195.0667s
Epoch: 5 cost time: 37.31019830703735
Epoch: 5, Steps: 318 | Train Loss: 0.1157127 Vali Loss: 0.1308014 Test Loss: 0.0979946
Validation loss decreased (0.130844 --> 0.130801).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1260256
	speed: 0.1193s/iter; left time: 177.8552s
	iters: 200, epoch: 6 | loss: 0.1094425
	speed: 0.1161s/iter; left time: 161.4262s
	iters: 300, epoch: 6 | loss: 0.1753195
	speed: 0.1175s/iter; left time: 151.6998s
Epoch: 6 cost time: 37.346707820892334
Epoch: 6, Steps: 318 | Train Loss: 0.1147781 Vali Loss: 0.1301366 Test Loss: 0.0977113
Validation loss decreased (0.130801 --> 0.130137).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0977213
	speed: 0.1173s/iter; left time: 137.5715s
	iters: 200, epoch: 7 | loss: 0.1393437
	speed: 0.1095s/iter; left time: 117.4511s
	iters: 300, epoch: 7 | loss: 0.1095192
	speed: 0.1126s/iter; left time: 109.5665s
Epoch: 7 cost time: 36.192262172698975
Epoch: 7, Steps: 318 | Train Loss: 0.1144595 Vali Loss: 0.1290340 Test Loss: 0.0978270
Validation loss decreased (0.130137 --> 0.129034).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1218724
	speed: 0.1253s/iter; left time: 107.1660s
	iters: 200, epoch: 8 | loss: 0.1539310
	speed: 0.1196s/iter; left time: 90.3146s
	iters: 300, epoch: 8 | loss: 0.0894634
	speed: 0.1170s/iter; left time: 76.6650s
Epoch: 8 cost time: 38.30592155456543
Epoch: 8, Steps: 318 | Train Loss: 0.1142877 Vali Loss: 0.1289202 Test Loss: 0.0977239
Validation loss decreased (0.129034 --> 0.128920).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1211810
	speed: 0.1317s/iter; left time: 70.7197s
	iters: 200, epoch: 9 | loss: 0.1329479
	speed: 0.1155s/iter; left time: 50.4815s
	iters: 300, epoch: 9 | loss: 0.1045375
	speed: 0.1078s/iter; left time: 36.3212s
Epoch: 9 cost time: 37.5740704536438
Epoch: 9, Steps: 318 | Train Loss: 0.1141741 Vali Loss: 0.1301001 Test Loss: 0.0977591
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0983596
	speed: 0.1147s/iter; left time: 25.1158s
	iters: 200, epoch: 10 | loss: 0.1075969
	speed: 0.1028s/iter; left time: 12.2330s
	iters: 300, epoch: 10 | loss: 0.1013397
	speed: 0.1114s/iter; left time: 2.1173s
Epoch: 10 cost time: 34.980353593826294
Epoch: 10, Steps: 318 | Train Loss: 0.1139657 Vali Loss: 0.1299181 Test Loss: 0.0977473
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.49915194511413574, mae:0.5436649918556213, rmse:0.7065068483352661, mape:0.02015397883951664, mspe:0.0006905397167429328, rse:0.31423741579055786, r2_score:0.8891011059925699, acc:0.9798460211604834
corr: [40.587116 40.64719  40.652912 41.08387  40.967117 40.898605 40.87703
 40.802998 40.828552 40.574913 40.83954  40.716167 40.884373 40.99147
 40.92477  41.01958  41.072983 41.3782   41.1978   40.9596  ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1855673
	speed: 0.1300s/iter; left time: 400.6142s
	iters: 200, epoch: 1 | loss: 0.1842497
	speed: 0.1151s/iter; left time: 343.0493s
	iters: 300, epoch: 1 | loss: 0.1194764
	speed: 0.1238s/iter; left time: 356.7759s
Epoch: 1 cost time: 38.82209777832031
Epoch: 1, Steps: 318 | Train Loss: 0.2404124 Vali Loss: 0.1483811 Test Loss: 0.1110143
Validation loss decreased (inf --> 0.148381).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1336384
	speed: 0.1262s/iter; left time: 348.6439s
	iters: 200, epoch: 2 | loss: 0.1107448
	speed: 0.1189s/iter; left time: 316.7542s
	iters: 300, epoch: 2 | loss: 0.1151009
	speed: 0.1128s/iter; left time: 289.2047s
Epoch: 2 cost time: 37.94348955154419
Epoch: 2, Steps: 318 | Train Loss: 0.1283604 Vali Loss: 0.1378013 Test Loss: 0.1021110
Validation loss decreased (0.148381 --> 0.137801).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1137665
	speed: 0.1191s/iter; left time: 291.2371s
	iters: 200, epoch: 3 | loss: 0.1506476
	speed: 0.1086s/iter; left time: 254.6400s
	iters: 300, epoch: 3 | loss: 0.1507400
	speed: 0.1084s/iter; left time: 243.3476s
Epoch: 3 cost time: 35.44498944282532
Epoch: 3, Steps: 318 | Train Loss: 0.1210069 Vali Loss: 0.1345474 Test Loss: 0.0998892
Validation loss decreased (0.137801 --> 0.134547).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1147172
	speed: 0.1190s/iter; left time: 253.0501s
	iters: 200, epoch: 4 | loss: 0.1313460
	speed: 0.1229s/iter; left time: 249.2037s
	iters: 300, epoch: 4 | loss: 0.1352647
	speed: 0.1248s/iter; left time: 240.5846s
Epoch: 4 cost time: 38.92769932746887
Epoch: 4, Steps: 318 | Train Loss: 0.1180727 Vali Loss: 0.1337801 Test Loss: 0.0987693
Validation loss decreased (0.134547 --> 0.133780).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0956803
	speed: 0.1122s/iter; left time: 202.9217s
	iters: 200, epoch: 5 | loss: 0.1045201
	speed: 0.1131s/iter; left time: 193.3175s
	iters: 300, epoch: 5 | loss: 0.1292384
	speed: 0.1193s/iter; left time: 191.9701s
Epoch: 5 cost time: 36.71371912956238
Epoch: 5, Steps: 318 | Train Loss: 0.1164639 Vali Loss: 0.1315390 Test Loss: 0.0978130
Validation loss decreased (0.133780 --> 0.131539).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1145834
	speed: 0.1322s/iter; left time: 197.0555s
	iters: 200, epoch: 6 | loss: 0.1217930
	speed: 0.1140s/iter; left time: 158.6193s
	iters: 300, epoch: 6 | loss: 0.1062421
	speed: 0.1108s/iter; left time: 143.0068s
Epoch: 6 cost time: 37.765681982040405
Epoch: 6, Steps: 318 | Train Loss: 0.1160100 Vali Loss: 0.1305943 Test Loss: 0.0975864
Validation loss decreased (0.131539 --> 0.130594).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1164178
	speed: 0.1141s/iter; left time: 133.8690s
	iters: 200, epoch: 7 | loss: 0.1338479
	speed: 0.1127s/iter; left time: 120.9782s
	iters: 300, epoch: 7 | loss: 0.1291674
	speed: 0.1162s/iter; left time: 113.0363s
Epoch: 7 cost time: 36.372541666030884
Epoch: 7, Steps: 318 | Train Loss: 0.1155610 Vali Loss: 0.1311841 Test Loss: 0.0977335
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1224166
	speed: 0.1213s/iter; left time: 103.6989s
	iters: 200, epoch: 8 | loss: 0.1218672
	speed: 0.1205s/iter; left time: 90.9824s
	iters: 300, epoch: 8 | loss: 0.1104682
	speed: 0.1182s/iter; left time: 77.4474s
Epoch: 8 cost time: 38.165486097335815
Epoch: 8, Steps: 318 | Train Loss: 0.1153358 Vali Loss: 0.1312525 Test Loss: 0.0975056
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0859403
	speed: 0.1135s/iter; left time: 60.9637s
	iters: 200, epoch: 9 | loss: 0.1209430
	speed: 0.1146s/iter; left time: 50.0839s
	iters: 300, epoch: 9 | loss: 0.1280399
	speed: 0.1205s/iter; left time: 40.6182s
Epoch: 9 cost time: 36.9919810295105
Epoch: 9, Steps: 318 | Train Loss: 0.1153173 Vali Loss: 0.1307937 Test Loss: 0.0975281
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.498449444770813, mae:0.5437927842140198, rmse:0.7060095071792603, mape:0.020144300535321236, mspe:0.0006878888234496117, rse:0.3137970566749573, r2_score:0.8904979288045105, acc:0.9798556994646788
corr: [40.674305 40.582043 40.55165  40.6709   40.804825 40.823364 40.864292
 40.82867  41.02347  40.521988 40.912025 40.90053  40.89182  40.92744
 40.94908  40.914585 41.16039  41.31129  41.282295 41.091507 41.051434
 41.212044 40.920784 41.187874 41.123745]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=1, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2228653
	speed: 0.1288s/iter; left time: 395.4105s
	iters: 200, epoch: 1 | loss: 0.1291044
	speed: 0.1176s/iter; left time: 349.4567s
	iters: 300, epoch: 1 | loss: 0.1779249
	speed: 0.1272s/iter; left time: 365.0781s
Epoch: 1 cost time: 39.713412046432495
Epoch: 1, Steps: 317 | Train Loss: 0.2672824 Vali Loss: 0.1521315 Test Loss: 0.1124174
Validation loss decreased (inf --> 0.152131).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1375920
	speed: 0.1703s/iter; left time: 469.0206s
	iters: 200, epoch: 2 | loss: 0.1360943
	speed: 0.1622s/iter; left time: 430.4428s
	iters: 300, epoch: 2 | loss: 0.1578018
	speed: 0.1596s/iter; left time: 407.5886s
Epoch: 2 cost time: 51.63283133506775
Epoch: 2, Steps: 317 | Train Loss: 0.1330156 Vali Loss: 0.1439615 Test Loss: 0.1054962
Validation loss decreased (0.152131 --> 0.143961).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1164944
	speed: 0.1268s/iter; left time: 308.9198s
	iters: 200, epoch: 3 | loss: 0.1007974
	speed: 0.1266s/iter; left time: 295.9262s
	iters: 300, epoch: 3 | loss: 0.1615773
	speed: 0.1305s/iter; left time: 291.8320s
Epoch: 3 cost time: 40.97267174720764
Epoch: 3, Steps: 317 | Train Loss: 0.1259461 Vali Loss: 0.1422302 Test Loss: 0.1023931
Validation loss decreased (0.143961 --> 0.142230).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1226821
	speed: 0.1302s/iter; left time: 276.1040s
	iters: 200, epoch: 4 | loss: 0.0898398
	speed: 0.1406s/iter; left time: 283.9624s
	iters: 300, epoch: 4 | loss: 0.1470565
	speed: 0.1633s/iter; left time: 313.4908s
Epoch: 4 cost time: 46.824158668518066
Epoch: 4, Steps: 317 | Train Loss: 0.1230821 Vali Loss: 0.1411959 Test Loss: 0.1038488
Validation loss decreased (0.142230 --> 0.141196).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1369960
	speed: 0.1924s/iter; left time: 346.9292s
	iters: 200, epoch: 5 | loss: 0.1409662
	speed: 0.1697s/iter; left time: 288.9310s
	iters: 300, epoch: 5 | loss: 0.1124834
	speed: 0.1596s/iter; left time: 255.8667s
Epoch: 5 cost time: 54.918097496032715
Epoch: 5, Steps: 317 | Train Loss: 0.1216269 Vali Loss: 0.1399225 Test Loss: 0.1021628
Validation loss decreased (0.141196 --> 0.139923).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1229218
	speed: 0.1861s/iter; left time: 276.5408s
	iters: 200, epoch: 6 | loss: 0.1190734
	speed: 0.1769s/iter; left time: 245.1851s
	iters: 300, epoch: 6 | loss: 0.1256171
	speed: 0.1381s/iter; left time: 177.5526s
Epoch: 6 cost time: 52.326090574264526
Epoch: 6, Steps: 317 | Train Loss: 0.1208820 Vali Loss: 0.1387973 Test Loss: 0.1014901
Validation loss decreased (0.139923 --> 0.138797).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1291042
	speed: 0.1032s/iter; left time: 120.6644s
	iters: 200, epoch: 7 | loss: 0.1142608
	speed: 0.1411s/iter; left time: 150.8356s
	iters: 300, epoch: 7 | loss: 0.1098072
	speed: 0.1356s/iter; left time: 131.4345s
Epoch: 7 cost time: 39.943596601486206
Epoch: 7, Steps: 317 | Train Loss: 0.1204555 Vali Loss: 0.1401544 Test Loss: 0.1023345
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1019092
	speed: 0.1309s/iter; left time: 111.5443s
	iters: 200, epoch: 8 | loss: 0.1242129
	speed: 0.1211s/iter; left time: 91.0371s
	iters: 300, epoch: 8 | loss: 0.0923403
	speed: 0.1301s/iter; left time: 84.8045s
Epoch: 8 cost time: 40.77907109260559
Epoch: 8, Steps: 317 | Train Loss: 0.1202905 Vali Loss: 0.1397016 Test Loss: 0.1018242
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1059499
	speed: 0.1138s/iter; left time: 60.9005s
	iters: 200, epoch: 9 | loss: 0.1011882
	speed: 0.1336s/iter; left time: 58.1086s
	iters: 300, epoch: 9 | loss: 0.1055783
	speed: 0.1195s/iter; left time: 40.0293s
Epoch: 9 cost time: 38.943671226501465
Epoch: 9, Steps: 317 | Train Loss: 0.1203111 Vali Loss: 0.1396510 Test Loss: 0.1020295
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el1_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.5183887481689453, mae:0.5576303601264954, rmse:0.7199921607971191, mape:0.02063337154686451, mspe:0.000711629691068083, rse:0.31977441906929016, r2_score:0.8867220385867285, acc:0.9793666284531355
corr: [40.769405 40.74847  40.774647 40.65342  40.818634 40.68374  40.987213
 40.75301  40.83622  40.80709  40.901203 40.95013  40.863804 40.905388
 40.94754  40.946007 40.954468 41.152035 41.011723 40.943382 41.077984
 41.11723  40.92124  40.96099  40.953518 41.232994 41.081116 41.370476
 41.36492  41.172195]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2029401
	speed: 0.0696s/iter; left time: 214.4443s
	iters: 200, epoch: 1 | loss: 0.1656564
	speed: 0.0476s/iter; left time: 141.8318s
	iters: 300, epoch: 1 | loss: 0.1494347
	speed: 0.0648s/iter; left time: 186.7495s
Epoch: 1 cost time: 19.581128120422363
Epoch: 1, Steps: 318 | Train Loss: 0.2623468 Vali Loss: 0.1338936 Test Loss: 0.1591938
Validation loss decreased (inf --> 0.133894).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1063454
	speed: 0.0546s/iter; left time: 150.8083s
	iters: 200, epoch: 2 | loss: 0.1260432
	speed: 0.0679s/iter; left time: 180.9368s
	iters: 300, epoch: 2 | loss: 0.1362402
	speed: 0.0481s/iter; left time: 123.3163s
Epoch: 2 cost time: 18.260159730911255
Epoch: 2, Steps: 318 | Train Loss: 0.1196737 Vali Loss: 0.1212071 Test Loss: 0.1981262
Validation loss decreased (0.133894 --> 0.121207).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1008552
	speed: 0.0527s/iter; left time: 128.8857s
	iters: 200, epoch: 3 | loss: 0.1064056
	speed: 0.0503s/iter; left time: 118.0316s
	iters: 300, epoch: 3 | loss: 0.1231013
	speed: 0.0644s/iter; left time: 144.5799s
Epoch: 3 cost time: 18.364261388778687
Epoch: 3, Steps: 318 | Train Loss: 0.1072484 Vali Loss: 0.1182503 Test Loss: 0.2144671
Validation loss decreased (0.121207 --> 0.118250).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0810842
	speed: 0.0479s/iter; left time: 101.8056s
	iters: 200, epoch: 4 | loss: 0.1097764
	speed: 0.0593s/iter; left time: 120.2619s
	iters: 300, epoch: 4 | loss: 0.0745463
	speed: 0.0351s/iter; left time: 67.5484s
Epoch: 4 cost time: 14.855778932571411
Epoch: 4, Steps: 318 | Train Loss: 0.1024412 Vali Loss: 0.1145071 Test Loss: 0.2270024
Validation loss decreased (0.118250 --> 0.114507).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0778616
	speed: 0.0649s/iter; left time: 117.3531s
	iters: 200, epoch: 5 | loss: 0.0898070
	speed: 0.0655s/iter; left time: 111.8823s
	iters: 300, epoch: 5 | loss: 0.1203071
	speed: 0.0525s/iter; left time: 84.4636s
Epoch: 5 cost time: 19.393534421920776
Epoch: 5, Steps: 318 | Train Loss: 0.1000499 Vali Loss: 0.1139567 Test Loss: 0.2307276
Validation loss decreased (0.114507 --> 0.113957).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0962217
	speed: 0.0598s/iter; left time: 89.2114s
	iters: 200, epoch: 6 | loss: 0.0911482
	speed: 0.0553s/iter; left time: 76.9354s
	iters: 300, epoch: 6 | loss: 0.0912031
	speed: 0.0403s/iter; left time: 51.9996s
Epoch: 6 cost time: 16.368475437164307
Epoch: 6, Steps: 318 | Train Loss: 0.0990410 Vali Loss: 0.1130342 Test Loss: 0.2319896
Validation loss decreased (0.113957 --> 0.113034).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1097816
	speed: 0.0546s/iter; left time: 64.0334s
	iters: 200, epoch: 7 | loss: 0.1016880
	speed: 0.0528s/iter; left time: 56.6285s
	iters: 300, epoch: 7 | loss: 0.0821487
	speed: 0.0390s/iter; left time: 37.9599s
Epoch: 7 cost time: 16.067290544509888
Epoch: 7, Steps: 318 | Train Loss: 0.0985070 Vali Loss: 0.1133193 Test Loss: 0.2337012
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0831788
	speed: 0.0482s/iter; left time: 41.2174s
	iters: 200, epoch: 8 | loss: 0.0816966
	speed: 0.0634s/iter; left time: 47.8866s
	iters: 300, epoch: 8 | loss: 0.0785360
	speed: 0.0500s/iter; left time: 32.7494s
Epoch: 8 cost time: 17.574342727661133
Epoch: 8, Steps: 318 | Train Loss: 0.0980935 Vali Loss: 0.1119383 Test Loss: 0.2341573
Validation loss decreased (0.113034 --> 0.111938).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0923501
	speed: 0.0705s/iter; left time: 37.8666s
	iters: 200, epoch: 9 | loss: 0.0896003
	speed: 0.0597s/iter; left time: 26.0795s
	iters: 300, epoch: 9 | loss: 0.1110010
	speed: 0.0462s/iter; left time: 15.5739s
Epoch: 9 cost time: 19.131741762161255
Epoch: 9, Steps: 318 | Train Loss: 0.0983125 Vali Loss: 0.1128149 Test Loss: 0.2345157
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1301639
	speed: 0.0610s/iter; left time: 13.3526s
	iters: 200, epoch: 10 | loss: 0.0973531
	speed: 0.0553s/iter; left time: 6.5824s
	iters: 300, epoch: 10 | loss: 0.0668995
	speed: 0.0458s/iter; left time: 0.8702s
Epoch: 10 cost time: 16.727746725082397
Epoch: 10, Steps: 318 | Train Loss: 0.0983150 Vali Loss: 0.1128936 Test Loss: 0.2345380
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:1.1960235834121704, mae:0.7909080386161804, rmse:1.0936286449432373, mape:0.02939753606915474, mspe:0.0017236806452274323, rse:0.486978143453598, r2_score:0.6383060045226849, acc:0.9706024639308453
corr: [37.633373 38.37079  38.15026  37.46212  38.6688   39.094254 38.30885
 37.51033  38.42969  36.67076 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1721164
	speed: 0.0673s/iter; left time: 207.4563s
	iters: 200, epoch: 1 | loss: 0.1741078
	speed: 0.0878s/iter; left time: 261.6265s
	iters: 300, epoch: 1 | loss: 0.1346078
	speed: 0.0472s/iter; left time: 136.0752s
Epoch: 1 cost time: 20.854465007781982
Epoch: 1, Steps: 318 | Train Loss: 0.2314326 Vali Loss: 0.1358196 Test Loss: 0.1214654
Validation loss decreased (inf --> 0.135820).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1022803
	speed: 0.0506s/iter; left time: 139.7581s
	iters: 200, epoch: 2 | loss: 0.1081019
	speed: 0.0475s/iter; left time: 126.5451s
	iters: 300, epoch: 2 | loss: 0.1133155
	speed: 0.0465s/iter; left time: 119.0728s
Epoch: 2 cost time: 16.553501844406128
Epoch: 2, Steps: 318 | Train Loss: 0.1117807 Vali Loss: 0.1195553 Test Loss: 0.1204427
Validation loss decreased (0.135820 --> 0.119555).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1044426
	speed: 0.0537s/iter; left time: 131.2423s
	iters: 200, epoch: 3 | loss: 0.0799373
	speed: 0.0552s/iter; left time: 129.3798s
	iters: 300, epoch: 3 | loss: 0.0950942
	speed: 0.0453s/iter; left time: 101.6837s
Epoch: 3 cost time: 16.711928129196167
Epoch: 3, Steps: 318 | Train Loss: 0.1013781 Vali Loss: 0.1206713 Test Loss: 0.1203646
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1038002
	speed: 0.0522s/iter; left time: 111.0503s
	iters: 200, epoch: 4 | loss: 0.0968105
	speed: 0.0598s/iter; left time: 121.1804s
	iters: 300, epoch: 4 | loss: 0.0980514
	speed: 0.0683s/iter; left time: 131.6039s
Epoch: 4 cost time: 19.447730779647827
Epoch: 4, Steps: 318 | Train Loss: 0.0977615 Vali Loss: 0.1178282 Test Loss: 0.1183263
Validation loss decreased (0.119555 --> 0.117828).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1088994
	speed: 0.0309s/iter; left time: 55.8103s
	iters: 200, epoch: 5 | loss: 0.0803929
	speed: 0.0604s/iter; left time: 103.1918s
	iters: 300, epoch: 5 | loss: 0.0856092
	speed: 0.0649s/iter; left time: 104.4334s
Epoch: 5 cost time: 16.382262468338013
Epoch: 5, Steps: 318 | Train Loss: 0.0961872 Vali Loss: 0.1201650 Test Loss: 0.1199567
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1098977
	speed: 0.0628s/iter; left time: 93.5789s
	iters: 200, epoch: 6 | loss: 0.0865173
	speed: 0.0430s/iter; left time: 59.8136s
	iters: 300, epoch: 6 | loss: 0.0978358
	speed: 0.0449s/iter; left time: 57.9555s
Epoch: 6 cost time: 15.891458749771118
Epoch: 6, Steps: 318 | Train Loss: 0.0951430 Vali Loss: 0.1177941 Test Loss: 0.1183075
Validation loss decreased (0.117828 --> 0.117794).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0867163
	speed: 0.0753s/iter; left time: 88.2704s
	iters: 200, epoch: 7 | loss: 0.0675657
	speed: 0.0462s/iter; left time: 49.6062s
	iters: 300, epoch: 7 | loss: 0.0997229
	speed: 0.0606s/iter; left time: 58.9721s
Epoch: 7 cost time: 19.534098863601685
Epoch: 7, Steps: 318 | Train Loss: 0.0947355 Vali Loss: 0.1187460 Test Loss: 0.1183516
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0883797
	speed: 0.0546s/iter; left time: 46.6564s
	iters: 200, epoch: 8 | loss: 0.1010033
	speed: 0.0385s/iter; left time: 29.0827s
	iters: 300, epoch: 8 | loss: 0.0960406
	speed: 0.0473s/iter; left time: 31.0080s
Epoch: 8 cost time: 14.389072179794312
Epoch: 8, Steps: 318 | Train Loss: 0.0944903 Vali Loss: 0.1182804 Test Loss: 0.1183906
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0919024
	speed: 0.0478s/iter; left time: 25.6440s
	iters: 200, epoch: 9 | loss: 0.0847855
	speed: 0.0701s/iter; left time: 30.6406s
	iters: 300, epoch: 9 | loss: 0.1182379
	speed: 0.0346s/iter; left time: 11.6667s
Epoch: 9 cost time: 16.73074698448181
Epoch: 9, Steps: 318 | Train Loss: 0.0943872 Vali Loss: 0.1184514 Test Loss: 0.1182960
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.6042883992195129, mae:0.5930575132369995, rmse:0.7773599028587341, mape:0.021908242255449295, mspe:0.0008209645166061819, rse:0.345967173576355, r2_score:0.8694916910528718, acc:0.9780917577445507
corr: [39.329277 39.00679  39.62178  39.915516 39.089188 40.32453  40.0941
 40.332504 40.55513  40.741028 40.143486 40.11313  39.475048 40.191154
 40.083748]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2535496
	speed: 0.0851s/iter; left time: 262.0927s
	iters: 200, epoch: 1 | loss: 0.1196506
	speed: 0.0847s/iter; left time: 252.4965s
	iters: 300, epoch: 1 | loss: 0.1638348
	speed: 0.0764s/iter; left time: 220.1675s
Epoch: 1 cost time: 26.863484144210815
Epoch: 1, Steps: 318 | Train Loss: 0.2252384 Vali Loss: 0.1527653 Test Loss: 0.1150262
Validation loss decreased (inf --> 0.152765).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1474215
	speed: 0.0834s/iter; left time: 230.4674s
	iters: 200, epoch: 2 | loss: 0.1541664
	speed: 0.0601s/iter; left time: 160.0753s
	iters: 300, epoch: 2 | loss: 0.1419950
	speed: 0.0774s/iter; left time: 198.4334s
Epoch: 2 cost time: 24.2552752494812
Epoch: 2, Steps: 318 | Train Loss: 0.1327214 Vali Loss: 0.1418319 Test Loss: 0.1086701
Validation loss decreased (0.152765 --> 0.141832).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1438755
	speed: 0.0836s/iter; left time: 204.3602s
	iters: 200, epoch: 3 | loss: 0.1053475
	speed: 0.0730s/iter; left time: 171.1103s
	iters: 300, epoch: 3 | loss: 0.1159792
	speed: 0.0732s/iter; left time: 164.2267s
Epoch: 3 cost time: 24.94844937324524
Epoch: 3, Steps: 318 | Train Loss: 0.1216497 Vali Loss: 0.1326441 Test Loss: 0.0976340
Validation loss decreased (0.141832 --> 0.132644).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1025919
	speed: 0.0848s/iter; left time: 180.3811s
	iters: 200, epoch: 4 | loss: 0.1340224
	speed: 0.0651s/iter; left time: 131.9250s
	iters: 300, epoch: 4 | loss: 0.1131919
	speed: 0.0770s/iter; left time: 148.3651s
Epoch: 4 cost time: 24.09234642982483
Epoch: 4, Steps: 318 | Train Loss: 0.1170534 Vali Loss: 0.1320662 Test Loss: 0.0981109
Validation loss decreased (0.132644 --> 0.132066).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1120045
	speed: 0.0920s/iter; left time: 166.4168s
	iters: 200, epoch: 5 | loss: 0.1241399
	speed: 0.0783s/iter; left time: 133.8114s
	iters: 300, epoch: 5 | loss: 0.1455327
	speed: 0.0993s/iter; left time: 159.7941s
Epoch: 5 cost time: 28.23263382911682
Epoch: 5, Steps: 318 | Train Loss: 0.1151109 Vali Loss: 0.1305760 Test Loss: 0.0960494
Validation loss decreased (0.132066 --> 0.130576).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1332262
	speed: 0.0940s/iter; left time: 140.0965s
	iters: 200, epoch: 6 | loss: 0.1006028
	speed: 0.0825s/iter; left time: 114.7783s
	iters: 300, epoch: 6 | loss: 0.1191028
	speed: 0.0947s/iter; left time: 122.2332s
Epoch: 6 cost time: 28.084805488586426
Epoch: 6, Steps: 318 | Train Loss: 0.1139493 Vali Loss: 0.1299625 Test Loss: 0.0951277
Validation loss decreased (0.130576 --> 0.129962).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1177815
	speed: 0.0834s/iter; left time: 97.8334s
	iters: 200, epoch: 7 | loss: 0.0948122
	speed: 0.0857s/iter; left time: 91.9391s
	iters: 300, epoch: 7 | loss: 0.0914821
	speed: 0.0698s/iter; left time: 67.9151s
Epoch: 7 cost time: 25.573185920715332
Epoch: 7, Steps: 318 | Train Loss: 0.1137494 Vali Loss: 0.1299451 Test Loss: 0.0948503
Validation loss decreased (0.129962 --> 0.129945).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1191262
	speed: 0.0952s/iter; left time: 81.3769s
	iters: 200, epoch: 8 | loss: 0.1301204
	speed: 0.0595s/iter; left time: 44.9392s
	iters: 300, epoch: 8 | loss: 0.1215694
	speed: 0.0926s/iter; left time: 60.6397s
Epoch: 8 cost time: 26.62697458267212
Epoch: 8, Steps: 318 | Train Loss: 0.1135245 Vali Loss: 0.1301304 Test Loss: 0.0948847
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1439507
	speed: 0.0790s/iter; left time: 42.3985s
	iters: 200, epoch: 9 | loss: 0.1312242
	speed: 0.0699s/iter; left time: 30.5262s
	iters: 300, epoch: 9 | loss: 0.0889921
	speed: 0.1037s/iter; left time: 34.9515s
Epoch: 9 cost time: 27.061971426010132
Epoch: 9, Steps: 318 | Train Loss: 0.1133249 Vali Loss: 0.1295750 Test Loss: 0.0946430
Validation loss decreased (0.129945 --> 0.129575).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0868640
	speed: 0.0771s/iter; left time: 16.8817s
	iters: 200, epoch: 10 | loss: 0.1301058
	speed: 0.0695s/iter; left time: 8.2718s
	iters: 300, epoch: 10 | loss: 0.1436776
	speed: 0.0839s/iter; left time: 1.5934s
Epoch: 10 cost time: 24.276589155197144
Epoch: 10, Steps: 318 | Train Loss: 0.1133857 Vali Loss: 0.1299860 Test Loss: 0.0947090
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.48341551423072815, mae:0.5400865077972412, rmse:0.695280909538269, mape:0.020012404769659042, mspe:0.0006674660835415125, rse:0.3092443645000458, r2_score:0.8934372629933895, acc:0.979987595230341
corr: [40.363297 40.287292 40.552284 40.42727  40.589428 40.695404 40.749985
 40.653503 40.852158 40.797092 40.689533 40.95026  40.77519  40.876198
 40.982838 40.915245 41.057903 41.229652 41.483402 41.299408]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2093274
	speed: 0.0869s/iter; left time: 267.6054s
	iters: 200, epoch: 1 | loss: 0.1059513
	speed: 0.0567s/iter; left time: 169.0730s
	iters: 300, epoch: 1 | loss: 0.1502394
	speed: 0.0583s/iter; left time: 167.8418s
Epoch: 1 cost time: 20.92579698562622
Epoch: 1, Steps: 318 | Train Loss: 0.2570451 Vali Loss: 0.1462551 Test Loss: 0.1201697
Validation loss decreased (inf --> 0.146255).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1362586
	speed: 0.0601s/iter; left time: 165.9865s
	iters: 200, epoch: 2 | loss: 0.1038920
	speed: 0.0681s/iter; left time: 181.2959s
	iters: 300, epoch: 2 | loss: 0.1429044
	speed: 0.0544s/iter; left time: 139.4090s
Epoch: 2 cost time: 19.28705668449402
Epoch: 2, Steps: 318 | Train Loss: 0.1318605 Vali Loss: 0.1392038 Test Loss: 0.1139229
Validation loss decreased (0.146255 --> 0.139204).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1456978
	speed: 0.0597s/iter; left time: 145.9630s
	iters: 200, epoch: 3 | loss: 0.1245652
	speed: 0.0756s/iter; left time: 177.3359s
	iters: 300, epoch: 3 | loss: 0.1352942
	speed: 0.0495s/iter; left time: 111.0448s
Epoch: 3 cost time: 19.275564908981323
Epoch: 3, Steps: 318 | Train Loss: 0.1262851 Vali Loss: 0.1376313 Test Loss: 0.1142841
Validation loss decreased (0.139204 --> 0.137631).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1112995
	speed: 0.0760s/iter; left time: 161.6752s
	iters: 200, epoch: 4 | loss: 0.1142762
	speed: 0.0624s/iter; left time: 126.4414s
	iters: 300, epoch: 4 | loss: 0.1241281
	speed: 0.0694s/iter; left time: 133.6601s
Epoch: 4 cost time: 21.766661405563354
Epoch: 4, Steps: 318 | Train Loss: 0.1237710 Vali Loss: 0.1373500 Test Loss: 0.1160681
Validation loss decreased (0.137631 --> 0.137350).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1095756
	speed: 0.0690s/iter; left time: 124.8898s
	iters: 200, epoch: 5 | loss: 0.1366712
	speed: 0.0544s/iter; left time: 92.9134s
	iters: 300, epoch: 5 | loss: 0.1827314
	speed: 0.0367s/iter; left time: 59.0963s
Epoch: 5 cost time: 16.841954231262207
Epoch: 5, Steps: 318 | Train Loss: 0.1229017 Vali Loss: 0.1369903 Test Loss: 0.1164678
Validation loss decreased (0.137350 --> 0.136990).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0866891
	speed: 0.0681s/iter; left time: 101.5598s
	iters: 200, epoch: 6 | loss: 0.1332235
	speed: 0.0686s/iter; left time: 95.4258s
	iters: 300, epoch: 6 | loss: 0.1233064
	speed: 0.0735s/iter; left time: 94.8709s
Epoch: 6 cost time: 21.692232370376587
Epoch: 6, Steps: 318 | Train Loss: 0.1222081 Vali Loss: 0.1358615 Test Loss: 0.1154825
Validation loss decreased (0.136990 --> 0.135862).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1347054
	speed: 0.0752s/iter; left time: 88.1780s
	iters: 200, epoch: 7 | loss: 0.1508025
	speed: 0.0531s/iter; left time: 56.9737s
	iters: 300, epoch: 7 | loss: 0.1012509
	speed: 0.0528s/iter; left time: 51.4112s
Epoch: 7 cost time: 18.777137517929077
Epoch: 7, Steps: 318 | Train Loss: 0.1218999 Vali Loss: 0.1360614 Test Loss: 0.1160182
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1192233
	speed: 0.0671s/iter; left time: 57.3419s
	iters: 200, epoch: 8 | loss: 0.1100175
	speed: 0.0573s/iter; left time: 43.2627s
	iters: 300, epoch: 8 | loss: 0.1095610
	speed: 0.0670s/iter; left time: 43.8993s
Epoch: 8 cost time: 20.921950340270996
Epoch: 8, Steps: 318 | Train Loss: 0.1218578 Vali Loss: 0.1354983 Test Loss: 0.1164947
Validation loss decreased (0.135862 --> 0.135498).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1076240
	speed: 0.0804s/iter; left time: 43.1524s
	iters: 200, epoch: 9 | loss: 0.1708853
	speed: 0.0587s/iter; left time: 25.6581s
	iters: 300, epoch: 9 | loss: 0.1212476
	speed: 0.0564s/iter; left time: 19.0120s
Epoch: 9 cost time: 20.863616466522217
Epoch: 9, Steps: 318 | Train Loss: 0.1218183 Vali Loss: 0.1354016 Test Loss: 0.1164892
Validation loss decreased (0.135498 --> 0.135402).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1527546
	speed: 0.0714s/iter; left time: 15.6420s
	iters: 200, epoch: 10 | loss: 0.1231552
	speed: 0.0681s/iter; left time: 8.1052s
	iters: 300, epoch: 10 | loss: 0.1125383
	speed: 0.0594s/iter; left time: 1.1290s
Epoch: 10 cost time: 21.242289066314697
Epoch: 10, Steps: 318 | Train Loss: 0.1216411 Vali Loss: 0.1353093 Test Loss: 0.1164527
Validation loss decreased (0.135402 --> 0.135309).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.5948143601417542, mae:0.5933201313018799, rmse:0.771242082118988, mape:0.02198196016252041, mspe:0.0008254721760749817, rse:0.3427906930446625, r2_score:0.8669597975574809, acc:0.9780180398374796
corr: [40.51234  40.686783 40.789455 40.74866  40.848503 40.88247  40.98411
 40.873543 40.983105 40.796284 40.8704   40.92337  41.043415 40.881565
 40.997036 41.024536 40.984287 41.02153  41.281948 41.181034 41.167534
 40.971516 40.902172 40.918903 40.9109  ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1624380
	speed: 0.0766s/iter; left time: 235.3622s
	iters: 200, epoch: 1 | loss: 0.1776483
	speed: 0.0934s/iter; left time: 277.5934s
	iters: 300, epoch: 1 | loss: 0.1123605
	speed: 0.0923s/iter; left time: 264.9397s
Epoch: 1 cost time: 27.760969877243042
Epoch: 1, Steps: 317 | Train Loss: 0.2222928 Vali Loss: 0.1702130 Test Loss: 0.1373757
Validation loss decreased (inf --> 0.170213).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1454009
	speed: 0.0884s/iter; left time: 243.3324s
	iters: 200, epoch: 2 | loss: 0.1184959
	speed: 0.1024s/iter; left time: 271.8727s
	iters: 300, epoch: 2 | loss: 0.1166157
	speed: 0.0773s/iter; left time: 197.3483s
Epoch: 2 cost time: 28.235596179962158
Epoch: 2, Steps: 317 | Train Loss: 0.1332138 Vali Loss: 0.1468243 Test Loss: 0.1138928
Validation loss decreased (0.170213 --> 0.146824).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1216308
	speed: 0.0865s/iter; left time: 210.8089s
	iters: 200, epoch: 3 | loss: 0.1024529
	speed: 0.0918s/iter; left time: 214.4656s
	iters: 300, epoch: 3 | loss: 0.1383339
	speed: 0.1153s/iter; left time: 257.8667s
Epoch: 3 cost time: 31.33906316757202
Epoch: 3, Steps: 317 | Train Loss: 0.1266023 Vali Loss: 0.1450274 Test Loss: 0.1155877
Validation loss decreased (0.146824 --> 0.145027).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1263283
	speed: 0.1336s/iter; left time: 283.2509s
	iters: 200, epoch: 4 | loss: 0.1320531
	speed: 0.1101s/iter; left time: 222.3602s
	iters: 300, epoch: 4 | loss: 0.1139442
	speed: 0.0950s/iter; left time: 182.4434s
Epoch: 4 cost time: 35.31880307197571
Epoch: 4, Steps: 317 | Train Loss: 0.1237160 Vali Loss: 0.1435288 Test Loss: 0.1121353
Validation loss decreased (0.145027 --> 0.143529).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1294352
	speed: 0.0922s/iter; left time: 166.2401s
	iters: 200, epoch: 5 | loss: 0.1214198
	speed: 0.1043s/iter; left time: 177.5652s
	iters: 300, epoch: 5 | loss: 0.1195853
	speed: 0.1067s/iter; left time: 170.9647s
Epoch: 5 cost time: 32.16602540016174
Epoch: 5, Steps: 317 | Train Loss: 0.1223329 Vali Loss: 0.1431451 Test Loss: 0.1126140
Validation loss decreased (0.143529 --> 0.143145).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0947735
	speed: 0.1200s/iter; left time: 178.3641s
	iters: 200, epoch: 6 | loss: 0.1020553
	speed: 0.0995s/iter; left time: 137.8599s
	iters: 300, epoch: 6 | loss: 0.1300655
	speed: 0.0879s/iter; left time: 113.0926s
Epoch: 6 cost time: 32.59619331359863
Epoch: 6, Steps: 317 | Train Loss: 0.1215870 Vali Loss: 0.1419183 Test Loss: 0.1114167
Validation loss decreased (0.143145 --> 0.141918).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1039781
	speed: 0.0710s/iter; left time: 82.9510s
	iters: 200, epoch: 7 | loss: 0.1403848
	speed: 0.1052s/iter; left time: 112.4997s
	iters: 300, epoch: 7 | loss: 0.1216644
	speed: 0.0862s/iter; left time: 83.5719s
Epoch: 7 cost time: 29.052019596099854
Epoch: 7, Steps: 317 | Train Loss: 0.1210460 Vali Loss: 0.1430516 Test Loss: 0.1119397
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1398229
	speed: 0.1192s/iter; left time: 101.5944s
	iters: 200, epoch: 8 | loss: 0.1262252
	speed: 0.1206s/iter; left time: 90.6873s
	iters: 300, epoch: 8 | loss: 0.0974048
	speed: 0.0841s/iter; left time: 54.8070s
Epoch: 8 cost time: 34.426689863204956
Epoch: 8, Steps: 317 | Train Loss: 0.1210683 Vali Loss: 0.1421803 Test Loss: 0.1118630
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1181037
	speed: 0.0985s/iter; left time: 52.6912s
	iters: 200, epoch: 9 | loss: 0.1177136
	speed: 0.0982s/iter; left time: 42.7321s
	iters: 300, epoch: 9 | loss: 0.1255885
	speed: 0.1148s/iter; left time: 38.4745s
Epoch: 9 cost time: 33.35862851142883
Epoch: 9, Steps: 317 | Train Loss: 0.1209403 Vali Loss: 0.1428168 Test Loss: 0.1120566
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.5690919160842896, mae:0.5837669372558594, rmse:0.7543818354606628, mape:0.02164485491812229, mspe:0.0007887352840043604, rse:0.335048109292984, r2_score:0.8757908199108615, acc:0.9783551450818777
corr: [41.011868 41.126694 41.296524 41.312675 41.30661  41.405792 41.32626
 41.336525 41.339428 41.273933 41.231056 41.254623 41.2105   41.30716
 41.31243  41.25559  41.2831   41.40073  41.331802 41.334793 41.15985
 41.344486 41.301823 41.27042  41.35651  41.412647 41.428696 41.431606
 41.50303  41.283897]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1937059
	speed: 0.1150s/iter; left time: 354.3134s
	iters: 200, epoch: 1 | loss: 0.1691769
	speed: 0.0753s/iter; left time: 224.5914s
	iters: 300, epoch: 1 | loss: 0.1420851
	speed: 0.0732s/iter; left time: 210.9680s
Epoch: 1 cost time: 27.941400289535522
Epoch: 1, Steps: 318 | Train Loss: 0.2449363 Vali Loss: 0.1424235 Test Loss: 0.1071284
Validation loss decreased (inf --> 0.142424).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1096651
	speed: 0.1084s/iter; left time: 299.4651s
	iters: 200, epoch: 2 | loss: 0.1322604
	speed: 0.1005s/iter; left time: 267.6855s
	iters: 300, epoch: 2 | loss: 0.1399484
	speed: 0.1013s/iter; left time: 259.5384s
Epoch: 2 cost time: 33.178465843200684
Epoch: 2, Steps: 318 | Train Loss: 0.1249886 Vali Loss: 0.1290342 Test Loss: 0.0957118
Validation loss decreased (0.142424 --> 0.129034).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1102382
	speed: 0.1025s/iter; left time: 250.5982s
	iters: 200, epoch: 3 | loss: 0.1154907
	speed: 0.0951s/iter; left time: 223.1233s
	iters: 300, epoch: 3 | loss: 0.1444932
	speed: 0.0978s/iter; left time: 219.5650s
Epoch: 3 cost time: 31.390491247177124
Epoch: 3, Steps: 318 | Train Loss: 0.1147333 Vali Loss: 0.1224603 Test Loss: 0.0896523
Validation loss decreased (0.129034 --> 0.122460).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0985848
	speed: 0.1118s/iter; left time: 237.7941s
	iters: 200, epoch: 4 | loss: 0.1055110
	speed: 0.0998s/iter; left time: 202.2792s
	iters: 300, epoch: 4 | loss: 0.0824190
	speed: 0.0979s/iter; left time: 188.7044s
Epoch: 4 cost time: 32.90418481826782
Epoch: 4, Steps: 318 | Train Loss: 0.1095989 Vali Loss: 0.1193895 Test Loss: 0.0885440
Validation loss decreased (0.122460 --> 0.119389).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0869834
	speed: 0.0981s/iter; left time: 177.4807s
	iters: 200, epoch: 5 | loss: 0.1015346
	speed: 0.0978s/iter; left time: 167.2123s
	iters: 300, epoch: 5 | loss: 0.1190298
	speed: 0.0939s/iter; left time: 151.1162s
Epoch: 5 cost time: 30.695159196853638
Epoch: 5, Steps: 318 | Train Loss: 0.1072696 Vali Loss: 0.1179992 Test Loss: 0.0872096
Validation loss decreased (0.119389 --> 0.117999).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0959009
	speed: 0.1035s/iter; left time: 154.2985s
	iters: 200, epoch: 6 | loss: 0.1076801
	speed: 0.0964s/iter; left time: 134.0866s
	iters: 300, epoch: 6 | loss: 0.1154251
	speed: 0.0980s/iter; left time: 126.4983s
Epoch: 6 cost time: 31.522178173065186
Epoch: 6, Steps: 318 | Train Loss: 0.1060815 Vali Loss: 0.1168560 Test Loss: 0.0866025
Validation loss decreased (0.117999 --> 0.116856).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1203679
	speed: 0.1029s/iter; left time: 120.7483s
	iters: 200, epoch: 7 | loss: 0.1230509
	speed: 0.0987s/iter; left time: 105.8544s
	iters: 300, epoch: 7 | loss: 0.0845557
	speed: 0.0868s/iter; left time: 84.4460s
Epoch: 7 cost time: 30.42715096473694
Epoch: 7, Steps: 318 | Train Loss: 0.1053640 Vali Loss: 0.1170371 Test Loss: 0.0866008
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0893668
	speed: 0.0972s/iter; left time: 83.0713s
	iters: 200, epoch: 8 | loss: 0.0856953
	speed: 0.0935s/iter; left time: 70.5858s
	iters: 300, epoch: 8 | loss: 0.0926672
	speed: 0.0936s/iter; left time: 61.2830s
Epoch: 8 cost time: 30.219019651412964
Epoch: 8, Steps: 318 | Train Loss: 0.1050763 Vali Loss: 0.1159240 Test Loss: 0.0864426
Validation loss decreased (0.116856 --> 0.115924).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1121477
	speed: 0.0990s/iter; left time: 53.1633s
	iters: 200, epoch: 9 | loss: 0.1035227
	speed: 0.0975s/iter; left time: 42.5937s
	iters: 300, epoch: 9 | loss: 0.1148517
	speed: 0.0908s/iter; left time: 30.5935s
Epoch: 9 cost time: 30.32361936569214
Epoch: 9, Steps: 318 | Train Loss: 0.1052431 Vali Loss: 0.1162534 Test Loss: 0.0864123
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1301801
	speed: 0.1005s/iter; left time: 22.0145s
	iters: 200, epoch: 10 | loss: 0.1037488
	speed: 0.0928s/iter; left time: 11.0403s
	iters: 300, epoch: 10 | loss: 0.0693081
	speed: 0.0965s/iter; left time: 1.8336s
Epoch: 10 cost time: 30.580370903015137
Epoch: 10, Steps: 318 | Train Loss: 0.1053031 Vali Loss: 0.1165126 Test Loss: 0.0863747
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.44152969121932983, mae:0.5122111439704895, rmse:0.6644769906997681, mape:0.019050227478146553, mspe:0.0006185933598317206, rse:0.2958827018737793, r2_score:0.904409710382448, acc:0.9809497725218534
corr: [40.793736 40.620358 40.687756 40.973755 40.90786  40.803585 40.864674
 41.446835 41.366142 41.284084]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1842778
	speed: 0.1363s/iter; left time: 419.8655s
	iters: 200, epoch: 1 | loss: 0.1901571
	speed: 0.1063s/iter; left time: 316.9410s
	iters: 300, epoch: 1 | loss: 0.1709994
	speed: 0.1139s/iter; left time: 328.2585s
Epoch: 1 cost time: 36.83704495429993
Epoch: 1, Steps: 318 | Train Loss: 0.2345952 Vali Loss: 0.1558288 Test Loss: 0.1178282
Validation loss decreased (inf --> 0.155829).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1196141
	speed: 0.1081s/iter; left time: 298.6689s
	iters: 200, epoch: 2 | loss: 0.1261702
	speed: 0.0980s/iter; left time: 260.8571s
	iters: 300, epoch: 2 | loss: 0.1404911
	speed: 0.0945s/iter; left time: 242.1784s
Epoch: 2 cost time: 31.92954993247986
Epoch: 2, Steps: 318 | Train Loss: 0.1336844 Vali Loss: 0.1411299 Test Loss: 0.1067112
Validation loss decreased (0.155829 --> 0.141130).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1262551
	speed: 0.1069s/iter; left time: 261.4352s
	iters: 200, epoch: 3 | loss: 0.0969689
	speed: 0.1033s/iter; left time: 242.3339s
	iters: 300, epoch: 3 | loss: 0.1028655
	speed: 0.1025s/iter; left time: 230.1263s
Epoch: 3 cost time: 33.30010986328125
Epoch: 3, Steps: 318 | Train Loss: 0.1230743 Vali Loss: 0.1372620 Test Loss: 0.1016492
Validation loss decreased (0.141130 --> 0.137262).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1315413
	speed: 0.1107s/iter; left time: 235.4559s
	iters: 200, epoch: 4 | loss: 0.1236674
	speed: 0.1002s/iter; left time: 203.0990s
	iters: 300, epoch: 4 | loss: 0.1141375
	speed: 0.0978s/iter; left time: 188.4914s
Epoch: 4 cost time: 32.41796875
Epoch: 4, Steps: 318 | Train Loss: 0.1182806 Vali Loss: 0.1340721 Test Loss: 0.0994281
Validation loss decreased (0.137262 --> 0.134072).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1301181
	speed: 0.1054s/iter; left time: 190.6398s
	iters: 200, epoch: 5 | loss: 0.1068378
	speed: 0.1017s/iter; left time: 173.8470s
	iters: 300, epoch: 5 | loss: 0.1002681
	speed: 0.1069s/iter; left time: 171.9944s
Epoch: 5 cost time: 33.11552810668945
Epoch: 5, Steps: 318 | Train Loss: 0.1162083 Vali Loss: 0.1317793 Test Loss: 0.0990220
Validation loss decreased (0.134072 --> 0.131779).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1460971
	speed: 0.0978s/iter; left time: 145.8265s
	iters: 200, epoch: 6 | loss: 0.1004830
	speed: 0.0893s/iter; left time: 124.1565s
	iters: 300, epoch: 6 | loss: 0.1169715
	speed: 0.1122s/iter; left time: 144.8210s
Epoch: 6 cost time: 31.978534698486328
Epoch: 6, Steps: 318 | Train Loss: 0.1149642 Vali Loss: 0.1306947 Test Loss: 0.0985023
Validation loss decreased (0.131779 --> 0.130695).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1213755
	speed: 0.1062s/iter; left time: 124.5230s
	iters: 200, epoch: 7 | loss: 0.0722567
	speed: 0.0969s/iter; left time: 104.0097s
	iters: 300, epoch: 7 | loss: 0.1157366
	speed: 0.0895s/iter; left time: 87.1103s
Epoch: 7 cost time: 31.077534437179565
Epoch: 7, Steps: 318 | Train Loss: 0.1143151 Vali Loss: 0.1320171 Test Loss: 0.0974971
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1030002
	speed: 0.1042s/iter; left time: 89.0989s
	iters: 200, epoch: 8 | loss: 0.1176639
	speed: 0.0953s/iter; left time: 71.9672s
	iters: 300, epoch: 8 | loss: 0.1346800
	speed: 0.1041s/iter; left time: 68.2048s
Epoch: 8 cost time: 32.27727437019348
Epoch: 8, Steps: 318 | Train Loss: 0.1142287 Vali Loss: 0.1301434 Test Loss: 0.0974606
Validation loss decreased (0.130695 --> 0.130143).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1016976
	speed: 0.1129s/iter; left time: 60.6367s
	iters: 200, epoch: 9 | loss: 0.0916917
	speed: 0.1000s/iter; left time: 43.7096s
	iters: 300, epoch: 9 | loss: 0.1534884
	speed: 0.1016s/iter; left time: 34.2400s
Epoch: 9 cost time: 33.205159187316895
Epoch: 9, Steps: 318 | Train Loss: 0.1138589 Vali Loss: 0.1309329 Test Loss: 0.0973755
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1206092
	speed: 0.1046s/iter; left time: 22.9142s
	iters: 200, epoch: 10 | loss: 0.0925521
	speed: 0.1004s/iter; left time: 11.9420s
	iters: 300, epoch: 10 | loss: 0.1130118
	speed: 0.1049s/iter; left time: 1.9925s
Epoch: 10 cost time: 32.855552434921265
Epoch: 10, Steps: 318 | Train Loss: 0.1139852 Vali Loss: 0.1307258 Test Loss: 0.0972985
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.4978072941303253, mae:0.5489833354949951, rmse:0.7055546045303345, mape:0.020347407087683678, mspe:0.0006891707889735699, rse:0.3140099346637726, r2_score:0.8921103947344253, acc:0.9796525929123163
corr: [41.083412 40.964752 41.151684 41.30254  41.135754 41.341705 41.273342
 40.943348 41.539078 41.30291  41.271492 41.347183 41.435482 41.80358
 41.536366]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2201419
	speed: 0.1209s/iter; left time: 372.6121s
	iters: 200, epoch: 1 | loss: 0.1023653
	speed: 0.0988s/iter; left time: 294.6388s
	iters: 300, epoch: 1 | loss: 0.1323147
	speed: 0.0990s/iter; left time: 285.3565s
Epoch: 1 cost time: 33.92721748352051
Epoch: 1, Steps: 318 | Train Loss: 0.1903993 Vali Loss: 0.1402802 Test Loss: 0.1020656
Validation loss decreased (inf --> 0.140280).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1165499
	speed: 0.1036s/iter; left time: 286.1917s
	iters: 200, epoch: 2 | loss: 0.1439673
	speed: 0.0923s/iter; left time: 245.9030s
	iters: 300, epoch: 2 | loss: 0.1226337
	speed: 0.0900s/iter; left time: 230.6745s
Epoch: 2 cost time: 30.24326229095459
Epoch: 2, Steps: 318 | Train Loss: 0.1178926 Vali Loss: 0.1334657 Test Loss: 0.0992557
Validation loss decreased (0.140280 --> 0.133466).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1260961
	speed: 0.0988s/iter; left time: 241.5427s
	iters: 200, epoch: 3 | loss: 0.0923781
	speed: 0.0970s/iter; left time: 227.4061s
	iters: 300, epoch: 3 | loss: 0.1072278
	speed: 0.1017s/iter; left time: 228.4054s
Epoch: 3 cost time: 31.91109609603882
Epoch: 3, Steps: 318 | Train Loss: 0.1091328 Vali Loss: 0.1273943 Test Loss: 0.0938779
Validation loss decreased (0.133466 --> 0.127394).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0949595
	speed: 0.0943s/iter; left time: 200.5355s
	iters: 200, epoch: 4 | loss: 0.1157421
	speed: 0.0981s/iter; left time: 198.8615s
	iters: 300, epoch: 4 | loss: 0.1072019
	speed: 0.0986s/iter; left time: 190.0602s
Epoch: 4 cost time: 30.63939595222473
Epoch: 4, Steps: 318 | Train Loss: 0.1057000 Vali Loss: 0.1268624 Test Loss: 0.0953242
Validation loss decreased (0.127394 --> 0.126862).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1025751
	speed: 0.1038s/iter; left time: 187.8235s
	iters: 200, epoch: 5 | loss: 0.1156353
	speed: 0.1034s/iter; left time: 176.6361s
	iters: 300, epoch: 5 | loss: 0.1214554
	speed: 0.1076s/iter; left time: 173.1639s
Epoch: 5 cost time: 33.3978431224823
Epoch: 5, Steps: 318 | Train Loss: 0.1040848 Vali Loss: 0.1263696 Test Loss: 0.0939331
Validation loss decreased (0.126862 --> 0.126370).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1127314
	speed: 0.0898s/iter; left time: 133.9602s
	iters: 200, epoch: 6 | loss: 0.0834121
	speed: 0.1055s/iter; left time: 146.7978s
	iters: 300, epoch: 6 | loss: 0.1125012
	speed: 0.1168s/iter; left time: 150.8161s
Epoch: 6 cost time: 32.78855848312378
Epoch: 6, Steps: 318 | Train Loss: 0.1032339 Vali Loss: 0.1260003 Test Loss: 0.0937348
Validation loss decreased (0.126370 --> 0.126000).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1045306
	speed: 0.1214s/iter; left time: 142.4485s
	iters: 200, epoch: 7 | loss: 0.0875579
	speed: 0.1051s/iter; left time: 112.7888s
	iters: 300, epoch: 7 | loss: 0.0795014
	speed: 0.1001s/iter; left time: 97.3985s
Epoch: 7 cost time: 34.71446514129639
Epoch: 7, Steps: 318 | Train Loss: 0.1029572 Vali Loss: 0.1260448 Test Loss: 0.0935799
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1095927
	speed: 0.0963s/iter; left time: 82.3324s
	iters: 200, epoch: 8 | loss: 0.1050153
	speed: 0.0949s/iter; left time: 71.6690s
	iters: 300, epoch: 8 | loss: 0.1160960
	speed: 0.1007s/iter; left time: 65.9553s
Epoch: 8 cost time: 31.301344394683838
Epoch: 8, Steps: 318 | Train Loss: 0.1027613 Vali Loss: 0.1261027 Test Loss: 0.0936245
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1340132
	speed: 0.1085s/iter; left time: 58.2744s
	iters: 200, epoch: 9 | loss: 0.1189425
	speed: 0.0957s/iter; left time: 41.8049s
	iters: 300, epoch: 9 | loss: 0.0749770
	speed: 0.0896s/iter; left time: 30.1839s
Epoch: 9 cost time: 31.048593044281006
Epoch: 9, Steps: 318 | Train Loss: 0.1026475 Vali Loss: 0.1258669 Test Loss: 0.0934956
Validation loss decreased (0.126000 --> 0.125867).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0773947
	speed: 0.1005s/iter; left time: 22.0140s
	iters: 200, epoch: 10 | loss: 0.1114955
	speed: 0.0986s/iter; left time: 11.7373s
	iters: 300, epoch: 10 | loss: 0.1221452
	speed: 0.0953s/iter; left time: 1.8105s
Epoch: 10 cost time: 31.429131984710693
Epoch: 10, Steps: 318 | Train Loss: 0.1024916 Vali Loss: 0.1261484 Test Loss: 0.0935419
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.4775548577308655, mae:0.5371485948562622, rmse:0.6910534501075745, mape:0.019948972389101982, mspe:0.000668109452817589, rse:0.3073640763759613, r2_score:0.8956841798926309, acc:0.980051027610898
corr: [40.5135   40.48238  40.52916  40.580055 40.62056  40.760223 40.926018
 40.702866 41.020054 40.804058 40.935574 41.032475 40.966694 41.0669
 41.093445 41.114506 41.087658 41.33273  41.53103  41.44144 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1923032
	speed: 0.1174s/iter; left time: 361.6339s
	iters: 200, epoch: 1 | loss: 0.1104860
	speed: 0.0957s/iter; left time: 285.1889s
	iters: 300, epoch: 1 | loss: 0.1490194
	speed: 0.1023s/iter; left time: 294.7385s
Epoch: 1 cost time: 33.166178464889526
Epoch: 1, Steps: 318 | Train Loss: 0.2361357 Vali Loss: 0.1430227 Test Loss: 0.1122685
Validation loss decreased (inf --> 0.143023).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1350655
	speed: 0.1115s/iter; left time: 308.1254s
	iters: 200, epoch: 2 | loss: 0.1038025
	speed: 0.1003s/iter; left time: 267.0633s
	iters: 300, epoch: 2 | loss: 0.1341969
	speed: 0.0998s/iter; left time: 255.7765s
Epoch: 2 cost time: 32.993353605270386
Epoch: 2, Steps: 318 | Train Loss: 0.1248506 Vali Loss: 0.1361588 Test Loss: 0.1062306
Validation loss decreased (0.143023 --> 0.136159).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1314465
	speed: 0.1048s/iter; left time: 256.3153s
	iters: 200, epoch: 3 | loss: 0.1196997
	speed: 0.0942s/iter; left time: 220.7854s
	iters: 300, epoch: 3 | loss: 0.1264431
	speed: 0.1001s/iter; left time: 224.7207s
Epoch: 3 cost time: 32.02948760986328
Epoch: 3, Steps: 318 | Train Loss: 0.1182287 Vali Loss: 0.1355848 Test Loss: 0.1064334
Validation loss decreased (0.136159 --> 0.135585).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1036770
	speed: 0.1027s/iter; left time: 218.3525s
	iters: 200, epoch: 4 | loss: 0.1070376
	speed: 0.0996s/iter; left time: 201.9171s
	iters: 300, epoch: 4 | loss: 0.1250514
	speed: 0.1028s/iter; left time: 198.1158s
Epoch: 4 cost time: 32.560603857040405
Epoch: 4, Steps: 318 | Train Loss: 0.1146722 Vali Loss: 0.1336356 Test Loss: 0.1057241
Validation loss decreased (0.135585 --> 0.133636).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0993715
	speed: 0.1069s/iter; left time: 193.3821s
	iters: 200, epoch: 5 | loss: 0.1268160
	speed: 0.1025s/iter; left time: 175.2097s
	iters: 300, epoch: 5 | loss: 0.1738562
	speed: 0.0921s/iter; left time: 148.2593s
Epoch: 5 cost time: 32.32980036735535
Epoch: 5, Steps: 318 | Train Loss: 0.1131229 Vali Loss: 0.1317741 Test Loss: 0.1048794
Validation loss decreased (0.133636 --> 0.131774).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0796838
	speed: 0.1152s/iter; left time: 171.7064s
	iters: 200, epoch: 6 | loss: 0.1176055
	speed: 0.1024s/iter; left time: 142.4022s
	iters: 300, epoch: 6 | loss: 0.1195800
	speed: 0.1001s/iter; left time: 129.2697s
Epoch: 6 cost time: 33.842414140701294
Epoch: 6, Steps: 318 | Train Loss: 0.1119462 Vali Loss: 0.1305956 Test Loss: 0.1038996
Validation loss decreased (0.131774 --> 0.130596).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1245147
	speed: 0.1088s/iter; left time: 127.6157s
	iters: 200, epoch: 7 | loss: 0.1303109
	speed: 0.1072s/iter; left time: 114.9816s
	iters: 300, epoch: 7 | loss: 0.0965834
	speed: 0.0999s/iter; left time: 97.2176s
Epoch: 7 cost time: 33.50758171081543
Epoch: 7, Steps: 318 | Train Loss: 0.1116467 Vali Loss: 0.1305866 Test Loss: 0.1036368
Validation loss decreased (0.130596 --> 0.130587).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1053154
	speed: 0.1000s/iter; left time: 85.5022s
	iters: 200, epoch: 8 | loss: 0.0994932
	speed: 0.0965s/iter; left time: 72.8469s
	iters: 300, epoch: 8 | loss: 0.0998393
	speed: 0.1035s/iter; left time: 67.8010s
Epoch: 8 cost time: 31.272037982940674
Epoch: 8, Steps: 318 | Train Loss: 0.1112584 Vali Loss: 0.1299514 Test Loss: 0.1035431
Validation loss decreased (0.130587 --> 0.129951).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1024389
	speed: 0.1036s/iter; left time: 55.6440s
	iters: 200, epoch: 9 | loss: 0.1596866
	speed: 0.1046s/iter; left time: 45.7156s
	iters: 300, epoch: 9 | loss: 0.1147140
	speed: 0.0934s/iter; left time: 31.4881s
Epoch: 9 cost time: 31.9235577583313
Epoch: 9, Steps: 318 | Train Loss: 0.1112967 Vali Loss: 0.1298597 Test Loss: 0.1035717
Validation loss decreased (0.129951 --> 0.129860).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1329866
	speed: 0.1020s/iter; left time: 22.3381s
	iters: 200, epoch: 10 | loss: 0.1160346
	speed: 0.0994s/iter; left time: 11.8235s
	iters: 300, epoch: 10 | loss: 0.0924445
	speed: 0.1052s/iter; left time: 1.9992s
Epoch: 10 cost time: 32.723206758499146
Epoch: 10, Steps: 318 | Train Loss: 0.1110806 Vali Loss: 0.1299551 Test Loss: 0.1035445
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.5290213823318481, mae:0.5613442063331604, rmse:0.7273385524749756, mape:0.02084309794008732, mspe:0.0007389714592136443, rse:0.323277086019516, r2_score:0.8856411685882428, acc:0.9791569020599127
corr: [40.8852   40.892292 40.88256  40.84399  40.849712 40.89819  40.97914
 40.89094  40.94308  40.968662 41.14731  41.218998 41.238594 41.358547
 41.279007 41.233475 41.18506  41.331375 41.488697 41.335922 41.294937
 41.27852  41.256115 41.337364 41.493042]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1655008
	speed: 0.1346s/iter; left time: 413.4520s
	iters: 200, epoch: 1 | loss: 0.1911643
	speed: 0.1279s/iter; left time: 379.9241s
	iters: 300, epoch: 1 | loss: 0.1190611
	speed: 0.1134s/iter; left time: 325.7028s
Epoch: 1 cost time: 39.769104957580566
Epoch: 1, Steps: 317 | Train Loss: 0.2194963 Vali Loss: 0.1695793 Test Loss: 0.1314469
Validation loss decreased (inf --> 0.169579).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1589704
	speed: 0.1233s/iter; left time: 339.5479s
	iters: 200, epoch: 2 | loss: 0.1240858
	speed: 0.1090s/iter; left time: 289.3226s
	iters: 300, epoch: 2 | loss: 0.1239330
	speed: 0.1193s/iter; left time: 304.5810s
Epoch: 2 cost time: 37.34192776679993
Epoch: 2, Steps: 317 | Train Loss: 0.1388333 Vali Loss: 0.1510128 Test Loss: 0.1134317
Validation loss decreased (0.169579 --> 0.151013).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1259820
	speed: 0.1239s/iter; left time: 301.9679s
	iters: 200, epoch: 3 | loss: 0.1103351
	speed: 0.0939s/iter; left time: 219.4131s
	iters: 300, epoch: 3 | loss: 0.1458481
	speed: 0.0973s/iter; left time: 217.6701s
Epoch: 3 cost time: 33.687705516815186
Epoch: 3, Steps: 317 | Train Loss: 0.1313896 Vali Loss: 0.1513987 Test Loss: 0.1160658
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1369366
	speed: 0.1110s/iter; left time: 235.3325s
	iters: 200, epoch: 4 | loss: 0.1354336
	speed: 0.1044s/iter; left time: 210.9743s
	iters: 300, epoch: 4 | loss: 0.1124104
	speed: 0.1007s/iter; left time: 193.2761s
Epoch: 4 cost time: 33.543567419052124
Epoch: 4, Steps: 317 | Train Loss: 0.1281693 Vali Loss: 0.1494249 Test Loss: 0.1116393
Validation loss decreased (0.151013 --> 0.149425).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1362561
	speed: 0.1058s/iter; left time: 190.6989s
	iters: 200, epoch: 5 | loss: 0.1314358
	speed: 0.1046s/iter; left time: 178.0565s
	iters: 300, epoch: 5 | loss: 0.1292830
	speed: 0.1048s/iter; left time: 168.0242s
Epoch: 5 cost time: 33.223581314086914
Epoch: 5, Steps: 317 | Train Loss: 0.1266837 Vali Loss: 0.1490657 Test Loss: 0.1124311
Validation loss decreased (0.149425 --> 0.149066).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0944608
	speed: 0.1153s/iter; left time: 171.3136s
	iters: 200, epoch: 6 | loss: 0.1105223
	speed: 0.0955s/iter; left time: 132.3512s
	iters: 300, epoch: 6 | loss: 0.1387989
	speed: 0.0953s/iter; left time: 122.6056s
Epoch: 6 cost time: 32.70538640022278
Epoch: 6, Steps: 317 | Train Loss: 0.1259174 Vali Loss: 0.1485315 Test Loss: 0.1115136
Validation loss decreased (0.149066 --> 0.148532).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1157900
	speed: 0.1344s/iter; left time: 157.1573s
	iters: 200, epoch: 7 | loss: 0.1456839
	speed: 0.1160s/iter; left time: 124.0522s
	iters: 300, epoch: 7 | loss: 0.1355427
	speed: 0.1093s/iter; left time: 105.8893s
Epoch: 7 cost time: 38.08825969696045
Epoch: 7, Steps: 317 | Train Loss: 0.1253980 Vali Loss: 0.1492108 Test Loss: 0.1118466
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1401968
	speed: 0.1149s/iter; left time: 97.9194s
	iters: 200, epoch: 8 | loss: 0.1228004
	speed: 0.1051s/iter; left time: 79.0219s
	iters: 300, epoch: 8 | loss: 0.1048405
	speed: 0.1000s/iter; left time: 65.2103s
Epoch: 8 cost time: 33.614898443222046
Epoch: 8, Steps: 317 | Train Loss: 0.1252677 Vali Loss: 0.1487360 Test Loss: 0.1117603
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1134223
	speed: 0.1099s/iter; left time: 58.8220s
	iters: 200, epoch: 9 | loss: 0.1118391
	speed: 0.1204s/iter; left time: 52.3725s
	iters: 300, epoch: 9 | loss: 0.1399349
	speed: 0.1166s/iter; left time: 39.0599s
Epoch: 9 cost time: 36.5372359752655
Epoch: 9, Steps: 317 | Train Loss: 0.1252542 Vali Loss: 0.1493115 Test Loss: 0.1121085
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.5695866942405701, mae:0.586248517036438, rmse:0.7547096610069275, mape:0.021720005199313164, mspe:0.0007852161070331931, rse:0.3351937234401703, r2_score:0.875294573782755, acc:0.9782799948006868
corr: [40.932983 40.969566 41.059345 40.995    41.247482 41.07798  41.16158
 41.00156  41.147858 41.033085 41.08987  41.07163  40.999332 41.15371
 41.17841  41.062927 41.006264 41.177193 41.203625 41.375935 41.287422
 41.26728  41.279175 41.370544 41.394165 41.50948  41.376663 41.44029
 41.281307 41.412956]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1937588
	speed: 0.1408s/iter; left time: 433.6709s
	iters: 200, epoch: 1 | loss: 0.1691349
	speed: 0.1084s/iter; left time: 323.0665s
	iters: 300, epoch: 1 | loss: 0.1417715
	speed: 0.1068s/iter; left time: 307.8197s
Epoch: 1 cost time: 37.44048571586609
Epoch: 1, Steps: 318 | Train Loss: 0.2445107 Vali Loss: 0.1424321 Test Loss: 0.1067624
Validation loss decreased (inf --> 0.142432).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1094744
	speed: 0.1194s/iter; left time: 329.9369s
	iters: 200, epoch: 2 | loss: 0.1324472
	speed: 0.1129s/iter; left time: 300.6862s
	iters: 300, epoch: 2 | loss: 0.1394487
	speed: 0.1016s/iter; left time: 260.4086s
Epoch: 2 cost time: 35.30177545547485
Epoch: 2, Steps: 318 | Train Loss: 0.1249535 Vali Loss: 0.1289952 Test Loss: 0.0956486
Validation loss decreased (0.142432 --> 0.128995).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1100756
	speed: 0.1150s/iter; left time: 281.2494s
	iters: 200, epoch: 3 | loss: 0.1154455
	speed: 0.1011s/iter; left time: 237.0210s
	iters: 300, epoch: 3 | loss: 0.1440521
	speed: 0.1087s/iter; left time: 244.1257s
Epoch: 3 cost time: 34.488001108169556
Epoch: 3, Steps: 318 | Train Loss: 0.1148598 Vali Loss: 0.1224419 Test Loss: 0.0895402
Validation loss decreased (0.128995 --> 0.122442).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0985276
	speed: 0.1146s/iter; left time: 243.8006s
	iters: 200, epoch: 4 | loss: 0.1056059
	speed: 0.1003s/iter; left time: 203.3801s
	iters: 300, epoch: 4 | loss: 0.0823886
	speed: 0.1006s/iter; left time: 193.8782s
Epoch: 4 cost time: 33.32522654533386
Epoch: 4, Steps: 318 | Train Loss: 0.1096548 Vali Loss: 0.1193082 Test Loss: 0.0884027
Validation loss decreased (0.122442 --> 0.119308).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0873297
	speed: 0.1147s/iter; left time: 207.4620s
	iters: 200, epoch: 5 | loss: 0.1014728
	speed: 0.1017s/iter; left time: 173.8248s
	iters: 300, epoch: 5 | loss: 0.1189111
	speed: 0.1069s/iter; left time: 171.9781s
Epoch: 5 cost time: 34.58075261116028
Epoch: 5, Steps: 318 | Train Loss: 0.1073585 Vali Loss: 0.1178461 Test Loss: 0.0871000
Validation loss decreased (0.119308 --> 0.117846).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0960281
	speed: 0.1162s/iter; left time: 173.3238s
	iters: 200, epoch: 6 | loss: 0.1077227
	speed: 0.1075s/iter; left time: 149.4921s
	iters: 300, epoch: 6 | loss: 0.1159125
	speed: 0.1086s/iter; left time: 140.1752s
Epoch: 6 cost time: 35.046857595443726
Epoch: 6, Steps: 318 | Train Loss: 0.1061969 Vali Loss: 0.1167297 Test Loss: 0.0865070
Validation loss decreased (0.117846 --> 0.116730).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1203517
	speed: 0.1161s/iter; left time: 136.1550s
	iters: 200, epoch: 7 | loss: 0.1226951
	speed: 0.0987s/iter; left time: 105.8794s
	iters: 300, epoch: 7 | loss: 0.0845717
	speed: 0.1156s/iter; left time: 112.4302s
Epoch: 7 cost time: 34.83963108062744
Epoch: 7, Steps: 318 | Train Loss: 0.1054714 Vali Loss: 0.1169048 Test Loss: 0.0865025
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0891059
	speed: 0.1074s/iter; left time: 91.8211s
	iters: 200, epoch: 8 | loss: 0.0857626
	speed: 0.1186s/iter; left time: 89.5263s
	iters: 300, epoch: 8 | loss: 0.0929008
	speed: 0.1112s/iter; left time: 72.8176s
Epoch: 8 cost time: 35.98531222343445
Epoch: 8, Steps: 318 | Train Loss: 0.1051905 Vali Loss: 0.1158102 Test Loss: 0.0863467
Validation loss decreased (0.116730 --> 0.115810).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1122529
	speed: 0.1111s/iter; left time: 59.6776s
	iters: 200, epoch: 9 | loss: 0.1038633
	speed: 0.1181s/iter; left time: 51.6115s
	iters: 300, epoch: 9 | loss: 0.1147547
	speed: 0.1045s/iter; left time: 35.2264s
Epoch: 9 cost time: 35.364041328430176
Epoch: 9, Steps: 318 | Train Loss: 0.1053529 Vali Loss: 0.1161267 Test Loss: 0.0863152
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1299941
	speed: 0.1130s/iter; left time: 24.7385s
	iters: 200, epoch: 10 | loss: 0.1036876
	speed: 0.1053s/iter; left time: 12.5280s
	iters: 300, epoch: 10 | loss: 0.0692241
	speed: 0.1122s/iter; left time: 2.1319s
Epoch: 10 cost time: 35.2207453250885
Epoch: 10, Steps: 318 | Train Loss: 0.1054186 Vali Loss: 0.1163967 Test Loss: 0.0862786
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.4410395622253418, mae:0.5119951963424683, rmse:0.6641080975532532, mape:0.01904197596013546, mspe:0.0006178747280500829, rse:0.2957184314727783, r2_score:0.9044791198982903, acc:0.9809580240398645
corr: [40.806858 40.62593  40.69644  40.974644 40.909363 40.802803 40.859787
 41.449795 41.353886 41.270283]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1845374
	speed: 0.1376s/iter; left time: 424.0722s
	iters: 200, epoch: 1 | loss: 0.1792635
	speed: 0.1164s/iter; left time: 347.0839s
	iters: 300, epoch: 1 | loss: 0.1576777
	speed: 0.1205s/iter; left time: 347.0523s
Epoch: 1 cost time: 39.65408444404602
Epoch: 1, Steps: 318 | Train Loss: 0.2278202 Vali Loss: 0.1513705 Test Loss: 0.1126642
Validation loss decreased (inf --> 0.151371).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1144759
	speed: 0.1190s/iter; left time: 328.7107s
	iters: 200, epoch: 2 | loss: 0.1156908
	speed: 0.1204s/iter; left time: 320.6980s
	iters: 300, epoch: 2 | loss: 0.1292247
	speed: 0.1167s/iter; left time: 299.2242s
Epoch: 2 cost time: 37.698866844177246
Epoch: 2, Steps: 318 | Train Loss: 0.1262994 Vali Loss: 0.1343701 Test Loss: 0.0987068
Validation loss decreased (0.151371 --> 0.134370).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1170092
	speed: 0.1243s/iter; left time: 303.9004s
	iters: 200, epoch: 3 | loss: 0.0892262
	speed: 0.1221s/iter; left time: 286.3846s
	iters: 300, epoch: 3 | loss: 0.0948937
	speed: 0.1200s/iter; left time: 269.3484s
Epoch: 3 cost time: 39.0325608253479
Epoch: 3, Steps: 318 | Train Loss: 0.1146131 Vali Loss: 0.1301134 Test Loss: 0.0946234
Validation loss decreased (0.134370 --> 0.130113).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1214337
	speed: 0.1290s/iter; left time: 274.3464s
	iters: 200, epoch: 4 | loss: 0.1115239
	speed: 0.1241s/iter; left time: 251.5130s
	iters: 300, epoch: 4 | loss: 0.1068508
	speed: 0.1192s/iter; left time: 229.6037s
Epoch: 4 cost time: 39.52592945098877
Epoch: 4, Steps: 318 | Train Loss: 0.1098905 Vali Loss: 0.1268522 Test Loss: 0.0920595
Validation loss decreased (0.130113 --> 0.126852).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1197305
	speed: 0.1284s/iter; left time: 232.3487s
	iters: 200, epoch: 5 | loss: 0.1035913
	speed: 0.1152s/iter; left time: 196.8482s
	iters: 300, epoch: 5 | loss: 0.0893719
	speed: 0.1150s/iter; left time: 185.0366s
Epoch: 5 cost time: 38.34353804588318
Epoch: 5, Steps: 318 | Train Loss: 0.1078408 Vali Loss: 0.1259595 Test Loss: 0.0922906
Validation loss decreased (0.126852 --> 0.125960).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1309960
	speed: 0.1290s/iter; left time: 192.3764s
	iters: 200, epoch: 6 | loss: 0.0959249
	speed: 0.1199s/iter; left time: 166.8345s
	iters: 300, epoch: 6 | loss: 0.1127120
	speed: 0.1221s/iter; left time: 157.6679s
Epoch: 6 cost time: 39.62071633338928
Epoch: 6, Steps: 318 | Train Loss: 0.1066829 Vali Loss: 0.1246375 Test Loss: 0.0913247
Validation loss decreased (0.125960 --> 0.124637).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1072723
	speed: 0.1382s/iter; left time: 162.0514s
	iters: 200, epoch: 7 | loss: 0.0787491
	speed: 0.1250s/iter; left time: 134.1254s
	iters: 300, epoch: 7 | loss: 0.1116880
	speed: 0.1329s/iter; left time: 129.3355s
Epoch: 7 cost time: 42.43366074562073
Epoch: 7, Steps: 318 | Train Loss: 0.1061393 Vali Loss: 0.1258115 Test Loss: 0.0906381
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0910338
	speed: 0.1516s/iter; left time: 129.6066s
	iters: 200, epoch: 8 | loss: 0.1032875
	speed: 0.1272s/iter; left time: 96.0338s
	iters: 300, epoch: 8 | loss: 0.1115161
	speed: 0.1186s/iter; left time: 77.7038s
Epoch: 8 cost time: 41.92951822280884
Epoch: 8, Steps: 318 | Train Loss: 0.1060856 Vali Loss: 0.1242212 Test Loss: 0.0907018
Validation loss decreased (0.124637 --> 0.124221).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0966627
	speed: 0.1469s/iter; left time: 78.8864s
	iters: 200, epoch: 9 | loss: 0.0911862
	speed: 0.1266s/iter; left time: 55.3116s
	iters: 300, epoch: 9 | loss: 0.1431439
	speed: 0.1254s/iter; left time: 42.2688s
Epoch: 9 cost time: 41.851566314697266
Epoch: 9, Steps: 318 | Train Loss: 0.1056909 Vali Loss: 0.1247572 Test Loss: 0.0906246
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1090283
	speed: 0.1280s/iter; left time: 28.0370s
	iters: 200, epoch: 10 | loss: 0.0902574
	speed: 0.1259s/iter; left time: 14.9775s
	iters: 300, epoch: 10 | loss: 0.1048302
	speed: 0.1206s/iter; left time: 2.2923s
Epoch: 10 cost time: 39.682098388671875
Epoch: 10, Steps: 318 | Train Loss: 0.1058181 Vali Loss: 0.1245749 Test Loss: 0.0905520
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.46328479051589966, mae:0.5300012230873108, rmse:0.6806502938270569, mape:0.01967553049325943, mspe:0.0006471360684372485, rse:0.3029261529445648, r2_score:0.9000643820379819, acc:0.9803244695067406
corr: [40.834854 40.771915 40.91343  40.986782 40.923027 41.04783  41.151886
 40.86523  41.481583 41.283997 41.24143  41.512703 41.41076  41.63186
 41.703674]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2195930
	speed: 0.1392s/iter; left time: 428.8744s
	iters: 200, epoch: 1 | loss: 0.1028866
	speed: 0.1166s/iter; left time: 347.6493s
	iters: 300, epoch: 1 | loss: 0.1319741
	speed: 0.1249s/iter; left time: 359.7175s
Epoch: 1 cost time: 40.00654649734497
Epoch: 1, Steps: 318 | Train Loss: 0.1914481 Vali Loss: 0.1401657 Test Loss: 0.1023201
Validation loss decreased (inf --> 0.140166).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1163762
	speed: 0.1142s/iter; left time: 315.4430s
	iters: 200, epoch: 2 | loss: 0.1442408
	speed: 0.1163s/iter; left time: 309.6144s
	iters: 300, epoch: 2 | loss: 0.1225564
	speed: 0.1127s/iter; left time: 288.8714s
Epoch: 2 cost time: 36.42617392539978
Epoch: 2, Steps: 318 | Train Loss: 0.1178464 Vali Loss: 0.1332674 Test Loss: 0.0994243
Validation loss decreased (0.140166 --> 0.133267).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1264211
	speed: 0.1166s/iter; left time: 285.0285s
	iters: 200, epoch: 3 | loss: 0.0919779
	speed: 0.1135s/iter; left time: 266.2227s
	iters: 300, epoch: 3 | loss: 0.1069324
	speed: 0.0996s/iter; left time: 223.7129s
Epoch: 3 cost time: 34.799744844436646
Epoch: 3, Steps: 318 | Train Loss: 0.1090689 Vali Loss: 0.1272194 Test Loss: 0.0940874
Validation loss decreased (0.133267 --> 0.127219).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0945475
	speed: 0.1143s/iter; left time: 243.1101s
	iters: 200, epoch: 4 | loss: 0.1167049
	speed: 0.1157s/iter; left time: 234.5834s
	iters: 300, epoch: 4 | loss: 0.1073906
	speed: 0.1118s/iter; left time: 215.4512s
Epoch: 4 cost time: 36.254462242126465
Epoch: 4, Steps: 318 | Train Loss: 0.1056148 Vali Loss: 0.1269727 Test Loss: 0.0956475
Validation loss decreased (0.127219 --> 0.126973).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1021629
	speed: 0.1238s/iter; left time: 224.0344s
	iters: 200, epoch: 5 | loss: 0.1158609
	speed: 0.1162s/iter; left time: 198.5316s
	iters: 300, epoch: 5 | loss: 0.1212833
	speed: 0.1061s/iter; left time: 170.7805s
Epoch: 5 cost time: 36.53822565078735
Epoch: 5, Steps: 318 | Train Loss: 0.1040172 Vali Loss: 0.1262162 Test Loss: 0.0941038
Validation loss decreased (0.126973 --> 0.126216).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1129868
	speed: 0.1187s/iter; left time: 177.0197s
	iters: 200, epoch: 6 | loss: 0.0831207
	speed: 0.1137s/iter; left time: 158.1239s
	iters: 300, epoch: 6 | loss: 0.1122701
	speed: 0.1095s/iter; left time: 141.3293s
Epoch: 6 cost time: 36.25243353843689
Epoch: 6, Steps: 318 | Train Loss: 0.1031498 Vali Loss: 0.1258815 Test Loss: 0.0939391
Validation loss decreased (0.126216 --> 0.125881).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1043668
	speed: 0.1211s/iter; left time: 142.1038s
	iters: 200, epoch: 7 | loss: 0.0871693
	speed: 0.1143s/iter; left time: 122.6538s
	iters: 300, epoch: 7 | loss: 0.0798281
	speed: 0.1074s/iter; left time: 104.5147s
Epoch: 7 cost time: 36.1181755065918
Epoch: 7, Steps: 318 | Train Loss: 0.1028716 Vali Loss: 0.1259033 Test Loss: 0.0937880
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1096283
	speed: 0.1159s/iter; left time: 99.0741s
	iters: 200, epoch: 8 | loss: 0.1049682
	speed: 0.1054s/iter; left time: 79.5461s
	iters: 300, epoch: 8 | loss: 0.1157249
	speed: 0.1154s/iter; left time: 75.5732s
Epoch: 8 cost time: 35.7057363986969
Epoch: 8, Steps: 318 | Train Loss: 0.1026679 Vali Loss: 0.1259633 Test Loss: 0.0938330
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1332934
	speed: 0.1190s/iter; left time: 63.8892s
	iters: 200, epoch: 9 | loss: 0.1182360
	speed: 0.1024s/iter; left time: 44.7703s
	iters: 300, epoch: 9 | loss: 0.0751233
	speed: 0.1081s/iter; left time: 36.4356s
Epoch: 9 cost time: 35.187355041503906
Epoch: 9, Steps: 318 | Train Loss: 0.1025614 Vali Loss: 0.1257399 Test Loss: 0.0937103
Validation loss decreased (0.125881 --> 0.125740).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0772910
	speed: 0.1155s/iter; left time: 25.3048s
	iters: 200, epoch: 10 | loss: 0.1116243
	speed: 0.1056s/iter; left time: 12.5700s
	iters: 300, epoch: 10 | loss: 0.1223088
	speed: 0.1096s/iter; left time: 2.0833s
Epoch: 10 cost time: 35.09649705886841
Epoch: 10, Steps: 318 | Train Loss: 0.1024188 Vali Loss: 0.1260036 Test Loss: 0.0937536
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.4786511957645416, mae:0.5377580523490906, rmse:0.6918462514877319, mape:0.01997566968202591, mspe:0.0006702099926769733, rse:0.3077166974544525, r2_score:0.8954901221258809, acc:0.9800243303179741
corr: [40.542625 40.511436 40.551765 40.60807  40.63511  40.79012  40.935448
 40.72611  41.03888  40.828747 40.948875 41.054848 40.988724 41.09287
 41.108963 41.126087 41.09905  41.344368 41.535168 41.454117]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1676739
	speed: 0.1351s/iter; left time: 416.3854s
	iters: 200, epoch: 1 | loss: 0.1078359
	speed: 0.1124s/iter; left time: 335.1534s
	iters: 300, epoch: 1 | loss: 0.1499729
	speed: 0.1141s/iter; left time: 328.6254s
Epoch: 1 cost time: 37.98753833770752
Epoch: 1, Steps: 318 | Train Loss: 0.2152253 Vali Loss: 0.1418918 Test Loss: 0.1085936
Validation loss decreased (inf --> 0.141892).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1338484
	speed: 0.1199s/iter; left time: 331.2917s
	iters: 200, epoch: 2 | loss: 0.1019594
	speed: 0.1127s/iter; left time: 300.2530s
	iters: 300, epoch: 2 | loss: 0.1316178
	speed: 0.1146s/iter; left time: 293.6115s
Epoch: 2 cost time: 37.00570774078369
Epoch: 2, Steps: 318 | Train Loss: 0.1232694 Vali Loss: 0.1354381 Test Loss: 0.1029936
Validation loss decreased (0.141892 --> 0.135438).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1279010
	speed: 0.1235s/iter; left time: 301.8461s
	iters: 200, epoch: 3 | loss: 0.1221561
	speed: 0.1126s/iter; left time: 264.1376s
	iters: 300, epoch: 3 | loss: 0.1220491
	speed: 0.1182s/iter; left time: 265.3232s
Epoch: 3 cost time: 37.26978063583374
Epoch: 3, Steps: 318 | Train Loss: 0.1163667 Vali Loss: 0.1350139 Test Loss: 0.1028319
Validation loss decreased (0.135438 --> 0.135014).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1002040
	speed: 0.1224s/iter; left time: 260.2748s
	iters: 200, epoch: 4 | loss: 0.1059726
	speed: 0.1138s/iter; left time: 230.6555s
	iters: 300, epoch: 4 | loss: 0.1233640
	speed: 0.1252s/iter; left time: 241.2232s
Epoch: 4 cost time: 37.80275774002075
Epoch: 4, Steps: 318 | Train Loss: 0.1127896 Vali Loss: 0.1333499 Test Loss: 0.1019777
Validation loss decreased (0.135014 --> 0.133350).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0964575
	speed: 0.1264s/iter; left time: 228.7206s
	iters: 200, epoch: 5 | loss: 0.1237118
	speed: 0.1171s/iter; left time: 200.1387s
	iters: 300, epoch: 5 | loss: 0.1713874
	speed: 0.1175s/iter; left time: 189.0547s
Epoch: 5 cost time: 38.27696084976196
Epoch: 5, Steps: 318 | Train Loss: 0.1112929 Vali Loss: 0.1316098 Test Loss: 0.1009967
Validation loss decreased (0.133350 --> 0.131610).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0776617
	speed: 0.1145s/iter; left time: 170.6937s
	iters: 200, epoch: 6 | loss: 0.1120230
	speed: 0.1203s/iter; left time: 167.2832s
	iters: 300, epoch: 6 | loss: 0.1181357
	speed: 0.1170s/iter; left time: 151.1058s
Epoch: 6 cost time: 37.48602890968323
Epoch: 6, Steps: 318 | Train Loss: 0.1101897 Vali Loss: 0.1303786 Test Loss: 0.1001413
Validation loss decreased (0.131610 --> 0.130379).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1227653
	speed: 0.1184s/iter; left time: 138.8452s
	iters: 200, epoch: 7 | loss: 0.1257816
	speed: 0.1164s/iter; left time: 124.8937s
	iters: 300, epoch: 7 | loss: 0.0966757
	speed: 0.1209s/iter; left time: 117.6708s
Epoch: 7 cost time: 38.05878829956055
Epoch: 7, Steps: 318 | Train Loss: 0.1098676 Vali Loss: 0.1304209 Test Loss: 0.0998756
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1032011
	speed: 0.1282s/iter; left time: 109.5938s
	iters: 200, epoch: 8 | loss: 0.0997025
	speed: 0.1187s/iter; left time: 89.6299s
	iters: 300, epoch: 8 | loss: 0.1000455
	speed: 0.1245s/iter; left time: 81.5742s
Epoch: 8 cost time: 39.25921702384949
Epoch: 8, Steps: 318 | Train Loss: 0.1093739 Vali Loss: 0.1298344 Test Loss: 0.0998433
Validation loss decreased (0.130379 --> 0.129834).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1043127
	speed: 0.1177s/iter; left time: 63.2048s
	iters: 200, epoch: 9 | loss: 0.1577496
	speed: 0.1168s/iter; left time: 51.0610s
	iters: 300, epoch: 9 | loss: 0.1104924
	speed: 0.1119s/iter; left time: 37.7157s
Epoch: 9 cost time: 36.59129500389099
Epoch: 9, Steps: 318 | Train Loss: 0.1094559 Vali Loss: 0.1297687 Test Loss: 0.0998364
Validation loss decreased (0.129834 --> 0.129769).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1311085
	speed: 0.1222s/iter; left time: 26.7627s
	iters: 200, epoch: 10 | loss: 0.1171459
	speed: 0.1192s/iter; left time: 14.1855s
	iters: 300, epoch: 10 | loss: 0.0900186
	speed: 0.1109s/iter; left time: 2.1075s
Epoch: 10 cost time: 37.32772183418274
Epoch: 10, Steps: 318 | Train Loss: 0.1092519 Vali Loss: 0.1298537 Test Loss: 0.0998219
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.5099419951438904, mae:0.5506705045700073, rmse:0.7141022086143494, mape:0.020440110936760902, mspe:0.0007100448710843921, rse:0.3173939883708954, r2_score:0.8896464332348447, acc:0.9795598890632391
corr: [40.74531  40.728664 40.734642 40.713062 40.78279  40.866585 40.96514
 40.804794 40.967632 40.95153  41.157066 41.202126 41.272167 41.30278
 41.29109  41.240795 41.160316 41.30085  41.464386 41.365475 41.331028
 41.301064 41.278812 41.286663 41.46313 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1615702
	speed: 0.1430s/iter; left time: 439.3021s
	iters: 200, epoch: 1 | loss: 0.1819711
	speed: 0.1247s/iter; left time: 370.3609s
	iters: 300, epoch: 1 | loss: 0.1133713
	speed: 0.1267s/iter; left time: 363.7518s
Epoch: 1 cost time: 41.482091665267944
Epoch: 1, Steps: 317 | Train Loss: 0.2135481 Vali Loss: 0.1685830 Test Loss: 0.1300321
Validation loss decreased (inf --> 0.168583).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1516589
	speed: 0.1375s/iter; left time: 378.7412s
	iters: 200, epoch: 2 | loss: 0.1245609
	speed: 0.1311s/iter; left time: 348.0606s
	iters: 300, epoch: 2 | loss: 0.1220728
	speed: 0.1372s/iter; left time: 350.3801s
Epoch: 2 cost time: 43.07216119766235
Epoch: 2, Steps: 317 | Train Loss: 0.1348572 Vali Loss: 0.1479111 Test Loss: 0.1104260
Validation loss decreased (0.168583 --> 0.147911).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1228843
	speed: 0.1447s/iter; left time: 352.6492s
	iters: 200, epoch: 3 | loss: 0.1080150
	speed: 0.1402s/iter; left time: 327.6860s
	iters: 300, epoch: 3 | loss: 0.1387623
	speed: 0.1432s/iter; left time: 320.4342s
Epoch: 3 cost time: 45.302830934524536
Epoch: 3, Steps: 317 | Train Loss: 0.1270584 Vali Loss: 0.1469878 Test Loss: 0.1106956
Validation loss decreased (0.147911 --> 0.146988).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1295454
	speed: 0.1436s/iter; left time: 304.3609s
	iters: 200, epoch: 4 | loss: 0.1298702
	speed: 0.1338s/iter; left time: 270.2498s
	iters: 300, epoch: 4 | loss: 0.1067071
	speed: 0.1414s/iter; left time: 271.5804s
Epoch: 4 cost time: 44.33567571640015
Epoch: 4, Steps: 317 | Train Loss: 0.1233703 Vali Loss: 0.1453167 Test Loss: 0.1079221
Validation loss decreased (0.146988 --> 0.145317).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1283209
	speed: 0.1403s/iter; left time: 253.0101s
	iters: 200, epoch: 5 | loss: 0.1238557
	speed: 0.1459s/iter; left time: 248.4714s
	iters: 300, epoch: 5 | loss: 0.1264085
	speed: 0.1372s/iter; left time: 219.8542s
Epoch: 5 cost time: 44.73696970939636
Epoch: 5, Steps: 317 | Train Loss: 0.1215051 Vali Loss: 0.1456319 Test Loss: 0.1089917
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0947534
	speed: 0.1456s/iter; left time: 216.2965s
	iters: 200, epoch: 6 | loss: 0.1071521
	speed: 0.1377s/iter; left time: 190.8694s
	iters: 300, epoch: 6 | loss: 0.1306292
	speed: 0.1403s/iter; left time: 180.3734s
Epoch: 6 cost time: 44.78299593925476
Epoch: 6, Steps: 317 | Train Loss: 0.1206100 Vali Loss: 0.1448583 Test Loss: 0.1076170
Validation loss decreased (0.145317 --> 0.144858).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1130612
	speed: 0.1372s/iter; left time: 160.4032s
	iters: 200, epoch: 7 | loss: 0.1320885
	speed: 0.1560s/iter; left time: 166.7459s
	iters: 300, epoch: 7 | loss: 0.1303302
	speed: 0.1386s/iter; left time: 134.3388s
Epoch: 7 cost time: 45.37746024131775
Epoch: 7, Steps: 317 | Train Loss: 0.1200689 Vali Loss: 0.1457341 Test Loss: 0.1080875
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1386960
	speed: 0.1414s/iter; left time: 120.4631s
	iters: 200, epoch: 8 | loss: 0.1182029
	speed: 0.1416s/iter; left time: 106.5059s
	iters: 300, epoch: 8 | loss: 0.1004116
	speed: 0.1445s/iter; left time: 94.2074s
Epoch: 8 cost time: 45.27010703086853
Epoch: 8, Steps: 317 | Train Loss: 0.1198881 Vali Loss: 0.1454841 Test Loss: 0.1081349
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1114861
	speed: 0.1474s/iter; left time: 78.8689s
	iters: 200, epoch: 9 | loss: 0.1091023
	speed: 0.1401s/iter; left time: 60.9408s
	iters: 300, epoch: 9 | loss: 0.1364885
	speed: 0.1337s/iter; left time: 44.7854s
Epoch: 9 cost time: 44.52866005897522
Epoch: 9, Steps: 317 | Train Loss: 0.1198571 Vali Loss: 0.1460250 Test Loss: 0.1085918
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.5496834516525269, mae:0.5741962194442749, rmse:0.7414063811302185, mape:0.021286003291606903, mspe:0.0007606408325955272, rse:0.32928523421287537, r2_score:0.8789022522277962, acc:0.9787139967083931
corr: [40.94652  40.99999  40.93197  40.914227 41.160175 40.925076 41.132046
 41.011227 41.155773 41.128468 41.216915 41.132755 41.092884 41.277203
 41.30898  41.281963 41.195587 41.370018 41.444023 41.450962 41.400284
 41.450054 41.265236 41.53113  41.523926 41.54479  41.493538 41.477604
 41.49162  41.55971 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1811388
	speed: 0.1498s/iter; left time: 461.5646s
	iters: 200, epoch: 1 | loss: 0.1638818
	speed: 0.1126s/iter; left time: 335.6452s
	iters: 300, epoch: 1 | loss: 0.1411534
	speed: 0.1213s/iter; left time: 349.4899s
Epoch: 1 cost time: 40.750805616378784
Epoch: 1, Steps: 318 | Train Loss: 0.2351090 Vali Loss: 0.1383977 Test Loss: 0.1044914
Validation loss decreased (inf --> 0.138398).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1022948
	speed: 0.1226s/iter; left time: 338.7703s
	iters: 200, epoch: 2 | loss: 0.1201142
	speed: 0.1245s/iter; left time: 331.4898s
	iters: 300, epoch: 2 | loss: 0.1337181
	speed: 0.1167s/iter; left time: 299.0779s
Epoch: 2 cost time: 38.52300000190735
Epoch: 2, Steps: 318 | Train Loss: 0.1209653 Vali Loss: 0.1236226 Test Loss: 0.0934548
Validation loss decreased (0.138398 --> 0.123623).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0994973
	speed: 0.1296s/iter; left time: 316.8328s
	iters: 200, epoch: 3 | loss: 0.1172795
	speed: 0.1160s/iter; left time: 271.9092s
	iters: 300, epoch: 3 | loss: 0.1305403
	speed: 0.1216s/iter; left time: 272.9645s
Epoch: 3 cost time: 39.141627073287964
Epoch: 3, Steps: 318 | Train Loss: 0.1086290 Vali Loss: 0.1165759 Test Loss: 0.0875862
Validation loss decreased (0.123623 --> 0.116576).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0837178
	speed: 0.1233s/iter; left time: 262.3207s
	iters: 200, epoch: 4 | loss: 0.1124803
	speed: 0.1247s/iter; left time: 252.7496s
	iters: 300, epoch: 4 | loss: 0.0739626
	speed: 0.1229s/iter; left time: 236.8185s
Epoch: 4 cost time: 39.24281668663025
Epoch: 4, Steps: 318 | Train Loss: 0.1033819 Vali Loss: 0.1142740 Test Loss: 0.0863299
Validation loss decreased (0.116576 --> 0.114274).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0741045
	speed: 0.1187s/iter; left time: 214.7254s
	iters: 200, epoch: 5 | loss: 0.0908824
	speed: 0.1278s/iter; left time: 218.4453s
	iters: 300, epoch: 5 | loss: 0.1302798
	speed: 0.1312s/iter; left time: 211.1325s
Epoch: 5 cost time: 40.07752227783203
Epoch: 5, Steps: 318 | Train Loss: 0.1008258 Vali Loss: 0.1136313 Test Loss: 0.0855094
Validation loss decreased (0.114274 --> 0.113631).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1012866
	speed: 0.1331s/iter; left time: 198.4864s
	iters: 200, epoch: 6 | loss: 0.0975245
	speed: 0.1152s/iter; left time: 160.2534s
	iters: 300, epoch: 6 | loss: 0.0999643
	speed: 0.1229s/iter; left time: 158.6465s
Epoch: 6 cost time: 39.63412094116211
Epoch: 6, Steps: 318 | Train Loss: 0.0995278 Vali Loss: 0.1120761 Test Loss: 0.0846910
Validation loss decreased (0.113631 --> 0.112076).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1151291
	speed: 0.1318s/iter; left time: 154.6187s
	iters: 200, epoch: 7 | loss: 0.1031164
	speed: 0.1181s/iter; left time: 126.7214s
	iters: 300, epoch: 7 | loss: 0.0809105
	speed: 0.1226s/iter; left time: 119.2694s
Epoch: 7 cost time: 39.77980422973633
Epoch: 7, Steps: 318 | Train Loss: 0.0992496 Vali Loss: 0.1129580 Test Loss: 0.0847159
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0861981
	speed: 0.1303s/iter; left time: 111.3928s
	iters: 200, epoch: 8 | loss: 0.0812186
	speed: 0.1215s/iter; left time: 91.7673s
	iters: 300, epoch: 8 | loss: 0.0780469
	speed: 0.1245s/iter; left time: 81.5730s
Epoch: 8 cost time: 39.704548358917236
Epoch: 8, Steps: 318 | Train Loss: 0.0986906 Vali Loss: 0.1116553 Test Loss: 0.0845310
Validation loss decreased (0.112076 --> 0.111655).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0997073
	speed: 0.1323s/iter; left time: 71.0258s
	iters: 200, epoch: 9 | loss: 0.0897960
	speed: 0.1205s/iter; left time: 52.6758s
	iters: 300, epoch: 9 | loss: 0.1150567
	speed: 0.1218s/iter; left time: 41.0559s
Epoch: 9 cost time: 39.65640735626221
Epoch: 9, Steps: 318 | Train Loss: 0.0989724 Vali Loss: 0.1122781 Test Loss: 0.0845629
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1158343
	speed: 0.1181s/iter; left time: 25.8539s
	iters: 200, epoch: 10 | loss: 0.1002514
	speed: 0.1295s/iter; left time: 15.4145s
	iters: 300, epoch: 10 | loss: 0.0727452
	speed: 0.1191s/iter; left time: 2.2638s
Epoch: 10 cost time: 38.85303854942322
Epoch: 10, Steps: 318 | Train Loss: 0.0988842 Vali Loss: 0.1123995 Test Loss: 0.0845260
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.43176546692848206, mae:0.5098187923431396, rmse:0.6570886373519897, mape:0.01894853450357914, mspe:0.0006046003545634449, rse:0.29259273409843445, r2_score:0.9059973664844714, acc:0.9810514654964209
corr: [40.578182 40.420944 40.24306  40.446114 40.810886 40.77227  40.68021
 41.022995 41.14361  41.14564 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1682309
	speed: 0.1828s/iter; left time: 563.3291s
	iters: 200, epoch: 1 | loss: 0.1767454
	speed: 0.1590s/iter; left time: 474.0547s
	iters: 300, epoch: 1 | loss: 0.1373661
	speed: 0.1696s/iter; left time: 488.5213s
Epoch: 1 cost time: 54.253310680389404
Epoch: 1, Steps: 318 | Train Loss: 0.1967036 Vali Loss: 0.1430206 Test Loss: 0.1047461
Validation loss decreased (inf --> 0.143021).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1065258
	speed: 0.1666s/iter; left time: 460.1833s
	iters: 200, epoch: 2 | loss: 0.1083062
	speed: 0.1726s/iter; left time: 459.6345s
	iters: 300, epoch: 2 | loss: 0.1122025
	speed: 0.1481s/iter; left time: 379.5872s
Epoch: 2 cost time: 51.797401905059814
Epoch: 2, Steps: 318 | Train Loss: 0.1150986 Vali Loss: 0.1280643 Test Loss: 0.0930202
Validation loss decreased (0.143021 --> 0.128064).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1073859
	speed: 0.1787s/iter; left time: 436.9684s
	iters: 200, epoch: 3 | loss: 0.0868482
	speed: 0.1670s/iter; left time: 391.5385s
	iters: 300, epoch: 3 | loss: 0.0944530
	speed: 0.1616s/iter; left time: 362.7333s
Epoch: 3 cost time: 53.722214221954346
Epoch: 3, Steps: 318 | Train Loss: 0.1044866 Vali Loss: 0.1243823 Test Loss: 0.0899497
Validation loss decreased (0.128064 --> 0.124382).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1041789
	speed: 0.1711s/iter; left time: 363.9270s
	iters: 200, epoch: 4 | loss: 0.0952979
	speed: 0.1504s/iter; left time: 304.8847s
	iters: 300, epoch: 4 | loss: 0.0922344
	speed: 0.1488s/iter; left time: 286.6464s
Epoch: 4 cost time: 50.388439893722534
Epoch: 4, Steps: 318 | Train Loss: 0.1005685 Vali Loss: 0.1230589 Test Loss: 0.0889819
Validation loss decreased (0.124382 --> 0.123059).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1121986
	speed: 0.1576s/iter; left time: 285.1062s
	iters: 200, epoch: 5 | loss: 0.0824216
	speed: 0.1534s/iter; left time: 262.0965s
	iters: 300, epoch: 5 | loss: 0.0893657
	speed: 0.1595s/iter; left time: 256.5603s
Epoch: 5 cost time: 50.489262104034424
Epoch: 5, Steps: 318 | Train Loss: 0.0987654 Vali Loss: 0.1217801 Test Loss: 0.0897011
Validation loss decreased (0.123059 --> 0.121780).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1136631
	speed: 0.1732s/iter; left time: 258.2694s
	iters: 200, epoch: 6 | loss: 0.0889820
	speed: 0.1585s/iter; left time: 220.4894s
	iters: 300, epoch: 6 | loss: 0.1048823
	speed: 0.1546s/iter; left time: 199.5425s
Epoch: 6 cost time: 51.43384552001953
Epoch: 6, Steps: 318 | Train Loss: 0.0978166 Vali Loss: 0.1202621 Test Loss: 0.0881560
Validation loss decreased (0.121780 --> 0.120262).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0986376
	speed: 0.1566s/iter; left time: 183.6609s
	iters: 200, epoch: 7 | loss: 0.0722110
	speed: 0.1621s/iter; left time: 173.8931s
	iters: 300, epoch: 7 | loss: 0.0987567
	speed: 0.1595s/iter; left time: 155.1639s
Epoch: 7 cost time: 50.640390157699585
Epoch: 7, Steps: 318 | Train Loss: 0.0972088 Vali Loss: 0.1214995 Test Loss: 0.0879650
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0910488
	speed: 0.1629s/iter; left time: 139.3117s
	iters: 200, epoch: 8 | loss: 0.0978027
	speed: 0.1641s/iter; left time: 123.8713s
	iters: 300, epoch: 8 | loss: 0.1021969
	speed: 0.1502s/iter; left time: 98.3502s
Epoch: 8 cost time: 50.976006507873535
Epoch: 8, Steps: 318 | Train Loss: 0.0971929 Vali Loss: 0.1206052 Test Loss: 0.0880390
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0995548
	speed: 0.1651s/iter; left time: 88.6576s
	iters: 200, epoch: 9 | loss: 0.0900812
	speed: 0.1550s/iter; left time: 67.7515s
	iters: 300, epoch: 9 | loss: 0.1229183
	speed: 0.1505s/iter; left time: 50.7063s
Epoch: 9 cost time: 50.32324838638306
Epoch: 9, Steps: 318 | Train Loss: 0.0968107 Vali Loss: 0.1208355 Test Loss: 0.0880110
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.45028138160705566, mae:0.5219496488571167, rmse:0.6710301041603088, mape:0.01937301829457283, mspe:0.0006283230031840503, rse:0.29864466190338135, r2_score:0.9046494164537433, acc:0.9806269817054272
corr: [40.42791  40.479576 40.610355 40.658787 40.80933  40.810207 40.757133
 40.73218  41.155304 41.15454  41.0565   41.211514 41.111275 41.329197
 41.278797]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2188933
	speed: 0.1576s/iter; left time: 485.4834s
	iters: 200, epoch: 1 | loss: 0.1024998
	speed: 0.1491s/iter; left time: 444.4715s
	iters: 300, epoch: 1 | loss: 0.1323294
	speed: 0.1458s/iter; left time: 420.0186s
Epoch: 1 cost time: 47.71672296524048
Epoch: 1, Steps: 318 | Train Loss: 0.1914116 Vali Loss: 0.1403206 Test Loss: 0.1025513
Validation loss decreased (inf --> 0.140321).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1156309
	speed: 0.1532s/iter; left time: 423.4193s
	iters: 200, epoch: 2 | loss: 0.1435850
	speed: 0.1389s/iter; left time: 369.9438s
	iters: 300, epoch: 2 | loss: 0.1226935
	speed: 0.1600s/iter; left time: 410.0990s
Epoch: 2 cost time: 47.72561049461365
Epoch: 2, Steps: 318 | Train Loss: 0.1177416 Vali Loss: 0.1330593 Test Loss: 0.0993931
Validation loss decreased (0.140321 --> 0.133059).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1267529
	speed: 0.1672s/iter; left time: 408.8784s
	iters: 200, epoch: 3 | loss: 0.0915413
	speed: 0.1362s/iter; left time: 319.3929s
	iters: 300, epoch: 3 | loss: 0.1065377
	speed: 0.1585s/iter; left time: 355.9242s
Epoch: 3 cost time: 49.57786822319031
Epoch: 3, Steps: 318 | Train Loss: 0.1089765 Vali Loss: 0.1271798 Test Loss: 0.0942391
Validation loss decreased (0.133059 --> 0.127180).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0948324
	speed: 0.1452s/iter; left time: 308.9214s
	iters: 200, epoch: 4 | loss: 0.1173366
	speed: 0.1393s/iter; left time: 282.4221s
	iters: 300, epoch: 4 | loss: 0.1074441
	speed: 0.1530s/iter; left time: 294.9174s
Epoch: 4 cost time: 46.786102533340454
Epoch: 4, Steps: 318 | Train Loss: 0.1055326 Vali Loss: 0.1269323 Test Loss: 0.0958358
Validation loss decreased (0.127180 --> 0.126932).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1021866
	speed: 0.1493s/iter; left time: 270.1414s
	iters: 200, epoch: 5 | loss: 0.1165962
	speed: 0.1445s/iter; left time: 246.9738s
	iters: 300, epoch: 5 | loss: 0.1214722
	speed: 0.1372s/iter; left time: 220.7861s
Epoch: 5 cost time: 45.65977668762207
Epoch: 5, Steps: 318 | Train Loss: 0.1039325 Vali Loss: 0.1261693 Test Loss: 0.0942685
Validation loss decreased (0.126932 --> 0.126169).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1128923
	speed: 0.1472s/iter; left time: 219.4260s
	iters: 200, epoch: 6 | loss: 0.0829857
	speed: 0.1469s/iter; left time: 204.3531s
	iters: 300, epoch: 6 | loss: 0.1126928
	speed: 0.1409s/iter; left time: 181.9429s
Epoch: 6 cost time: 46.16950488090515
Epoch: 6, Steps: 318 | Train Loss: 0.1030718 Vali Loss: 0.1258679 Test Loss: 0.0941097
Validation loss decreased (0.126169 --> 0.125868).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1042455
	speed: 0.1475s/iter; left time: 173.0674s
	iters: 200, epoch: 7 | loss: 0.0868849
	speed: 0.1497s/iter; left time: 160.5912s
	iters: 300, epoch: 7 | loss: 0.0803526
	speed: 0.1382s/iter; left time: 134.4365s
Epoch: 7 cost time: 45.90878915786743
Epoch: 7, Steps: 318 | Train Loss: 0.1027967 Vali Loss: 0.1258835 Test Loss: 0.0939624
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1097436
	speed: 0.1523s/iter; left time: 130.2216s
	iters: 200, epoch: 8 | loss: 0.1044907
	speed: 0.1378s/iter; left time: 104.0197s
	iters: 300, epoch: 8 | loss: 0.1149869
	speed: 0.1469s/iter; left time: 96.2062s
Epoch: 8 cost time: 46.56017184257507
Epoch: 8, Steps: 318 | Train Loss: 0.1025903 Vali Loss: 0.1259416 Test Loss: 0.0940025
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1327738
	speed: 0.1575s/iter; left time: 84.5735s
	iters: 200, epoch: 9 | loss: 0.1177389
	speed: 0.1348s/iter; left time: 58.8890s
	iters: 300, epoch: 9 | loss: 0.0754269
	speed: 0.1394s/iter; left time: 46.9868s
Epoch: 9 cost time: 45.999282360076904
Epoch: 9, Steps: 318 | Train Loss: 0.1024760 Vali Loss: 0.1257347 Test Loss: 0.0938856
Validation loss decreased (0.125868 --> 0.125735).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0771042
	speed: 0.1495s/iter; left time: 32.7329s
	iters: 200, epoch: 10 | loss: 0.1119505
	speed: 0.1343s/iter; left time: 15.9854s
	iters: 300, epoch: 10 | loss: 0.1226650
	speed: 0.1428s/iter; left time: 2.7134s
Epoch: 10 cost time: 45.64467811584473
Epoch: 10, Steps: 318 | Train Loss: 0.1023384 Vali Loss: 0.1259845 Test Loss: 0.0939313
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.47954675555229187, mae:0.537948727607727, rmse:0.6924931406974792, mape:0.019984150305390358, mspe:0.000671605987008661, rse:0.3080044388771057, r2_score:0.8953859903275596, acc:0.9800158496946096
corr: [40.541622 40.51515  40.552933 40.612236 40.621883 40.779167 40.93588
 40.72141  41.027252 40.819942 40.940403 41.057796 41.00319  41.11254
 41.125477 41.134266 41.100826 41.3476   41.522644 41.476246]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1827700
	speed: 0.1493s/iter; left time: 460.0067s
	iters: 200, epoch: 1 | loss: 0.1093155
	speed: 0.1310s/iter; left time: 390.5836s
	iters: 300, epoch: 1 | loss: 0.1500600
	speed: 0.1235s/iter; left time: 355.6936s
Epoch: 1 cost time: 42.680675745010376
Epoch: 1, Steps: 318 | Train Loss: 0.2225179 Vali Loss: 0.1414936 Test Loss: 0.1085139
Validation loss decreased (inf --> 0.141494).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1346538
	speed: 0.1427s/iter; left time: 394.3524s
	iters: 200, epoch: 2 | loss: 0.1027718
	speed: 0.1253s/iter; left time: 333.6413s
	iters: 300, epoch: 2 | loss: 0.1318742
	speed: 0.1310s/iter; left time: 335.8471s
Epoch: 2 cost time: 42.37275838851929
Epoch: 2, Steps: 318 | Train Loss: 0.1237444 Vali Loss: 0.1354673 Test Loss: 0.1033049
Validation loss decreased (0.141494 --> 0.135467).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1285213
	speed: 0.1354s/iter; left time: 331.1400s
	iters: 200, epoch: 3 | loss: 0.1216273
	speed: 0.1241s/iter; left time: 290.9833s
	iters: 300, epoch: 3 | loss: 0.1254826
	speed: 0.1248s/iter; left time: 280.1654s
Epoch: 3 cost time: 40.78339147567749
Epoch: 3, Steps: 318 | Train Loss: 0.1168448 Vali Loss: 0.1351416 Test Loss: 0.1031605
Validation loss decreased (0.135467 --> 0.135142).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1007525
	speed: 0.1292s/iter; left time: 274.7325s
	iters: 200, epoch: 4 | loss: 0.1046304
	speed: 0.1302s/iter; left time: 263.8985s
	iters: 300, epoch: 4 | loss: 0.1239569
	speed: 0.1294s/iter; left time: 249.4159s
Epoch: 4 cost time: 41.26522707939148
Epoch: 4, Steps: 318 | Train Loss: 0.1132630 Vali Loss: 0.1332227 Test Loss: 0.1023667
Validation loss decreased (0.135142 --> 0.133223).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0965836
	speed: 0.1390s/iter; left time: 251.3939s
	iters: 200, epoch: 5 | loss: 0.1243239
	speed: 0.1320s/iter; left time: 225.6280s
	iters: 300, epoch: 5 | loss: 0.1724240
	speed: 0.1276s/iter; left time: 205.3504s
Epoch: 5 cost time: 41.861472368240356
Epoch: 5, Steps: 318 | Train Loss: 0.1117675 Vali Loss: 0.1313275 Test Loss: 0.1013013
Validation loss decreased (0.133223 --> 0.131328).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0785381
	speed: 0.1274s/iter; left time: 189.9868s
	iters: 200, epoch: 6 | loss: 0.1124793
	speed: 0.1300s/iter; left time: 180.8934s
	iters: 300, epoch: 6 | loss: 0.1172054
	speed: 0.1321s/iter; left time: 170.5067s
Epoch: 6 cost time: 41.2815260887146
Epoch: 6, Steps: 318 | Train Loss: 0.1106306 Vali Loss: 0.1302845 Test Loss: 0.1004539
Validation loss decreased (0.131328 --> 0.130284).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1238193
	speed: 0.1402s/iter; left time: 164.4434s
	iters: 200, epoch: 7 | loss: 0.1287080
	speed: 0.1315s/iter; left time: 141.1371s
	iters: 300, epoch: 7 | loss: 0.0961121
	speed: 0.1278s/iter; left time: 124.3535s
Epoch: 7 cost time: 42.11509704589844
Epoch: 7, Steps: 318 | Train Loss: 0.1103278 Vali Loss: 0.1303434 Test Loss: 0.1002406
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1043152
	speed: 0.1322s/iter; left time: 113.0097s
	iters: 200, epoch: 8 | loss: 0.0985227
	speed: 0.1288s/iter; left time: 97.2727s
	iters: 300, epoch: 8 | loss: 0.0993994
	speed: 0.1340s/iter; left time: 87.7736s
Epoch: 8 cost time: 41.92509627342224
Epoch: 8, Steps: 318 | Train Loss: 0.1098834 Vali Loss: 0.1297422 Test Loss: 0.1002170
Validation loss decreased (0.130284 --> 0.129742).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1022363
	speed: 0.1405s/iter; left time: 75.4351s
	iters: 200, epoch: 9 | loss: 0.1580523
	speed: 0.1287s/iter; left time: 56.2374s
	iters: 300, epoch: 9 | loss: 0.1119911
	speed: 0.1211s/iter; left time: 40.8102s
Epoch: 9 cost time: 41.48620080947876
Epoch: 9, Steps: 318 | Train Loss: 0.1099442 Vali Loss: 0.1296591 Test Loss: 0.1002234
Validation loss decreased (0.129742 --> 0.129659).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1317570
	speed: 0.1329s/iter; left time: 29.0963s
	iters: 200, epoch: 10 | loss: 0.1169211
	speed: 0.1293s/iter; left time: 15.3871s
	iters: 300, epoch: 10 | loss: 0.0901615
	speed: 0.1200s/iter; left time: 2.2798s
Epoch: 10 cost time: 40.44676160812378
Epoch: 10, Steps: 318 | Train Loss: 0.1097580 Vali Loss: 0.1297690 Test Loss: 0.1002157
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.5119186639785767, mae:0.5526743531227112, rmse:0.7154849171638489, mape:0.020512528717517853, mspe:0.0007127337739802897, rse:0.31800854206085205, r2_score:0.8893148536777715, acc:0.9794874712824821
corr: [40.77266  40.753227 40.75657  40.71341  40.78269  40.857574 40.964565
 40.80887  41.022846 40.920784 41.158722 41.199944 41.215923 41.29068
 41.248158 41.22708  41.147533 41.26876  41.45818  41.335587 41.309273
 41.29515  41.272907 41.290653 41.443726]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1474868
	speed: 0.1902s/iter; left time: 584.0556s
	iters: 200, epoch: 1 | loss: 0.1727473
	speed: 0.1620s/iter; left time: 481.4295s
	iters: 300, epoch: 1 | loss: 0.1095477
	speed: 0.1725s/iter; left time: 495.2534s
Epoch: 1 cost time: 56.196746826171875
Epoch: 1, Steps: 317 | Train Loss: 0.2018273 Vali Loss: 0.1623399 Test Loss: 0.1215611
Validation loss decreased (inf --> 0.162340).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1412449
	speed: 0.1700s/iter; left time: 468.1659s
	iters: 200, epoch: 2 | loss: 0.1217925
	speed: 0.1666s/iter; left time: 442.1393s
	iters: 300, epoch: 2 | loss: 0.1181535
	speed: 0.1794s/iter; left time: 458.1012s
Epoch: 2 cost time: 54.52193570137024
Epoch: 2, Steps: 317 | Train Loss: 0.1268419 Vali Loss: 0.1456950 Test Loss: 0.1046008
Validation loss decreased (0.162340 --> 0.145695).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1195694
	speed: 0.1732s/iter; left time: 422.1644s
	iters: 200, epoch: 3 | loss: 0.0977460
	speed: 0.1631s/iter; left time: 381.1830s
	iters: 300, epoch: 3 | loss: 0.1237683
	speed: 0.1693s/iter; left time: 378.6517s
Epoch: 3 cost time: 53.691110610961914
Epoch: 3, Steps: 317 | Train Loss: 0.1187571 Vali Loss: 0.1439846 Test Loss: 0.1037557
Validation loss decreased (0.145695 --> 0.143985).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1155897
	speed: 0.1772s/iter; left time: 375.6621s
	iters: 200, epoch: 4 | loss: 0.1230195
	speed: 0.1643s/iter; left time: 331.8062s
	iters: 300, epoch: 4 | loss: 0.0953460
	speed: 0.1692s/iter; left time: 324.7915s
Epoch: 4 cost time: 53.982447147369385
Epoch: 4, Steps: 317 | Train Loss: 0.1146697 Vali Loss: 0.1425244 Test Loss: 0.1021711
Validation loss decreased (0.143985 --> 0.142524).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1098721
	speed: 0.1732s/iter; left time: 312.3286s
	iters: 200, epoch: 5 | loss: 0.1108399
	speed: 0.1727s/iter; left time: 294.1738s
	iters: 300, epoch: 5 | loss: 0.1126679
	speed: 0.1698s/iter; left time: 272.1683s
Epoch: 5 cost time: 54.306440114974976
Epoch: 5, Steps: 317 | Train Loss: 0.1126386 Vali Loss: 0.1429751 Test Loss: 0.1032344
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0840381
	speed: 0.1696s/iter; left time: 252.0907s
	iters: 200, epoch: 6 | loss: 0.0973894
	speed: 0.1699s/iter; left time: 235.4409s
	iters: 300, epoch: 6 | loss: 0.1151683
	speed: 0.1688s/iter; left time: 217.0996s
Epoch: 6 cost time: 53.57646584510803
Epoch: 6, Steps: 317 | Train Loss: 0.1117826 Vali Loss: 0.1415152 Test Loss: 0.1015175
Validation loss decreased (0.142524 --> 0.141515).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1020295
	speed: 0.1744s/iter; left time: 203.8655s
	iters: 200, epoch: 7 | loss: 0.1257838
	speed: 0.1594s/iter; left time: 170.4107s
	iters: 300, epoch: 7 | loss: 0.1149633
	speed: 0.1689s/iter; left time: 163.6784s
Epoch: 7 cost time: 53.608402967453
Epoch: 7, Steps: 317 | Train Loss: 0.1112243 Vali Loss: 0.1427247 Test Loss: 0.1020600
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1257205
	speed: 0.1917s/iter; left time: 163.3388s
	iters: 200, epoch: 8 | loss: 0.1078344
	speed: 0.1740s/iter; left time: 130.8718s
	iters: 300, epoch: 8 | loss: 0.0995794
	speed: 0.1575s/iter; left time: 102.6920s
Epoch: 8 cost time: 55.14425253868103
Epoch: 8, Steps: 317 | Train Loss: 0.1110585 Vali Loss: 0.1424466 Test Loss: 0.1022427
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1070625
	speed: 0.1790s/iter; left time: 95.7613s
	iters: 200, epoch: 9 | loss: 0.0991628
	speed: 0.1772s/iter; left time: 77.0945s
	iters: 300, epoch: 9 | loss: 0.1184860
	speed: 0.1704s/iter; left time: 57.0720s
Epoch: 9 cost time: 55.81701874732971
Epoch: 9, Steps: 317 | Train Loss: 0.1109320 Vali Loss: 0.1429314 Test Loss: 0.1027112
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.5185289978981018, mae:0.5575534105300903, rmse:0.7200895547866821, mape:0.020680654793977737, mspe:0.0007204315043054521, rse:0.3198176920413971, r2_score:0.8871263306923628, acc:0.9793193452060223
corr: [40.71247  40.780388 40.76155  40.699932 40.814808 40.895935 40.904335
 40.838642 41.111538 40.970467 41.08571  41.088936 41.11085  41.231895
 41.2904   41.212425 41.185272 41.32991  41.41639  41.542454 41.51555
 41.455914 41.307    41.537907 41.61446  41.63467  41.457127 41.519203
 41.57416  41.556873]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1833644
	speed: 0.1535s/iter; left time: 472.8378s
	iters: 200, epoch: 1 | loss: 0.1624835
	speed: 0.1361s/iter; left time: 405.6086s
	iters: 300, epoch: 1 | loss: 0.1365592
	speed: 0.1390s/iter; left time: 400.4048s
Epoch: 1 cost time: 45.40846037864685
Epoch: 1, Steps: 318 | Train Loss: 0.2350132 Vali Loss: 0.1369566 Test Loss: 0.1028451
Validation loss decreased (inf --> 0.136957).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1022279
	speed: 0.1419s/iter; left time: 392.0813s
	iters: 200, epoch: 2 | loss: 0.1148713
	speed: 0.1382s/iter; left time: 368.0942s
	iters: 300, epoch: 2 | loss: 0.1294114
	speed: 0.1208s/iter; left time: 309.4916s
Epoch: 2 cost time: 42.63991189002991
Epoch: 2, Steps: 318 | Train Loss: 0.1187774 Vali Loss: 0.1211190 Test Loss: 0.0911550
Validation loss decreased (0.136957 --> 0.121119).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0978654
	speed: 0.1413s/iter; left time: 345.5787s
	iters: 200, epoch: 3 | loss: 0.1158289
	speed: 0.1326s/iter; left time: 310.9731s
	iters: 300, epoch: 3 | loss: 0.1308092
	speed: 0.1357s/iter; left time: 304.7048s
Epoch: 3 cost time: 43.22779369354248
Epoch: 3, Steps: 318 | Train Loss: 0.1067117 Vali Loss: 0.1146418 Test Loss: 0.0857347
Validation loss decreased (0.121119 --> 0.114642).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0831301
	speed: 0.1393s/iter; left time: 296.3867s
	iters: 200, epoch: 4 | loss: 0.1099257
	speed: 0.1390s/iter; left time: 281.8395s
	iters: 300, epoch: 4 | loss: 0.0735838
	speed: 0.1258s/iter; left time: 242.4346s
Epoch: 4 cost time: 43.02917742729187
Epoch: 4, Steps: 318 | Train Loss: 0.1018198 Vali Loss: 0.1123857 Test Loss: 0.0846347
Validation loss decreased (0.114642 --> 0.112386).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0716593
	speed: 0.1406s/iter; left time: 254.2925s
	iters: 200, epoch: 5 | loss: 0.0908846
	speed: 0.1324s/iter; left time: 226.2483s
	iters: 300, epoch: 5 | loss: 0.1314301
	speed: 0.1381s/iter; left time: 222.1929s
Epoch: 5 cost time: 43.60097289085388
Epoch: 5, Steps: 318 | Train Loss: 0.0994757 Vali Loss: 0.1119393 Test Loss: 0.0839688
Validation loss decreased (0.112386 --> 0.111939).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1013118
	speed: 0.1412s/iter; left time: 210.5122s
	iters: 200, epoch: 6 | loss: 0.0965291
	speed: 0.1361s/iter; left time: 189.3345s
	iters: 300, epoch: 6 | loss: 0.0968316
	speed: 0.1361s/iter; left time: 175.6424s
Epoch: 6 cost time: 44.017382860183716
Epoch: 6, Steps: 318 | Train Loss: 0.0982494 Vali Loss: 0.1103930 Test Loss: 0.0831675
Validation loss decreased (0.111939 --> 0.110393).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1133018
	speed: 0.1401s/iter; left time: 164.3330s
	iters: 200, epoch: 7 | loss: 0.1013027
	speed: 0.1310s/iter; left time: 140.6045s
	iters: 300, epoch: 7 | loss: 0.0787738
	speed: 0.1381s/iter; left time: 134.3599s
Epoch: 7 cost time: 43.37391495704651
Epoch: 7, Steps: 318 | Train Loss: 0.0980558 Vali Loss: 0.1110718 Test Loss: 0.0831861
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0838636
	speed: 0.1374s/iter; left time: 117.4656s
	iters: 200, epoch: 8 | loss: 0.0788473
	speed: 0.1333s/iter; left time: 100.6063s
	iters: 300, epoch: 8 | loss: 0.0774260
	speed: 0.1206s/iter; left time: 78.9736s
Epoch: 8 cost time: 41.48841404914856
Epoch: 8, Steps: 318 | Train Loss: 0.0974744 Vali Loss: 0.1098698 Test Loss: 0.0830219
Validation loss decreased (0.110393 --> 0.109870).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0979873
	speed: 0.1382s/iter; left time: 74.2019s
	iters: 200, epoch: 9 | loss: 0.0905193
	speed: 0.1277s/iter; left time: 55.7976s
	iters: 300, epoch: 9 | loss: 0.1142740
	speed: 0.1266s/iter; left time: 42.6769s
Epoch: 9 cost time: 41.84381914138794
Epoch: 9, Steps: 318 | Train Loss: 0.0977808 Vali Loss: 0.1104280 Test Loss: 0.0830405
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1150293
	speed: 0.1326s/iter; left time: 29.0480s
	iters: 200, epoch: 10 | loss: 0.0996455
	speed: 0.1350s/iter; left time: 16.0597s
	iters: 300, epoch: 10 | loss: 0.0712105
	speed: 0.1308s/iter; left time: 2.4853s
Epoch: 10 cost time: 42.31044840812683
Epoch: 10, Steps: 318 | Train Loss: 0.0976973 Vali Loss: 0.1105501 Test Loss: 0.0830059
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.4240575134754181, mae:0.5054945945739746, rmse:0.6511969566345215, mape:0.01879366673529148, mspe:0.0005946617457084358, rse:0.289969265460968, r2_score:0.9080777713353102, acc:0.9812063332647085
corr: [40.571724 40.42215  40.237675 40.438835 40.798637 40.77003  40.674152
 41.012264 41.16642  41.15241 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1681785
	speed: 0.2024s/iter; left time: 623.6521s
	iters: 200, epoch: 1 | loss: 0.1769608
	speed: 0.1922s/iter; left time: 572.9551s
	iters: 300, epoch: 1 | loss: 0.1375855
	speed: 0.2041s/iter; left time: 588.0109s
Epoch: 1 cost time: 63.713613510131836
Epoch: 1, Steps: 318 | Train Loss: 0.1968738 Vali Loss: 0.1430046 Test Loss: 0.1046822
Validation loss decreased (inf --> 0.143005).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1065771
	speed: 0.2107s/iter; left time: 582.0783s
	iters: 200, epoch: 2 | loss: 0.1082136
	speed: 0.2005s/iter; left time: 534.0088s
	iters: 300, epoch: 2 | loss: 0.1121488
	speed: 0.2133s/iter; left time: 546.6691s
Epoch: 2 cost time: 65.72714042663574
Epoch: 2, Steps: 318 | Train Loss: 0.1150515 Vali Loss: 0.1281225 Test Loss: 0.0930134
Validation loss decreased (0.143005 --> 0.128123).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1072454
	speed: 0.1816s/iter; left time: 444.0179s
	iters: 200, epoch: 3 | loss: 0.0867527
	speed: 0.2084s/iter; left time: 488.6417s
	iters: 300, epoch: 3 | loss: 0.0943675
	speed: 0.1803s/iter; left time: 404.7480s
Epoch: 3 cost time: 61.159485816955566
Epoch: 3, Steps: 318 | Train Loss: 0.1044490 Vali Loss: 0.1244521 Test Loss: 0.0899493
Validation loss decreased (0.128123 --> 0.124452).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1041525
	speed: 0.2007s/iter; left time: 426.7944s
	iters: 200, epoch: 4 | loss: 0.0950993
	speed: 0.1849s/iter; left time: 374.7766s
	iters: 300, epoch: 4 | loss: 0.0922548
	speed: 0.1733s/iter; left time: 333.8560s
Epoch: 4 cost time: 59.72922658920288
Epoch: 4, Steps: 318 | Train Loss: 0.1005298 Vali Loss: 0.1231423 Test Loss: 0.0889904
Validation loss decreased (0.124452 --> 0.123142).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1121283
	speed: 0.1869s/iter; left time: 338.1493s
	iters: 200, epoch: 5 | loss: 0.0823429
	speed: 0.1718s/iter; left time: 293.6186s
	iters: 300, epoch: 5 | loss: 0.0892852
	speed: 0.1975s/iter; left time: 317.8201s
Epoch: 5 cost time: 59.2173535823822
Epoch: 5, Steps: 318 | Train Loss: 0.0987269 Vali Loss: 0.1218484 Test Loss: 0.0897064
Validation loss decreased (0.123142 --> 0.121848).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1136358
	speed: 0.1976s/iter; left time: 294.6388s
	iters: 200, epoch: 6 | loss: 0.0889055
	speed: 0.1853s/iter; left time: 257.7103s
	iters: 300, epoch: 6 | loss: 0.1047521
	speed: 0.1978s/iter; left time: 255.3768s
Epoch: 6 cost time: 61.49047255516052
Epoch: 6, Steps: 318 | Train Loss: 0.0977760 Vali Loss: 0.1203312 Test Loss: 0.0881557
Validation loss decreased (0.121848 --> 0.120331).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0986372
	speed: 0.1875s/iter; left time: 219.9203s
	iters: 200, epoch: 7 | loss: 0.0720871
	speed: 0.1820s/iter; left time: 195.3201s
	iters: 300, epoch: 7 | loss: 0.0988428
	speed: 0.1818s/iter; left time: 176.9239s
Epoch: 7 cost time: 57.89460468292236
Epoch: 7, Steps: 318 | Train Loss: 0.0971703 Vali Loss: 0.1215790 Test Loss: 0.0879686
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0910182
	speed: 0.1747s/iter; left time: 149.3716s
	iters: 200, epoch: 8 | loss: 0.0977841
	speed: 0.1585s/iter; left time: 119.6365s
	iters: 300, epoch: 8 | loss: 0.1020832
	speed: 0.1565s/iter; left time: 102.5041s
Epoch: 8 cost time: 51.403953552246094
Epoch: 8, Steps: 318 | Train Loss: 0.0971565 Vali Loss: 0.1206785 Test Loss: 0.0880421
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0995342
	speed: 0.1396s/iter; left time: 74.9747s
	iters: 200, epoch: 9 | loss: 0.0900767
	speed: 0.1417s/iter; left time: 61.9014s
	iters: 300, epoch: 9 | loss: 0.1227585
	speed: 0.1599s/iter; left time: 53.8873s
Epoch: 9 cost time: 47.60585641860962
Epoch: 9, Steps: 318 | Train Loss: 0.0967710 Vali Loss: 0.1209105 Test Loss: 0.0880122
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.45027992129325867, mae:0.5219206809997559, rmse:0.6710290312767029, mape:0.019372040405869484, mspe:0.0006283302791416645, rse:0.29864415526390076, r2_score:0.9046595612040652, acc:0.9806279595941305
corr: [40.427887 40.482224 40.61094  40.6578   40.808712 40.809334 40.75708
 40.733036 41.15313  41.15245  41.057167 41.21293  41.115765 41.329964
 41.28039 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2185034
	speed: 0.1939s/iter; left time: 597.3696s
	iters: 200, epoch: 1 | loss: 0.1024709
	speed: 0.1663s/iter; left time: 495.8061s
	iters: 300, epoch: 1 | loss: 0.1327111
	speed: 0.1862s/iter; left time: 536.4744s
Epoch: 1 cost time: 57.58727979660034
Epoch: 1, Steps: 318 | Train Loss: 0.1916073 Vali Loss: 0.1404865 Test Loss: 0.1026470
Validation loss decreased (inf --> 0.140486).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1152910
	speed: 0.1878s/iter; left time: 518.8257s
	iters: 200, epoch: 2 | loss: 0.1435152
	speed: 0.1783s/iter; left time: 474.8132s
	iters: 300, epoch: 2 | loss: 0.1225774
	speed: 0.1776s/iter; left time: 455.1564s
Epoch: 2 cost time: 57.2987756729126
Epoch: 2, Steps: 318 | Train Loss: 0.1176434 Vali Loss: 0.1331061 Test Loss: 0.0994375
Validation loss decreased (0.140486 --> 0.133106).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1269926
	speed: 0.1815s/iter; left time: 443.6630s
	iters: 200, epoch: 3 | loss: 0.0912507
	speed: 0.1826s/iter; left time: 428.3123s
	iters: 300, epoch: 3 | loss: 0.1061774
	speed: 0.1733s/iter; left time: 389.0302s
Epoch: 3 cost time: 57.35001802444458
Epoch: 3, Steps: 318 | Train Loss: 0.1088878 Vali Loss: 0.1272921 Test Loss: 0.0943421
Validation loss decreased (0.133106 --> 0.127292).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0946722
	speed: 0.1916s/iter; left time: 407.5544s
	iters: 200, epoch: 4 | loss: 0.1175274
	speed: 0.1685s/iter; left time: 341.5335s
	iters: 300, epoch: 4 | loss: 0.1071017
	speed: 0.1818s/iter; left time: 350.3663s
Epoch: 4 cost time: 57.36391854286194
Epoch: 4, Steps: 318 | Train Loss: 0.1054544 Vali Loss: 0.1270524 Test Loss: 0.0959723
Validation loss decreased (0.127292 --> 0.127052).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1021777
	speed: 0.1889s/iter; left time: 341.7504s
	iters: 200, epoch: 5 | loss: 0.1167461
	speed: 0.1733s/iter; left time: 296.1045s
	iters: 300, epoch: 5 | loss: 0.1215778
	speed: 0.1817s/iter; left time: 292.4066s
Epoch: 5 cost time: 57.47981262207031
Epoch: 5, Steps: 318 | Train Loss: 0.1038587 Vali Loss: 0.1262959 Test Loss: 0.0943931
Validation loss decreased (0.127052 --> 0.126296).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1131090
	speed: 0.1804s/iter; left time: 268.9671s
	iters: 200, epoch: 6 | loss: 0.0828932
	speed: 0.1729s/iter; left time: 240.4504s
	iters: 300, epoch: 6 | loss: 0.1128975
	speed: 0.1784s/iter; left time: 230.2935s
Epoch: 6 cost time: 56.25892353057861
Epoch: 6, Steps: 318 | Train Loss: 0.1030031 Vali Loss: 0.1260015 Test Loss: 0.0942316
Validation loss decreased (0.126296 --> 0.126002).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1043171
	speed: 0.1872s/iter; left time: 219.5689s
	iters: 200, epoch: 7 | loss: 0.0867328
	speed: 0.1771s/iter; left time: 190.0539s
	iters: 300, epoch: 7 | loss: 0.0806187
	speed: 0.1825s/iter; left time: 177.5271s
Epoch: 7 cost time: 58.059688091278076
Epoch: 7, Steps: 318 | Train Loss: 0.1027253 Vali Loss: 0.1260103 Test Loss: 0.0940814
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1097910
	speed: 0.1854s/iter; left time: 158.5168s
	iters: 200, epoch: 8 | loss: 0.1043473
	speed: 0.1782s/iter; left time: 134.5730s
	iters: 300, epoch: 8 | loss: 0.1142530
	speed: 0.1784s/iter; left time: 116.8347s
Epoch: 8 cost time: 57.34665870666504
Epoch: 8, Steps: 318 | Train Loss: 0.1025241 Vali Loss: 0.1260751 Test Loss: 0.0941235
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1325086
	speed: 0.1880s/iter; left time: 100.9388s
	iters: 200, epoch: 9 | loss: 0.1173458
	speed: 0.1795s/iter; left time: 78.4403s
	iters: 300, epoch: 9 | loss: 0.0756217
	speed: 0.1692s/iter; left time: 57.0230s
Epoch: 9 cost time: 57.21666693687439
Epoch: 9, Steps: 318 | Train Loss: 0.1024040 Vali Loss: 0.1258752 Test Loss: 0.0940078
Validation loss decreased (0.126002 --> 0.125875).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0770616
	speed: 0.1877s/iter; left time: 41.1157s
	iters: 200, epoch: 10 | loss: 0.1120283
	speed: 0.1718s/iter; left time: 20.4424s
	iters: 300, epoch: 10 | loss: 0.1226410
	speed: 0.1844s/iter; left time: 3.5036s
Epoch: 10 cost time: 57.57864546775818
Epoch: 10, Steps: 318 | Train Loss: 0.1022723 Vali Loss: 0.1261213 Test Loss: 0.0940552
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.48017075657844543, mae:0.5381878018379211, rmse:0.6929435729980469, mape:0.019993696361780167, mspe:0.0006725038401782513, rse:0.3082047402858734, r2_score:0.8953053668512378, acc:0.9800063036382198
corr: [40.54927  40.521103 40.56152  40.61874  40.62426  40.78371  40.938152
 40.72494  41.022564 40.821667 40.939083 41.0613   41.016537 41.12705
 41.13893  41.1471   41.112427 41.356693 41.516323 41.48277 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1687933
	speed: 0.1724s/iter; left time: 531.2716s
	iters: 200, epoch: 1 | loss: 0.1082791
	speed: 0.1536s/iter; left time: 457.9522s
	iters: 300, epoch: 1 | loss: 0.1498287
	speed: 0.1474s/iter; left time: 424.7015s
Epoch: 1 cost time: 50.09626531600952
Epoch: 1, Steps: 318 | Train Loss: 0.2141324 Vali Loss: 0.1423317 Test Loss: 0.1091237
Validation loss decreased (inf --> 0.142332).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1344104
	speed: 0.1543s/iter; left time: 426.2845s
	iters: 200, epoch: 2 | loss: 0.1016201
	speed: 0.1608s/iter; left time: 428.3241s
	iters: 300, epoch: 2 | loss: 0.1316971
	speed: 0.1696s/iter; left time: 434.7111s
Epoch: 2 cost time: 50.905749559402466
Epoch: 2, Steps: 318 | Train Loss: 0.1234318 Vali Loss: 0.1358563 Test Loss: 0.1033830
Validation loss decreased (0.142332 --> 0.135856).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1273365
	speed: 0.1489s/iter; left time: 363.9920s
	iters: 200, epoch: 3 | loss: 0.1220437
	speed: 0.1521s/iter; left time: 356.5952s
	iters: 300, epoch: 3 | loss: 0.1217579
	speed: 0.1357s/iter; left time: 304.5371s
Epoch: 3 cost time: 46.220059633255005
Epoch: 3, Steps: 318 | Train Loss: 0.1163195 Vali Loss: 0.1355005 Test Loss: 0.1031077
Validation loss decreased (0.135856 --> 0.135500).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0999319
	speed: 0.1473s/iter; left time: 313.3847s
	iters: 200, epoch: 4 | loss: 0.1062610
	speed: 0.1467s/iter; left time: 297.3324s
	iters: 300, epoch: 4 | loss: 0.1236678
	speed: 0.1438s/iter; left time: 277.1686s
Epoch: 4 cost time: 46.86829900741577
Epoch: 4, Steps: 318 | Train Loss: 0.1126868 Vali Loss: 0.1337327 Test Loss: 0.1020371
Validation loss decreased (0.135500 --> 0.133733).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0958894
	speed: 0.1603s/iter; left time: 289.9837s
	iters: 200, epoch: 5 | loss: 0.1237663
	speed: 0.1526s/iter; left time: 260.7720s
	iters: 300, epoch: 5 | loss: 0.1717943
	speed: 0.1511s/iter; left time: 243.0558s
Epoch: 5 cost time: 49.60505270957947
Epoch: 5, Steps: 318 | Train Loss: 0.1111754 Vali Loss: 0.1320486 Test Loss: 0.1010521
Validation loss decreased (0.133733 --> 0.132049).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0773628
	speed: 0.1557s/iter; left time: 232.0841s
	iters: 200, epoch: 6 | loss: 0.1121118
	speed: 0.1530s/iter; left time: 212.7539s
	iters: 300, epoch: 6 | loss: 0.1183665
	speed: 0.1532s/iter; left time: 197.8174s
Epoch: 6 cost time: 48.80505919456482
Epoch: 6, Steps: 318 | Train Loss: 0.1100377 Vali Loss: 0.1307803 Test Loss: 0.1001841
Validation loss decreased (0.132049 --> 0.130780).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1224782
	speed: 0.1513s/iter; left time: 177.5189s
	iters: 200, epoch: 7 | loss: 0.1255517
	speed: 0.1586s/iter; left time: 170.1917s
	iters: 300, epoch: 7 | loss: 0.0968827
	speed: 0.1549s/iter; left time: 150.7601s
Epoch: 7 cost time: 49.469032764434814
Epoch: 7, Steps: 318 | Train Loss: 0.1097247 Vali Loss: 0.1308449 Test Loss: 0.0999228
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1037564
	speed: 0.1599s/iter; left time: 136.7156s
	iters: 200, epoch: 8 | loss: 0.1000944
	speed: 0.1502s/iter; left time: 113.4105s
	iters: 300, epoch: 8 | loss: 0.0999985
	speed: 0.1473s/iter; left time: 96.4769s
Epoch: 8 cost time: 48.93169951438904
Epoch: 8, Steps: 318 | Train Loss: 0.1092424 Vali Loss: 0.1302598 Test Loss: 0.0998780
Validation loss decreased (0.130780 --> 0.130260).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1041927
	speed: 0.1592s/iter; left time: 85.4748s
	iters: 200, epoch: 9 | loss: 0.1576773
	speed: 0.1462s/iter; left time: 63.8846s
	iters: 300, epoch: 9 | loss: 0.1102798
	speed: 0.1570s/iter; left time: 52.9079s
Epoch: 9 cost time: 48.72280144691467
Epoch: 9, Steps: 318 | Train Loss: 0.1092977 Vali Loss: 0.1302044 Test Loss: 0.0998729
Validation loss decreased (0.130260 --> 0.130204).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1306346
	speed: 0.1600s/iter; left time: 35.0400s
	iters: 200, epoch: 10 | loss: 0.1174934
	speed: 0.1498s/iter; left time: 17.8255s
	iters: 300, epoch: 10 | loss: 0.0893835
	speed: 0.1510s/iter; left time: 2.8690s
Epoch: 10 cost time: 49.03154730796814
Epoch: 10, Steps: 318 | Train Loss: 0.1091054 Vali Loss: 0.1302781 Test Loss: 0.0998593
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.5101284384727478, mae:0.5508878827095032, rmse:0.7142327427864075, mape:0.020449098199605942, mspe:0.0007105394033715129, rse:0.31745201349258423, r2_score:0.8894047288291579, acc:0.9795509018003941
corr: [40.741383 40.725643 40.714954 40.69806  40.784958 40.858345 40.97035
 40.800526 40.950058 40.92983  41.136017 41.18091  41.26172  41.301212
 41.285404 41.250126 41.168137 41.29412  41.46678  41.365    41.336044
 41.29234  41.271076 41.28906  41.479523]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1454042
	speed: 0.2078s/iter; left time: 638.2294s
	iters: 200, epoch: 1 | loss: 0.1732274
	speed: 0.1839s/iter; left time: 546.3053s
	iters: 300, epoch: 1 | loss: 0.1102968
	speed: 0.1856s/iter; left time: 532.8142s
Epoch: 1 cost time: 61.21918797492981
Epoch: 1, Steps: 317 | Train Loss: 0.1971648 Vali Loss: 0.1611747 Test Loss: 0.1208677
Validation loss decreased (inf --> 0.161175).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1415165
	speed: 0.1940s/iter; left time: 534.4102s
	iters: 200, epoch: 2 | loss: 0.1207035
	speed: 0.1986s/iter; left time: 527.1208s
	iters: 300, epoch: 2 | loss: 0.1169835
	speed: 0.2042s/iter; left time: 521.5728s
Epoch: 2 cost time: 63.46862053871155
Epoch: 2, Steps: 317 | Train Loss: 0.1265193 Vali Loss: 0.1454633 Test Loss: 0.1047953
Validation loss decreased (0.161175 --> 0.145463).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1194507
	speed: 0.2113s/iter; left time: 515.0117s
	iters: 200, epoch: 3 | loss: 0.0988517
	speed: 0.1975s/iter; left time: 461.6076s
	iters: 300, epoch: 3 | loss: 0.1241976
	speed: 0.2001s/iter; left time: 447.5897s
Epoch: 3 cost time: 64.52870202064514
Epoch: 3, Steps: 317 | Train Loss: 0.1187486 Vali Loss: 0.1442081 Test Loss: 0.1044150
Validation loss decreased (0.145463 --> 0.144208).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1155642
	speed: 0.2140s/iter; left time: 453.5886s
	iters: 200, epoch: 4 | loss: 0.1211913
	speed: 0.1991s/iter; left time: 402.2071s
	iters: 300, epoch: 4 | loss: 0.0968114
	speed: 0.2033s/iter; left time: 390.2420s
Epoch: 4 cost time: 65.20727396011353
Epoch: 4, Steps: 317 | Train Loss: 0.1146003 Vali Loss: 0.1427966 Test Loss: 0.1027250
Validation loss decreased (0.144208 --> 0.142797).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1104562
	speed: 0.2162s/iter; left time: 389.7574s
	iters: 200, epoch: 5 | loss: 0.1096759
	speed: 0.1977s/iter; left time: 336.6798s
	iters: 300, epoch: 5 | loss: 0.1118077
	speed: 0.1940s/iter; left time: 310.9766s
Epoch: 5 cost time: 64.22277545928955
Epoch: 5, Steps: 317 | Train Loss: 0.1125595 Vali Loss: 0.1430604 Test Loss: 0.1036657
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0833278
	speed: 0.2057s/iter; left time: 305.5999s
	iters: 200, epoch: 6 | loss: 0.0974556
	speed: 0.2150s/iter; left time: 298.0182s
	iters: 300, epoch: 6 | loss: 0.1150011
	speed: 0.2069s/iter; left time: 266.0684s
Epoch: 6 cost time: 66.12712979316711
Epoch: 6, Steps: 317 | Train Loss: 0.1116796 Vali Loss: 0.1416560 Test Loss: 0.1020251
Validation loss decreased (0.142797 --> 0.141656).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1023393
	speed: 0.2397s/iter; left time: 280.2498s
	iters: 200, epoch: 7 | loss: 0.1265721
	speed: 0.2305s/iter; left time: 246.4379s
	iters: 300, epoch: 7 | loss: 0.1151235
	speed: 0.2104s/iter; left time: 203.8944s
Epoch: 7 cost time: 71.51903486251831
Epoch: 7, Steps: 317 | Train Loss: 0.1111152 Vali Loss: 0.1427547 Test Loss: 0.1024710
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1251414
	speed: 0.2388s/iter; left time: 203.4261s
	iters: 200, epoch: 8 | loss: 0.1079500
	speed: 0.2258s/iter; left time: 169.7953s
	iters: 300, epoch: 8 | loss: 0.0979594
	speed: 0.2090s/iter; left time: 136.2763s
Epoch: 8 cost time: 71.02733635902405
Epoch: 8, Steps: 317 | Train Loss: 0.1109268 Vali Loss: 0.1424297 Test Loss: 0.1026626
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1061173
	speed: 0.2274s/iter; left time: 121.6587s
	iters: 200, epoch: 9 | loss: 0.1006525
	speed: 0.2183s/iter; left time: 94.9595s
	iters: 300, epoch: 9 | loss: 0.1193882
	speed: 0.2127s/iter; left time: 71.2694s
Epoch: 9 cost time: 69.64512753486633
Epoch: 9, Steps: 317 | Train Loss: 0.1107915 Vali Loss: 0.1429137 Test Loss: 0.1030830
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el1_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.5211217403411865, mae:0.5589107871055603, rmse:0.7218876481056213, mape:0.020734362304210663, mspe:0.0007243548170663416, rse:0.3206162750720978, r2_score:0.8865309441299172, acc:0.9792656376957893
corr: [40.75609  40.84038  40.811375 40.757275 40.860382 40.93203  40.92419
 40.870113 41.089878 40.97462  41.105503 41.08334  41.11169  41.270885
 41.29904  41.217228 41.191772 41.34634  41.399254 41.529194 41.503822
 41.465683 41.312843 41.50124  41.535187 41.580708 41.429756 41.47993
 41.541344 41.524857]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1859394
	speed: 0.1379s/iter; left time: 424.8406s
	iters: 200, epoch: 1 | loss: 0.1324706
	speed: 0.1080s/iter; left time: 322.0841s
	iters: 300, epoch: 1 | loss: 0.1847101
	speed: 0.0878s/iter; left time: 253.0939s
Epoch: 1 cost time: 35.045044898986816
Epoch: 1, Steps: 318 | Train Loss: 0.2482157 Vali Loss: 0.1517891 Test Loss: 0.1418188
Validation loss decreased (inf --> 0.151789).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1215236
	speed: 0.1004s/iter; left time: 277.4640s
	iters: 200, epoch: 2 | loss: 0.1315065
	speed: 0.0787s/iter; left time: 209.6444s
	iters: 300, epoch: 2 | loss: 0.1400682
	speed: 0.0727s/iter; left time: 186.4100s
Epoch: 2 cost time: 26.28515911102295
Epoch: 2, Steps: 318 | Train Loss: 0.1294638 Vali Loss: 0.1388265 Test Loss: 0.1334577
Validation loss decreased (0.151789 --> 0.138827).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1060138
	speed: 0.0695s/iter; left time: 169.8206s
	iters: 200, epoch: 3 | loss: 0.1161514
	speed: 0.0711s/iter; left time: 166.7344s
	iters: 300, epoch: 3 | loss: 0.1446512
	speed: 0.0568s/iter; left time: 127.4148s
Epoch: 3 cost time: 20.8517644405365
Epoch: 3, Steps: 318 | Train Loss: 0.1239997 Vali Loss: 0.1363369 Test Loss: 0.1271686
Validation loss decreased (0.138827 --> 0.136337).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1064729
	speed: 0.0763s/iter; left time: 162.2234s
	iters: 200, epoch: 4 | loss: 0.1453965
	speed: 0.0671s/iter; left time: 135.9475s
	iters: 300, epoch: 4 | loss: 0.1327806
	speed: 0.0620s/iter; left time: 119.5649s
Epoch: 4 cost time: 21.234549283981323
Epoch: 4, Steps: 318 | Train Loss: 0.1207644 Vali Loss: 0.1344506 Test Loss: 0.1247390
Validation loss decreased (0.136337 --> 0.134451).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1232913
	speed: 0.0774s/iter; left time: 139.9464s
	iters: 200, epoch: 5 | loss: 0.0823102
	speed: 0.0653s/iter; left time: 111.6202s
	iters: 300, epoch: 5 | loss: 0.1031655
	speed: 0.0553s/iter; left time: 88.9586s
Epoch: 5 cost time: 21.129441261291504
Epoch: 5, Steps: 318 | Train Loss: 0.1183637 Vali Loss: 0.1345512 Test Loss: 0.1224151
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1029825
	speed: 0.0758s/iter; left time: 113.0413s
	iters: 200, epoch: 6 | loss: 0.1119926
	speed: 0.0537s/iter; left time: 74.7248s
	iters: 300, epoch: 6 | loss: 0.1186582
	speed: 0.0526s/iter; left time: 67.9020s
Epoch: 6 cost time: 19.422884225845337
Epoch: 6, Steps: 318 | Train Loss: 0.1175666 Vali Loss: 0.1298648 Test Loss: 0.1216699
Validation loss decreased (0.134451 --> 0.129865).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1222235
	speed: 0.0749s/iter; left time: 87.8883s
	iters: 200, epoch: 7 | loss: 0.1218578
	speed: 0.0578s/iter; left time: 61.9858s
	iters: 300, epoch: 7 | loss: 0.1257740
	speed: 0.0676s/iter; left time: 65.7934s
Epoch: 7 cost time: 21.24971342086792
Epoch: 7, Steps: 318 | Train Loss: 0.1176896 Vali Loss: 0.1309018 Test Loss: 0.1210409
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1007640
	speed: 0.0762s/iter; left time: 65.1667s
	iters: 200, epoch: 8 | loss: 0.1235137
	speed: 0.0704s/iter; left time: 53.1561s
	iters: 300, epoch: 8 | loss: 0.1353857
	speed: 0.0755s/iter; left time: 49.4606s
Epoch: 8 cost time: 23.494335412979126
Epoch: 8, Steps: 318 | Train Loss: 0.1165102 Vali Loss: 0.1326786 Test Loss: 0.1209246
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1022594
	speed: 0.0763s/iter; left time: 40.9543s
	iters: 200, epoch: 9 | loss: 0.0866474
	speed: 0.0749s/iter; left time: 32.7252s
	iters: 300, epoch: 9 | loss: 0.1201763
	speed: 0.0617s/iter; left time: 20.7767s
Epoch: 9 cost time: 22.49208164215088
Epoch: 9, Steps: 318 | Train Loss: 0.1163919 Vali Loss: 0.1308673 Test Loss: 0.1209983
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.6214627623558044, mae:0.6079419255256653, rmse:0.7883291244506836, mape:0.022554848343133926, mspe:0.0008697580196894705, rse:0.3510323762893677, r2_score:0.8582934081839058, acc:0.9774451516568661
corr: [41.45553  41.222023 41.382324 41.471455 41.485012 41.578304 41.50041
 41.562527 41.640785 41.40987 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2240036
	speed: 0.0796s/iter; left time: 245.1655s
	iters: 200, epoch: 1 | loss: 0.1550628
	speed: 0.0665s/iter; left time: 198.1337s
	iters: 300, epoch: 1 | loss: 0.1062988
	speed: 0.0710s/iter; left time: 204.6481s
Epoch: 1 cost time: 22.83531665802002
Epoch: 1, Steps: 318 | Train Loss: 0.2721505 Vali Loss: 0.1520691 Test Loss: 0.1272266
Validation loss decreased (inf --> 0.152069).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1308652
	speed: 0.0764s/iter; left time: 211.0420s
	iters: 200, epoch: 2 | loss: 0.1509307
	speed: 0.0669s/iter; left time: 178.0416s
	iters: 300, epoch: 2 | loss: 0.1216723
	speed: 0.0721s/iter; left time: 184.7279s
Epoch: 2 cost time: 22.703981637954712
Epoch: 2, Steps: 318 | Train Loss: 0.1343630 Vali Loss: 0.1460567 Test Loss: 0.1342333
Validation loss decreased (0.152069 --> 0.146057).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1432249
	speed: 0.0781s/iter; left time: 190.9595s
	iters: 200, epoch: 3 | loss: 0.1240504
	speed: 0.0703s/iter; left time: 164.9272s
	iters: 300, epoch: 3 | loss: 0.1241912
	speed: 0.0514s/iter; left time: 115.4500s
Epoch: 3 cost time: 21.34850025177002
Epoch: 3, Steps: 318 | Train Loss: 0.1279744 Vali Loss: 0.1437659 Test Loss: 0.1385884
Validation loss decreased (0.146057 --> 0.143766).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1566738
	speed: 0.0755s/iter; left time: 160.4859s
	iters: 200, epoch: 4 | loss: 0.1441887
	speed: 0.0590s/iter; left time: 119.6019s
	iters: 300, epoch: 4 | loss: 0.0950631
	speed: 0.0697s/iter; left time: 134.3449s
Epoch: 4 cost time: 21.70889639854431
Epoch: 4, Steps: 318 | Train Loss: 0.1258300 Vali Loss: 0.1405146 Test Loss: 0.1388791
Validation loss decreased (0.143766 --> 0.140515).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1243473
	speed: 0.0664s/iter; left time: 120.0316s
	iters: 200, epoch: 5 | loss: 0.1431474
	speed: 0.0751s/iter; left time: 128.3512s
	iters: 300, epoch: 5 | loss: 0.1127791
	speed: 0.0714s/iter; left time: 114.8749s
Epoch: 5 cost time: 22.681365489959717
Epoch: 5, Steps: 318 | Train Loss: 0.1244015 Vali Loss: 0.1399065 Test Loss: 0.1433086
Validation loss decreased (0.140515 --> 0.139907).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1403826
	speed: 0.1029s/iter; left time: 153.4168s
	iters: 200, epoch: 6 | loss: 0.1006974
	speed: 0.0891s/iter; left time: 123.9051s
	iters: 300, epoch: 6 | loss: 0.1171765
	speed: 0.0930s/iter; left time: 120.0012s
Epoch: 6 cost time: 30.531719207763672
Epoch: 6, Steps: 318 | Train Loss: 0.1235128 Vali Loss: 0.1401042 Test Loss: 0.1470525
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0995728
	speed: 0.1099s/iter; left time: 128.9256s
	iters: 200, epoch: 7 | loss: 0.1185705
	speed: 0.0915s/iter; left time: 98.1681s
	iters: 300, epoch: 7 | loss: 0.1056953
	speed: 0.0893s/iter; left time: 86.9354s
Epoch: 7 cost time: 31.141281843185425
Epoch: 7, Steps: 318 | Train Loss: 0.1233586 Vali Loss: 0.1402401 Test Loss: 0.1475108
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1003468
	speed: 0.1008s/iter; left time: 86.1453s
	iters: 200, epoch: 8 | loss: 0.0988884
	speed: 0.1089s/iter; left time: 82.2519s
	iters: 300, epoch: 8 | loss: 0.1053097
	speed: 0.1093s/iter; left time: 71.5741s
Epoch: 8 cost time: 33.33646202087402
Epoch: 8, Steps: 318 | Train Loss: 0.1229938 Vali Loss: 0.1380137 Test Loss: 0.1478352
Validation loss decreased (0.139907 --> 0.138014).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1167366
	speed: 0.1127s/iter; left time: 60.5146s
	iters: 200, epoch: 9 | loss: 0.1193250
	speed: 0.0893s/iter; left time: 39.0125s
	iters: 300, epoch: 9 | loss: 0.1778158
	speed: 0.1042s/iter; left time: 35.1299s
Epoch: 9 cost time: 32.323784589767456
Epoch: 9, Steps: 318 | Train Loss: 0.1232307 Vali Loss: 0.1393264 Test Loss: 0.1480173
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1639835
	speed: 0.1106s/iter; left time: 24.2279s
	iters: 200, epoch: 10 | loss: 0.1015068
	speed: 0.0933s/iter; left time: 11.0970s
	iters: 300, epoch: 10 | loss: 0.0841545
	speed: 0.1112s/iter; left time: 2.1121s
Epoch: 10 cost time: 33.310781478881836
Epoch: 10, Steps: 318 | Train Loss: 0.1228486 Vali Loss: 0.1388754 Test Loss: 0.1480414
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.7551091909408569, mae:0.6434576511383057, rmse:0.8689702153205872, mape:0.02394508570432663, mspe:0.0010746698826551437, rse:0.38673868775367737, r2_score:0.833379793869243, acc:0.9760549142956734
corr: [38.38315  38.542263 38.960716 39.036037 39.111294 39.67812  39.525055
 39.560658 39.610146 39.945763 39.986595 40.227867 40.480316 40.477745
 40.801224]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2624086
	speed: 0.1317s/iter; left time: 405.6855s
	iters: 200, epoch: 1 | loss: 0.1782992
	speed: 0.1136s/iter; left time: 338.7046s
	iters: 300, epoch: 1 | loss: 0.1741954
	speed: 0.1177s/iter; left time: 339.0784s
Epoch: 1 cost time: 38.43384146690369
Epoch: 1, Steps: 318 | Train Loss: 0.2892690 Vali Loss: 0.1561246 Test Loss: 0.1273108
Validation loss decreased (inf --> 0.156125).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1475484
	speed: 0.1315s/iter; left time: 363.3016s
	iters: 200, epoch: 2 | loss: 0.1427889
	speed: 0.1211s/iter; left time: 322.5220s
	iters: 300, epoch: 2 | loss: 0.1433933
	speed: 0.1195s/iter; left time: 306.1506s
Epoch: 2 cost time: 39.65404725074768
Epoch: 2, Steps: 318 | Train Loss: 0.1389526 Vali Loss: 0.1502828 Test Loss: 0.1172759
Validation loss decreased (0.156125 --> 0.150283).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1158253
	speed: 0.1347s/iter; left time: 329.4405s
	iters: 200, epoch: 3 | loss: 0.1385483
	speed: 0.1181s/iter; left time: 277.0576s
	iters: 300, epoch: 3 | loss: 0.1483822
	speed: 0.1189s/iter; left time: 266.9917s
Epoch: 3 cost time: 39.32038354873657
Epoch: 3, Steps: 318 | Train Loss: 0.1334394 Vali Loss: 0.1464783 Test Loss: 0.1147981
Validation loss decreased (0.150283 --> 0.146478).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1223879
	speed: 0.1312s/iter; left time: 279.0389s
	iters: 200, epoch: 4 | loss: 0.1392123
	speed: 0.1158s/iter; left time: 234.8100s
	iters: 300, epoch: 4 | loss: 0.1525454
	speed: 0.1270s/iter; left time: 244.8252s
Epoch: 4 cost time: 39.34186053276062
Epoch: 4, Steps: 318 | Train Loss: 0.1311545 Vali Loss: 0.1470848 Test Loss: 0.1132036
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1092239
	speed: 0.1382s/iter; left time: 249.9264s
	iters: 200, epoch: 5 | loss: 0.1453518
	speed: 0.1042s/iter; left time: 178.0844s
	iters: 300, epoch: 5 | loss: 0.1040307
	speed: 0.1352s/iter; left time: 217.5468s
Epoch: 5 cost time: 40.47182822227478
Epoch: 5, Steps: 318 | Train Loss: 0.1302877 Vali Loss: 0.1469699 Test Loss: 0.1128846
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0907838
	speed: 0.1281s/iter; left time: 191.0503s
	iters: 200, epoch: 6 | loss: 0.0967067
	speed: 0.1345s/iter; left time: 187.1554s
	iters: 300, epoch: 6 | loss: 0.1558964
	speed: 0.1155s/iter; left time: 149.0888s
Epoch: 6 cost time: 39.76333928108215
Epoch: 6, Steps: 318 | Train Loss: 0.1297835 Vali Loss: 0.1466523 Test Loss: 0.1121626
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.5863634347915649, mae:0.5926274061203003, rmse:0.7657437324523926, mape:0.02190679870545864, mspe:0.0008004909614101052, rse:0.34058451652526855, r2_score:0.872233103029162, acc:0.9780932012945414
corr: [40.951164 41.02073  40.988564 40.95178  40.999466 40.791786 40.909225
 40.905563 41.050205 40.873436 40.99332  40.973583 40.95267  41.190346
 41.13278  41.10002  41.220608 41.525215 41.604702 41.437904]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2265160
	speed: 0.1079s/iter; left time: 332.3341s
	iters: 200, epoch: 1 | loss: 0.1823132
	speed: 0.0819s/iter; left time: 244.0344s
	iters: 300, epoch: 1 | loss: 0.1213185
	speed: 0.0847s/iter; left time: 244.0935s
Epoch: 1 cost time: 28.463242053985596
Epoch: 1, Steps: 318 | Train Loss: 0.3136191 Vali Loss: 0.1727866 Test Loss: 0.1513072
Validation loss decreased (inf --> 0.172787).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1658206
	speed: 0.0909s/iter; left time: 251.0730s
	iters: 200, epoch: 2 | loss: 0.1233472
	speed: 0.0967s/iter; left time: 257.6322s
	iters: 300, epoch: 2 | loss: 0.1465342
	speed: 0.0858s/iter; left time: 219.8316s
Epoch: 2 cost time: 29.275714635849
Epoch: 2, Steps: 318 | Train Loss: 0.1489102 Vali Loss: 0.1607538 Test Loss: 0.1427086
Validation loss decreased (0.172787 --> 0.160754).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1548121
	speed: 0.1032s/iter; left time: 252.3264s
	iters: 200, epoch: 3 | loss: 0.1075926
	speed: 0.0925s/iter; left time: 216.8155s
	iters: 300, epoch: 3 | loss: 0.1498369
	speed: 0.0890s/iter; left time: 199.7657s
Epoch: 3 cost time: 30.25916600227356
Epoch: 3, Steps: 318 | Train Loss: 0.1419607 Vali Loss: 0.1587245 Test Loss: 0.1375619
Validation loss decreased (0.160754 --> 0.158724).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1160659
	speed: 0.0936s/iter; left time: 199.0848s
	iters: 200, epoch: 4 | loss: 0.1256055
	speed: 0.0898s/iter; left time: 181.9346s
	iters: 300, epoch: 4 | loss: 0.1320097
	speed: 0.0801s/iter; left time: 154.4075s
Epoch: 4 cost time: 28.129748106002808
Epoch: 4, Steps: 318 | Train Loss: 0.1397649 Vali Loss: 0.1557783 Test Loss: 0.1316387
Validation loss decreased (0.158724 --> 0.155778).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1413530
	speed: 0.1022s/iter; left time: 184.8213s
	iters: 200, epoch: 5 | loss: 0.1581773
	speed: 0.0936s/iter; left time: 159.9160s
	iters: 300, epoch: 5 | loss: 0.1230241
	speed: 0.0796s/iter; left time: 128.0627s
Epoch: 5 cost time: 29.115710496902466
Epoch: 5, Steps: 318 | Train Loss: 0.1382755 Vali Loss: 0.1541935 Test Loss: 0.1307662
Validation loss decreased (0.155778 --> 0.154194).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1312565
	speed: 0.0997s/iter; left time: 148.6713s
	iters: 200, epoch: 6 | loss: 0.1610050
	speed: 0.0895s/iter; left time: 124.4612s
	iters: 300, epoch: 6 | loss: 0.1257612
	speed: 0.0877s/iter; left time: 113.1741s
Epoch: 6 cost time: 29.39465856552124
Epoch: 6, Steps: 318 | Train Loss: 0.1377824 Vali Loss: 0.1543851 Test Loss: 0.1307620
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1466324
	speed: 0.0867s/iter; left time: 101.6857s
	iters: 200, epoch: 7 | loss: 0.1462994
	speed: 0.0867s/iter; left time: 93.0668s
	iters: 300, epoch: 7 | loss: 0.1390265
	speed: 0.0887s/iter; left time: 86.2658s
Epoch: 7 cost time: 27.677070379257202
Epoch: 7, Steps: 318 | Train Loss: 0.1374037 Vali Loss: 0.1540492 Test Loss: 0.1303953
Validation loss decreased (0.154194 --> 0.154049).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1453096
	speed: 0.0950s/iter; left time: 81.2576s
	iters: 200, epoch: 8 | loss: 0.1340643
	speed: 0.0855s/iter; left time: 64.5553s
	iters: 300, epoch: 8 | loss: 0.1225463
	speed: 0.0883s/iter; left time: 57.8141s
Epoch: 8 cost time: 28.982548713684082
Epoch: 8, Steps: 318 | Train Loss: 0.1372858 Vali Loss: 0.1536645 Test Loss: 0.1302742
Validation loss decreased (0.154049 --> 0.153665).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1398291
	speed: 0.0973s/iter; left time: 52.2529s
	iters: 200, epoch: 9 | loss: 0.1198388
	speed: 0.0916s/iter; left time: 40.0448s
	iters: 300, epoch: 9 | loss: 0.1507925
	speed: 0.0872s/iter; left time: 29.4032s
Epoch: 9 cost time: 29.1845486164093
Epoch: 9, Steps: 318 | Train Loss: 0.1373461 Vali Loss: 0.1539261 Test Loss: 0.1301838
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1585292
	speed: 0.0976s/iter; left time: 21.3673s
	iters: 200, epoch: 10 | loss: 0.1456515
	speed: 0.1132s/iter; left time: 13.4713s
	iters: 300, epoch: 10 | loss: 0.1090428
	speed: 0.1123s/iter; left time: 2.1336s
Epoch: 10 cost time: 33.71987557411194
Epoch: 10, Steps: 318 | Train Loss: 0.1370240 Vali Loss: 0.1534572 Test Loss: 0.1301644
Validation loss decreased (0.153665 --> 0.153457).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.6648511290550232, mae:0.6411545872688293, rmse:0.815384030342102, mape:0.023703237995505333, mspe:0.0009094594861380756, rse:0.3624102771282196, r2_score:0.8614077571640357, acc:0.9762967620044947
corr: [41.478424 41.252277 41.218704 41.507835 41.228737 41.305477 41.57787
 41.372276 41.495766 41.43228  41.330338 41.44584  41.52658  41.3579
 41.142254 41.221363 41.16973  41.300602 41.334675 41.16918  41.18599
 40.948185 41.123028 40.838524 40.751625]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2553749
	speed: 0.1275s/iter; left time: 391.6883s
	iters: 200, epoch: 1 | loss: 0.1486940
	speed: 0.0889s/iter; left time: 263.9756s
	iters: 300, epoch: 1 | loss: 0.1455756
	speed: 0.0923s/iter; left time: 264.9036s
Epoch: 1 cost time: 32.35553288459778
Epoch: 1, Steps: 317 | Train Loss: 0.3222084 Vali Loss: 0.1715885 Test Loss: 0.1538446
Validation loss decreased (inf --> 0.171589).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1604776
	speed: 0.0989s/iter; left time: 272.5020s
	iters: 200, epoch: 2 | loss: 0.1630477
	speed: 0.0841s/iter; left time: 223.2279s
	iters: 300, epoch: 2 | loss: 0.1489390
	speed: 0.0886s/iter; left time: 226.2173s
Epoch: 2 cost time: 29.1309916973114
Epoch: 2, Steps: 317 | Train Loss: 0.1599254 Vali Loss: 0.1607619 Test Loss: 0.1316378
Validation loss decreased (0.171589 --> 0.160762).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1709892
	speed: 0.0955s/iter; left time: 232.8030s
	iters: 200, epoch: 3 | loss: 0.1259501
	speed: 0.0792s/iter; left time: 185.1804s
	iters: 300, epoch: 3 | loss: 0.1445145
	speed: 0.0917s/iter; left time: 205.0792s
Epoch: 3 cost time: 28.26106548309326
Epoch: 3, Steps: 317 | Train Loss: 0.1493156 Vali Loss: 0.1558281 Test Loss: 0.1259845
Validation loss decreased (0.160762 --> 0.155828).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1323928
	speed: 0.0897s/iter; left time: 190.2366s
	iters: 200, epoch: 4 | loss: 0.1482224
	speed: 0.0844s/iter; left time: 170.4110s
	iters: 300, epoch: 4 | loss: 0.1260621
	speed: 0.0825s/iter; left time: 158.3390s
Epoch: 4 cost time: 26.96073007583618
Epoch: 4, Steps: 317 | Train Loss: 0.1458956 Vali Loss: 0.1557350 Test Loss: 0.1248171
Validation loss decreased (0.155828 --> 0.155735).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1625949
	speed: 0.1007s/iter; left time: 181.6320s
	iters: 200, epoch: 5 | loss: 0.1289783
	speed: 0.0913s/iter; left time: 155.4404s
	iters: 300, epoch: 5 | loss: 0.1701036
	speed: 0.0801s/iter; left time: 128.3911s
Epoch: 5 cost time: 28.540350198745728
Epoch: 5, Steps: 317 | Train Loss: 0.1441133 Vali Loss: 0.1554079 Test Loss: 0.1238945
Validation loss decreased (0.155735 --> 0.155408).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1414386
	speed: 0.0891s/iter; left time: 132.4006s
	iters: 200, epoch: 6 | loss: 0.1268500
	speed: 0.0844s/iter; left time: 116.9431s
	iters: 300, epoch: 6 | loss: 0.1624575
	speed: 0.0898s/iter; left time: 115.4227s
Epoch: 6 cost time: 27.98567032814026
Epoch: 6, Steps: 317 | Train Loss: 0.1434854 Vali Loss: 0.1548736 Test Loss: 0.1234493
Validation loss decreased (0.155408 --> 0.154874).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1612790
	speed: 0.0868s/iter; left time: 101.4985s
	iters: 200, epoch: 7 | loss: 0.1204067
	speed: 0.0861s/iter; left time: 92.0716s
	iters: 300, epoch: 7 | loss: 0.1241940
	speed: 0.0873s/iter; left time: 84.5947s
Epoch: 7 cost time: 27.316838264465332
Epoch: 7, Steps: 317 | Train Loss: 0.1426853 Vali Loss: 0.1553499 Test Loss: 0.1233530
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1414232
	speed: 0.0963s/iter; left time: 82.0823s
	iters: 200, epoch: 8 | loss: 0.1163497
	speed: 0.0855s/iter; left time: 64.3218s
	iters: 300, epoch: 8 | loss: 0.1286120
	speed: 0.0889s/iter; left time: 57.9357s
Epoch: 8 cost time: 28.698955059051514
Epoch: 8, Steps: 317 | Train Loss: 0.1427819 Vali Loss: 0.1545754 Test Loss: 0.1232979
Validation loss decreased (0.154874 --> 0.154575).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1437226
	speed: 0.0948s/iter; left time: 50.7237s
	iters: 200, epoch: 9 | loss: 0.2047657
	speed: 0.0792s/iter; left time: 34.4355s
	iters: 300, epoch: 9 | loss: 0.1189325
	speed: 0.0773s/iter; left time: 25.8961s
Epoch: 9 cost time: 27.16250252723694
Epoch: 9, Steps: 317 | Train Loss: 0.1428386 Vali Loss: 0.1544126 Test Loss: 0.1232729
Validation loss decreased (0.154575 --> 0.154413).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1334783
	speed: 0.0861s/iter; left time: 18.7652s
	iters: 200, epoch: 10 | loss: 0.1608917
	speed: 0.0878s/iter; left time: 10.3616s
	iters: 300, epoch: 10 | loss: 0.1326721
	speed: 0.0970s/iter; left time: 1.7458s
Epoch: 10 cost time: 29.132550954818726
Epoch: 10, Steps: 317 | Train Loss: 0.1426281 Vali Loss: 0.1549422 Test Loss: 0.1232571
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.6296505928039551, mae:0.6133695244789124, rmse:0.7935052514076233, mape:0.022729460150003433, mspe:0.0008688241359777749, rse:0.3524242341518402, r2_score:0.8623569514642815, acc:0.9772705398499966
corr: [40.672337 40.642517 40.898384 41.02158  41.01232  40.968826 41.112297
 41.172943 41.127026 41.27108  41.251045 41.205177 41.21827  41.294872
 40.938244 41.051476 41.08463  41.110893 41.090824 40.99635  40.892376
 40.84503  41.281475 41.23864  41.14374  41.282635 41.282784 41.241364
 41.462826 41.493095]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1561519
	speed: 0.1298s/iter; left time: 400.0634s
	iters: 200, epoch: 1 | loss: 0.1227602
	speed: 0.1098s/iter; left time: 327.2761s
	iters: 300, epoch: 1 | loss: 0.1675342
	speed: 0.1012s/iter; left time: 291.4902s
Epoch: 1 cost time: 36.07141065597534
Epoch: 1, Steps: 318 | Train Loss: 0.2237136 Vali Loss: 0.1479953 Test Loss: 0.1114128
Validation loss decreased (inf --> 0.147995).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1053919
	speed: 0.1292s/iter; left time: 357.0694s
	iters: 200, epoch: 2 | loss: 0.1204868
	speed: 0.0982s/iter; left time: 261.5762s
	iters: 300, epoch: 2 | loss: 0.1454773
	speed: 0.1084s/iter; left time: 277.9351s
Epoch: 2 cost time: 35.51562285423279
Epoch: 2, Steps: 318 | Train Loss: 0.1252857 Vali Loss: 0.1375494 Test Loss: 0.1016762
Validation loss decreased (0.147995 --> 0.137549).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0986441
	speed: 0.0980s/iter; left time: 239.5035s
	iters: 200, epoch: 3 | loss: 0.1113787
	speed: 0.1099s/iter; left time: 257.6901s
	iters: 300, epoch: 3 | loss: 0.1486212
	speed: 0.1027s/iter; left time: 230.5874s
Epoch: 3 cost time: 32.860761880874634
Epoch: 3, Steps: 318 | Train Loss: 0.1171973 Vali Loss: 0.1327079 Test Loss: 0.0947477
Validation loss decreased (0.137549 --> 0.132708).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0984384
	speed: 0.1076s/iter; left time: 228.8108s
	iters: 200, epoch: 4 | loss: 0.1377853
	speed: 0.1042s/iter; left time: 211.1981s
	iters: 300, epoch: 4 | loss: 0.1259950
	speed: 0.1084s/iter; left time: 208.8820s
Epoch: 4 cost time: 33.2906060218811
Epoch: 4, Steps: 318 | Train Loss: 0.1125929 Vali Loss: 0.1291146 Test Loss: 0.0920607
Validation loss decreased (0.132708 --> 0.129115).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1102206
	speed: 0.1098s/iter; left time: 198.6279s
	iters: 200, epoch: 5 | loss: 0.0777751
	speed: 0.1077s/iter; left time: 184.0954s
	iters: 300, epoch: 5 | loss: 0.0964978
	speed: 0.1029s/iter; left time: 165.5814s
Epoch: 5 cost time: 33.98349404335022
Epoch: 5, Steps: 318 | Train Loss: 0.1097773 Vali Loss: 0.1271126 Test Loss: 0.0909438
Validation loss decreased (0.129115 --> 0.127113).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0943364
	speed: 0.1075s/iter; left time: 160.2168s
	iters: 200, epoch: 6 | loss: 0.1088738
	speed: 0.0950s/iter; left time: 132.2127s
	iters: 300, epoch: 6 | loss: 0.1191053
	speed: 0.1086s/iter; left time: 140.2165s
Epoch: 6 cost time: 32.48557710647583
Epoch: 6, Steps: 318 | Train Loss: 0.1085492 Vali Loss: 0.1251251 Test Loss: 0.0902424
Validation loss decreased (0.127113 --> 0.125125).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1186713
	speed: 0.1066s/iter; left time: 125.0251s
	iters: 200, epoch: 7 | loss: 0.1192577
	speed: 0.1015s/iter; left time: 108.8802s
	iters: 300, epoch: 7 | loss: 0.1269354
	speed: 0.1087s/iter; left time: 105.7634s
Epoch: 7 cost time: 33.67429041862488
Epoch: 7, Steps: 318 | Train Loss: 0.1079984 Vali Loss: 0.1260000 Test Loss: 0.0898341
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0979677
	speed: 0.1204s/iter; left time: 102.9594s
	iters: 200, epoch: 8 | loss: 0.1180290
	speed: 0.1154s/iter; left time: 87.1122s
	iters: 300, epoch: 8 | loss: 0.1238214
	speed: 0.1334s/iter; left time: 87.3756s
Epoch: 8 cost time: 39.43078541755676
Epoch: 8, Steps: 318 | Train Loss: 0.1077806 Vali Loss: 0.1268524 Test Loss: 0.0896411
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0744999
	speed: 0.1157s/iter; left time: 62.1352s
	iters: 200, epoch: 9 | loss: 0.0889896
	speed: 0.1059s/iter; left time: 46.2983s
	iters: 300, epoch: 9 | loss: 0.1091865
	speed: 0.1276s/iter; left time: 42.9904s
Epoch: 9 cost time: 37.072157859802246
Epoch: 9, Steps: 318 | Train Loss: 0.1074624 Vali Loss: 0.1260321 Test Loss: 0.0895769
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.46093812584877014, mae:0.529042661190033, rmse:0.6789242625236511, mape:0.01961229182779789, mspe:0.000638976227492094, rse:0.3023158609867096, r2_score:0.9001797307814051, acc:0.9803877081722021
corr: [40.90439  40.85075  40.87257  40.922634 40.908176 41.047646 40.97553
 41.175278 41.222702 41.434963]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2142573
	speed: 0.1524s/iter; left time: 469.4577s
	iters: 200, epoch: 1 | loss: 0.1558676
	speed: 0.1051s/iter; left time: 313.3891s
	iters: 300, epoch: 1 | loss: 0.1003188
	speed: 0.0993s/iter; left time: 285.9950s
Epoch: 1 cost time: 37.767263412475586
Epoch: 1, Steps: 318 | Train Loss: 0.2196229 Vali Loss: 0.1442473 Test Loss: 0.1114961
Validation loss decreased (inf --> 0.144247).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1333983
	speed: 0.1107s/iter; left time: 305.9615s
	iters: 200, epoch: 2 | loss: 0.1355237
	speed: 0.1031s/iter; left time: 274.5040s
	iters: 300, epoch: 2 | loss: 0.1170183
	speed: 0.0978s/iter; left time: 250.6304s
Epoch: 2 cost time: 33.32626533508301
Epoch: 2, Steps: 318 | Train Loss: 0.1278637 Vali Loss: 0.1400508 Test Loss: 0.1031662
Validation loss decreased (0.144247 --> 0.140051).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1379066
	speed: 0.1011s/iter; left time: 247.2352s
	iters: 200, epoch: 3 | loss: 0.1271043
	speed: 0.1028s/iter; left time: 241.0758s
	iters: 300, epoch: 3 | loss: 0.1197668
	speed: 0.1052s/iter; left time: 236.1972s
Epoch: 3 cost time: 33.18478846549988
Epoch: 3, Steps: 318 | Train Loss: 0.1218257 Vali Loss: 0.1374470 Test Loss: 0.1034463
Validation loss decreased (0.140051 --> 0.137447).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1488263
	speed: 0.1095s/iter; left time: 232.8353s
	iters: 200, epoch: 4 | loss: 0.1378916
	speed: 0.1065s/iter; left time: 215.8425s
	iters: 300, epoch: 4 | loss: 0.0891677
	speed: 0.0998s/iter; left time: 192.3622s
Epoch: 4 cost time: 33.532108545303345
Epoch: 4, Steps: 318 | Train Loss: 0.1189619 Vali Loss: 0.1333236 Test Loss: 0.0981352
Validation loss decreased (0.137447 --> 0.133324).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1215554
	speed: 0.1042s/iter; left time: 188.5043s
	iters: 200, epoch: 5 | loss: 0.1256597
	speed: 0.1001s/iter; left time: 171.0459s
	iters: 300, epoch: 5 | loss: 0.1099151
	speed: 0.1052s/iter; left time: 169.3360s
Epoch: 5 cost time: 33.18477964401245
Epoch: 5, Steps: 318 | Train Loss: 0.1173695 Vali Loss: 0.1325021 Test Loss: 0.0969481
Validation loss decreased (0.133324 --> 0.132502).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1291319
	speed: 0.1065s/iter; left time: 158.7655s
	iters: 200, epoch: 6 | loss: 0.1006575
	speed: 0.1167s/iter; left time: 162.3124s
	iters: 300, epoch: 6 | loss: 0.1098345
	speed: 0.1130s/iter; left time: 145.8767s
Epoch: 6 cost time: 35.45091938972473
Epoch: 6, Steps: 318 | Train Loss: 0.1163180 Vali Loss: 0.1329311 Test Loss: 0.0968209
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0902083
	speed: 0.1131s/iter; left time: 132.6548s
	iters: 200, epoch: 7 | loss: 0.1140902
	speed: 0.1089s/iter; left time: 116.8564s
	iters: 300, epoch: 7 | loss: 0.1018915
	speed: 0.1035s/iter; left time: 100.6584s
Epoch: 7 cost time: 34.81813335418701
Epoch: 7, Steps: 318 | Train Loss: 0.1159865 Vali Loss: 0.1326063 Test Loss: 0.0963782
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1014782
	speed: 0.1093s/iter; left time: 93.4416s
	iters: 200, epoch: 8 | loss: 0.0973348
	speed: 0.1079s/iter; left time: 81.4577s
	iters: 300, epoch: 8 | loss: 0.1069053
	speed: 0.1093s/iter; left time: 71.5867s
Epoch: 8 cost time: 34.450218200683594
Epoch: 8, Steps: 318 | Train Loss: 0.1156219 Vali Loss: 0.1309311 Test Loss: 0.0963642
Validation loss decreased (0.132502 --> 0.130931).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1103958
	speed: 0.1239s/iter; left time: 66.5603s
	iters: 200, epoch: 9 | loss: 0.1232707
	speed: 0.1108s/iter; left time: 48.4002s
	iters: 300, epoch: 9 | loss: 0.1669071
	speed: 0.1036s/iter; left time: 34.9026s
Epoch: 9 cost time: 36.01581001281738
Epoch: 9, Steps: 318 | Train Loss: 0.1157059 Vali Loss: 0.1317721 Test Loss: 0.0962569
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1664789
	speed: 0.1079s/iter; left time: 23.6211s
	iters: 200, epoch: 10 | loss: 0.0975445
	speed: 0.1059s/iter; left time: 12.6045s
	iters: 300, epoch: 10 | loss: 0.0787906
	speed: 0.0984s/iter; left time: 1.8690s
Epoch: 10 cost time: 33.30477023124695
Epoch: 10, Steps: 318 | Train Loss: 0.1152856 Vali Loss: 0.1312634 Test Loss: 0.0962198
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.49220699071884155, mae:0.5428293943405151, rmse:0.7015746235847473, mape:0.020106414332985878, mspe:0.000678611162584275, rse:0.3122386336326599, r2_score:0.892685981043148, acc:0.9798935856670141
corr: [41.07556  41.088768 40.90775  40.865883 40.93574  41.278725 40.971214
 41.108192 41.03238  41.152214 40.876446 40.931175 41.046963 41.130466
 41.40793 ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2230204
	speed: 0.1373s/iter; left time: 423.1352s
	iters: 200, epoch: 1 | loss: 0.1499043
	speed: 0.1328s/iter; left time: 395.8810s
	iters: 300, epoch: 1 | loss: 0.1592821
	speed: 0.1242s/iter; left time: 357.8526s
Epoch: 1 cost time: 41.06118440628052
Epoch: 1, Steps: 318 | Train Loss: 0.2566716 Vali Loss: 0.1475968 Test Loss: 0.1150456
Validation loss decreased (inf --> 0.147597).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1303062
	speed: 0.1169s/iter; left time: 322.9795s
	iters: 200, epoch: 2 | loss: 0.1265422
	speed: 0.1107s/iter; left time: 294.8311s
	iters: 300, epoch: 2 | loss: 0.1316336
	speed: 0.1007s/iter; left time: 258.0549s
Epoch: 2 cost time: 34.89273500442505
Epoch: 2, Steps: 318 | Train Loss: 0.1309663 Vali Loss: 0.1469712 Test Loss: 0.1090050
Validation loss decreased (0.147597 --> 0.146971).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1143135
	speed: 0.1103s/iter; left time: 269.7831s
	iters: 200, epoch: 3 | loss: 0.1316990
	speed: 0.1208s/iter; left time: 283.2354s
	iters: 300, epoch: 3 | loss: 0.1413897
	speed: 0.1220s/iter; left time: 273.8056s
Epoch: 3 cost time: 38.09842014312744
Epoch: 3, Steps: 318 | Train Loss: 0.1266194 Vali Loss: 0.1412124 Test Loss: 0.1071937
Validation loss decreased (0.146971 --> 0.141212).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1233220
	speed: 0.1376s/iter; left time: 292.5785s
	iters: 200, epoch: 4 | loss: 0.1397919
	speed: 0.1333s/iter; left time: 270.1694s
	iters: 300, epoch: 4 | loss: 0.1438001
	speed: 0.1688s/iter; left time: 325.2095s
Epoch: 4 cost time: 46.02653670310974
Epoch: 4, Steps: 318 | Train Loss: 0.1245102 Vali Loss: 0.1422738 Test Loss: 0.1059202
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0988843
	speed: 0.1389s/iter; left time: 251.2948s
	iters: 200, epoch: 5 | loss: 0.1396969
	speed: 0.1565s/iter; left time: 267.5048s
	iters: 300, epoch: 5 | loss: 0.0944278
	speed: 0.1358s/iter; left time: 218.4605s
Epoch: 5 cost time: 45.87505841255188
Epoch: 5, Steps: 318 | Train Loss: 0.1234247 Vali Loss: 0.1420963 Test Loss: 0.1058299
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0863228
	speed: 0.1386s/iter; left time: 206.6918s
	iters: 200, epoch: 6 | loss: 0.0955013
	speed: 0.1242s/iter; left time: 172.8218s
	iters: 300, epoch: 6 | loss: 0.1536781
	speed: 0.1208s/iter; left time: 155.9372s
Epoch: 6 cost time: 40.77388620376587
Epoch: 6, Steps: 318 | Train Loss: 0.1226658 Vali Loss: 0.1410982 Test Loss: 0.1052004
Validation loss decreased (0.141212 --> 0.141098).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1054072
	speed: 0.1277s/iter; left time: 149.8066s
	iters: 200, epoch: 7 | loss: 0.1377835
	speed: 0.1261s/iter; left time: 135.2612s
	iters: 300, epoch: 7 | loss: 0.1286404
	speed: 0.1208s/iter; left time: 117.5599s
Epoch: 7 cost time: 39.47701287269592
Epoch: 7, Steps: 318 | Train Loss: 0.1224924 Vali Loss: 0.1408261 Test Loss: 0.1050512
Validation loss decreased (0.141098 --> 0.140826).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1132231
	speed: 0.1199s/iter; left time: 102.4841s
	iters: 200, epoch: 8 | loss: 0.1622558
	speed: 0.1103s/iter; left time: 83.2927s
	iters: 300, epoch: 8 | loss: 0.1188705
	speed: 0.1149s/iter; left time: 75.2486s
Epoch: 8 cost time: 36.568952798843384
Epoch: 8, Steps: 318 | Train Loss: 0.1223216 Vali Loss: 0.1402437 Test Loss: 0.1049979
Validation loss decreased (0.140826 --> 0.140244).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1130739
	speed: 0.1277s/iter; left time: 68.5654s
	iters: 200, epoch: 9 | loss: 0.1584869
	speed: 0.1217s/iter; left time: 53.1907s
	iters: 300, epoch: 9 | loss: 0.1063245
	speed: 0.1136s/iter; left time: 38.2751s
Epoch: 9 cost time: 38.19533944129944
Epoch: 9, Steps: 318 | Train Loss: 0.1221459 Vali Loss: 0.1408012 Test Loss: 0.1049816
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1287293
	speed: 0.1186s/iter; left time: 25.9839s
	iters: 200, epoch: 10 | loss: 0.1145211
	speed: 0.1185s/iter; left time: 14.0974s
	iters: 300, epoch: 10 | loss: 0.1285243
	speed: 0.1152s/iter; left time: 2.1887s
Epoch: 10 cost time: 37.76002049446106
Epoch: 10, Steps: 318 | Train Loss: 0.1220658 Vali Loss: 0.1405475 Test Loss: 0.1049635
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.5363060832023621, mae:0.5664393901824951, rmse:0.7323291897773743, mape:0.020978039130568504, mspe:0.0007398162269964814, rse:0.3257225453853607, r2_score:0.8823574117555614, acc:0.9790219608694315
corr: [41.071686 41.289925 41.130695 41.08313  41.213535 41.178246 41.191456
 41.267895 41.198463 41.195152 41.241707 41.146122 41.280247 41.337914
 41.414402 41.385235 41.36226  41.444263 41.363045 41.5244  ]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1495470
	speed: 0.1446s/iter; left time: 445.3845s
	iters: 200, epoch: 1 | loss: 0.1687858
	speed: 0.0994s/iter; left time: 296.3090s
	iters: 300, epoch: 1 | loss: 0.1088210
	speed: 0.0930s/iter; left time: 267.8052s
Epoch: 1 cost time: 35.94086766242981
Epoch: 1, Steps: 318 | Train Loss: 0.2297682 Vali Loss: 0.1486136 Test Loss: 0.1157348
Validation loss decreased (inf --> 0.148614).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1495125
	speed: 0.1308s/iter; left time: 361.3120s
	iters: 200, epoch: 2 | loss: 0.1034537
	speed: 0.1190s/iter; left time: 316.9043s
	iters: 300, epoch: 2 | loss: 0.1329351
	speed: 0.1097s/iter; left time: 281.2001s
Epoch: 2 cost time: 38.128334760665894
Epoch: 2, Steps: 318 | Train Loss: 0.1310924 Vali Loss: 0.1452001 Test Loss: 0.1090988
Validation loss decreased (0.148614 --> 0.145200).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1349916
	speed: 0.1198s/iter; left time: 292.8583s
	iters: 200, epoch: 3 | loss: 0.0967168
	speed: 0.1195s/iter; left time: 280.1787s
	iters: 300, epoch: 3 | loss: 0.1446950
	speed: 0.1121s/iter; left time: 251.7344s
Epoch: 3 cost time: 37.67875051498413
Epoch: 3, Steps: 318 | Train Loss: 0.1260326 Vali Loss: 0.1431407 Test Loss: 0.1095754
Validation loss decreased (0.145200 --> 0.143141).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0993577
	speed: 0.1265s/iter; left time: 269.1344s
	iters: 200, epoch: 4 | loss: 0.1047250
	speed: 0.1140s/iter; left time: 231.0046s
	iters: 300, epoch: 4 | loss: 0.1123498
	speed: 0.1133s/iter; left time: 218.2481s
Epoch: 4 cost time: 37.464548110961914
Epoch: 4, Steps: 318 | Train Loss: 0.1236650 Vali Loss: 0.1409101 Test Loss: 0.1072691
Validation loss decreased (0.143141 --> 0.140910).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1278549
	speed: 0.1213s/iter; left time: 219.4620s
	iters: 200, epoch: 5 | loss: 0.1563790
	speed: 0.1106s/iter; left time: 188.9639s
	iters: 300, epoch: 5 | loss: 0.1005529
	speed: 0.1112s/iter; left time: 178.9811s
Epoch: 5 cost time: 36.25422263145447
Epoch: 5, Steps: 318 | Train Loss: 0.1223386 Vali Loss: 0.1408331 Test Loss: 0.1077113
Validation loss decreased (0.140910 --> 0.140833).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1069437
	speed: 0.1227s/iter; left time: 182.8742s
	iters: 200, epoch: 6 | loss: 0.1370194
	speed: 0.1112s/iter; left time: 154.7309s
	iters: 300, epoch: 6 | loss: 0.1107134
	speed: 0.1124s/iter; left time: 145.1206s
Epoch: 6 cost time: 36.69037699699402
Epoch: 6, Steps: 318 | Train Loss: 0.1216070 Vali Loss: 0.1408779 Test Loss: 0.1077125
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1408691
	speed: 0.1262s/iter; left time: 147.9750s
	iters: 200, epoch: 7 | loss: 0.1339859
	speed: 0.1033s/iter; left time: 110.8618s
	iters: 300, epoch: 7 | loss: 0.1182911
	speed: 0.1020s/iter; left time: 99.2344s
Epoch: 7 cost time: 34.91338276863098
Epoch: 7, Steps: 318 | Train Loss: 0.1212840 Vali Loss: 0.1406550 Test Loss: 0.1074238
Validation loss decreased (0.140833 --> 0.140655).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1304674
	speed: 0.1177s/iter; left time: 100.6584s
	iters: 200, epoch: 8 | loss: 0.1230957
	speed: 0.1150s/iter; left time: 86.7959s
	iters: 300, epoch: 8 | loss: 0.1150979
	speed: 0.1016s/iter; left time: 66.5549s
Epoch: 8 cost time: 35.665262937545776
Epoch: 8, Steps: 318 | Train Loss: 0.1209745 Vali Loss: 0.1403069 Test Loss: 0.1072660
Validation loss decreased (0.140655 --> 0.140307).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1193470
	speed: 0.1156s/iter; left time: 62.0552s
	iters: 200, epoch: 9 | loss: 0.1073977
	speed: 0.1158s/iter; left time: 50.5895s
	iters: 300, epoch: 9 | loss: 0.1236023
	speed: 0.1080s/iter; left time: 36.3866s
Epoch: 9 cost time: 36.18847155570984
Epoch: 9, Steps: 318 | Train Loss: 0.1210367 Vali Loss: 0.1404531 Test Loss: 0.1072095
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1230015
	speed: 0.1411s/iter; left time: 30.8966s
	iters: 200, epoch: 10 | loss: 0.1234332
	speed: 0.1247s/iter; left time: 14.8427s
	iters: 300, epoch: 10 | loss: 0.1004074
	speed: 0.1174s/iter; left time: 2.2306s
Epoch: 10 cost time: 40.37507367134094
Epoch: 10, Steps: 318 | Train Loss: 0.1209182 Vali Loss: 0.1400751 Test Loss: 0.1071781
Validation loss decreased (0.140307 --> 0.140075).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.547441840171814, mae:0.5735779404640198, rmse:0.7398931384086609, mape:0.02122987061738968, mspe:0.0007526600384153426, rse:0.3288571536540985, r2_score:0.880268344538749, acc:0.9787701293826103
corr: [41.14068  40.96716  40.953693 41.214554 40.94176  40.98081  41.152946
 41.09229  41.1796   41.038715 41.074394 41.13097  41.141598 41.02989
 41.08686  41.146038 41.14741  41.173813 40.976723 41.128487 41.01452
 40.981995 41.06584  41.168262 41.083942]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2084000
	speed: 0.1737s/iter; left time: 533.4378s
	iters: 200, epoch: 1 | loss: 0.1316802
	speed: 0.1236s/iter; left time: 367.2561s
	iters: 300, epoch: 1 | loss: 0.1213828
	speed: 0.1278s/iter; left time: 366.7707s
Epoch: 1 cost time: 44.674683809280396
Epoch: 1, Steps: 317 | Train Loss: 0.2741157 Vali Loss: 0.1556236 Test Loss: 0.1148137
Validation loss decreased (inf --> 0.155624).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1402069
	speed: 0.1270s/iter; left time: 349.7722s
	iters: 200, epoch: 2 | loss: 0.1406717
	speed: 0.1178s/iter; left time: 312.6691s
	iters: 300, epoch: 2 | loss: 0.1226687
	speed: 0.1235s/iter; left time: 315.4573s
Epoch: 2 cost time: 39.01596140861511
Epoch: 2, Steps: 317 | Train Loss: 0.1362682 Vali Loss: 0.1466273 Test Loss: 0.1094751
Validation loss decreased (0.155624 --> 0.146627).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1451697
	speed: 0.1370s/iter; left time: 333.9875s
	iters: 200, epoch: 3 | loss: 0.1139015
	speed: 0.1231s/iter; left time: 287.6740s
	iters: 300, epoch: 3 | loss: 0.1164504
	speed: 0.1162s/iter; left time: 259.9204s
Epoch: 3 cost time: 39.77653479576111
Epoch: 3, Steps: 317 | Train Loss: 0.1311221 Vali Loss: 0.1437463 Test Loss: 0.1090360
Validation loss decreased (0.146627 --> 0.143746).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1209405
	speed: 0.1149s/iter; left time: 243.6637s
	iters: 200, epoch: 4 | loss: 0.1318176
	speed: 0.1032s/iter; left time: 208.4054s
	iters: 300, epoch: 4 | loss: 0.1151354
	speed: 0.1081s/iter; left time: 207.5178s
Epoch: 4 cost time: 34.88141918182373
Epoch: 4, Steps: 317 | Train Loss: 0.1291104 Vali Loss: 0.1436204 Test Loss: 0.1085869
Validation loss decreased (0.143746 --> 0.143620).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1402502
	speed: 0.1224s/iter; left time: 220.6732s
	iters: 200, epoch: 5 | loss: 0.1093273
	speed: 0.1099s/iter; left time: 187.2187s
	iters: 300, epoch: 5 | loss: 0.1490249
	speed: 0.1103s/iter; left time: 176.8114s
Epoch: 5 cost time: 36.37642693519592
Epoch: 5, Steps: 317 | Train Loss: 0.1279805 Vali Loss: 0.1435996 Test Loss: 0.1086050
Validation loss decreased (0.143620 --> 0.143600).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1229555
	speed: 0.1263s/iter; left time: 187.7030s
	iters: 200, epoch: 6 | loss: 0.1104335
	speed: 0.1093s/iter; left time: 151.4357s
	iters: 300, epoch: 6 | loss: 0.1423598
	speed: 0.1183s/iter; left time: 152.1712s
Epoch: 6 cost time: 37.22263789176941
Epoch: 6, Steps: 317 | Train Loss: 0.1276188 Vali Loss: 0.1429670 Test Loss: 0.1088037
Validation loss decreased (0.143600 --> 0.142967).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1389314
	speed: 0.1231s/iter; left time: 143.9003s
	iters: 200, epoch: 7 | loss: 0.1159784
	speed: 0.1151s/iter; left time: 123.0046s
	iters: 300, epoch: 7 | loss: 0.1202267
	speed: 0.1071s/iter; left time: 103.7834s
Epoch: 7 cost time: 36.38852882385254
Epoch: 7, Steps: 317 | Train Loss: 0.1269934 Vali Loss: 0.1430652 Test Loss: 0.1087843
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1287649
	speed: 0.1270s/iter; left time: 108.2339s
	iters: 200, epoch: 8 | loss: 0.1089961
	speed: 0.1113s/iter; left time: 83.7344s
	iters: 300, epoch: 8 | loss: 0.1180367
	speed: 0.1151s/iter; left time: 75.0356s
Epoch: 8 cost time: 37.72993445396423
Epoch: 8, Steps: 317 | Train Loss: 0.1269528 Vali Loss: 0.1424018 Test Loss: 0.1087265
Validation loss decreased (0.142967 --> 0.142402).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1270311
	speed: 0.1200s/iter; left time: 64.2106s
	iters: 200, epoch: 9 | loss: 0.1768949
	speed: 0.1099s/iter; left time: 47.7926s
	iters: 300, epoch: 9 | loss: 0.1037800
	speed: 0.1090s/iter; left time: 36.5216s
Epoch: 9 cost time: 35.869778871536255
Epoch: 9, Steps: 317 | Train Loss: 0.1270346 Vali Loss: 0.1420466 Test Loss: 0.1087412
Validation loss decreased (0.142402 --> 0.142047).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1089128
	speed: 0.1136s/iter; left time: 24.7671s
	iters: 200, epoch: 10 | loss: 0.1354981
	speed: 0.1115s/iter; left time: 13.1600s
	iters: 300, epoch: 10 | loss: 0.1179065
	speed: 0.1123s/iter; left time: 2.0219s
Epoch: 10 cost time: 35.872328996658325
Epoch: 10, Steps: 317 | Train Loss: 0.1270713 Vali Loss: 0.1425186 Test Loss: 0.1087380
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.5554260611534119, mae:0.5748105049133301, rmse:0.7452691197395325, mape:0.02127978205680847, mspe:0.000765167351346463, rse:0.33100083470344543, r2_score:0.8765387378545266, acc:0.9787202179431915
corr: [40.78575  40.881607 40.85005  40.842598 40.87669  40.84479  41.06118
 41.015415 41.0015   40.98267  41.15756  41.297134 41.18354  41.201912
 41.26156  41.067    41.19949  41.1099   41.247936 41.226627 41.16914
 40.95152  41.10198  41.09997  41.156357 41.03299  41.061314 41.11726
 41.235977 41.146862]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1562938
	speed: 0.1621s/iter; left time: 499.4794s
	iters: 200, epoch: 1 | loss: 0.1236566
	speed: 0.1277s/iter; left time: 380.6014s
	iters: 300, epoch: 1 | loss: 0.1671539
	speed: 0.1173s/iter; left time: 337.8029s
Epoch: 1 cost time: 43.172271490097046
Epoch: 1, Steps: 318 | Train Loss: 0.2242741 Vali Loss: 0.1486583 Test Loss: 0.1119224
Validation loss decreased (inf --> 0.148658).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1051310
	speed: 0.1322s/iter; left time: 365.3379s
	iters: 200, epoch: 2 | loss: 0.1203473
	speed: 0.1179s/iter; left time: 313.9677s
	iters: 300, epoch: 2 | loss: 0.1444313
	speed: 0.1177s/iter; left time: 301.5590s
Epoch: 2 cost time: 39.27437424659729
Epoch: 2, Steps: 318 | Train Loss: 0.1250444 Vali Loss: 0.1373410 Test Loss: 0.1010716
Validation loss decreased (0.148658 --> 0.137341).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0983520
	speed: 0.1344s/iter; left time: 328.5901s
	iters: 200, epoch: 3 | loss: 0.1108149
	speed: 0.1209s/iter; left time: 283.4990s
	iters: 300, epoch: 3 | loss: 0.1479244
	speed: 0.1261s/iter; left time: 283.1314s
Epoch: 3 cost time: 40.5799446105957
Epoch: 3, Steps: 318 | Train Loss: 0.1162185 Vali Loss: 0.1320032 Test Loss: 0.0937162
Validation loss decreased (0.137341 --> 0.132003).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0977782
	speed: 0.1377s/iter; left time: 292.7960s
	iters: 200, epoch: 4 | loss: 0.1358711
	speed: 0.1375s/iter; left time: 278.6652s
	iters: 300, epoch: 4 | loss: 0.1262352
	speed: 0.1369s/iter; left time: 263.7752s
Epoch: 4 cost time: 44.074509143829346
Epoch: 4, Steps: 318 | Train Loss: 0.1114478 Vali Loss: 0.1281446 Test Loss: 0.0910702
Validation loss decreased (0.132003 --> 0.128145).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1082496
	speed: 0.1442s/iter; left time: 260.8360s
	iters: 200, epoch: 5 | loss: 0.0772333
	speed: 0.1409s/iter; left time: 240.8777s
	iters: 300, epoch: 5 | loss: 0.0958214
	speed: 0.1456s/iter; left time: 234.2920s
Epoch: 5 cost time: 45.979315519332886
Epoch: 5, Steps: 318 | Train Loss: 0.1086381 Vali Loss: 0.1263372 Test Loss: 0.0901002
Validation loss decreased (0.128145 --> 0.126337).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0929497
	speed: 0.1560s/iter; left time: 232.5777s
	iters: 200, epoch: 6 | loss: 0.1082198
	speed: 0.1295s/iter; left time: 180.1912s
	iters: 300, epoch: 6 | loss: 0.1176208
	speed: 0.1209s/iter; left time: 156.0871s
Epoch: 6 cost time: 42.798237323760986
Epoch: 6, Steps: 318 | Train Loss: 0.1074703 Vali Loss: 0.1244439 Test Loss: 0.0894319
Validation loss decreased (0.126337 --> 0.124444).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1172462
	speed: 0.1267s/iter; left time: 148.6745s
	iters: 200, epoch: 7 | loss: 0.1190875
	speed: 0.1250s/iter; left time: 134.1498s
	iters: 300, epoch: 7 | loss: 0.1266866
	speed: 0.1208s/iter; left time: 117.5265s
Epoch: 7 cost time: 39.5677764415741
Epoch: 7, Steps: 318 | Train Loss: 0.1069493 Vali Loss: 0.1253150 Test Loss: 0.0890601
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0967938
	speed: 0.1324s/iter; left time: 113.2185s
	iters: 200, epoch: 8 | loss: 0.1159272
	speed: 0.1222s/iter; left time: 92.2399s
	iters: 300, epoch: 8 | loss: 0.1215214
	speed: 0.1143s/iter; left time: 74.8454s
Epoch: 8 cost time: 39.458218574523926
Epoch: 8, Steps: 318 | Train Loss: 0.1067355 Vali Loss: 0.1262265 Test Loss: 0.0888814
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0743707
	speed: 0.1379s/iter; left time: 74.0760s
	iters: 200, epoch: 9 | loss: 0.0884190
	speed: 0.1290s/iter; left time: 56.3777s
	iters: 300, epoch: 9 | loss: 0.1077288
	speed: 0.1212s/iter; left time: 40.8518s
Epoch: 9 cost time: 41.0028760433197
Epoch: 9, Steps: 318 | Train Loss: 0.1064368 Vali Loss: 0.1253705 Test Loss: 0.0888270
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.45679807662963867, mae:0.5265169739723206, rmse:0.6758683919906616, mape:0.01952364668250084, mspe:0.0006337167578749359, rse:0.3009551167488098, r2_score:0.9011864999486731, acc:0.9804763533174992
corr: [40.897564 40.83039  40.858337 40.909435 40.901554 41.048214 40.969807
 41.161556 41.212666 41.416367]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2143899
	speed: 0.1968s/iter; left time: 606.2037s
	iters: 200, epoch: 1 | loss: 0.1554427
	speed: 0.1876s/iter; left time: 559.3570s
	iters: 300, epoch: 1 | loss: 0.1004813
	speed: 0.1720s/iter; left time: 495.5220s
Epoch: 1 cost time: 58.75853514671326
Epoch: 1, Steps: 318 | Train Loss: 0.2197618 Vali Loss: 0.1443381 Test Loss: 0.1116157
Validation loss decreased (inf --> 0.144338).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1331887
	speed: 0.1417s/iter; left time: 391.6241s
	iters: 200, epoch: 2 | loss: 0.1356939
	speed: 0.1349s/iter; left time: 359.1723s
	iters: 300, epoch: 2 | loss: 0.1168136
	speed: 0.1380s/iter; left time: 353.5705s
Epoch: 2 cost time: 44.06044602394104
Epoch: 2, Steps: 318 | Train Loss: 0.1280052 Vali Loss: 0.1403084 Test Loss: 0.1034686
Validation loss decreased (0.144338 --> 0.140308).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1379211
	speed: 0.1309s/iter; left time: 320.0279s
	iters: 200, epoch: 3 | loss: 0.1270387
	speed: 0.1247s/iter; left time: 292.4939s
	iters: 300, epoch: 3 | loss: 0.1200298
	speed: 0.1268s/iter; left time: 284.7294s
Epoch: 3 cost time: 40.68292498588562
Epoch: 3, Steps: 318 | Train Loss: 0.1221244 Vali Loss: 0.1377664 Test Loss: 0.1038823
Validation loss decreased (0.140308 --> 0.137766).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1489009
	speed: 0.1362s/iter; left time: 289.7024s
	iters: 200, epoch: 4 | loss: 0.1379212
	speed: 0.1217s/iter; left time: 246.7127s
	iters: 300, epoch: 4 | loss: 0.0896062
	speed: 0.1251s/iter; left time: 241.0473s
Epoch: 4 cost time: 40.9433012008667
Epoch: 4, Steps: 318 | Train Loss: 0.1193634 Vali Loss: 0.1337123 Test Loss: 0.0986461
Validation loss decreased (0.137766 --> 0.133712).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1224194
	speed: 0.1319s/iter; left time: 238.5181s
	iters: 200, epoch: 5 | loss: 0.1259838
	speed: 0.1224s/iter; left time: 209.1925s
	iters: 300, epoch: 5 | loss: 0.1102733
	speed: 0.1236s/iter; left time: 198.8105s
Epoch: 5 cost time: 40.21468901634216
Epoch: 5, Steps: 318 | Train Loss: 0.1177966 Vali Loss: 0.1329266 Test Loss: 0.0974753
Validation loss decreased (0.133712 --> 0.132927).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1299467
	speed: 0.1352s/iter; left time: 201.5833s
	iters: 200, epoch: 6 | loss: 0.1005640
	speed: 0.1210s/iter; left time: 168.3528s
	iters: 300, epoch: 6 | loss: 0.1103548
	speed: 0.1218s/iter; left time: 157.2681s
Epoch: 6 cost time: 39.852205753326416
Epoch: 6, Steps: 318 | Train Loss: 0.1167648 Vali Loss: 0.1333438 Test Loss: 0.0973499
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0901157
	speed: 0.1399s/iter; left time: 164.0648s
	iters: 200, epoch: 7 | loss: 0.1140374
	speed: 0.1154s/iter; left time: 123.7915s
	iters: 300, epoch: 7 | loss: 0.1021252
	speed: 0.1191s/iter; left time: 115.9076s
Epoch: 7 cost time: 39.71308135986328
Epoch: 7, Steps: 318 | Train Loss: 0.1164350 Vali Loss: 0.1330370 Test Loss: 0.0969164
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1012709
	speed: 0.1269s/iter; left time: 108.4926s
	iters: 200, epoch: 8 | loss: 0.0971403
	speed: 0.1238s/iter; left time: 93.4706s
	iters: 300, epoch: 8 | loss: 0.1061970
	speed: 0.1315s/iter; left time: 86.1557s
Epoch: 8 cost time: 40.58664131164551
Epoch: 8, Steps: 318 | Train Loss: 0.1160634 Vali Loss: 0.1313413 Test Loss: 0.0968998
Validation loss decreased (0.132927 --> 0.131341).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1111655
	speed: 0.1439s/iter; left time: 77.2678s
	iters: 200, epoch: 9 | loss: 0.1239962
	speed: 0.1363s/iter; left time: 59.5660s
	iters: 300, epoch: 9 | loss: 0.1673580
	speed: 0.1363s/iter; left time: 45.9381s
Epoch: 9 cost time: 44.02539801597595
Epoch: 9, Steps: 318 | Train Loss: 0.1161560 Vali Loss: 0.1321894 Test Loss: 0.0967993
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1655559
	speed: 0.1481s/iter; left time: 32.4269s
	iters: 200, epoch: 10 | loss: 0.0973645
	speed: 0.1266s/iter; left time: 15.0635s
	iters: 300, epoch: 10 | loss: 0.0786371
	speed: 0.1280s/iter; left time: 2.4317s
Epoch: 10 cost time: 42.734822511672974
Epoch: 10, Steps: 318 | Train Loss: 0.1157358 Vali Loss: 0.1316926 Test Loss: 0.0967629
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.494942843914032, mae:0.5444679260253906, rmse:0.703521728515625, mape:0.020166898146271706, mspe:0.0006823436124250293, rse:0.31310519576072693, r2_score:0.8919585184478653, acc:0.9798331018537283
corr: [41.06398  41.070694 40.89     40.859543 40.933266 41.26587  40.955517
 41.09945  41.029716 41.149666 40.874794 40.92584  41.040653 41.12133
 41.399796]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2403581
	speed: 0.1892s/iter; left time: 582.8656s
	iters: 200, epoch: 1 | loss: 0.1536946
	speed: 0.1760s/iter; left time: 524.7492s
	iters: 300, epoch: 1 | loss: 0.1581313
	speed: 0.1879s/iter; left time: 541.4820s
Epoch: 1 cost time: 59.03496742248535
Epoch: 1, Steps: 318 | Train Loss: 0.2568999 Vali Loss: 0.1478863 Test Loss: 0.1153418
Validation loss decreased (inf --> 0.147886).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1304836
	speed: 0.1828s/iter; left time: 505.0453s
	iters: 200, epoch: 2 | loss: 0.1314306
	speed: 0.1800s/iter; left time: 479.3306s
	iters: 300, epoch: 2 | loss: 0.1319684
	speed: 0.1683s/iter; left time: 431.4520s
Epoch: 2 cost time: 56.26860761642456
Epoch: 2, Steps: 318 | Train Loss: 0.1311446 Vali Loss: 0.1473672 Test Loss: 0.1098525
Validation loss decreased (0.147886 --> 0.147367).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1142220
	speed: 0.1764s/iter; left time: 431.3423s
	iters: 200, epoch: 3 | loss: 0.1337705
	speed: 0.1720s/iter; left time: 403.4387s
	iters: 300, epoch: 3 | loss: 0.1431358
	speed: 0.1850s/iter; left time: 415.3768s
Epoch: 3 cost time: 56.56570100784302
Epoch: 3, Steps: 318 | Train Loss: 0.1269624 Vali Loss: 0.1419673 Test Loss: 0.1083767
Validation loss decreased (0.147367 --> 0.141967).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1199296
	speed: 0.1857s/iter; left time: 394.9187s
	iters: 200, epoch: 4 | loss: 0.1388654
	speed: 0.1745s/iter; left time: 353.6502s
	iters: 300, epoch: 4 | loss: 0.1438449
	speed: 0.1735s/iter; left time: 334.3193s
Epoch: 4 cost time: 56.43799662590027
Epoch: 4, Steps: 318 | Train Loss: 0.1246399 Vali Loss: 0.1430933 Test Loss: 0.1071800
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1012433
	speed: 0.1908s/iter; left time: 345.1650s
	iters: 200, epoch: 5 | loss: 0.1403407
	speed: 0.1683s/iter; left time: 287.5582s
	iters: 300, epoch: 5 | loss: 0.0936387
	speed: 0.1758s/iter; left time: 282.8501s
Epoch: 5 cost time: 56.60510206222534
Epoch: 5, Steps: 318 | Train Loss: 0.1235041 Vali Loss: 0.1429031 Test Loss: 0.1071228
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0845789
	speed: 0.1845s/iter; left time: 275.1049s
	iters: 200, epoch: 6 | loss: 0.0965576
	speed: 0.1866s/iter; left time: 259.5712s
	iters: 300, epoch: 6 | loss: 0.1515195
	speed: 0.1660s/iter; left time: 214.2844s
Epoch: 6 cost time: 56.7378625869751
Epoch: 6, Steps: 318 | Train Loss: 0.1227490 Vali Loss: 0.1418730 Test Loss: 0.1064389
Validation loss decreased (0.141967 --> 0.141873).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1038077
	speed: 0.1825s/iter; left time: 214.0940s
	iters: 200, epoch: 7 | loss: 0.1395765
	speed: 0.1797s/iter; left time: 192.7681s
	iters: 300, epoch: 7 | loss: 0.1298003
	speed: 0.1708s/iter; left time: 166.1425s
Epoch: 7 cost time: 56.484445095062256
Epoch: 7, Steps: 318 | Train Loss: 0.1225791 Vali Loss: 0.1416515 Test Loss: 0.1063778
Validation loss decreased (0.141873 --> 0.141651).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1154806
	speed: 0.1823s/iter; left time: 155.8945s
	iters: 200, epoch: 8 | loss: 0.1627547
	speed: 0.1800s/iter; left time: 135.8971s
	iters: 300, epoch: 8 | loss: 0.1197594
	speed: 0.1818s/iter; left time: 119.1091s
Epoch: 8 cost time: 57.64866280555725
Epoch: 8, Steps: 318 | Train Loss: 0.1224262 Vali Loss: 0.1409982 Test Loss: 0.1063550
Validation loss decreased (0.141651 --> 0.140998).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1148981
	speed: 0.1775s/iter; left time: 95.3166s
	iters: 200, epoch: 9 | loss: 0.1581509
	speed: 0.1664s/iter; left time: 72.7325s
	iters: 300, epoch: 9 | loss: 0.1059632
	speed: 0.1715s/iter; left time: 57.7986s
Epoch: 9 cost time: 55.529802083969116
Epoch: 9, Steps: 318 | Train Loss: 0.1221573 Vali Loss: 0.1415899 Test Loss: 0.1063105
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1273486
	speed: 0.1796s/iter; left time: 39.3259s
	iters: 200, epoch: 10 | loss: 0.1171082
	speed: 0.1937s/iter; left time: 23.0529s
	iters: 300, epoch: 10 | loss: 0.1314182
	speed: 0.1700s/iter; left time: 3.2305s
Epoch: 10 cost time: 57.403284788131714
Epoch: 10, Steps: 318 | Train Loss: 0.1220597 Vali Loss: 0.1413291 Test Loss: 0.1063039
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.5432375073432922, mae:0.5713064670562744, rmse:0.737046480178833, mape:0.021147994324564934, mspe:0.0007476964965462685, rse:0.32782068848609924, r2_score:0.8802651852167708, acc:0.9788520056754351
corr: [41.008472 41.195564 41.059258 40.968575 41.231827 41.20367  41.095165
 41.062428 41.11194  41.092056 41.113144 41.12079  41.08677  41.154007
 41.19418  41.235176 41.321102 41.34117  41.273968 41.371544]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1491289
	speed: 0.1785s/iter; left time: 549.8476s
	iters: 200, epoch: 1 | loss: 0.1685131
	speed: 0.1274s/iter; left time: 379.6887s
	iters: 300, epoch: 1 | loss: 0.1082209
	speed: 0.1311s/iter; left time: 377.8292s
Epoch: 1 cost time: 46.39798069000244
Epoch: 1, Steps: 318 | Train Loss: 0.2295297 Vali Loss: 0.1486673 Test Loss: 0.1155843
Validation loss decreased (inf --> 0.148667).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1495317
	speed: 0.1529s/iter; left time: 422.4141s
	iters: 200, epoch: 2 | loss: 0.1034297
	speed: 0.1283s/iter; left time: 341.7294s
	iters: 300, epoch: 2 | loss: 0.1329310
	speed: 0.1352s/iter; left time: 346.5896s
Epoch: 2 cost time: 44.214550733566284
Epoch: 2, Steps: 318 | Train Loss: 0.1309127 Vali Loss: 0.1453778 Test Loss: 0.1086146
Validation loss decreased (0.148667 --> 0.145378).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1349722
	speed: 0.1266s/iter; left time: 309.6243s
	iters: 200, epoch: 3 | loss: 0.0965181
	speed: 0.1183s/iter; left time: 277.4692s
	iters: 300, epoch: 3 | loss: 0.1446412
	speed: 0.1325s/iter; left time: 297.4770s
Epoch: 3 cost time: 40.13371253013611
Epoch: 3, Steps: 318 | Train Loss: 0.1258587 Vali Loss: 0.1432124 Test Loss: 0.1094741
Validation loss decreased (0.145378 --> 0.143212).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0992021
	speed: 0.1340s/iter; left time: 285.1148s
	iters: 200, epoch: 4 | loss: 0.1048639
	speed: 0.1293s/iter; left time: 262.0291s
	iters: 300, epoch: 4 | loss: 0.1124341
	speed: 0.1175s/iter; left time: 226.3449s
Epoch: 4 cost time: 40.097403049468994
Epoch: 4, Steps: 318 | Train Loss: 0.1235458 Vali Loss: 0.1409584 Test Loss: 0.1072415
Validation loss decreased (0.143212 --> 0.140958).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1280588
	speed: 0.1281s/iter; left time: 231.7692s
	iters: 200, epoch: 5 | loss: 0.1563886
	speed: 0.1201s/iter; left time: 205.2049s
	iters: 300, epoch: 5 | loss: 0.1005490
	speed: 0.1258s/iter; left time: 202.3735s
Epoch: 5 cost time: 39.6491973400116
Epoch: 5, Steps: 318 | Train Loss: 0.1222351 Vali Loss: 0.1408466 Test Loss: 0.1076298
Validation loss decreased (0.140958 --> 0.140847).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1067621
	speed: 0.1309s/iter; left time: 195.1494s
	iters: 200, epoch: 6 | loss: 0.1369073
	speed: 0.1095s/iter; left time: 152.2595s
	iters: 300, epoch: 6 | loss: 0.1104852
	speed: 0.1186s/iter; left time: 153.1374s
Epoch: 6 cost time: 37.88560724258423
Epoch: 6, Steps: 318 | Train Loss: 0.1215245 Vali Loss: 0.1409059 Test Loss: 0.1076294
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1407931
	speed: 0.1267s/iter; left time: 148.6745s
	iters: 200, epoch: 7 | loss: 0.1324014
	speed: 0.1212s/iter; left time: 130.0867s
	iters: 300, epoch: 7 | loss: 0.1186293
	speed: 0.1286s/iter; left time: 125.1745s
Epoch: 7 cost time: 39.76369547843933
Epoch: 7, Steps: 318 | Train Loss: 0.1211865 Vali Loss: 0.1406667 Test Loss: 0.1073575
Validation loss decreased (0.140847 --> 0.140667).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1303960
	speed: 0.1298s/iter; left time: 110.9803s
	iters: 200, epoch: 8 | loss: 0.1230051
	speed: 0.1165s/iter; left time: 87.9491s
	iters: 300, epoch: 8 | loss: 0.1152277
	speed: 0.1228s/iter; left time: 80.4456s
Epoch: 8 cost time: 39.15501022338867
Epoch: 8, Steps: 318 | Train Loss: 0.1208945 Vali Loss: 0.1403196 Test Loss: 0.1071997
Validation loss decreased (0.140667 --> 0.140320).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1189585
	speed: 0.1323s/iter; left time: 71.0243s
	iters: 200, epoch: 9 | loss: 0.1073164
	speed: 0.1208s/iter; left time: 52.7978s
	iters: 300, epoch: 9 | loss: 0.1231024
	speed: 0.1191s/iter; left time: 40.1245s
Epoch: 9 cost time: 39.538915395736694
Epoch: 9, Steps: 318 | Train Loss: 0.1209443 Vali Loss: 0.1404659 Test Loss: 0.1071462
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1228167
	speed: 0.1267s/iter; left time: 27.7374s
	iters: 200, epoch: 10 | loss: 0.1235343
	speed: 0.1210s/iter; left time: 14.4041s
	iters: 300, epoch: 10 | loss: 0.1003279
	speed: 0.1194s/iter; left time: 2.2690s
Epoch: 10 cost time: 39.02598977088928
Epoch: 10, Steps: 318 | Train Loss: 0.1208355 Vali Loss: 0.1400916 Test Loss: 0.1071138
Validation loss decreased (0.140320 --> 0.140092).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.5471135973930359, mae:0.5732167363166809, rmse:0.7396712899208069, mape:0.021217819303274155, mspe:0.0007524269167333841, rse:0.3287585377693176, r2_score:0.8803249881234813, acc:0.9787821806967258
corr: [41.14647  40.971027 40.949497 41.221096 40.95468  40.98741  41.14264
 41.091328 41.17893  41.03667  41.068264 41.1239   41.131016 41.01025
 41.075375 41.124268 41.12771  41.150883 40.964737 41.12894  40.999012
 40.97366  41.051914 41.15358  41.077644]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2075809
	speed: 0.1613s/iter; left time: 495.2132s
	iters: 200, epoch: 1 | loss: 0.1313083
	speed: 0.1457s/iter; left time: 432.8012s
	iters: 300, epoch: 1 | loss: 0.1211869
	speed: 0.1622s/iter; left time: 465.6779s
Epoch: 1 cost time: 49.11396503448486
Epoch: 1, Steps: 317 | Train Loss: 0.2734653 Vali Loss: 0.1553843 Test Loss: 0.1147442
Validation loss decreased (inf --> 0.155384).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1402596
	speed: 0.1462s/iter; left time: 402.6416s
	iters: 200, epoch: 2 | loss: 0.1404825
	speed: 0.1526s/iter; left time: 405.0150s
	iters: 300, epoch: 2 | loss: 0.1226428
	speed: 0.1619s/iter; left time: 413.5855s
Epoch: 2 cost time: 48.498206615448
Epoch: 2, Steps: 317 | Train Loss: 0.1361408 Vali Loss: 0.1464104 Test Loss: 0.1093445
Validation loss decreased (0.155384 --> 0.146410).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1451703
	speed: 0.1442s/iter; left time: 351.4493s
	iters: 200, epoch: 3 | loss: 0.1138509
	speed: 0.1432s/iter; left time: 334.6448s
	iters: 300, epoch: 3 | loss: 0.1162234
	speed: 0.1449s/iter; left time: 324.0887s
Epoch: 3 cost time: 45.66039776802063
Epoch: 3, Steps: 317 | Train Loss: 0.1309947 Vali Loss: 0.1435796 Test Loss: 0.1089457
Validation loss decreased (0.146410 --> 0.143580).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1207642
	speed: 0.1464s/iter; left time: 310.3514s
	iters: 200, epoch: 4 | loss: 0.1319214
	speed: 0.1296s/iter; left time: 261.6983s
	iters: 300, epoch: 4 | loss: 0.1150643
	speed: 0.1351s/iter; left time: 259.3556s
Epoch: 4 cost time: 43.69999647140503
Epoch: 4, Steps: 317 | Train Loss: 0.1289729 Vali Loss: 0.1434281 Test Loss: 0.1084946
Validation loss decreased (0.143580 --> 0.143428).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1397126
	speed: 0.1569s/iter; left time: 282.9637s
	iters: 200, epoch: 5 | loss: 0.1090737
	speed: 0.1418s/iter; left time: 241.4731s
	iters: 300, epoch: 5 | loss: 0.1488663
	speed: 0.1406s/iter; left time: 225.3491s
Epoch: 5 cost time: 46.13992428779602
Epoch: 5, Steps: 317 | Train Loss: 0.1278453 Vali Loss: 0.1434261 Test Loss: 0.1085066
Validation loss decreased (0.143428 --> 0.143426).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1228646
	speed: 0.1523s/iter; left time: 226.3731s
	iters: 200, epoch: 6 | loss: 0.1101924
	speed: 0.1294s/iter; left time: 179.3219s
	iters: 300, epoch: 6 | loss: 0.1421096
	speed: 0.1318s/iter; left time: 169.4442s
Epoch: 6 cost time: 43.768555641174316
Epoch: 6, Steps: 317 | Train Loss: 0.1274823 Vali Loss: 0.1427943 Test Loss: 0.1087048
Validation loss decreased (0.143426 --> 0.142794).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1385838
	speed: 0.1462s/iter; left time: 170.9403s
	iters: 200, epoch: 7 | loss: 0.1160183
	speed: 0.1261s/iter; left time: 134.8120s
	iters: 300, epoch: 7 | loss: 0.1197403
	speed: 0.1329s/iter; left time: 128.8010s
Epoch: 7 cost time: 42.695372104644775
Epoch: 7, Steps: 317 | Train Loss: 0.1268547 Vali Loss: 0.1428914 Test Loss: 0.1086788
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1284006
	speed: 0.1470s/iter; left time: 125.2164s
	iters: 200, epoch: 8 | loss: 0.1089253
	speed: 0.1282s/iter; left time: 96.3800s
	iters: 300, epoch: 8 | loss: 0.1179588
	speed: 0.1270s/iter; left time: 82.8252s
Epoch: 8 cost time: 42.15187168121338
Epoch: 8, Steps: 317 | Train Loss: 0.1268049 Vali Loss: 0.1422277 Test Loss: 0.1086158
Validation loss decreased (0.142794 --> 0.142228).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1268326
	speed: 0.1365s/iter; left time: 73.0487s
	iters: 200, epoch: 9 | loss: 0.1770427
	speed: 0.1369s/iter; left time: 59.5670s
	iters: 300, epoch: 9 | loss: 0.1036714
	speed: 0.1366s/iter; left time: 45.7498s
Epoch: 9 cost time: 43.578065395355225
Epoch: 9, Steps: 317 | Train Loss: 0.1268911 Vali Loss: 0.1418704 Test Loss: 0.1086287
Validation loss decreased (0.142228 --> 0.141870).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1089473
	speed: 0.1293s/iter; left time: 28.1876s
	iters: 200, epoch: 10 | loss: 0.1353343
	speed: 0.1217s/iter; left time: 14.3593s
	iters: 300, epoch: 10 | loss: 0.1169041
	speed: 0.1286s/iter; left time: 2.3149s
Epoch: 10 cost time: 40.43627858161926
Epoch: 10, Steps: 317 | Train Loss: 0.1269298 Vali Loss: 0.1423371 Test Loss: 0.1086241
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
mse:0.5548511743545532, mae:0.5743990540504456, rmse:0.7448833584785461, mape:0.021265089511871338, mspe:0.0007644770666956902, rse:0.3308294713497162, r2_score:0.8766342792677045, acc:0.9787349104881287
corr: [40.78774  40.887775 40.862926 40.844215 40.87461  40.845184 41.0634
 41.02065  40.998177 40.97916  41.15366  41.297474 41.181824 41.203053
 41.25408  41.066326 41.199295 41.107933 41.249397 41.22416  41.170456
 40.949463 41.10296  41.103237 41.159576 41.032143 41.0676   41.122673
 41.236572 41.150757]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1560653
	speed: 0.1664s/iter; left time: 512.8170s
	iters: 200, epoch: 1 | loss: 0.1228211
	speed: 0.1489s/iter; left time: 443.7800s
	iters: 300, epoch: 1 | loss: 0.1667139
	speed: 0.1857s/iter; left time: 535.0157s
Epoch: 1 cost time: 53.00744843482971
Epoch: 1, Steps: 318 | Train Loss: 0.2251669 Vali Loss: 0.1489203 Test Loss: 0.1121406
Validation loss decreased (inf --> 0.148920).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1063915
	speed: 0.1629s/iter; left time: 450.0064s
	iters: 200, epoch: 2 | loss: 0.1202640
	speed: 0.1332s/iter; left time: 354.6868s
	iters: 300, epoch: 2 | loss: 0.1449163
	speed: 0.1553s/iter; left time: 398.0048s
Epoch: 2 cost time: 47.683653116226196
Epoch: 2, Steps: 318 | Train Loss: 0.1251474 Vali Loss: 0.1375479 Test Loss: 0.1012583
Validation loss decreased (0.148920 --> 0.137548).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0989343
	speed: 0.1729s/iter; left time: 422.7771s
	iters: 200, epoch: 3 | loss: 0.1110058
	speed: 0.1721s/iter; left time: 403.5920s
	iters: 300, epoch: 3 | loss: 0.1478886
	speed: 0.1531s/iter; left time: 343.7095s
Epoch: 3 cost time: 52.657581090927124
Epoch: 3, Steps: 318 | Train Loss: 0.1164595 Vali Loss: 0.1321364 Test Loss: 0.0938288
Validation loss decreased (0.137548 --> 0.132136).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0974688
	speed: 0.1655s/iter; left time: 351.9345s
	iters: 200, epoch: 4 | loss: 0.1366215
	speed: 0.1509s/iter; left time: 305.9307s
	iters: 300, epoch: 4 | loss: 0.1257180
	speed: 0.1632s/iter; left time: 314.5242s
Epoch: 4 cost time: 51.07649278640747
Epoch: 4, Steps: 318 | Train Loss: 0.1116186 Vali Loss: 0.1283166 Test Loss: 0.0911452
Validation loss decreased (0.132136 --> 0.128317).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1085046
	speed: 0.1653s/iter; left time: 299.0040s
	iters: 200, epoch: 5 | loss: 0.0768756
	speed: 0.1555s/iter; left time: 265.8028s
	iters: 300, epoch: 5 | loss: 0.0961007
	speed: 0.1623s/iter; left time: 261.1718s
Epoch: 5 cost time: 51.08863306045532
Epoch: 5, Steps: 318 | Train Loss: 0.1088028 Vali Loss: 0.1262658 Test Loss: 0.0900956
Validation loss decreased (0.128317 --> 0.126266).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0929810
	speed: 0.1602s/iter; left time: 238.8108s
	iters: 200, epoch: 6 | loss: 0.1080326
	speed: 0.1691s/iter; left time: 235.1716s
	iters: 300, epoch: 6 | loss: 0.1177114
	speed: 0.1547s/iter; left time: 199.7419s
Epoch: 6 cost time: 51.2581524848938
Epoch: 6, Steps: 318 | Train Loss: 0.1075926 Vali Loss: 0.1243849 Test Loss: 0.0894257
Validation loss decreased (0.126266 --> 0.124385).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1170913
	speed: 0.1783s/iter; left time: 209.1134s
	iters: 200, epoch: 7 | loss: 0.1189828
	speed: 0.1555s/iter; left time: 166.8883s
	iters: 300, epoch: 7 | loss: 0.1269581
	speed: 0.1608s/iter; left time: 156.4966s
Epoch: 7 cost time: 52.454365253448486
Epoch: 7, Steps: 318 | Train Loss: 0.1070839 Vali Loss: 0.1252308 Test Loss: 0.0890408
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0963150
	speed: 0.1924s/iter; left time: 164.4955s
	iters: 200, epoch: 8 | loss: 0.1154499
	speed: 0.1655s/iter; left time: 124.9810s
	iters: 300, epoch: 8 | loss: 0.1219789
	speed: 0.1754s/iter; left time: 114.9192s
Epoch: 8 cost time: 56.58364224433899
Epoch: 8, Steps: 318 | Train Loss: 0.1068792 Vali Loss: 0.1260934 Test Loss: 0.0888521
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0741654
	speed: 0.1739s/iter; left time: 93.3931s
	iters: 200, epoch: 9 | loss: 0.0883135
	speed: 0.1725s/iter; left time: 75.3799s
	iters: 300, epoch: 9 | loss: 0.1083490
	speed: 0.1817s/iter; left time: 61.2271s
Epoch: 9 cost time: 56.293121576309204
Epoch: 9, Steps: 318 | Train Loss: 0.1065877 Vali Loss: 0.1252538 Test Loss: 0.0887976
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
mse:0.4567667543888092, mae:0.5264679193496704, rmse:0.675845205783844, mape:0.01952262781560421, mspe:0.00063378392951563, rse:0.30094480514526367, r2_score:0.9012657561352082, acc:0.9804773721843958
corr: [40.92038  40.85086  40.874603 40.91393  40.907413 41.051144 40.98376
 41.173588 41.22297  41.400253]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2061045
	speed: 0.1948s/iter; left time: 600.2492s
	iters: 200, epoch: 1 | loss: 0.1586326
	speed: 0.1735s/iter; left time: 517.3302s
	iters: 300, epoch: 1 | loss: 0.0975732
	speed: 0.1731s/iter; left time: 498.6724s
Epoch: 1 cost time: 57.30812120437622
Epoch: 1, Steps: 318 | Train Loss: 0.2210057 Vali Loss: 0.1442263 Test Loss: 0.1125902
Validation loss decreased (inf --> 0.144226).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1301806
	speed: 0.1916s/iter; left time: 529.4318s
	iters: 200, epoch: 2 | loss: 0.1369324
	speed: 0.1769s/iter; left time: 471.0561s
	iters: 300, epoch: 2 | loss: 0.1163263
	speed: 0.1814s/iter; left time: 464.9340s
Epoch: 2 cost time: 58.4438591003418
Epoch: 2, Steps: 318 | Train Loss: 0.1277638 Vali Loss: 0.1404496 Test Loss: 0.1034382
Validation loss decreased (0.144226 --> 0.140450).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1365151
	speed: 0.2124s/iter; left time: 519.2160s
	iters: 200, epoch: 3 | loss: 0.1272704
	speed: 0.1861s/iter; left time: 436.3958s
	iters: 300, epoch: 3 | loss: 0.1203951
	speed: 0.1929s/iter; left time: 432.9852s
Epoch: 3 cost time: 62.42140078544617
Epoch: 3, Steps: 318 | Train Loss: 0.1206608 Vali Loss: 0.1373411 Test Loss: 0.1021931
Validation loss decreased (0.140450 --> 0.137341).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1412810
	speed: 0.1999s/iter; left time: 425.2527s
	iters: 200, epoch: 4 | loss: 0.1349831
	speed: 0.1888s/iter; left time: 382.7179s
	iters: 300, epoch: 4 | loss: 0.0905243
	speed: 0.1858s/iter; left time: 358.1143s
Epoch: 4 cost time: 60.800983905792236
Epoch: 4, Steps: 318 | Train Loss: 0.1169489 Vali Loss: 0.1334408 Test Loss: 0.0965956
Validation loss decreased (0.137341 --> 0.133441).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1196399
	speed: 0.1825s/iter; left time: 330.0929s
	iters: 200, epoch: 5 | loss: 0.1232953
	speed: 0.1858s/iter; left time: 317.4810s
	iters: 300, epoch: 5 | loss: 0.1010084
	speed: 0.1526s/iter; left time: 245.5421s
Epoch: 5 cost time: 54.957446575164795
Epoch: 5, Steps: 318 | Train Loss: 0.1148407 Vali Loss: 0.1330797 Test Loss: 0.0956422
Validation loss decreased (0.133441 --> 0.133080).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1252132
	speed: 0.1618s/iter; left time: 241.2961s
	iters: 200, epoch: 6 | loss: 0.1044876
	speed: 0.1662s/iter; left time: 231.1222s
	iters: 300, epoch: 6 | loss: 0.1117337
	speed: 0.1705s/iter; left time: 220.1150s
Epoch: 6 cost time: 53.086591720581055
Epoch: 6, Steps: 318 | Train Loss: 0.1133844 Vali Loss: 0.1339952 Test Loss: 0.0954617
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0856463
	speed: 0.1735s/iter; left time: 203.5070s
	iters: 200, epoch: 7 | loss: 0.1122941
	speed: 0.1574s/iter; left time: 168.8611s
	iters: 300, epoch: 7 | loss: 0.1023489
	speed: 0.1700s/iter; left time: 165.3948s
Epoch: 7 cost time: 53.02482581138611
Epoch: 7, Steps: 318 | Train Loss: 0.1129044 Vali Loss: 0.1335551 Test Loss: 0.0947778
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0944592
	speed: 0.1625s/iter; left time: 138.9195s
	iters: 200, epoch: 8 | loss: 0.0955762
	speed: 0.1657s/iter; left time: 125.1346s
	iters: 300, epoch: 8 | loss: 0.1029184
	speed: 0.1772s/iter; left time: 116.0462s
Epoch: 8 cost time: 53.882986545562744
Epoch: 8, Steps: 318 | Train Loss: 0.1124285 Vali Loss: 0.1317465 Test Loss: 0.0946874
Validation loss decreased (0.133080 --> 0.131746).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1125239
	speed: 0.1676s/iter; left time: 90.0013s
	iters: 200, epoch: 9 | loss: 0.1236269
	speed: 0.1592s/iter; left time: 69.5764s
	iters: 300, epoch: 9 | loss: 0.1602299
	speed: 0.1587s/iter; left time: 53.4783s
Epoch: 9 cost time: 51.67236137390137
Epoch: 9, Steps: 318 | Train Loss: 0.1124687 Vali Loss: 0.1326297 Test Loss: 0.0945896
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1566174
	speed: 0.1750s/iter; left time: 38.3292s
	iters: 200, epoch: 10 | loss: 0.0971786
	speed: 0.1546s/iter; left time: 18.4018s
	iters: 300, epoch: 10 | loss: 0.0735337
	speed: 0.1622s/iter; left time: 3.0820s
Epoch: 10 cost time: 52.38605737686157
Epoch: 10, Steps: 318 | Train Loss: 0.1120277 Vali Loss: 0.1323977 Test Loss: 0.0945468
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
mse:0.4836423993110657, mae:0.5412278771400452, rmse:0.6954440474510193, mape:0.020045025274157524, mspe:0.000667294894810766, rse:0.309510201215744, r2_score:0.8937939778370545, acc:0.9799549747258425
corr: [40.55848  40.70997  40.683353 40.63583  40.758068 41.06459  40.87067
 40.922935 40.7605   40.958153 40.86643  40.93211  41.0269   40.978626
 41.093002]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2282358
	speed: 0.3168s/iter; left time: 976.0576s
	iters: 200, epoch: 1 | loss: 0.1529724
	speed: 0.3142s/iter; left time: 936.4820s
	iters: 300, epoch: 1 | loss: 0.1597251
	speed: 0.3446s/iter; left time: 992.9006s
Epoch: 1 cost time: 104.33543181419373
Epoch: 1, Steps: 318 | Train Loss: 0.2551338 Vali Loss: 0.1472980 Test Loss: 0.1147697
Validation loss decreased (inf --> 0.147298).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1312203
	speed: 0.2369s/iter; left time: 654.4832s
	iters: 200, epoch: 2 | loss: 0.1266198
	speed: 0.2535s/iter; left time: 675.1132s
	iters: 300, epoch: 2 | loss: 0.1322710
	speed: 0.2260s/iter; left time: 579.2992s
Epoch: 2 cost time: 76.14510202407837
Epoch: 2, Steps: 318 | Train Loss: 0.1308237 Vali Loss: 0.1466892 Test Loss: 0.1084881
Validation loss decreased (0.147298 --> 0.146689).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1149740
	speed: 0.2491s/iter; left time: 609.1292s
	iters: 200, epoch: 3 | loss: 0.1307615
	speed: 0.2496s/iter; left time: 585.1991s
	iters: 300, epoch: 3 | loss: 0.1402722
	speed: 0.2314s/iter; left time: 519.4551s
Epoch: 3 cost time: 77.24972724914551
Epoch: 3, Steps: 318 | Train Loss: 0.1263379 Vali Loss: 0.1409260 Test Loss: 0.1061943
Validation loss decreased (0.146689 --> 0.140926).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1210648
	speed: 0.2563s/iter; left time: 545.2295s
	iters: 200, epoch: 4 | loss: 0.1376953
	speed: 0.2539s/iter; left time: 514.7118s
	iters: 300, epoch: 4 | loss: 0.1414513
	speed: 0.2255s/iter; left time: 434.5169s
Epoch: 4 cost time: 77.21715641021729
Epoch: 4, Steps: 318 | Train Loss: 0.1240195 Vali Loss: 0.1417845 Test Loss: 0.1045975
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1008831
	speed: 0.2618s/iter; left time: 473.5501s
	iters: 200, epoch: 5 | loss: 0.1421209
	speed: 0.2480s/iter; left time: 423.7516s
	iters: 300, epoch: 5 | loss: 0.0950460
	speed: 0.2485s/iter; left time: 399.8833s
Epoch: 5 cost time: 80.21086311340332
Epoch: 5, Steps: 318 | Train Loss: 0.1228764 Vali Loss: 0.1414341 Test Loss: 0.1044029
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0848490
	speed: 0.2577s/iter; left time: 384.1594s
	iters: 200, epoch: 6 | loss: 0.0958557
	speed: 0.2336s/iter; left time: 324.9081s
	iters: 300, epoch: 6 | loss: 0.1533258
	speed: 0.2329s/iter; left time: 300.6420s
Epoch: 6 cost time: 78.0543155670166
Epoch: 6, Steps: 318 | Train Loss: 0.1220219 Vali Loss: 0.1404348 Test Loss: 0.1035824
Validation loss decreased (0.140926 --> 0.140435).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1043041
	speed: 0.2603s/iter; left time: 305.3731s
	iters: 200, epoch: 7 | loss: 0.1364204
	speed: 0.2566s/iter; left time: 275.3280s
	iters: 300, epoch: 7 | loss: 0.1272097
	speed: 0.2393s/iter; left time: 232.8408s
Epoch: 7 cost time: 80.23078465461731
Epoch: 7, Steps: 318 | Train Loss: 0.1218310 Vali Loss: 0.1401378 Test Loss: 0.1034698
Validation loss decreased (0.140435 --> 0.140138).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1131908
	speed: 0.2563s/iter; left time: 219.1666s
	iters: 200, epoch: 8 | loss: 0.1638842
	speed: 0.2592s/iter; left time: 195.6757s
	iters: 300, epoch: 8 | loss: 0.1184586
	speed: 0.2559s/iter; left time: 167.6331s
Epoch: 8 cost time: 81.55170726776123
Epoch: 8, Steps: 318 | Train Loss: 0.1216284 Vali Loss: 0.1395798 Test Loss: 0.1033782
Validation loss decreased (0.140138 --> 0.139580).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1144093
	speed: 0.2680s/iter; left time: 143.9014s
	iters: 200, epoch: 9 | loss: 0.1579336
	speed: 0.2528s/iter; left time: 110.4826s
	iters: 300, epoch: 9 | loss: 0.1088279
	speed: 0.2383s/iter; left time: 80.2941s
Epoch: 9 cost time: 80.60282111167908
Epoch: 9, Steps: 318 | Train Loss: 0.1214376 Vali Loss: 0.1401436 Test Loss: 0.1033489
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1269387
	speed: 0.2697s/iter; left time: 59.0720s
	iters: 200, epoch: 10 | loss: 0.1141433
	speed: 0.3239s/iter; left time: 38.5383s
	iters: 300, epoch: 10 | loss: 0.1275380
	speed: 0.3081s/iter; left time: 5.8532s
Epoch: 10 cost time: 94.61088275909424
Epoch: 10, Steps: 318 | Train Loss: 0.1213613 Vali Loss: 0.1398263 Test Loss: 0.1033245
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
mse:0.5280327796936035, mae:0.563015341758728, rmse:0.7266586422920227, mape:0.020853646099567413, mspe:0.0007292713853530586, rse:0.32320043444633484, r2_score:0.884518398558849, acc:0.9791463539004326
corr: [41.076782 41.3153   41.15148  41.07964  41.18781  41.19326  41.194317
 41.274902 41.19196  41.16246  41.256844 41.137363 41.292324 41.346333
 41.415794 41.416317 41.38915  41.46086  41.35947  41.538334]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1477166
	speed: 0.1940s/iter; left time: 597.5790s
	iters: 200, epoch: 1 | loss: 0.1680862
	speed: 0.1519s/iter; left time: 452.9306s
	iters: 300, epoch: 1 | loss: 0.1081203
	speed: 0.1219s/iter; left time: 351.1031s
Epoch: 1 cost time: 49.07327580451965
Epoch: 1, Steps: 318 | Train Loss: 0.2296859 Vali Loss: 0.1487557 Test Loss: 0.1155031
Validation loss decreased (inf --> 0.148756).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1498626
	speed: 0.1336s/iter; left time: 369.1449s
	iters: 200, epoch: 2 | loss: 0.1034478
	speed: 0.1420s/iter; left time: 378.0778s
	iters: 300, epoch: 2 | loss: 0.1329811
	speed: 0.1239s/iter; left time: 317.5710s
Epoch: 2 cost time: 42.22756505012512
Epoch: 2, Steps: 318 | Train Loss: 0.1307858 Vali Loss: 0.1453249 Test Loss: 0.1084358
Validation loss decreased (0.148756 --> 0.145325).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1349186
	speed: 0.1367s/iter; left time: 334.1622s
	iters: 200, epoch: 3 | loss: 0.0969172
	speed: 0.1298s/iter; left time: 304.4765s
	iters: 300, epoch: 3 | loss: 0.1446978
	speed: 0.1422s/iter; left time: 319.3121s
Epoch: 3 cost time: 43.236804246902466
Epoch: 3, Steps: 318 | Train Loss: 0.1257926 Vali Loss: 0.1432444 Test Loss: 0.1093712
Validation loss decreased (0.145325 --> 0.143244).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0991845
	speed: 0.1390s/iter; left time: 295.6432s
	iters: 200, epoch: 4 | loss: 0.1045550
	speed: 0.1409s/iter; left time: 285.5960s
	iters: 300, epoch: 4 | loss: 0.1123001
	speed: 0.1292s/iter; left time: 248.9982s
Epoch: 4 cost time: 43.21632647514343
Epoch: 4, Steps: 318 | Train Loss: 0.1235211 Vali Loss: 0.1409378 Test Loss: 0.1071564
Validation loss decreased (0.143244 --> 0.140938).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1277620
	speed: 0.1319s/iter; left time: 238.5227s
	iters: 200, epoch: 5 | loss: 0.1562084
	speed: 0.1337s/iter; left time: 228.5693s
	iters: 300, epoch: 5 | loss: 0.1008269
	speed: 0.1313s/iter; left time: 211.3297s
Epoch: 5 cost time: 42.22493815422058
Epoch: 5, Steps: 318 | Train Loss: 0.1222203 Vali Loss: 0.1407260 Test Loss: 0.1076536
Validation loss decreased (0.140938 --> 0.140726).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1067975
	speed: 0.1454s/iter; left time: 216.8512s
	iters: 200, epoch: 6 | loss: 0.1370500
	speed: 0.1337s/iter; left time: 186.0266s
	iters: 300, epoch: 6 | loss: 0.1105929
	speed: 0.1276s/iter; left time: 164.7879s
Epoch: 6 cost time: 43.22203493118286
Epoch: 6, Steps: 318 | Train Loss: 0.1215211 Vali Loss: 0.1408150 Test Loss: 0.1076491
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1411343
	speed: 0.1437s/iter; left time: 168.5418s
	iters: 200, epoch: 7 | loss: 0.1313424
	speed: 0.1456s/iter; left time: 156.2372s
	iters: 300, epoch: 7 | loss: 0.1180965
	speed: 0.1497s/iter; left time: 145.6332s
Epoch: 7 cost time: 46.054636001586914
Epoch: 7, Steps: 318 | Train Loss: 0.1211823 Vali Loss: 0.1405343 Test Loss: 0.1073779
Validation loss decreased (0.140726 --> 0.140534).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1303336
	speed: 0.1396s/iter; left time: 119.3313s
	iters: 200, epoch: 8 | loss: 0.1229821
	speed: 0.1381s/iter; left time: 104.2290s
	iters: 300, epoch: 8 | loss: 0.1152014
	speed: 0.1321s/iter; left time: 86.5108s
Epoch: 8 cost time: 43.4444854259491
Epoch: 8, Steps: 318 | Train Loss: 0.1208772 Vali Loss: 0.1401947 Test Loss: 0.1072146
Validation loss decreased (0.140534 --> 0.140195).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1191575
	speed: 0.1442s/iter; left time: 77.4201s
	iters: 200, epoch: 9 | loss: 0.1074513
	speed: 0.1635s/iter; left time: 71.4632s
	iters: 300, epoch: 9 | loss: 0.1233639
	speed: 0.1374s/iter; left time: 46.3156s
Epoch: 9 cost time: 47.1170220375061
Epoch: 9, Steps: 318 | Train Loss: 0.1209377 Vali Loss: 0.1403484 Test Loss: 0.1071583
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1231984
	speed: 0.1468s/iter; left time: 32.1468s
	iters: 200, epoch: 10 | loss: 0.1234145
	speed: 0.1311s/iter; left time: 15.6026s
	iters: 300, epoch: 10 | loss: 0.1003134
	speed: 0.1292s/iter; left time: 2.4548s
Epoch: 10 cost time: 43.05561304092407
Epoch: 10, Steps: 318 | Train Loss: 0.1208166 Vali Loss: 0.1399751 Test Loss: 0.1071250
Validation loss decreased (0.140195 --> 0.139975).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
mse:0.5471707582473755, mae:0.5733807682991028, rmse:0.7397099137306213, mape:0.021224835887551308, mspe:0.0007525728433392942, rse:0.32877570390701294, r2_score:0.8802608591601073, acc:0.9787751641124487
corr: [41.130535 40.954956 40.931477 41.19654  40.935608 40.978615 41.12781
 41.075733 41.16087  41.024868 41.051456 41.11172  41.12799  40.996162
 41.061947 41.115215 41.120697 41.12314  40.943554 41.1273   40.98716
 40.9574   41.05558  41.162083 41.081467]
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2011463
	speed: 0.2100s/iter; left time: 645.0579s
	iters: 200, epoch: 1 | loss: 0.1302771
	speed: 0.2146s/iter; left time: 637.5892s
	iters: 300, epoch: 1 | loss: 0.1308539
	speed: 0.2436s/iter; left time: 699.4123s
Epoch: 1 cost time: 71.33859634399414
Epoch: 1, Steps: 317 | Train Loss: 0.2676095 Vali Loss: 0.1595564 Test Loss: 0.1172628
Validation loss decreased (inf --> 0.159556).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1411699
	speed: 0.2994s/iter; left time: 824.6180s
	iters: 200, epoch: 2 | loss: 0.1370559
	speed: 0.2648s/iter; left time: 702.8817s
	iters: 300, epoch: 2 | loss: 0.1147158
	speed: 0.3322s/iter; left time: 848.4587s
Epoch: 2 cost time: 95.02615666389465
Epoch: 2, Steps: 317 | Train Loss: 0.1376127 Vali Loss: 0.1589525 Test Loss: 0.1132754
Validation loss decreased (0.159556 --> 0.158952).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1368913
	speed: 0.2787s/iter; left time: 679.2651s
	iters: 200, epoch: 3 | loss: 0.1228893
	speed: 0.2963s/iter; left time: 692.4128s
	iters: 300, epoch: 3 | loss: 0.1106166
	speed: 0.2746s/iter; left time: 614.3602s
Epoch: 3 cost time: 90.9534981250763
Epoch: 3, Steps: 317 | Train Loss: 0.1290519 Vali Loss: 0.1561712 Test Loss: 0.1133750
Validation loss decreased (0.158952 --> 0.156171).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1157920
	speed: 0.3003s/iter; left time: 636.5409s
	iters: 200, epoch: 4 | loss: 0.1320222
	speed: 0.2349s/iter; left time: 474.5710s
	iters: 300, epoch: 4 | loss: 0.1155472
	speed: 0.2240s/iter; left time: 430.0024s
Epoch: 4 cost time: 79.85082340240479
Epoch: 4, Steps: 317 | Train Loss: 0.1257565 Vali Loss: 0.1561390 Test Loss: 0.1121049
Validation loss decreased (0.156171 --> 0.156139).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1376842
	speed: 0.2519s/iter; left time: 454.1659s
	iters: 200, epoch: 5 | loss: 0.1060524
	speed: 0.2350s/iter; left time: 400.2290s
	iters: 300, epoch: 5 | loss: 0.1372638
	speed: 0.2615s/iter; left time: 419.2243s
Epoch: 5 cost time: 78.89273619651794
Epoch: 5, Steps: 317 | Train Loss: 0.1240038 Vali Loss: 0.1574295 Test Loss: 0.1121924
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1148861
	speed: 0.2527s/iter; left time: 375.5274s
	iters: 200, epoch: 6 | loss: 0.1057520
	speed: 0.2591s/iter; left time: 359.0560s
	iters: 300, epoch: 6 | loss: 0.1361804
	speed: 0.2399s/iter; left time: 308.4641s
Epoch: 6 cost time: 79.59688425064087
Epoch: 6, Steps: 317 | Train Loss: 0.1232460 Vali Loss: 0.1570066 Test Loss: 0.1131789
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1382293
	speed: 0.2413s/iter; left time: 282.0427s
	iters: 200, epoch: 7 | loss: 0.1182216
	speed: 0.2590s/iter; left time: 276.8645s
	iters: 300, epoch: 7 | loss: 0.1111065
	speed: 0.2300s/iter; left time: 222.8879s
Epoch: 7 cost time: 77.16419577598572
Epoch: 7, Steps: 317 | Train Loss: 0.1227162 Vali Loss: 0.1579593 Test Loss: 0.1131254
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1563423
	speed: 0.1849s/iter; left time: 569.5620s
	iters: 200, epoch: 1 | loss: 0.1237887
	speed: 0.1657s/iter; left time: 494.0817s
	iters: 300, epoch: 1 | loss: 0.1672333
	speed: 0.1585s/iter; left time: 456.6244s
Epoch: 1 cost time: 54.08600854873657
Epoch: 1, Steps: 318 | Train Loss: 0.2255415 Vali Loss: 0.1488994 Test Loss: 0.1118992
Validation loss decreased (inf --> 0.148899).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1064899
	speed: 0.1656s/iter; left time: 457.5345s
	iters: 200, epoch: 2 | loss: 0.1208556
	speed: 0.1694s/iter; left time: 451.1698s
	iters: 300, epoch: 2 | loss: 0.1446416
	speed: 0.1730s/iter; left time: 443.4393s
Epoch: 2 cost time: 54.056934118270874
Epoch: 2, Steps: 318 | Train Loss: 0.1254187 Vali Loss: 0.1377433 Test Loss: 0.1013399
Validation loss decreased (0.148899 --> 0.137743).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0995544
	speed: 0.2001s/iter; left time: 489.2290s
	iters: 200, epoch: 3 | loss: 0.1111098
	speed: 0.1777s/iter; left time: 416.6356s
	iters: 300, epoch: 3 | loss: 0.1484879
	speed: 0.1752s/iter; left time: 393.2438s
Epoch: 3 cost time: 58.755813121795654
Epoch: 3, Steps: 318 | Train Loss: 0.1169327 Vali Loss: 0.1324340 Test Loss: 0.0940305
Validation loss decreased (0.137743 --> 0.132434).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0977352
	speed: 0.1499s/iter; left time: 318.8840s
	iters: 200, epoch: 4 | loss: 0.1379297
	speed: 0.1408s/iter; left time: 285.4925s
	iters: 300, epoch: 4 | loss: 0.1263553
	speed: 0.1420s/iter; left time: 273.6848s
Epoch: 4 cost time: 46.1666305065155
Epoch: 4, Steps: 318 | Train Loss: 0.1122658 Vali Loss: 0.1287892 Test Loss: 0.0913863
Validation loss decreased (0.132434 --> 0.128789).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1097450
	speed: 0.1679s/iter; left time: 303.7930s
	iters: 200, epoch: 5 | loss: 0.0766677
	speed: 0.1650s/iter; left time: 282.0626s
	iters: 300, epoch: 5 | loss: 0.0969328
	speed: 0.1568s/iter; left time: 252.3594s
Epoch: 5 cost time: 51.85735273361206
Epoch: 5, Steps: 318 | Train Loss: 0.1094961 Vali Loss: 0.1268243 Test Loss: 0.0903220
Validation loss decreased (0.128789 --> 0.126824).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0933077
	speed: 0.1814s/iter; left time: 270.4848s
	iters: 200, epoch: 6 | loss: 0.1087088
	speed: 0.1959s/iter; left time: 272.4564s
	iters: 300, epoch: 6 | loss: 0.1188270
	speed: 0.1864s/iter; left time: 240.6829s
Epoch: 6 cost time: 59.18990755081177
Epoch: 6, Steps: 318 | Train Loss: 0.1083037 Vali Loss: 0.1248242 Test Loss: 0.0896376
Validation loss decreased (0.126824 --> 0.124824).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1177648
	speed: 0.2046s/iter; left time: 239.9519s
	iters: 200, epoch: 7 | loss: 0.1192181
	speed: 0.1727s/iter; left time: 185.3028s
	iters: 300, epoch: 7 | loss: 0.1284251
	speed: 0.1759s/iter; left time: 171.1712s
Epoch: 7 cost time: 59.3307363986969
Epoch: 7, Steps: 318 | Train Loss: 0.1078097 Vali Loss: 0.1257386 Test Loss: 0.0892318
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0972261
	speed: 0.1624s/iter; left time: 138.8336s
	iters: 200, epoch: 8 | loss: 0.1165629
	speed: 0.1778s/iter; left time: 134.2601s
	iters: 300, epoch: 8 | loss: 0.1235861
	speed: 0.1867s/iter; left time: 122.2631s
Epoch: 8 cost time: 56.18127727508545
Epoch: 8, Steps: 318 | Train Loss: 0.1075858 Vali Loss: 0.1265884 Test Loss: 0.0890432
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0755981
	speed: 0.1904s/iter; left time: 102.2492s
	iters: 200, epoch: 9 | loss: 0.0890397
	speed: 0.1727s/iter; left time: 75.4725s
	iters: 300, epoch: 9 | loss: 0.1092093
	speed: 0.1733s/iter; left time: 58.3955s
Epoch: 9 cost time: 57.31452226638794
Epoch: 9, Steps: 318 | Train Loss: 0.1072993 Vali Loss: 0.1257643 Test Loss: 0.0889846
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2059775
	speed: 0.1967s/iter; left time: 605.9008s
	iters: 200, epoch: 1 | loss: 0.1580786
	speed: 0.1801s/iter; left time: 536.7843s
	iters: 300, epoch: 1 | loss: 0.0975866
	speed: 0.1925s/iter; left time: 554.5379s
Epoch: 1 cost time: 60.363689661026
Epoch: 1, Steps: 318 | Train Loss: 0.2212397 Vali Loss: 0.1442270 Test Loss: 0.1126176
Validation loss decreased (inf --> 0.144227).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1301744
	speed: 0.1965s/iter; left time: 542.8153s
	iters: 200, epoch: 2 | loss: 0.1369144
	speed: 0.1911s/iter; left time: 509.0040s
	iters: 300, epoch: 2 | loss: 0.1166811
	speed: 0.1977s/iter; left time: 506.5787s
Epoch: 2 cost time: 62.011130809783936
Epoch: 2, Steps: 318 | Train Loss: 0.1277355 Vali Loss: 0.1404426 Test Loss: 0.1035395
Validation loss decreased (0.144227 --> 0.140443).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1367874
	speed: 0.1911s/iter; left time: 467.1200s
	iters: 200, epoch: 3 | loss: 0.1273143
	speed: 0.1815s/iter; left time: 425.6142s
	iters: 300, epoch: 3 | loss: 0.1200925
	speed: 0.1746s/iter; left time: 392.0813s
Epoch: 3 cost time: 58.05517840385437
Epoch: 3, Steps: 318 | Train Loss: 0.1207371 Vali Loss: 0.1373942 Test Loss: 0.1022025
Validation loss decreased (0.140443 --> 0.137394).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1439968
	speed: 0.1653s/iter; left time: 351.5474s
	iters: 200, epoch: 4 | loss: 0.1347925
	speed: 0.1618s/iter; left time: 327.9295s
	iters: 300, epoch: 4 | loss: 0.0904172
	speed: 0.1628s/iter; left time: 313.6525s
Epoch: 4 cost time: 52.188302993774414
Epoch: 4, Steps: 318 | Train Loss: 0.1169176 Vali Loss: 0.1334504 Test Loss: 0.0966392
Validation loss decreased (0.137394 --> 0.133450).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1203426
	speed: 0.1679s/iter; left time: 303.6463s
	iters: 200, epoch: 5 | loss: 0.1244985
	speed: 0.1780s/iter; left time: 304.1965s
	iters: 300, epoch: 5 | loss: 0.1009120
	speed: 0.1534s/iter; left time: 246.7914s
Epoch: 5 cost time: 53.214200496673584
Epoch: 5, Steps: 318 | Train Loss: 0.1149911 Vali Loss: 0.1331658 Test Loss: 0.0956963
Validation loss decreased (0.133450 --> 0.133166).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1252333
	speed: 0.1624s/iter; left time: 242.1975s
	iters: 200, epoch: 6 | loss: 0.1044349
	speed: 0.1628s/iter; left time: 226.3954s
	iters: 300, epoch: 6 | loss: 0.1121197
	speed: 0.1526s/iter; left time: 197.0039s
Epoch: 6 cost time: 50.94261312484741
Epoch: 6, Steps: 318 | Train Loss: 0.1135445 Vali Loss: 0.1339683 Test Loss: 0.0955280
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0856603
	speed: 0.1915s/iter; left time: 224.5740s
	iters: 200, epoch: 7 | loss: 0.1128682
	speed: 0.1782s/iter; left time: 191.2200s
	iters: 300, epoch: 7 | loss: 0.1021654
	speed: 0.1739s/iter; left time: 169.1584s
Epoch: 7 cost time: 57.479275941848755
Epoch: 7, Steps: 318 | Train Loss: 0.1131123 Vali Loss: 0.1335632 Test Loss: 0.0949094
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0946197
	speed: 0.1843s/iter; left time: 157.6108s
	iters: 200, epoch: 8 | loss: 0.0955949
	speed: 0.1772s/iter; left time: 133.7493s
	iters: 300, epoch: 8 | loss: 0.1031329
	speed: 0.1827s/iter; left time: 119.6985s
Epoch: 8 cost time: 57.127010345458984
Epoch: 8, Steps: 318 | Train Loss: 0.1126664 Vali Loss: 0.1317440 Test Loss: 0.0948070
Validation loss decreased (0.133166 --> 0.131744).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1129472
	speed: 0.1624s/iter; left time: 87.2198s
	iters: 200, epoch: 9 | loss: 0.1237377
	speed: 0.1543s/iter; left time: 67.4264s
	iters: 300, epoch: 9 | loss: 0.1608655
	speed: 0.1611s/iter; left time: 54.2792s
Epoch: 9 cost time: 50.434001207351685
Epoch: 9, Steps: 318 | Train Loss: 0.1126920 Vali Loss: 0.1325989 Test Loss: 0.0947228
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1557602
	speed: 0.1618s/iter; left time: 35.4263s
	iters: 200, epoch: 10 | loss: 0.0973167
	speed: 0.1533s/iter; left time: 18.2408s
	iters: 300, epoch: 10 | loss: 0.0736471
	speed: 0.1610s/iter; left time: 3.0591s
Epoch: 10 cost time: 50.03142786026001
Epoch: 10, Steps: 318 | Train Loss: 0.1123028 Vali Loss: 0.1323689 Test Loss: 0.0946746
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2289447
	speed: 0.2390s/iter; left time: 736.3512s
	iters: 200, epoch: 1 | loss: 0.1548297
	speed: 0.2365s/iter; left time: 705.0581s
	iters: 300, epoch: 1 | loss: 0.1596849
	speed: 0.2859s/iter; left time: 823.7617s
Epoch: 1 cost time: 81.71722459793091
Epoch: 1, Steps: 318 | Train Loss: 0.2567710 Vali Loss: 0.1473422 Test Loss: 0.1147375
Validation loss decreased (inf --> 0.147342).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1318716
	speed: 0.2871s/iter; left time: 793.3037s
	iters: 200, epoch: 2 | loss: 0.1271832
	speed: 0.2802s/iter; left time: 746.1980s
	iters: 300, epoch: 2 | loss: 0.1317311
	speed: 0.2809s/iter; left time: 720.0146s
Epoch: 2 cost time: 89.33637595176697
Epoch: 2, Steps: 318 | Train Loss: 0.1306670 Vali Loss: 0.1469023 Test Loss: 0.1083674
Validation loss decreased (0.147342 --> 0.146902).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1152800
	speed: 0.2927s/iter; left time: 715.5995s
	iters: 200, epoch: 3 | loss: 0.1299007
	speed: 0.2878s/iter; left time: 674.8896s
	iters: 300, epoch: 3 | loss: 0.1401583
	speed: 0.3093s/iter; left time: 694.3122s
Epoch: 3 cost time: 94.50221848487854
Epoch: 3, Steps: 318 | Train Loss: 0.1261259 Vali Loss: 0.1409160 Test Loss: 0.1057262
Validation loss decreased (0.146902 --> 0.140916).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1202607
	speed: 0.2968s/iter; left time: 631.3250s
	iters: 200, epoch: 4 | loss: 0.1366905
	speed: 0.2750s/iter; left time: 557.3688s
	iters: 300, epoch: 4 | loss: 0.1415030
	speed: 0.2862s/iter; left time: 551.5274s
Epoch: 4 cost time: 90.64130997657776
Epoch: 4, Steps: 318 | Train Loss: 0.1236816 Vali Loss: 0.1418027 Test Loss: 0.1041821
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1018458
	speed: 0.2916s/iter; left time: 527.5641s
	iters: 200, epoch: 5 | loss: 0.1429142
	speed: 0.2901s/iter; left time: 495.7279s
	iters: 300, epoch: 5 | loss: 0.0950891
	speed: 0.2865s/iter; left time: 461.0105s
Epoch: 5 cost time: 92.37449336051941
Epoch: 5, Steps: 318 | Train Loss: 0.1224889 Vali Loss: 0.1413153 Test Loss: 0.1037625
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0817932
	speed: 0.2970s/iter; left time: 442.8392s
	iters: 200, epoch: 6 | loss: 0.0965958
	speed: 0.2626s/iter; left time: 365.2602s
	iters: 300, epoch: 6 | loss: 0.1519606
	speed: 0.3457s/iter; left time: 446.2519s
Epoch: 6 cost time: 96.05443072319031
Epoch: 6, Steps: 318 | Train Loss: 0.1216255 Vali Loss: 0.1403638 Test Loss: 0.1029915
Validation loss decreased (0.140916 --> 0.140364).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1011049
	speed: 0.3562s/iter; left time: 417.7756s
	iters: 200, epoch: 7 | loss: 0.1353580
	speed: 0.3445s/iter; left time: 369.6067s
	iters: 300, epoch: 7 | loss: 0.1261363
	speed: 0.3602s/iter; left time: 350.5120s
Epoch: 7 cost time: 111.88393306732178
Epoch: 7, Steps: 318 | Train Loss: 0.1214290 Vali Loss: 0.1400750 Test Loss: 0.1028863
Validation loss decreased (0.140364 --> 0.140075).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1134453
	speed: 0.2947s/iter; left time: 251.9550s
	iters: 200, epoch: 8 | loss: 0.1644152
	speed: 0.2700s/iter; left time: 203.8312s
	iters: 300, epoch: 8 | loss: 0.1178901
	speed: 0.2813s/iter; left time: 184.2439s
Epoch: 8 cost time: 90.74078226089478
Epoch: 8, Steps: 318 | Train Loss: 0.1212008 Vali Loss: 0.1395063 Test Loss: 0.1027720
Validation loss decreased (0.140075 --> 0.139506).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1150625
	speed: 0.2885s/iter; left time: 154.8993s
	iters: 200, epoch: 9 | loss: 0.1567253
	speed: 0.2667s/iter; left time: 116.5683s
	iters: 300, epoch: 9 | loss: 0.1099779
	speed: 0.2826s/iter; left time: 95.2363s
Epoch: 9 cost time: 89.41273736953735
Epoch: 9, Steps: 318 | Train Loss: 0.1210076 Vali Loss: 0.1400811 Test Loss: 0.1027327
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1264601
	speed: 0.2827s/iter; left time: 61.9169s
	iters: 200, epoch: 10 | loss: 0.1137992
	speed: 0.2852s/iter; left time: 33.9404s
	iters: 300, epoch: 10 | loss: 0.1269982
	speed: 0.2763s/iter; left time: 5.2488s
Epoch: 10 cost time: 89.27370524406433
Epoch: 10, Steps: 318 | Train Loss: 0.1209349 Vali Loss: 0.1397398 Test Loss: 0.1027085
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1478470
	speed: 0.1788s/iter; left time: 550.7772s
	iters: 200, epoch: 1 | loss: 0.1679428
	speed: 0.1732s/iter; left time: 516.1696s
	iters: 300, epoch: 1 | loss: 0.1080318
	speed: 0.1576s/iter; left time: 454.1437s
Epoch: 1 cost time: 54.13275504112244
Epoch: 1, Steps: 318 | Train Loss: 0.2298482 Vali Loss: 0.1487382 Test Loss: 0.1153259
Validation loss decreased (inf --> 0.148738).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1495225
	speed: 0.1634s/iter; left time: 451.5759s
	iters: 200, epoch: 2 | loss: 0.1034765
	speed: 0.1647s/iter; left time: 438.6507s
	iters: 300, epoch: 2 | loss: 0.1329499
	speed: 0.1550s/iter; left time: 397.1991s
Epoch: 2 cost time: 51.131364583969116
Epoch: 2, Steps: 318 | Train Loss: 0.1308149 Vali Loss: 0.1453534 Test Loss: 0.1084413
Validation loss decreased (0.148738 --> 0.145353).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1349675
	speed: 0.1680s/iter; left time: 410.6523s
	iters: 200, epoch: 3 | loss: 0.0962052
	speed: 0.1623s/iter; left time: 380.7041s
	iters: 300, epoch: 3 | loss: 0.1447228
	speed: 0.1713s/iter; left time: 384.5945s
Epoch: 3 cost time: 52.84512901306152
Epoch: 3, Steps: 318 | Train Loss: 0.1258040 Vali Loss: 0.1433142 Test Loss: 0.1093693
Validation loss decreased (0.145353 --> 0.143314).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0993342
	speed: 0.1597s/iter; left time: 339.7812s
	iters: 200, epoch: 4 | loss: 0.1045709
	speed: 0.1655s/iter; left time: 335.4551s
	iters: 300, epoch: 4 | loss: 0.1122184
	speed: 0.1646s/iter; left time: 317.1776s
Epoch: 4 cost time: 52.05515766143799
Epoch: 4, Steps: 318 | Train Loss: 0.1235414 Vali Loss: 0.1410188 Test Loss: 0.1071179
Validation loss decreased (0.143314 --> 0.141019).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1279871
	speed: 0.1776s/iter; left time: 321.3526s
	iters: 200, epoch: 5 | loss: 0.1567023
	speed: 0.1597s/iter; left time: 272.9872s
	iters: 300, epoch: 5 | loss: 0.1007278
	speed: 0.1679s/iter; left time: 270.1302s
Epoch: 5 cost time: 52.78291630744934
Epoch: 5, Steps: 318 | Train Loss: 0.1222530 Vali Loss: 0.1408505 Test Loss: 0.1074691
Validation loss decreased (0.141019 --> 0.140851).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1065061
	speed: 0.1813s/iter; left time: 270.2688s
	iters: 200, epoch: 6 | loss: 0.1373592
	speed: 0.1808s/iter; left time: 251.5101s
	iters: 300, epoch: 6 | loss: 0.1104936
	speed: 0.1680s/iter; left time: 216.8638s
Epoch: 6 cost time: 57.03892159461975
Epoch: 6, Steps: 318 | Train Loss: 0.1215548 Vali Loss: 0.1409396 Test Loss: 0.1075341
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1408557
	speed: 0.1803s/iter; left time: 211.5504s
	iters: 200, epoch: 7 | loss: 0.1330729
	speed: 0.1769s/iter; left time: 189.7644s
	iters: 300, epoch: 7 | loss: 0.1185622
	speed: 0.1865s/iter; left time: 181.4480s
Epoch: 7 cost time: 58.24392509460449
Epoch: 7, Steps: 318 | Train Loss: 0.1212397 Vali Loss: 0.1406900 Test Loss: 0.1072490
Validation loss decreased (0.140851 --> 0.140690).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1304743
	speed: 0.1737s/iter; left time: 148.4993s
	iters: 200, epoch: 8 | loss: 0.1229410
	speed: 0.1661s/iter; left time: 125.4365s
	iters: 300, epoch: 8 | loss: 0.1150740
	speed: 0.1584s/iter; left time: 103.7748s
Epoch: 8 cost time: 53.055060625076294
Epoch: 8, Steps: 318 | Train Loss: 0.1209299 Vali Loss: 0.1403450 Test Loss: 0.1070895
Validation loss decreased (0.140690 --> 0.140345).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1191175
	speed: 0.1785s/iter; left time: 95.8423s
	iters: 200, epoch: 9 | loss: 0.1075022
	speed: 0.1531s/iter; left time: 66.9212s
	iters: 300, epoch: 9 | loss: 0.1231337
	speed: 0.1633s/iter; left time: 55.0204s
Epoch: 9 cost time: 52.879019260406494
Epoch: 9, Steps: 318 | Train Loss: 0.1209995 Vali Loss: 0.1404963 Test Loss: 0.1070319
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1231672
	speed: 0.1792s/iter; left time: 39.2342s
	iters: 200, epoch: 10 | loss: 0.1233986
	speed: 0.1677s/iter; left time: 19.9587s
	iters: 300, epoch: 10 | loss: 0.1006002
	speed: 0.1668s/iter; left time: 3.1698s
Epoch: 10 cost time: 53.937219858169556
Epoch: 10, Steps: 318 | Train Loss: 0.1208835 Vali Loss: 0.1401194 Test Loss: 0.1070016
Validation loss decreased (0.140345 --> 0.140119).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=2, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1923026
	speed: 0.2361s/iter; left time: 724.9994s
	iters: 200, epoch: 1 | loss: 0.1275900
	speed: 0.2448s/iter; left time: 727.3604s
	iters: 300, epoch: 1 | loss: 0.1255615
	speed: 0.2352s/iter; left time: 675.2431s
Epoch: 1 cost time: 76.16905403137207
Epoch: 1, Steps: 317 | Train Loss: 0.2713702 Vali Loss: 0.1601427 Test Loss: 0.1201125
Validation loss decreased (inf --> 0.160143).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1466668
	speed: 0.2597s/iter; left time: 715.3229s
	iters: 200, epoch: 2 | loss: 0.1450510
	speed: 0.2551s/iter; left time: 677.1454s
	iters: 300, epoch: 2 | loss: 0.1228525
	speed: 0.2720s/iter; left time: 694.7421s
Epoch: 2 cost time: 82.96016907691956
Epoch: 2, Steps: 317 | Train Loss: 0.1395378 Vali Loss: 0.1499666 Test Loss: 0.1112975
Validation loss decreased (0.160143 --> 0.149967).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1418581
	speed: 0.2794s/iter; left time: 680.8784s
	iters: 200, epoch: 3 | loss: 0.1173116
	speed: 0.2712s/iter; left time: 633.7015s
	iters: 300, epoch: 3 | loss: 0.1159301
	speed: 0.2795s/iter; left time: 625.3286s
Epoch: 3 cost time: 86.84546852111816
Epoch: 3, Steps: 317 | Train Loss: 0.1330861 Vali Loss: 0.1484091 Test Loss: 0.1101182
Validation loss decreased (0.149967 --> 0.148409).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1220903
	speed: 0.2666s/iter; left time: 565.2327s
	iters: 200, epoch: 4 | loss: 0.1311732
	speed: 0.2660s/iter; left time: 537.3150s
	iters: 300, epoch: 4 | loss: 0.1181594
	speed: 0.2797s/iter; left time: 536.9329s
Epoch: 4 cost time: 86.09616923332214
Epoch: 4, Steps: 317 | Train Loss: 0.1304839 Vali Loss: 0.1488149 Test Loss: 0.1090003
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1346133
	speed: 0.2933s/iter; left time: 528.8766s
	iters: 200, epoch: 5 | loss: 0.1127495
	speed: 0.2878s/iter; left time: 490.2060s
	iters: 300, epoch: 5 | loss: 0.1434728
	speed: 0.2719s/iter; left time: 435.8191s
Epoch: 5 cost time: 90.77431225776672
Epoch: 5, Steps: 317 | Train Loss: 0.1290925 Vali Loss: 0.1491472 Test Loss: 0.1087771
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1219709
	speed: 0.3487s/iter; left time: 518.2330s
	iters: 200, epoch: 6 | loss: 0.1062841
	speed: 0.3372s/iter; left time: 467.3510s
	iters: 300, epoch: 6 | loss: 0.1414890
	speed: 0.3165s/iter; left time: 407.0095s
Epoch: 6 cost time: 105.25641393661499
Epoch: 6, Steps: 317 | Train Loss: 0.1285210 Vali Loss: 0.1492169 Test Loss: 0.1086890
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2262128
	speed: 0.1185s/iter; left time: 365.0496s
	iters: 200, epoch: 1 | loss: 0.1533412
	speed: 0.0885s/iter; left time: 263.9077s
	iters: 300, epoch: 1 | loss: 0.1383378
	speed: 0.0972s/iter; left time: 279.9146s
Epoch: 1 cost time: 32.59438371658325
Epoch: 1, Steps: 318 | Train Loss: 0.2329428 Vali Loss: 0.1506268 Test Loss: 0.1255136
Validation loss decreased (inf --> 0.150627).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1370840
	speed: 0.1014s/iter; left time: 280.1517s
	iters: 200, epoch: 2 | loss: 0.1139953
	speed: 0.0979s/iter; left time: 260.7272s
	iters: 300, epoch: 2 | loss: 0.1067277
	speed: 0.0923s/iter; left time: 236.5456s
Epoch: 2 cost time: 30.196975469589233
Epoch: 2, Steps: 318 | Train Loss: 0.1280286 Vali Loss: 0.1288304 Test Loss: 0.1067571
Validation loss decreased (0.150627 --> 0.128830).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0926328
	speed: 0.1027s/iter; left time: 251.0114s
	iters: 200, epoch: 3 | loss: 0.1415122
	speed: 0.0913s/iter; left time: 214.0172s
	iters: 300, epoch: 3 | loss: 0.0927100
	speed: 0.0979s/iter; left time: 219.8124s
Epoch: 3 cost time: 31.262367486953735
Epoch: 3, Steps: 318 | Train Loss: 0.1089392 Vali Loss: 0.1233441 Test Loss: 0.0995685
Validation loss decreased (0.128830 --> 0.123344).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0941165
	speed: 0.1067s/iter; left time: 226.8938s
	iters: 200, epoch: 4 | loss: 0.1057029
	speed: 0.0977s/iter; left time: 197.9727s
	iters: 300, epoch: 4 | loss: 0.1157278
	speed: 0.0983s/iter; left time: 189.5106s
Epoch: 4 cost time: 32.356882095336914
Epoch: 4, Steps: 318 | Train Loss: 0.1025712 Vali Loss: 0.1161989 Test Loss: 0.0978504
Validation loss decreased (0.123344 --> 0.116199).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0969965
	speed: 0.1008s/iter; left time: 182.2643s
	iters: 200, epoch: 5 | loss: 0.0889733
	speed: 0.1033s/iter; left time: 176.5016s
	iters: 300, epoch: 5 | loss: 0.0943033
	speed: 0.0976s/iter; left time: 157.0017s
Epoch: 5 cost time: 31.85228395462036
Epoch: 5, Steps: 318 | Train Loss: 0.0992617 Vali Loss: 0.1154038 Test Loss: 0.0957222
Validation loss decreased (0.116199 --> 0.115404).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0910285
	speed: 0.1000s/iter; left time: 149.0358s
	iters: 200, epoch: 6 | loss: 0.1050237
	speed: 0.0815s/iter; left time: 113.3509s
	iters: 300, epoch: 6 | loss: 0.1033713
	speed: 0.0942s/iter; left time: 121.6438s
Epoch: 6 cost time: 29.430155038833618
Epoch: 6, Steps: 318 | Train Loss: 0.0977503 Vali Loss: 0.1135876 Test Loss: 0.0946038
Validation loss decreased (0.115404 --> 0.113588).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0876000
	speed: 0.1005s/iter; left time: 117.8301s
	iters: 200, epoch: 7 | loss: 0.0875335
	speed: 0.0936s/iter; left time: 100.4366s
	iters: 300, epoch: 7 | loss: 0.1127080
	speed: 0.0994s/iter; left time: 96.7600s
Epoch: 7 cost time: 30.99505925178528
Epoch: 7, Steps: 318 | Train Loss: 0.0970758 Vali Loss: 0.1126144 Test Loss: 0.0947726
Validation loss decreased (0.113588 --> 0.112614).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1008741
	speed: 0.1052s/iter; left time: 89.9463s
	iters: 200, epoch: 8 | loss: 0.0807177
	speed: 0.0954s/iter; left time: 72.0615s
	iters: 300, epoch: 8 | loss: 0.0757554
	speed: 0.0918s/iter; left time: 60.1280s
Epoch: 8 cost time: 31.122594833374023
Epoch: 8, Steps: 318 | Train Loss: 0.0969851 Vali Loss: 0.1136538 Test Loss: 0.0948701
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0745945
	speed: 0.1048s/iter; left time: 56.2894s
	iters: 200, epoch: 9 | loss: 0.0979804
	speed: 0.0948s/iter; left time: 41.4241s
	iters: 300, epoch: 9 | loss: 0.0850406
	speed: 0.0930s/iter; left time: 31.3528s
Epoch: 9 cost time: 31.360507488250732
Epoch: 9, Steps: 318 | Train Loss: 0.0969716 Vali Loss: 0.1143410 Test Loss: 0.0947404
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0779160
	speed: 0.0588s/iter; left time: 12.8784s
	iters: 200, epoch: 10 | loss: 0.1052958
	speed: 0.0780s/iter; left time: 9.2879s
	iters: 300, epoch: 10 | loss: 0.0977955
	speed: 0.0637s/iter; left time: 1.2103s
Epoch: 10 cost time: 21.387811183929443
Epoch: 10, Steps: 318 | Train Loss: 0.0964796 Vali Loss: 0.1128826 Test Loss: 0.0947393
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1775768
	speed: 0.0884s/iter; left time: 272.3091s
	iters: 200, epoch: 1 | loss: 0.1566616
	speed: 0.0683s/iter; left time: 203.5728s
	iters: 300, epoch: 1 | loss: 0.1448325
	speed: 0.0715s/iter; left time: 205.9160s
Epoch: 1 cost time: 24.32214665412903
Epoch: 1, Steps: 318 | Train Loss: 0.3242860 Vali Loss: 0.1554801 Test Loss: 0.2597677
Validation loss decreased (inf --> 0.155480).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1597462
	speed: 0.0764s/iter; left time: 210.9843s
	iters: 200, epoch: 2 | loss: 0.1650135
	speed: 0.0776s/iter; left time: 206.6811s
	iters: 300, epoch: 2 | loss: 0.1209245
	speed: 0.0685s/iter; left time: 175.4878s
Epoch: 2 cost time: 23.750166654586792
Epoch: 2, Steps: 318 | Train Loss: 0.1427615 Vali Loss: 0.1469397 Test Loss: 0.2401853
Validation loss decreased (0.155480 --> 0.146940).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1793867
	speed: 0.0762s/iter; left time: 186.2130s
	iters: 200, epoch: 3 | loss: 0.1138462
	speed: 0.0619s/iter; left time: 145.1994s
	iters: 300, epoch: 3 | loss: 0.1217783
	speed: 0.0590s/iter; left time: 132.4963s
Epoch: 3 cost time: 20.45797610282898
Epoch: 3, Steps: 318 | Train Loss: 0.1352620 Vali Loss: 0.1416630 Test Loss: 0.2293420
Validation loss decreased (0.146940 --> 0.141663).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1165664
	speed: 0.1070s/iter; left time: 227.6672s
	iters: 200, epoch: 4 | loss: 0.1425366
	speed: 0.1020s/iter; left time: 206.6885s
	iters: 300, epoch: 4 | loss: 0.1828393
	speed: 0.1041s/iter; left time: 200.5201s
Epoch: 4 cost time: 33.02846598625183
Epoch: 4, Steps: 318 | Train Loss: 0.1312408 Vali Loss: 0.1408751 Test Loss: 0.2325321
Validation loss decreased (0.141663 --> 0.140875).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1265586
	speed: 0.1086s/iter; left time: 196.4459s
	iters: 200, epoch: 5 | loss: 0.1427221
	speed: 0.1142s/iter; left time: 195.2085s
	iters: 300, epoch: 5 | loss: 0.1419928
	speed: 0.1153s/iter; left time: 185.4804s
Epoch: 5 cost time: 35.96979570388794
Epoch: 5, Steps: 318 | Train Loss: 0.1294403 Vali Loss: 0.1394830 Test Loss: 0.2318320
Validation loss decreased (0.140875 --> 0.139483).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1269589
	speed: 0.1222s/iter; left time: 182.2024s
	iters: 200, epoch: 6 | loss: 0.1184578
	speed: 0.1113s/iter; left time: 154.7885s
	iters: 300, epoch: 6 | loss: 0.1700092
	speed: 0.1059s/iter; left time: 136.6811s
Epoch: 6 cost time: 35.92222285270691
Epoch: 6, Steps: 318 | Train Loss: 0.1288549 Vali Loss: 0.1402591 Test Loss: 0.2316328
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1497675
	speed: 0.1153s/iter; left time: 135.2560s
	iters: 200, epoch: 7 | loss: 0.0982838
	speed: 0.0902s/iter; left time: 96.8093s
	iters: 300, epoch: 7 | loss: 0.1226816
	speed: 0.0913s/iter; left time: 88.8148s
Epoch: 7 cost time: 31.50801110267639
Epoch: 7, Steps: 318 | Train Loss: 0.1287085 Vali Loss: 0.1400174 Test Loss: 0.2311072
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0929536
	speed: 0.1124s/iter; left time: 96.0667s
	iters: 200, epoch: 8 | loss: 0.0949906
	speed: 0.1026s/iter; left time: 77.4491s
	iters: 300, epoch: 8 | loss: 0.1574810
	speed: 0.1016s/iter; left time: 66.5352s
Epoch: 8 cost time: 33.303773403167725
Epoch: 8, Steps: 318 | Train Loss: 0.1283480 Vali Loss: 0.1383591 Test Loss: 0.2303324
Validation loss decreased (0.139483 --> 0.138359).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1487114
	speed: 0.1192s/iter; left time: 64.0232s
	iters: 200, epoch: 9 | loss: 0.0981748
	speed: 0.1119s/iter; left time: 48.8840s
	iters: 300, epoch: 9 | loss: 0.1314074
	speed: 0.0911s/iter; left time: 30.7149s
Epoch: 9 cost time: 34.174376249313354
Epoch: 9, Steps: 318 | Train Loss: 0.1284205 Vali Loss: 0.1391809 Test Loss: 0.2305758
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1580509
	speed: 0.1147s/iter; left time: 25.1107s
	iters: 200, epoch: 10 | loss: 0.1162706
	speed: 0.1045s/iter; left time: 12.4380s
	iters: 300, epoch: 10 | loss: 0.1390802
	speed: 0.0977s/iter; left time: 1.8569s
Epoch: 10 cost time: 33.534961462020874
Epoch: 10, Steps: 318 | Train Loss: 0.1279876 Vali Loss: 0.1389329 Test Loss: 0.2305399
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1455004
	speed: 0.1242s/iter; left time: 382.5872s
	iters: 200, epoch: 1 | loss: 0.2277997
	speed: 0.1054s/iter; left time: 314.2112s
	iters: 300, epoch: 1 | loss: 0.1784630
	speed: 0.1356s/iter; left time: 390.6623s
Epoch: 1 cost time: 39.31326150894165
Epoch: 1, Steps: 318 | Train Loss: 0.2519311 Vali Loss: 0.1454024 Test Loss: 0.1165875
Validation loss decreased (inf --> 0.145402).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1238961
	speed: 0.1305s/iter; left time: 360.5604s
	iters: 200, epoch: 2 | loss: 0.1046839
	speed: 0.1411s/iter; left time: 375.8681s
	iters: 300, epoch: 2 | loss: 0.1275143
	speed: 0.1561s/iter; left time: 399.9753s
Epoch: 2 cost time: 45.856443643569946
Epoch: 2, Steps: 318 | Train Loss: 0.1268898 Vali Loss: 0.1338397 Test Loss: 0.1122164
Validation loss decreased (0.145402 --> 0.133840).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0861275
	speed: 0.1443s/iter; left time: 352.7584s
	iters: 200, epoch: 3 | loss: 0.0929578
	speed: 0.1201s/iter; left time: 281.5646s
	iters: 300, epoch: 3 | loss: 0.1161771
	speed: 0.1218s/iter; left time: 273.4833s
Epoch: 3 cost time: 40.7349169254303
Epoch: 3, Steps: 318 | Train Loss: 0.1186327 Vali Loss: 0.1296543 Test Loss: 0.1041145
Validation loss decreased (0.133840 --> 0.129654).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1060367
	speed: 0.1346s/iter; left time: 286.2240s
	iters: 200, epoch: 4 | loss: 0.0937029
	speed: 0.1414s/iter; left time: 286.6628s
	iters: 300, epoch: 4 | loss: 0.1020235
	speed: 0.1360s/iter; left time: 262.1429s
Epoch: 4 cost time: 43.537959575653076
Epoch: 4, Steps: 318 | Train Loss: 0.1137781 Vali Loss: 0.1303616 Test Loss: 0.1014874
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1067071
	speed: 0.1156s/iter; left time: 209.2019s
	iters: 200, epoch: 5 | loss: 0.1251682
	speed: 0.1210s/iter; left time: 206.7921s
	iters: 300, epoch: 5 | loss: 0.1517974
	speed: 0.1504s/iter; left time: 242.0644s
Epoch: 5 cost time: 41.1328022480011
Epoch: 5, Steps: 318 | Train Loss: 0.1121330 Vali Loss: 0.1263255 Test Loss: 0.0990036
Validation loss decreased (0.129654 --> 0.126326).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1034847
	speed: 0.1450s/iter; left time: 216.2521s
	iters: 200, epoch: 6 | loss: 0.1190544
	speed: 0.1502s/iter; left time: 208.9343s
	iters: 300, epoch: 6 | loss: 0.1236167
	speed: 0.1348s/iter; left time: 173.9720s
Epoch: 6 cost time: 45.724456548690796
Epoch: 6, Steps: 318 | Train Loss: 0.1112195 Vali Loss: 0.1259233 Test Loss: 0.0988575
Validation loss decreased (0.126326 --> 0.125923).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0957098
	speed: 0.1500s/iter; left time: 175.9389s
	iters: 200, epoch: 7 | loss: 0.0801224
	speed: 0.1259s/iter; left time: 135.0516s
	iters: 300, epoch: 7 | loss: 0.1136830
	speed: 0.1460s/iter; left time: 142.0804s
Epoch: 7 cost time: 43.740466594696045
Epoch: 7, Steps: 318 | Train Loss: 0.1108058 Vali Loss: 0.1253727 Test Loss: 0.0987733
Validation loss decreased (0.125923 --> 0.125373).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0904532
	speed: 0.1567s/iter; left time: 133.9921s
	iters: 200, epoch: 8 | loss: 0.1161620
	speed: 0.1572s/iter; left time: 118.7020s
	iters: 300, epoch: 8 | loss: 0.1080476
	speed: 0.1495s/iter; left time: 97.9429s
Epoch: 8 cost time: 48.64660954475403
Epoch: 8, Steps: 318 | Train Loss: 0.1104813 Vali Loss: 0.1257442 Test Loss: 0.0984689
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1378264
	speed: 0.1672s/iter; left time: 89.7945s
	iters: 200, epoch: 9 | loss: 0.0957053
	speed: 0.1577s/iter; left time: 68.9205s
	iters: 300, epoch: 9 | loss: 0.0943190
	speed: 0.1429s/iter; left time: 48.1625s
Epoch: 9 cost time: 49.46043014526367
Epoch: 9, Steps: 318 | Train Loss: 0.1105107 Vali Loss: 0.1262003 Test Loss: 0.0984568
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1212990
	speed: 0.1645s/iter; left time: 36.0170s
	iters: 200, epoch: 10 | loss: 0.1217876
	speed: 0.1451s/iter; left time: 17.2641s
	iters: 300, epoch: 10 | loss: 0.0957062
	speed: 0.1402s/iter; left time: 2.6646s
Epoch: 10 cost time: 47.580843448638916
Epoch: 10, Steps: 318 | Train Loss: 0.1102773 Vali Loss: 0.1260286 Test Loss: 0.0983945
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1599680
	speed: 0.1676s/iter; left time: 516.2883s
	iters: 200, epoch: 1 | loss: 0.1440165
	speed: 0.1479s/iter; left time: 440.7553s
	iters: 300, epoch: 1 | loss: 0.1088220
	speed: 0.1443s/iter; left time: 415.7065s
Epoch: 1 cost time: 47.28303909301758
Epoch: 1, Steps: 318 | Train Loss: 0.2544434 Vali Loss: 0.1448251 Test Loss: 0.1190276
Validation loss decreased (inf --> 0.144825).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1481287
	speed: 0.1427s/iter; left time: 394.3152s
	iters: 200, epoch: 2 | loss: 0.1577473
	speed: 0.1496s/iter; left time: 398.2897s
	iters: 300, epoch: 2 | loss: 0.1330876
	speed: 0.1550s/iter; left time: 397.1398s
Epoch: 2 cost time: 47.340625524520874
Epoch: 2, Steps: 318 | Train Loss: 0.1303238 Vali Loss: 0.1367940 Test Loss: 0.1202422
Validation loss decreased (0.144825 --> 0.136794).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1515972
	speed: 0.1555s/iter; left time: 380.1828s
	iters: 200, epoch: 3 | loss: 0.1317524
	speed: 0.1321s/iter; left time: 309.7502s
	iters: 300, epoch: 3 | loss: 0.1295490
	speed: 0.1379s/iter; left time: 309.5725s
Epoch: 3 cost time: 45.242939949035645
Epoch: 3, Steps: 318 | Train Loss: 0.1227376 Vali Loss: 0.1348720 Test Loss: 0.1243608
Validation loss decreased (0.136794 --> 0.134872).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1287831
	speed: 0.1893s/iter; left time: 402.6130s
	iters: 200, epoch: 4 | loss: 0.1188221
	speed: 0.1515s/iter; left time: 307.0108s
	iters: 300, epoch: 4 | loss: 0.1221820
	speed: 0.1615s/iter; left time: 311.1352s
Epoch: 4 cost time: 53.681790828704834
Epoch: 4, Steps: 318 | Train Loss: 0.1189163 Vali Loss: 0.1314293 Test Loss: 0.1241946
Validation loss decreased (0.134872 --> 0.131429).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0975218
	speed: 0.1615s/iter; left time: 292.1363s
	iters: 200, epoch: 5 | loss: 0.1445165
	speed: 0.1774s/iter; left time: 303.2513s
	iters: 300, epoch: 5 | loss: 0.1480466
	speed: 0.1874s/iter; left time: 301.4528s
Epoch: 5 cost time: 55.51204538345337
Epoch: 5, Steps: 318 | Train Loss: 0.1173806 Vali Loss: 0.1310282 Test Loss: 0.1245735
Validation loss decreased (0.131429 --> 0.131028).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1200619
	speed: 0.1741s/iter; left time: 259.5987s
	iters: 200, epoch: 6 | loss: 0.0944829
	speed: 0.1643s/iter; left time: 228.5075s
	iters: 300, epoch: 6 | loss: 0.1238597
	speed: 0.1624s/iter; left time: 209.7099s
Epoch: 6 cost time: 53.330658197402954
Epoch: 6, Steps: 318 | Train Loss: 0.1162598 Vali Loss: 0.1294155 Test Loss: 0.1246173
Validation loss decreased (0.131028 --> 0.129416).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0959520
	speed: 0.1634s/iter; left time: 191.7182s
	iters: 200, epoch: 7 | loss: 0.1135959
	speed: 0.1525s/iter; left time: 163.6126s
	iters: 300, epoch: 7 | loss: 0.0925948
	speed: 0.1728s/iter; left time: 168.1754s
Epoch: 7 cost time: 52.74682426452637
Epoch: 7, Steps: 318 | Train Loss: 0.1159783 Vali Loss: 0.1292799 Test Loss: 0.1241836
Validation loss decreased (0.129416 --> 0.129280).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1331648
	speed: 0.1484s/iter; left time: 126.9072s
	iters: 200, epoch: 8 | loss: 0.1154466
	speed: 0.1472s/iter; left time: 111.1340s
	iters: 300, epoch: 8 | loss: 0.1144220
	speed: 0.1429s/iter; left time: 93.6320s
Epoch: 8 cost time: 46.52054047584534
Epoch: 8, Steps: 318 | Train Loss: 0.1155652 Vali Loss: 0.1292200 Test Loss: 0.1243233
Validation loss decreased (0.129280 --> 0.129220).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1044069
	speed: 0.1562s/iter; left time: 83.8825s
	iters: 200, epoch: 9 | loss: 0.1158420
	speed: 0.1433s/iter; left time: 62.6121s
	iters: 300, epoch: 9 | loss: 0.1025766
	speed: 0.1379s/iter; left time: 46.4663s
Epoch: 9 cost time: 45.64714217185974
Epoch: 9, Steps: 318 | Train Loss: 0.1155305 Vali Loss: 0.1290460 Test Loss: 0.1241986
Validation loss decreased (0.129220 --> 0.129046).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0957934
	speed: 0.1526s/iter; left time: 33.4135s
	iters: 200, epoch: 10 | loss: 0.1266681
	speed: 0.1418s/iter; left time: 16.8771s
	iters: 300, epoch: 10 | loss: 0.1406640
	speed: 0.1475s/iter; left time: 2.8026s
Epoch: 10 cost time: 46.95528292655945
Epoch: 10, Steps: 318 | Train Loss: 0.1154400 Vali Loss: 0.1291533 Test Loss: 0.1242328
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2687047
	speed: 0.1216s/iter; left time: 373.4493s
	iters: 200, epoch: 1 | loss: 0.1589080
	speed: 0.0987s/iter; left time: 293.2371s
	iters: 300, epoch: 1 | loss: 0.1644300
	speed: 0.1018s/iter; left time: 292.3607s
Epoch: 1 cost time: 34.352919816970825
Epoch: 1, Steps: 317 | Train Loss: 0.2973852 Vali Loss: 0.1583795 Test Loss: 0.1322014
Validation loss decreased (inf --> 0.158379).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1379375
	speed: 0.1155s/iter; left time: 318.1211s
	iters: 200, epoch: 2 | loss: 0.1412176
	speed: 0.1110s/iter; left time: 294.5967s
	iters: 300, epoch: 2 | loss: 0.1321414
	speed: 0.1037s/iter; left time: 264.7273s
Epoch: 2 cost time: 34.755308389663696
Epoch: 2, Steps: 317 | Train Loss: 0.1466801 Vali Loss: 0.1524697 Test Loss: 0.1232096
Validation loss decreased (0.158379 --> 0.152470).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1795408
	speed: 0.1097s/iter; left time: 267.3726s
	iters: 200, epoch: 3 | loss: 0.1400698
	speed: 0.1056s/iter; left time: 246.8176s
	iters: 300, epoch: 3 | loss: 0.1370121
	speed: 0.1054s/iter; left time: 235.7590s
Epoch: 3 cost time: 33.94765377044678
Epoch: 3, Steps: 317 | Train Loss: 0.1393273 Vali Loss: 0.1493321 Test Loss: 0.1191252
Validation loss decreased (0.152470 --> 0.149332).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1575104
	speed: 0.1182s/iter; left time: 250.5368s
	iters: 200, epoch: 4 | loss: 0.1627229
	speed: 0.0985s/iter; left time: 198.9536s
	iters: 300, epoch: 4 | loss: 0.1234943
	speed: 0.1056s/iter; left time: 202.7235s
Epoch: 4 cost time: 34.16588568687439
Epoch: 4, Steps: 317 | Train Loss: 0.1368071 Vali Loss: 0.1487624 Test Loss: 0.1192005
Validation loss decreased (0.149332 --> 0.148762).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1452613
	speed: 0.1093s/iter; left time: 197.0875s
	iters: 200, epoch: 5 | loss: 0.1578083
	speed: 0.0978s/iter; left time: 166.6165s
	iters: 300, epoch: 5 | loss: 0.1432920
	speed: 0.1016s/iter; left time: 162.8573s
Epoch: 5 cost time: 32.774563789367676
Epoch: 5, Steps: 317 | Train Loss: 0.1353481 Vali Loss: 0.1480359 Test Loss: 0.1183777
Validation loss decreased (0.148762 --> 0.148036).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1534694
	speed: 0.1150s/iter; left time: 170.8452s
	iters: 200, epoch: 6 | loss: 0.1315576
	speed: 0.1022s/iter; left time: 141.6540s
	iters: 300, epoch: 6 | loss: 0.1585642
	speed: 0.0992s/iter; left time: 127.5182s
Epoch: 6 cost time: 33.55089282989502
Epoch: 6, Steps: 317 | Train Loss: 0.1350549 Vali Loss: 0.1473087 Test Loss: 0.1174220
Validation loss decreased (0.148036 --> 0.147309).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1269601
	speed: 0.1035s/iter; left time: 121.0029s
	iters: 200, epoch: 7 | loss: 0.1388304
	speed: 0.1017s/iter; left time: 108.7171s
	iters: 300, epoch: 7 | loss: 0.1141947
	speed: 0.1090s/iter; left time: 105.6053s
Epoch: 7 cost time: 33.39911103248596
Epoch: 7, Steps: 317 | Train Loss: 0.1348104 Vali Loss: 0.1477878 Test Loss: 0.1169837
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1390996
	speed: 0.1165s/iter; left time: 99.2309s
	iters: 200, epoch: 8 | loss: 0.1418410
	speed: 0.1076s/iter; left time: 80.9078s
	iters: 300, epoch: 8 | loss: 0.1474173
	speed: 0.1038s/iter; left time: 67.6882s
Epoch: 8 cost time: 34.76738977432251
Epoch: 8, Steps: 317 | Train Loss: 0.1344746 Vali Loss: 0.1471422 Test Loss: 0.1170312
Validation loss decreased (0.147309 --> 0.147142).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1208746
	speed: 0.1144s/iter; left time: 61.2149s
	iters: 200, epoch: 9 | loss: 0.1696168
	speed: 0.1036s/iter; left time: 45.0829s
	iters: 300, epoch: 9 | loss: 0.1235650
	speed: 0.0999s/iter; left time: 33.4750s
Epoch: 9 cost time: 33.78644514083862
Epoch: 9, Steps: 317 | Train Loss: 0.1344295 Vali Loss: 0.1472094 Test Loss: 0.1170425
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1354023
	speed: 0.1072s/iter; left time: 23.3670s
	iters: 200, epoch: 10 | loss: 0.1459703
	speed: 0.0935s/iter; left time: 11.0302s
	iters: 300, epoch: 10 | loss: 0.1318328
	speed: 0.0932s/iter; left time: 1.6776s
Epoch: 10 cost time: 31.059855937957764
Epoch: 10, Steps: 317 | Train Loss: 0.1343176 Vali Loss: 0.1465663 Test Loss: 0.1170290
Validation loss decreased (0.147142 --> 0.146566).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2264286
	speed: 0.1495s/iter; left time: 460.6746s
	iters: 200, epoch: 1 | loss: 0.1531208
	speed: 0.1256s/iter; left time: 374.3954s
	iters: 300, epoch: 1 | loss: 0.1323944
	speed: 0.1188s/iter; left time: 342.3784s
Epoch: 1 cost time: 41.51722860336304
Epoch: 1, Steps: 318 | Train Loss: 0.2205782 Vali Loss: 0.1443078 Test Loss: 0.1092710
Validation loss decreased (inf --> 0.144308).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1384629
	speed: 0.1165s/iter; left time: 321.8696s
	iters: 200, epoch: 2 | loss: 0.1049988
	speed: 0.1128s/iter; left time: 300.4035s
	iters: 300, epoch: 2 | loss: 0.1046109
	speed: 0.1225s/iter; left time: 313.9766s
Epoch: 2 cost time: 37.761263608932495
Epoch: 2, Steps: 318 | Train Loss: 0.1202801 Vali Loss: 0.1229186 Test Loss: 0.0927544
Validation loss decreased (0.144308 --> 0.122919).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0789963
	speed: 0.1527s/iter; left time: 373.4589s
	iters: 200, epoch: 3 | loss: 0.1203535
	speed: 0.1563s/iter; left time: 366.4860s
	iters: 300, epoch: 3 | loss: 0.0936349
	speed: 0.1224s/iter; left time: 274.8618s
Epoch: 3 cost time: 45.36821985244751
Epoch: 3, Steps: 318 | Train Loss: 0.1045796 Vali Loss: 0.1172229 Test Loss: 0.0843677
Validation loss decreased (0.122919 --> 0.117223).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0895234
	speed: 0.1266s/iter; left time: 269.3136s
	iters: 200, epoch: 4 | loss: 0.0980110
	speed: 0.1099s/iter; left time: 222.8325s
	iters: 300, epoch: 4 | loss: 0.1260787
	speed: 0.1100s/iter; left time: 212.0000s
Epoch: 4 cost time: 36.65806031227112
Epoch: 4, Steps: 318 | Train Loss: 0.0998403 Vali Loss: 0.1100192 Test Loss: 0.0807681
Validation loss decreased (0.117223 --> 0.110019).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0999771
	speed: 0.1316s/iter; left time: 238.0706s
	iters: 200, epoch: 5 | loss: 0.0962441
	speed: 0.1248s/iter; left time: 213.3466s
	iters: 300, epoch: 5 | loss: 0.0953518
	speed: 0.1136s/iter; left time: 182.7555s
Epoch: 5 cost time: 39.56303024291992
Epoch: 5, Steps: 318 | Train Loss: 0.0970896 Vali Loss: 0.1098229 Test Loss: 0.0797502
Validation loss decreased (0.110019 --> 0.109823).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0956559
	speed: 0.1278s/iter; left time: 190.5654s
	iters: 200, epoch: 6 | loss: 0.1099975
	speed: 0.1173s/iter; left time: 163.1408s
	iters: 300, epoch: 6 | loss: 0.1079547
	speed: 0.1026s/iter; left time: 132.5135s
Epoch: 6 cost time: 36.89247226715088
Epoch: 6, Steps: 318 | Train Loss: 0.0958222 Vali Loss: 0.1082204 Test Loss: 0.0793145
Validation loss decreased (0.109823 --> 0.108220).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0826674
	speed: 0.1212s/iter; left time: 142.1683s
	iters: 200, epoch: 7 | loss: 0.0792158
	speed: 0.1109s/iter; left time: 119.0307s
	iters: 300, epoch: 7 | loss: 0.1128084
	speed: 0.1161s/iter; left time: 112.9342s
Epoch: 7 cost time: 36.93095827102661
Epoch: 7, Steps: 318 | Train Loss: 0.0953598 Vali Loss: 0.1073444 Test Loss: 0.0791500
Validation loss decreased (0.108220 --> 0.107344).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1033674
	speed: 0.1249s/iter; left time: 106.7608s
	iters: 200, epoch: 8 | loss: 0.0682272
	speed: 0.1080s/iter; left time: 81.5424s
	iters: 300, epoch: 8 | loss: 0.0734411
	speed: 0.1094s/iter; left time: 71.6800s
Epoch: 8 cost time: 36.45699644088745
Epoch: 8, Steps: 318 | Train Loss: 0.0955373 Vali Loss: 0.1082386 Test Loss: 0.0792445
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0732076
	speed: 0.1272s/iter; left time: 68.2946s
	iters: 200, epoch: 9 | loss: 0.1017447
	speed: 0.1169s/iter; left time: 51.0827s
	iters: 300, epoch: 9 | loss: 0.0872800
	speed: 0.1191s/iter; left time: 40.1359s
Epoch: 9 cost time: 38.67012667655945
Epoch: 9, Steps: 318 | Train Loss: 0.0953549 Vali Loss: 0.1089646 Test Loss: 0.0791788
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0843648
	speed: 0.1252s/iter; left time: 27.4297s
	iters: 200, epoch: 10 | loss: 0.1141489
	speed: 0.1121s/iter; left time: 13.3402s
	iters: 300, epoch: 10 | loss: 0.1052348
	speed: 0.1199s/iter; left time: 2.2779s
Epoch: 10 cost time: 37.87661170959473
Epoch: 10, Steps: 318 | Train Loss: 0.0947392 Vali Loss: 0.1077294 Test Loss: 0.0791778
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1063511
	speed: 0.1535s/iter; left time: 473.0197s
	iters: 200, epoch: 1 | loss: 0.1338561
	speed: 0.1376s/iter; left time: 410.2443s
	iters: 300, epoch: 1 | loss: 0.1312719
	speed: 0.1378s/iter; left time: 396.9074s
Epoch: 1 cost time: 45.48761487007141
Epoch: 1, Steps: 318 | Train Loss: 0.2190747 Vali Loss: 0.1421810 Test Loss: 0.1038119
Validation loss decreased (inf --> 0.142181).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1300991
	speed: 0.1541s/iter; left time: 425.7124s
	iters: 200, epoch: 2 | loss: 0.1304458
	speed: 0.1421s/iter; left time: 378.4099s
	iters: 300, epoch: 2 | loss: 0.0885512
	speed: 0.1535s/iter; left time: 393.5259s
Epoch: 2 cost time: 47.431639194488525
Epoch: 2, Steps: 318 | Train Loss: 0.1156979 Vali Loss: 0.1233117 Test Loss: 0.0902082
Validation loss decreased (0.142181 --> 0.123312).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1431832
	speed: 0.1520s/iter; left time: 371.6546s
	iters: 200, epoch: 3 | loss: 0.0914898
	speed: 0.1347s/iter; left time: 315.8941s
	iters: 300, epoch: 3 | loss: 0.0830697
	speed: 0.1393s/iter; left time: 312.6739s
Epoch: 3 cost time: 45.153409481048584
Epoch: 3, Steps: 318 | Train Loss: 0.1058769 Vali Loss: 0.1168654 Test Loss: 0.0874764
Validation loss decreased (0.123312 --> 0.116865).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1106099
	speed: 0.1459s/iter; left time: 310.2962s
	iters: 200, epoch: 4 | loss: 0.0990804
	speed: 0.1425s/iter; left time: 288.7575s
	iters: 300, epoch: 4 | loss: 0.1277038
	speed: 0.1353s/iter; left time: 260.7311s
Epoch: 4 cost time: 45.10149383544922
Epoch: 4, Steps: 318 | Train Loss: 0.1016806 Vali Loss: 0.1151992 Test Loss: 0.0861973
Validation loss decreased (0.116865 --> 0.115199).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0856251
	speed: 0.1374s/iter; left time: 248.6380s
	iters: 200, epoch: 5 | loss: 0.1101354
	speed: 0.1440s/iter; left time: 246.0749s
	iters: 300, epoch: 5 | loss: 0.1141923
	speed: 0.1601s/iter; left time: 257.6626s
Epoch: 5 cost time: 46.89882206916809
Epoch: 5, Steps: 318 | Train Loss: 0.0999943 Vali Loss: 0.1144341 Test Loss: 0.0853356
Validation loss decreased (0.115199 --> 0.114434).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1135742
	speed: 0.1466s/iter; left time: 218.5893s
	iters: 200, epoch: 6 | loss: 0.0941223
	speed: 0.1260s/iter; left time: 175.2879s
	iters: 300, epoch: 6 | loss: 0.1246041
	speed: 0.1404s/iter; left time: 181.3164s
Epoch: 6 cost time: 43.66676712036133
Epoch: 6, Steps: 318 | Train Loss: 0.0990218 Vali Loss: 0.1146785 Test Loss: 0.0857315
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0926146
	speed: 0.1382s/iter; left time: 162.0580s
	iters: 200, epoch: 7 | loss: 0.0820544
	speed: 0.1329s/iter; left time: 142.5997s
	iters: 300, epoch: 7 | loss: 0.1102260
	speed: 0.1408s/iter; left time: 136.9905s
Epoch: 7 cost time: 44.020628929138184
Epoch: 7, Steps: 318 | Train Loss: 0.0988452 Vali Loss: 0.1141478 Test Loss: 0.0852318
Validation loss decreased (0.114434 --> 0.114148).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0690513
	speed: 0.1459s/iter; left time: 124.7616s
	iters: 200, epoch: 8 | loss: 0.0823417
	speed: 0.1280s/iter; left time: 96.6077s
	iters: 300, epoch: 8 | loss: 0.1223105
	speed: 0.1356s/iter; left time: 88.8395s
Epoch: 8 cost time: 43.663268089294434
Epoch: 8, Steps: 318 | Train Loss: 0.0985768 Vali Loss: 0.1130749 Test Loss: 0.0852386
Validation loss decreased (0.114148 --> 0.113075).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1180412
	speed: 0.1478s/iter; left time: 79.3652s
	iters: 200, epoch: 9 | loss: 0.0810186
	speed: 0.1289s/iter; left time: 56.3485s
	iters: 300, epoch: 9 | loss: 0.1121410
	speed: 0.1300s/iter; left time: 43.8125s
Epoch: 9 cost time: 43.489365100860596
Epoch: 9, Steps: 318 | Train Loss: 0.0984991 Vali Loss: 0.1141297 Test Loss: 0.0851304
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0986205
	speed: 0.1417s/iter; left time: 31.0389s
	iters: 200, epoch: 10 | loss: 0.0928116
	speed: 0.1304s/iter; left time: 15.5129s
	iters: 300, epoch: 10 | loss: 0.1044993
	speed: 0.1264s/iter; left time: 2.4023s
Epoch: 10 cost time: 42.663456201553345
Epoch: 10, Steps: 318 | Train Loss: 0.0984333 Vali Loss: 0.1133954 Test Loss: 0.0851247
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1440065
	speed: 0.1433s/iter; left time: 441.5508s
	iters: 200, epoch: 1 | loss: 0.1202447
	speed: 0.1608s/iter; left time: 479.4327s
	iters: 300, epoch: 1 | loss: 0.1712406
	speed: 0.1729s/iter; left time: 498.0477s
Epoch: 1 cost time: 50.428179025650024
Epoch: 1, Steps: 318 | Train Loss: 0.2234129 Vali Loss: 0.1428504 Test Loss: 0.1071998
Validation loss decreased (inf --> 0.142850).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1238996
	speed: 0.1376s/iter; left time: 380.1830s
	iters: 200, epoch: 2 | loss: 0.1026307
	speed: 0.1213s/iter; left time: 323.0334s
	iters: 300, epoch: 2 | loss: 0.1271786
	speed: 0.1125s/iter; left time: 288.2918s
Epoch: 2 cost time: 39.28654980659485
Epoch: 2, Steps: 318 | Train Loss: 0.1235611 Vali Loss: 0.1330864 Test Loss: 0.1010248
Validation loss decreased (0.142850 --> 0.133086).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0880392
	speed: 0.1258s/iter; left time: 307.5613s
	iters: 200, epoch: 3 | loss: 0.0860022
	speed: 0.1170s/iter; left time: 274.3307s
	iters: 300, epoch: 3 | loss: 0.1069580
	speed: 0.1142s/iter; left time: 256.4062s
Epoch: 3 cost time: 37.94968557357788
Epoch: 3, Steps: 318 | Train Loss: 0.1159865 Vali Loss: 0.1312283 Test Loss: 0.0975904
Validation loss decreased (0.133086 --> 0.131228).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1084202
	speed: 0.1249s/iter; left time: 265.6913s
	iters: 200, epoch: 4 | loss: 0.1006109
	speed: 0.1169s/iter; left time: 237.0073s
	iters: 300, epoch: 4 | loss: 0.1051491
	speed: 0.1162s/iter; left time: 223.8760s
Epoch: 4 cost time: 37.8903546333313
Epoch: 4, Steps: 318 | Train Loss: 0.1116838 Vali Loss: 0.1320073 Test Loss: 0.0981980
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1094284
	speed: 0.1197s/iter; left time: 216.5281s
	iters: 200, epoch: 5 | loss: 0.1226788
	speed: 0.1148s/iter; left time: 196.2575s
	iters: 300, epoch: 5 | loss: 0.1548680
	speed: 0.1189s/iter; left time: 191.3685s
Epoch: 5 cost time: 37.83490753173828
Epoch: 5, Steps: 318 | Train Loss: 0.1098550 Vali Loss: 0.1287785 Test Loss: 0.0965244
Validation loss decreased (0.131228 --> 0.128778).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1064533
	speed: 0.1277s/iter; left time: 190.4279s
	iters: 200, epoch: 6 | loss: 0.1088582
	speed: 0.1181s/iter; left time: 164.2121s
	iters: 300, epoch: 6 | loss: 0.1228731
	speed: 0.1153s/iter; left time: 148.8614s
Epoch: 6 cost time: 38.25499248504639
Epoch: 6, Steps: 318 | Train Loss: 0.1092875 Vali Loss: 0.1276375 Test Loss: 0.0960859
Validation loss decreased (0.128778 --> 0.127637).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0892605
	speed: 0.1231s/iter; left time: 144.3993s
	iters: 200, epoch: 7 | loss: 0.0796069
	speed: 0.1164s/iter; left time: 124.8536s
	iters: 300, epoch: 7 | loss: 0.1160352
	speed: 0.1218s/iter; left time: 118.5339s
Epoch: 7 cost time: 38.57168126106262
Epoch: 7, Steps: 318 | Train Loss: 0.1087032 Vali Loss: 0.1276194 Test Loss: 0.0965492
Validation loss decreased (0.127637 --> 0.127619).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0831847
	speed: 0.1248s/iter; left time: 106.7201s
	iters: 200, epoch: 8 | loss: 0.1093222
	speed: 0.1226s/iter; left time: 92.5663s
	iters: 300, epoch: 8 | loss: 0.1102880
	speed: 0.1206s/iter; left time: 78.9867s
Epoch: 8 cost time: 39.00196433067322
Epoch: 8, Steps: 318 | Train Loss: 0.1084448 Vali Loss: 0.1276851 Test Loss: 0.0962122
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1370606
	speed: 0.1224s/iter; left time: 65.7106s
	iters: 200, epoch: 9 | loss: 0.0968189
	speed: 0.1156s/iter; left time: 50.5354s
	iters: 300, epoch: 9 | loss: 0.0963580
	speed: 0.1220s/iter; left time: 41.1054s
Epoch: 9 cost time: 38.50657320022583
Epoch: 9, Steps: 318 | Train Loss: 0.1084642 Vali Loss: 0.1282816 Test Loss: 0.0961623
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1239004
	speed: 0.1292s/iter; left time: 28.3008s
	iters: 200, epoch: 10 | loss: 0.1107272
	speed: 0.1161s/iter; left time: 13.8119s
	iters: 300, epoch: 10 | loss: 0.0943509
	speed: 0.1278s/iter; left time: 2.4291s
Epoch: 10 cost time: 39.741687059402466
Epoch: 10, Steps: 318 | Train Loss: 0.1081946 Vali Loss: 0.1280431 Test Loss: 0.0961193
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1581064
	speed: 0.1365s/iter; left time: 420.5843s
	iters: 200, epoch: 1 | loss: 0.1427229
	speed: 0.1121s/iter; left time: 334.2199s
	iters: 300, epoch: 1 | loss: 0.1102852
	speed: 0.1155s/iter; left time: 332.8062s
Epoch: 1 cost time: 38.47772932052612
Epoch: 1, Steps: 318 | Train Loss: 0.2369703 Vali Loss: 0.1505922 Test Loss: 0.1156093
Validation loss decreased (inf --> 0.150592).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1408512
	speed: 0.1271s/iter; left time: 351.0772s
	iters: 200, epoch: 2 | loss: 0.1541272
	speed: 0.1160s/iter; left time: 308.8764s
	iters: 300, epoch: 2 | loss: 0.1311717
	speed: 0.1162s/iter; left time: 297.8016s
Epoch: 2 cost time: 37.653921365737915
Epoch: 2, Steps: 318 | Train Loss: 0.1289819 Vali Loss: 0.1399082 Test Loss: 0.1048569
Validation loss decreased (0.150592 --> 0.139908).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1556707
	speed: 0.1236s/iter; left time: 302.2859s
	iters: 200, epoch: 3 | loss: 0.1277527
	speed: 0.1050s/iter; left time: 246.1833s
	iters: 300, epoch: 3 | loss: 0.1255925
	speed: 0.1109s/iter; left time: 248.9898s
Epoch: 3 cost time: 36.05768918991089
Epoch: 3, Steps: 318 | Train Loss: 0.1223218 Vali Loss: 0.1392533 Test Loss: 0.1028804
Validation loss decreased (0.139908 --> 0.139253).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1277442
	speed: 0.1168s/iter; left time: 248.5197s
	iters: 200, epoch: 4 | loss: 0.1197339
	speed: 0.1128s/iter; left time: 228.6992s
	iters: 300, epoch: 4 | loss: 0.1200256
	speed: 0.1135s/iter; left time: 218.7769s
Epoch: 4 cost time: 36.25984597206116
Epoch: 4, Steps: 318 | Train Loss: 0.1190013 Vali Loss: 0.1357999 Test Loss: 0.1015453
Validation loss decreased (0.139253 --> 0.135800).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0934377
	speed: 0.1187s/iter; left time: 214.7541s
	iters: 200, epoch: 5 | loss: 0.1435954
	speed: 0.1183s/iter; left time: 202.2448s
	iters: 300, epoch: 5 | loss: 0.1510040
	speed: 0.1107s/iter; left time: 178.1302s
Epoch: 5 cost time: 36.90191292762756
Epoch: 5, Steps: 318 | Train Loss: 0.1171350 Vali Loss: 0.1359111 Test Loss: 0.1026071
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1233243
	speed: 0.1144s/iter; left time: 170.5312s
	iters: 200, epoch: 6 | loss: 0.0944990
	speed: 0.1173s/iter; left time: 163.1183s
	iters: 300, epoch: 6 | loss: 0.1268565
	speed: 0.1298s/iter; left time: 167.5634s
Epoch: 6 cost time: 38.36476135253906
Epoch: 6, Steps: 318 | Train Loss: 0.1162747 Vali Loss: 0.1342792 Test Loss: 0.1007253
Validation loss decreased (0.135800 --> 0.134279).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0941469
	speed: 0.1161s/iter; left time: 136.1982s
	iters: 200, epoch: 7 | loss: 0.1106537
	speed: 0.1088s/iter; left time: 116.7622s
	iters: 300, epoch: 7 | loss: 0.0904892
	speed: 0.1237s/iter; left time: 120.3657s
Epoch: 7 cost time: 37.212292194366455
Epoch: 7, Steps: 318 | Train Loss: 0.1158808 Vali Loss: 0.1343089 Test Loss: 0.1002388
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1389770
	speed: 0.1243s/iter; left time: 106.2588s
	iters: 200, epoch: 8 | loss: 0.1190111
	speed: 0.1145s/iter; left time: 86.4164s
	iters: 300, epoch: 8 | loss: 0.1156580
	speed: 0.1270s/iter; left time: 83.1855s
Epoch: 8 cost time: 39.227620124816895
Epoch: 8, Steps: 318 | Train Loss: 0.1154755 Vali Loss: 0.1340413 Test Loss: 0.1004416
Validation loss decreased (0.134279 --> 0.134041).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1082764
	speed: 0.1174s/iter; left time: 63.0530s
	iters: 200, epoch: 9 | loss: 0.1189412
	speed: 0.1032s/iter; left time: 45.0884s
	iters: 300, epoch: 9 | loss: 0.0977963
	speed: 0.1136s/iter; left time: 38.2795s
Epoch: 9 cost time: 35.57105612754822
Epoch: 9, Steps: 318 | Train Loss: 0.1154278 Vali Loss: 0.1341987 Test Loss: 0.1001904
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0927280
	speed: 0.1263s/iter; left time: 27.6675s
	iters: 200, epoch: 10 | loss: 0.1233100
	speed: 0.1240s/iter; left time: 14.7503s
	iters: 300, epoch: 10 | loss: 0.1319475
	speed: 0.1303s/iter; left time: 2.4758s
Epoch: 10 cost time: 40.35871410369873
Epoch: 10, Steps: 318 | Train Loss: 0.1153684 Vali Loss: 0.1340915 Test Loss: 0.1001967
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2001804
	speed: 0.1958s/iter; left time: 601.4025s
	iters: 200, epoch: 1 | loss: 0.1375309
	speed: 0.1724s/iter; left time: 512.2798s
	iters: 300, epoch: 1 | loss: 0.1472243
	speed: 0.1571s/iter; left time: 450.9691s
Epoch: 1 cost time: 55.591875076293945
Epoch: 1, Steps: 317 | Train Loss: 0.2194179 Vali Loss: 0.1470814 Test Loss: 0.1146982
Validation loss decreased (inf --> 0.147081).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1278092
	speed: 0.1840s/iter; left time: 506.7086s
	iters: 200, epoch: 2 | loss: 0.1258306
	speed: 0.1780s/iter; left time: 472.4449s
	iters: 300, epoch: 2 | loss: 0.1393587
	speed: 0.1569s/iter; left time: 400.8076s
Epoch: 2 cost time: 54.19168186187744
Epoch: 2, Steps: 317 | Train Loss: 0.1294657 Vali Loss: 0.1472913 Test Loss: 0.1097962
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1627876
	speed: 0.1325s/iter; left time: 322.8363s
	iters: 200, epoch: 3 | loss: 0.1249641
	speed: 0.1228s/iter; left time: 287.0303s
	iters: 300, epoch: 3 | loss: 0.1203184
	speed: 0.1296s/iter; left time: 289.9162s
Epoch: 3 cost time: 40.833274126052856
Epoch: 3, Steps: 317 | Train Loss: 0.1244433 Vali Loss: 0.1403550 Test Loss: 0.1039199
Validation loss decreased (0.147081 --> 0.140355).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1381217
	speed: 0.1400s/iter; left time: 296.7091s
	iters: 200, epoch: 4 | loss: 0.1401565
	speed: 0.1243s/iter; left time: 251.0321s
	iters: 300, epoch: 4 | loss: 0.1115368
	speed: 0.1167s/iter; left time: 223.9860s
Epoch: 4 cost time: 40.47251272201538
Epoch: 4, Steps: 317 | Train Loss: 0.1210917 Vali Loss: 0.1414598 Test Loss: 0.1076361
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1332116
	speed: 0.1375s/iter; left time: 247.9677s
	iters: 200, epoch: 5 | loss: 0.1285719
	speed: 0.1242s/iter; left time: 211.5039s
	iters: 300, epoch: 5 | loss: 0.1338976
	speed: 0.1191s/iter; left time: 190.9243s
Epoch: 5 cost time: 40.20079326629639
Epoch: 5, Steps: 317 | Train Loss: 0.1193668 Vali Loss: 0.1379776 Test Loss: 0.1033330
Validation loss decreased (0.140355 --> 0.137978).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1252078
	speed: 0.1293s/iter; left time: 192.1331s
	iters: 200, epoch: 6 | loss: 0.1188007
	speed: 0.1416s/iter; left time: 196.2849s
	iters: 300, epoch: 6 | loss: 0.1371888
	speed: 0.1230s/iter; left time: 158.2367s
Epoch: 6 cost time: 41.491668462753296
Epoch: 6, Steps: 317 | Train Loss: 0.1186919 Vali Loss: 0.1370862 Test Loss: 0.1023119
Validation loss decreased (0.137978 --> 0.137086).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1094400
	speed: 0.1243s/iter; left time: 145.3090s
	iters: 200, epoch: 7 | loss: 0.1214849
	speed: 0.1309s/iter; left time: 139.9434s
	iters: 300, epoch: 7 | loss: 0.1093615
	speed: 0.1184s/iter; left time: 114.7527s
Epoch: 7 cost time: 39.35457110404968
Epoch: 7, Steps: 317 | Train Loss: 0.1182012 Vali Loss: 0.1374377 Test Loss: 0.1022633
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1326592
	speed: 0.1211s/iter; left time: 103.2139s
	iters: 200, epoch: 8 | loss: 0.1213345
	speed: 0.1144s/iter; left time: 85.9917s
	iters: 300, epoch: 8 | loss: 0.1234631
	speed: 0.1281s/iter; left time: 83.5520s
Epoch: 8 cost time: 38.83620309829712
Epoch: 8, Steps: 317 | Train Loss: 0.1178578 Vali Loss: 0.1366917 Test Loss: 0.1021498
Validation loss decreased (0.137086 --> 0.136692).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1023700
	speed: 0.1289s/iter; left time: 68.9641s
	iters: 200, epoch: 9 | loss: 0.1533249
	speed: 0.1184s/iter; left time: 51.4967s
	iters: 300, epoch: 9 | loss: 0.1008665
	speed: 0.1173s/iter; left time: 39.2871s
Epoch: 9 cost time: 38.65855121612549
Epoch: 9, Steps: 317 | Train Loss: 0.1179557 Vali Loss: 0.1368543 Test Loss: 0.1022160
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1172643
	speed: 0.1345s/iter; left time: 29.3236s
	iters: 200, epoch: 10 | loss: 0.1321576
	speed: 0.1204s/iter; left time: 14.2116s
	iters: 300, epoch: 10 | loss: 0.1133170
	speed: 0.1211s/iter; left time: 2.1806s
Epoch: 10 cost time: 39.67417073249817
Epoch: 10, Steps: 317 | Train Loss: 0.1176681 Vali Loss: 0.1363534 Test Loss: 0.1022825
Validation loss decreased (0.136692 --> 0.136353).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2262540
	speed: 0.1693s/iter; left time: 521.5191s
	iters: 200, epoch: 1 | loss: 0.1530337
	speed: 0.1582s/iter; left time: 471.6260s
	iters: 300, epoch: 1 | loss: 0.1321594
	speed: 0.1614s/iter; left time: 465.1243s
Epoch: 1 cost time: 52.056644439697266
Epoch: 1, Steps: 318 | Train Loss: 0.2203892 Vali Loss: 0.1444935 Test Loss: 0.1094081
Validation loss decreased (inf --> 0.144493).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1385567
	speed: 0.1713s/iter; left time: 473.4287s
	iters: 200, epoch: 2 | loss: 0.1053670
	speed: 0.1939s/iter; left time: 516.2868s
	iters: 300, epoch: 2 | loss: 0.1049204
	speed: 0.1773s/iter; left time: 454.3850s
Epoch: 2 cost time: 57.59428954124451
Epoch: 2, Steps: 318 | Train Loss: 0.1205709 Vali Loss: 0.1230282 Test Loss: 0.0927744
Validation loss decreased (0.144493 --> 0.123028).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0792384
	speed: 0.2058s/iter; left time: 503.2319s
	iters: 200, epoch: 3 | loss: 0.1206039
	speed: 0.1906s/iter; left time: 446.9416s
	iters: 300, epoch: 3 | loss: 0.0936591
	speed: 0.2199s/iter; left time: 493.6363s
Epoch: 3 cost time: 65.6096875667572
Epoch: 3, Steps: 318 | Train Loss: 0.1047690 Vali Loss: 0.1171958 Test Loss: 0.0843995
Validation loss decreased (0.123028 --> 0.117196).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0896177
	speed: 0.2138s/iter; left time: 454.6529s
	iters: 200, epoch: 4 | loss: 0.0980495
	speed: 0.2206s/iter; left time: 447.1951s
	iters: 300, epoch: 4 | loss: 0.1263845
	speed: 0.2250s/iter; left time: 433.4801s
Epoch: 4 cost time: 70.4868438243866
Epoch: 4, Steps: 318 | Train Loss: 0.0999462 Vali Loss: 0.1099241 Test Loss: 0.0807485
Validation loss decreased (0.117196 --> 0.109924).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0999101
	speed: 0.2058s/iter; left time: 372.2905s
	iters: 200, epoch: 5 | loss: 0.0962834
	speed: 0.2253s/iter; left time: 385.1231s
	iters: 300, epoch: 5 | loss: 0.0954927
	speed: 0.2258s/iter; left time: 363.3899s
Epoch: 5 cost time: 69.70223069190979
Epoch: 5, Steps: 318 | Train Loss: 0.0971652 Vali Loss: 0.1097161 Test Loss: 0.0797249
Validation loss decreased (0.109924 --> 0.109716).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0955947
	speed: 0.2246s/iter; left time: 334.9028s
	iters: 200, epoch: 6 | loss: 0.1100695
	speed: 0.2014s/iter; left time: 280.1196s
	iters: 300, epoch: 6 | loss: 0.1084054
	speed: 0.1929s/iter; left time: 248.9955s
Epoch: 6 cost time: 65.49294018745422
Epoch: 6, Steps: 318 | Train Loss: 0.0958874 Vali Loss: 0.1081123 Test Loss: 0.0792827
Validation loss decreased (0.109716 --> 0.108112).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0827915
	speed: 0.1970s/iter; left time: 231.0339s
	iters: 200, epoch: 7 | loss: 0.0792828
	speed: 0.2032s/iter; left time: 218.0023s
	iters: 300, epoch: 7 | loss: 0.1129750
	speed: 0.1935s/iter; left time: 188.2363s
Epoch: 7 cost time: 62.96526908874512
Epoch: 7, Steps: 318 | Train Loss: 0.0954128 Vali Loss: 0.1072356 Test Loss: 0.0791237
Validation loss decreased (0.108112 --> 0.107236).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1034068
	speed: 0.2046s/iter; left time: 174.9166s
	iters: 200, epoch: 8 | loss: 0.0680800
	speed: 0.1885s/iter; left time: 142.3294s
	iters: 300, epoch: 8 | loss: 0.0734092
	speed: 0.2186s/iter; left time: 143.1589s
Epoch: 8 cost time: 64.719801902771
Epoch: 8, Steps: 318 | Train Loss: 0.0955953 Vali Loss: 0.1081294 Test Loss: 0.0792130
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0734822
	speed: 0.2084s/iter; left time: 111.9100s
	iters: 200, epoch: 9 | loss: 0.1019492
	speed: 0.1964s/iter; left time: 85.8208s
	iters: 300, epoch: 9 | loss: 0.0871851
	speed: 0.2006s/iter; left time: 67.6122s
Epoch: 9 cost time: 64.46007108688354
Epoch: 9, Steps: 318 | Train Loss: 0.0954141 Vali Loss: 0.1088582 Test Loss: 0.0791486
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0840949
	speed: 0.2256s/iter; left time: 49.4066s
	iters: 200, epoch: 10 | loss: 0.1141929
	speed: 0.2050s/iter; left time: 24.4005s
	iters: 300, epoch: 10 | loss: 0.1051825
	speed: 0.1946s/iter; left time: 3.6980s
Epoch: 10 cost time: 66.21114134788513
Epoch: 10, Steps: 318 | Train Loss: 0.0947987 Vali Loss: 0.1076203 Test Loss: 0.0791487
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1058954
	speed: 0.1534s/iter; left time: 472.7349s
	iters: 200, epoch: 1 | loss: 0.1331280
	speed: 0.1347s/iter; left time: 401.4124s
	iters: 300, epoch: 1 | loss: 0.1298762
	speed: 0.1341s/iter; left time: 386.4378s
Epoch: 1 cost time: 44.75773549079895
Epoch: 1, Steps: 318 | Train Loss: 0.2192017 Vali Loss: 0.1413953 Test Loss: 0.1035114
Validation loss decreased (inf --> 0.141395).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1291814
	speed: 0.1433s/iter; left time: 395.9422s
	iters: 200, epoch: 2 | loss: 0.1310811
	speed: 0.1379s/iter; left time: 367.2074s
	iters: 300, epoch: 2 | loss: 0.0880725
	speed: 0.1425s/iter; left time: 365.2260s
Epoch: 2 cost time: 45.18828296661377
Epoch: 2, Steps: 318 | Train Loss: 0.1154338 Vali Loss: 0.1224631 Test Loss: 0.0903262
Validation loss decreased (0.141395 --> 0.122463).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1436103
	speed: 0.1433s/iter; left time: 350.2479s
	iters: 200, epoch: 3 | loss: 0.0916685
	speed: 0.1447s/iter; left time: 339.2142s
	iters: 300, epoch: 3 | loss: 0.0823984
	speed: 0.1396s/iter; left time: 313.3837s
Epoch: 3 cost time: 45.244338512420654
Epoch: 3, Steps: 318 | Train Loss: 0.1057534 Vali Loss: 0.1163401 Test Loss: 0.0873586
Validation loss decreased (0.122463 --> 0.116340).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1102081
	speed: 0.1398s/iter; left time: 297.3077s
	iters: 200, epoch: 4 | loss: 0.0986149
	speed: 0.1460s/iter; left time: 295.9104s
	iters: 300, epoch: 4 | loss: 0.1273152
	speed: 0.1385s/iter; left time: 266.8751s
Epoch: 4 cost time: 44.99704360961914
Epoch: 4, Steps: 318 | Train Loss: 0.1016146 Vali Loss: 0.1145863 Test Loss: 0.0862116
Validation loss decreased (0.116340 --> 0.114586).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0857905
	speed: 0.1455s/iter; left time: 263.2512s
	iters: 200, epoch: 5 | loss: 0.1100295
	speed: 0.1501s/iter; left time: 256.5591s
	iters: 300, epoch: 5 | loss: 0.1144881
	speed: 0.1306s/iter; left time: 210.0672s
Epoch: 5 cost time: 44.94359350204468
Epoch: 5, Steps: 318 | Train Loss: 0.0999795 Vali Loss: 0.1138618 Test Loss: 0.0853014
Validation loss decreased (0.114586 --> 0.113862).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1134430
	speed: 0.1426s/iter; left time: 212.6063s
	iters: 200, epoch: 6 | loss: 0.0932149
	speed: 0.1338s/iter; left time: 186.1524s
	iters: 300, epoch: 6 | loss: 0.1256441
	speed: 0.1437s/iter; left time: 185.4528s
Epoch: 6 cost time: 44.377872705459595
Epoch: 6, Steps: 318 | Train Loss: 0.0989879 Vali Loss: 0.1141183 Test Loss: 0.0857933
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0931170
	speed: 0.1432s/iter; left time: 167.9534s
	iters: 200, epoch: 7 | loss: 0.0816511
	speed: 0.1299s/iter; left time: 139.3528s
	iters: 300, epoch: 7 | loss: 0.1101556
	speed: 0.1236s/iter; left time: 120.2270s
Epoch: 7 cost time: 42.65349316596985
Epoch: 7, Steps: 318 | Train Loss: 0.0988146 Vali Loss: 0.1135318 Test Loss: 0.0852534
Validation loss decreased (0.113862 --> 0.113532).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0687668
	speed: 0.1518s/iter; left time: 129.7735s
	iters: 200, epoch: 8 | loss: 0.0823732
	speed: 0.1305s/iter; left time: 98.5498s
	iters: 300, epoch: 8 | loss: 0.1229967
	speed: 0.1340s/iter; left time: 87.8025s
Epoch: 8 cost time: 43.787222385406494
Epoch: 8, Steps: 318 | Train Loss: 0.0985584 Vali Loss: 0.1124942 Test Loss: 0.0852436
Validation loss decreased (0.113532 --> 0.112494).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1176085
	speed: 0.1441s/iter; left time: 77.3938s
	iters: 200, epoch: 9 | loss: 0.0814756
	speed: 0.1359s/iter; left time: 59.3755s
	iters: 300, epoch: 9 | loss: 0.1123874
	speed: 0.1498s/iter; left time: 50.4861s
Epoch: 9 cost time: 45.53799629211426
Epoch: 9, Steps: 318 | Train Loss: 0.0984886 Vali Loss: 0.1135243 Test Loss: 0.0851372
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0980762
	speed: 0.1399s/iter; left time: 30.6372s
	iters: 200, epoch: 10 | loss: 0.0928346
	speed: 0.1238s/iter; left time: 14.7328s
	iters: 300, epoch: 10 | loss: 0.1050022
	speed: 0.1363s/iter; left time: 2.5893s
Epoch: 10 cost time: 42.6338312625885
Epoch: 10, Steps: 318 | Train Loss: 0.0984196 Vali Loss: 0.1128208 Test Loss: 0.0851287
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1625136
	speed: 0.1596s/iter; left time: 491.6713s
	iters: 200, epoch: 1 | loss: 0.1211386
	speed: 0.1274s/iter; left time: 379.6426s
	iters: 300, epoch: 1 | loss: 0.1680416
	speed: 0.1417s/iter; left time: 408.1362s
Epoch: 1 cost time: 45.28600358963013
Epoch: 1, Steps: 318 | Train Loss: 0.2355206 Vali Loss: 0.1459691 Test Loss: 0.1084278
Validation loss decreased (inf --> 0.145969).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1244507
	speed: 0.1466s/iter; left time: 405.1745s
	iters: 200, epoch: 2 | loss: 0.0909891
	speed: 0.1355s/iter; left time: 360.8428s
	iters: 300, epoch: 2 | loss: 0.1108245
	speed: 0.1312s/iter; left time: 336.2644s
Epoch: 2 cost time: 43.96050477027893
Epoch: 2, Steps: 318 | Train Loss: 0.1224134 Vali Loss: 0.1379150 Test Loss: 0.1002611
Validation loss decreased (0.145969 --> 0.137915).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0824748
	speed: 0.1505s/iter; left time: 368.0600s
	iters: 200, epoch: 3 | loss: 0.0851603
	speed: 0.1437s/iter; left time: 336.8595s
	iters: 300, epoch: 3 | loss: 0.1200088
	speed: 0.1304s/iter; left time: 292.8502s
Epoch: 3 cost time: 44.75387668609619
Epoch: 3, Steps: 318 | Train Loss: 0.1129966 Vali Loss: 0.1319584 Test Loss: 0.0943776
Validation loss decreased (0.137915 --> 0.131958).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1037714
	speed: 0.1369s/iter; left time: 291.2197s
	iters: 200, epoch: 4 | loss: 0.1071919
	speed: 0.1363s/iter; left time: 276.2117s
	iters: 300, epoch: 4 | loss: 0.1094843
	speed: 0.1343s/iter; left time: 258.8449s
Epoch: 4 cost time: 43.278120279312134
Epoch: 4, Steps: 318 | Train Loss: 0.1083616 Vali Loss: 0.1355674 Test Loss: 0.0948906
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0906623
	speed: 0.1498s/iter; left time: 271.0713s
	iters: 200, epoch: 5 | loss: 0.1107833
	speed: 0.1445s/iter; left time: 247.0322s
	iters: 300, epoch: 5 | loss: 0.1425900
	speed: 0.1367s/iter; left time: 220.0131s
Epoch: 5 cost time: 45.442774534225464
Epoch: 5, Steps: 318 | Train Loss: 0.1063586 Vali Loss: 0.1335220 Test Loss: 0.0945659
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1050348
	speed: 0.1403s/iter; left time: 209.1145s
	iters: 200, epoch: 6 | loss: 0.0926492
	speed: 0.1432s/iter; left time: 199.2303s
	iters: 300, epoch: 6 | loss: 0.1215721
	speed: 0.1387s/iter; left time: 178.9983s
Epoch: 6 cost time: 44.73355221748352
Epoch: 6, Steps: 318 | Train Loss: 0.1057445 Vali Loss: 0.1333868 Test Loss: 0.0942006
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1585889
	speed: 0.1542s/iter; left time: 474.9935s
	iters: 200, epoch: 1 | loss: 0.1421929
	speed: 0.1183s/iter; left time: 352.7559s
	iters: 300, epoch: 1 | loss: 0.1099317
	speed: 0.1259s/iter; left time: 362.7005s
Epoch: 1 cost time: 42.10560059547424
Epoch: 1, Steps: 318 | Train Loss: 0.2359206 Vali Loss: 0.1503623 Test Loss: 0.1154722
Validation loss decreased (inf --> 0.150362).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1406158
	speed: 0.1368s/iter; left time: 377.8607s
	iters: 200, epoch: 2 | loss: 0.1533362
	speed: 0.1272s/iter; left time: 338.8065s
	iters: 300, epoch: 2 | loss: 0.1313935
	speed: 0.1192s/iter; left time: 305.6137s
Epoch: 2 cost time: 40.898513078689575
Epoch: 2, Steps: 318 | Train Loss: 0.1285780 Vali Loss: 0.1393962 Test Loss: 0.1041491
Validation loss decreased (0.150362 --> 0.139396).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1541864
	speed: 0.1346s/iter; left time: 329.1974s
	iters: 200, epoch: 3 | loss: 0.1267525
	speed: 0.1249s/iter; left time: 292.9524s
	iters: 300, epoch: 3 | loss: 0.1250991
	speed: 0.1308s/iter; left time: 293.5794s
Epoch: 3 cost time: 41.456990003585815
Epoch: 3, Steps: 318 | Train Loss: 0.1216140 Vali Loss: 0.1384749 Test Loss: 0.1019203
Validation loss decreased (0.139396 --> 0.138475).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1263642
	speed: 0.1338s/iter; left time: 284.5476s
	iters: 200, epoch: 4 | loss: 0.1182154
	speed: 0.1324s/iter; left time: 268.2824s
	iters: 300, epoch: 4 | loss: 0.1186684
	speed: 0.1289s/iter; left time: 248.4243s
Epoch: 4 cost time: 41.86338138580322
Epoch: 4, Steps: 318 | Train Loss: 0.1181666 Vali Loss: 0.1350640 Test Loss: 0.1005009
Validation loss decreased (0.138475 --> 0.135064).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0929285
	speed: 0.1392s/iter; left time: 251.7524s
	iters: 200, epoch: 5 | loss: 0.1430182
	speed: 0.1223s/iter; left time: 209.0554s
	iters: 300, epoch: 5 | loss: 0.1484015
	speed: 0.1345s/iter; left time: 216.4535s
Epoch: 5 cost time: 41.941285371780396
Epoch: 5, Steps: 318 | Train Loss: 0.1163078 Vali Loss: 0.1351586 Test Loss: 0.1015872
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1229459
	speed: 0.1472s/iter; left time: 219.4452s
	iters: 200, epoch: 6 | loss: 0.0934717
	speed: 0.1211s/iter; left time: 168.4400s
	iters: 300, epoch: 6 | loss: 0.1272911
	speed: 0.1178s/iter; left time: 152.0590s
Epoch: 6 cost time: 40.74775505065918
Epoch: 6, Steps: 318 | Train Loss: 0.1154828 Vali Loss: 0.1335761 Test Loss: 0.0996725
Validation loss decreased (0.135064 --> 0.133576).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0932771
	speed: 0.1400s/iter; left time: 164.1757s
	iters: 200, epoch: 7 | loss: 0.1105842
	speed: 0.1381s/iter; left time: 148.2330s
	iters: 300, epoch: 7 | loss: 0.0898489
	speed: 0.1363s/iter; left time: 132.6327s
Epoch: 7 cost time: 43.66653561592102
Epoch: 7, Steps: 318 | Train Loss: 0.1151069 Vali Loss: 0.1336373 Test Loss: 0.0992107
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1376946
	speed: 0.1318s/iter; left time: 112.6947s
	iters: 200, epoch: 8 | loss: 0.1168576
	speed: 0.1266s/iter; left time: 95.6206s
	iters: 300, epoch: 8 | loss: 0.1150796
	speed: 0.1301s/iter; left time: 85.1855s
Epoch: 8 cost time: 41.225892305374146
Epoch: 8, Steps: 318 | Train Loss: 0.1147137 Vali Loss: 0.1333836 Test Loss: 0.0994101
Validation loss decreased (0.133576 --> 0.133384).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1078905
	speed: 0.1390s/iter; left time: 74.6168s
	iters: 200, epoch: 9 | loss: 0.1184746
	speed: 0.1211s/iter; left time: 52.9359s
	iters: 300, epoch: 9 | loss: 0.0967472
	speed: 0.1285s/iter; left time: 43.3131s
Epoch: 9 cost time: 41.23696494102478
Epoch: 9, Steps: 318 | Train Loss: 0.1146701 Vali Loss: 0.1335190 Test Loss: 0.0991753
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0928871
	speed: 0.1441s/iter; left time: 31.5547s
	iters: 200, epoch: 10 | loss: 0.1225652
	speed: 0.1241s/iter; left time: 14.7662s
	iters: 300, epoch: 10 | loss: 0.1309950
	speed: 0.1277s/iter; left time: 2.4257s
Epoch: 10 cost time: 42.257814168930054
Epoch: 10, Steps: 318 | Train Loss: 0.1146170 Vali Loss: 0.1334101 Test Loss: 0.0991773
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1975258
	speed: 0.2069s/iter; left time: 635.3780s
	iters: 200, epoch: 1 | loss: 0.1513449
	speed: 0.1852s/iter; left time: 550.2357s
	iters: 300, epoch: 1 | loss: 0.1463677
	speed: 0.1887s/iter; left time: 541.8705s
Epoch: 1 cost time: 61.20478010177612
Epoch: 1, Steps: 317 | Train Loss: 0.2199345 Vali Loss: 0.1467967 Test Loss: 0.1138490
Validation loss decreased (inf --> 0.146797).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1271007
	speed: 0.1901s/iter; left time: 523.4298s
	iters: 200, epoch: 2 | loss: 0.1259005
	speed: 0.1852s/iter; left time: 491.5737s
	iters: 300, epoch: 2 | loss: 0.1385082
	speed: 0.1701s/iter; left time: 434.4281s
Epoch: 2 cost time: 57.77038788795471
Epoch: 2, Steps: 317 | Train Loss: 0.1289680 Vali Loss: 0.1468983 Test Loss: 0.1090882
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1627501
	speed: 0.1713s/iter; left time: 417.5547s
	iters: 200, epoch: 3 | loss: 0.1246617
	speed: 0.1942s/iter; left time: 453.7766s
	iters: 300, epoch: 3 | loss: 0.1202709
	speed: 0.1800s/iter; left time: 402.6846s
Epoch: 3 cost time: 57.62878465652466
Epoch: 3, Steps: 317 | Train Loss: 0.1238310 Vali Loss: 0.1398886 Test Loss: 0.1033216
Validation loss decreased (0.146797 --> 0.139889).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1373285
	speed: 0.1958s/iter; left time: 415.0845s
	iters: 200, epoch: 4 | loss: 0.1387703
	speed: 0.1758s/iter; left time: 355.1259s
	iters: 300, epoch: 4 | loss: 0.1113564
	speed: 0.1806s/iter; left time: 346.8257s
Epoch: 4 cost time: 58.569918394088745
Epoch: 4, Steps: 317 | Train Loss: 0.1204441 Vali Loss: 0.1409886 Test Loss: 0.1068798
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1331531
	speed: 0.1826s/iter; left time: 329.1937s
	iters: 200, epoch: 5 | loss: 0.1275216
	speed: 0.1880s/iter; left time: 320.1497s
	iters: 300, epoch: 5 | loss: 0.1327928
	speed: 0.1911s/iter; left time: 306.3770s
Epoch: 5 cost time: 59.47712993621826
Epoch: 5, Steps: 317 | Train Loss: 0.1186845 Vali Loss: 0.1376169 Test Loss: 0.1027910
Validation loss decreased (0.139889 --> 0.137617).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1242134
	speed: 0.1883s/iter; left time: 279.7850s
	iters: 200, epoch: 6 | loss: 0.1178719
	speed: 0.1663s/iter; left time: 230.4490s
	iters: 300, epoch: 6 | loss: 0.1359481
	speed: 0.1938s/iter; left time: 249.2717s
Epoch: 6 cost time: 58.61949419975281
Epoch: 6, Steps: 317 | Train Loss: 0.1180172 Vali Loss: 0.1366450 Test Loss: 0.1016794
Validation loss decreased (0.137617 --> 0.136645).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1077376
	speed: 0.1935s/iter; left time: 226.1443s
	iters: 200, epoch: 7 | loss: 0.1207904
	speed: 0.1881s/iter; left time: 201.0958s
	iters: 300, epoch: 7 | loss: 0.1092700
	speed: 0.1741s/iter; left time: 168.6926s
Epoch: 7 cost time: 58.945061922073364
Epoch: 7, Steps: 317 | Train Loss: 0.1175036 Vali Loss: 0.1370009 Test Loss: 0.1016580
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1320203
	speed: 0.1867s/iter; left time: 159.0602s
	iters: 200, epoch: 8 | loss: 0.1218684
	speed: 0.1847s/iter; left time: 138.8792s
	iters: 300, epoch: 8 | loss: 0.1233666
	speed: 0.1922s/iter; left time: 125.3365s
Epoch: 8 cost time: 59.13742756843567
Epoch: 8, Steps: 317 | Train Loss: 0.1171567 Vali Loss: 0.1362583 Test Loss: 0.1015466
Validation loss decreased (0.136645 --> 0.136258).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1021071
	speed: 0.1873s/iter; left time: 100.2265s
	iters: 200, epoch: 9 | loss: 0.1532379
	speed: 0.1820s/iter; left time: 79.1877s
	iters: 300, epoch: 9 | loss: 0.0995580
	speed: 0.1792s/iter; left time: 60.0159s
Epoch: 9 cost time: 58.079833984375
Epoch: 9, Steps: 317 | Train Loss: 0.1172793 Vali Loss: 0.1364401 Test Loss: 0.1016141
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1161429
	speed: 0.1983s/iter; left time: 43.2225s
	iters: 200, epoch: 10 | loss: 0.1315627
	speed: 0.1861s/iter; left time: 21.9622s
	iters: 300, epoch: 10 | loss: 0.1127781
	speed: 0.1970s/iter; left time: 3.5465s
Epoch: 10 cost time: 62.208022356033325
Epoch: 10, Steps: 317 | Train Loss: 0.1169672 Vali Loss: 0.1359476 Test Loss: 0.1016913
Validation loss decreased (0.136258 --> 0.135948).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2264114
	speed: 0.1940s/iter; left time: 597.7840s
	iters: 200, epoch: 1 | loss: 0.1535085
	speed: 0.1924s/iter; left time: 573.4751s
	iters: 300, epoch: 1 | loss: 0.1320995
	speed: 0.2248s/iter; left time: 647.6333s
Epoch: 1 cost time: 65.45128965377808
Epoch: 1, Steps: 318 | Train Loss: 0.2208673 Vali Loss: 0.1445954 Test Loss: 0.1094667
Validation loss decreased (inf --> 0.144595).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1386140
	speed: 0.2040s/iter; left time: 563.7847s
	iters: 200, epoch: 2 | loss: 0.1055426
	speed: 0.1908s/iter; left time: 508.1841s
	iters: 300, epoch: 2 | loss: 0.1057280
	speed: 0.2043s/iter; left time: 523.7474s
Epoch: 2 cost time: 64.30897402763367
Epoch: 2, Steps: 318 | Train Loss: 0.1207261 Vali Loss: 0.1232647 Test Loss: 0.0929792
Validation loss decreased (0.144595 --> 0.123265).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0793974
	speed: 0.2287s/iter; left time: 559.2876s
	iters: 200, epoch: 3 | loss: 0.1209628
	speed: 0.2270s/iter; left time: 532.3254s
	iters: 300, epoch: 3 | loss: 0.0935350
	speed: 0.2231s/iter; left time: 500.8778s
Epoch: 3 cost time: 71.61095023155212
Epoch: 3, Steps: 318 | Train Loss: 0.1048915 Vali Loss: 0.1172071 Test Loss: 0.0844756
Validation loss decreased (0.123265 --> 0.117207).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0895931
	speed: 0.2407s/iter; left time: 512.0243s
	iters: 200, epoch: 4 | loss: 0.0981262
	speed: 0.2188s/iter; left time: 443.5913s
	iters: 300, epoch: 4 | loss: 0.1266590
	speed: 0.2260s/iter; left time: 435.4431s
Epoch: 4 cost time: 73.65008854866028
Epoch: 4, Steps: 318 | Train Loss: 0.0999786 Vali Loss: 0.1098603 Test Loss: 0.0807299
Validation loss decreased (0.117207 --> 0.109860).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0999072
	speed: 0.0971s/iter; left time: 175.7124s
	iters: 200, epoch: 5 | loss: 0.0959968
	speed: 0.0958s/iter; left time: 163.7941s
	iters: 300, epoch: 5 | loss: 0.0953314
	speed: 0.0964s/iter; left time: 155.1269s
Epoch: 5 cost time: 30.730377435684204
Epoch: 5, Steps: 318 | Train Loss: 0.0971614 Vali Loss: 0.1096380 Test Loss: 0.0797084
Validation loss decreased (0.109860 --> 0.109638).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0958979
	speed: 0.1010s/iter; left time: 150.5714s
	iters: 200, epoch: 6 | loss: 0.1101289
	speed: 0.0949s/iter; left time: 132.0657s
	iters: 300, epoch: 6 | loss: 0.1083586
	speed: 0.0960s/iter; left time: 123.9824s
Epoch: 6 cost time: 30.95155906677246
Epoch: 6, Steps: 318 | Train Loss: 0.0958826 Vali Loss: 0.1080478 Test Loss: 0.0792665
Validation loss decreased (0.109638 --> 0.108048).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0826907
	speed: 0.1013s/iter; left time: 118.8704s
	iters: 200, epoch: 7 | loss: 0.0792240
	speed: 0.0963s/iter; left time: 103.2831s
	iters: 300, epoch: 7 | loss: 0.1134179
	speed: 0.0982s/iter; left time: 95.5636s
Epoch: 7 cost time: 31.42584252357483
Epoch: 7, Steps: 318 | Train Loss: 0.0954004 Vali Loss: 0.1071810 Test Loss: 0.0791149
Validation loss decreased (0.108048 --> 0.107181).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1031402
	speed: 0.1004s/iter; left time: 85.8624s
	iters: 200, epoch: 8 | loss: 0.0680479
	speed: 0.0972s/iter; left time: 73.3565s
	iters: 300, epoch: 8 | loss: 0.0735073
	speed: 0.0912s/iter; left time: 59.7312s
Epoch: 8 cost time: 30.71429204940796
Epoch: 8, Steps: 318 | Train Loss: 0.0955771 Vali Loss: 0.1080630 Test Loss: 0.0792041
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0736263
	speed: 0.0999s/iter; left time: 53.6451s
	iters: 200, epoch: 9 | loss: 0.1021876
	speed: 0.0960s/iter; left time: 41.9590s
	iters: 300, epoch: 9 | loss: 0.0871142
	speed: 0.0962s/iter; left time: 32.4220s
Epoch: 9 cost time: 31.023470640182495
Epoch: 9, Steps: 318 | Train Loss: 0.0953903 Vali Loss: 0.1087887 Test Loss: 0.0791483
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0841258
	speed: 0.0987s/iter; left time: 21.6180s
	iters: 200, epoch: 10 | loss: 0.1141296
	speed: 0.0962s/iter; left time: 11.4516s
	iters: 300, epoch: 10 | loss: 0.1045672
	speed: 0.1018s/iter; left time: 1.9345s
Epoch: 10 cost time: 31.395604610443115
Epoch: 10, Steps: 318 | Train Loss: 0.0947726 Vali Loss: 0.1075517 Test Loss: 0.0791480
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1041570
	speed: 0.1138s/iter; left time: 350.7240s
	iters: 200, epoch: 1 | loss: 0.1321282
	speed: 0.0974s/iter; left time: 290.2124s
	iters: 300, epoch: 1 | loss: 0.1295129
	speed: 0.0925s/iter; left time: 266.4103s
Epoch: 1 cost time: 32.204803466796875
Epoch: 1, Steps: 318 | Train Loss: 0.2196571 Vali Loss: 0.1407006 Test Loss: 0.1030225
Validation loss decreased (inf --> 0.140701).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1279725
	speed: 0.0959s/iter; left time: 264.8421s
	iters: 200, epoch: 2 | loss: 0.1316029
	speed: 0.1001s/iter; left time: 266.4351s
	iters: 300, epoch: 2 | loss: 0.0873572
	speed: 0.0993s/iter; left time: 254.5906s
Epoch: 2 cost time: 31.307700157165527
Epoch: 2, Steps: 318 | Train Loss: 0.1149422 Vali Loss: 0.1220124 Test Loss: 0.0901823
Validation loss decreased (0.140701 --> 0.122012).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1440221
	speed: 0.0982s/iter; left time: 240.0649s
	iters: 200, epoch: 3 | loss: 0.0918036
	speed: 0.0985s/iter; left time: 230.9280s
	iters: 300, epoch: 3 | loss: 0.0823484
	speed: 0.0947s/iter; left time: 212.6429s
Epoch: 3 cost time: 30.736745595932007
Epoch: 3, Steps: 318 | Train Loss: 0.1056410 Vali Loss: 0.1162955 Test Loss: 0.0872416
Validation loss decreased (0.122012 --> 0.116295).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1104573
	speed: 0.1012s/iter; left time: 215.3077s
	iters: 200, epoch: 4 | loss: 0.0986511
	speed: 0.0978s/iter; left time: 198.2635s
	iters: 300, epoch: 4 | loss: 0.1275066
	speed: 0.0991s/iter; left time: 190.9622s
Epoch: 4 cost time: 31.57535457611084
Epoch: 4, Steps: 318 | Train Loss: 0.1016256 Vali Loss: 0.1145795 Test Loss: 0.0861686
Validation loss decreased (0.116295 --> 0.114580).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0859875
	speed: 0.0995s/iter; left time: 179.9513s
	iters: 200, epoch: 5 | loss: 0.1107276
	speed: 0.0985s/iter; left time: 168.4188s
	iters: 300, epoch: 5 | loss: 0.1149003
	speed: 0.0985s/iter; left time: 158.4980s
Epoch: 5 cost time: 31.42097306251526
Epoch: 5, Steps: 318 | Train Loss: 0.1000094 Vali Loss: 0.1138435 Test Loss: 0.0852191
Validation loss decreased (0.114580 --> 0.113844).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1137677
	speed: 0.1005s/iter; left time: 149.8860s
	iters: 200, epoch: 6 | loss: 0.0928802
	speed: 0.0980s/iter; left time: 136.2936s
	iters: 300, epoch: 6 | loss: 0.1250376
	speed: 0.0985s/iter; left time: 127.1556s
Epoch: 6 cost time: 31.475354194641113
Epoch: 6, Steps: 318 | Train Loss: 0.0990486 Vali Loss: 0.1141584 Test Loss: 0.0857218
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0930397
	speed: 0.1024s/iter; left time: 120.0680s
	iters: 200, epoch: 7 | loss: 0.0811780
	speed: 0.0961s/iter; left time: 103.1212s
	iters: 300, epoch: 7 | loss: 0.1104522
	speed: 0.0995s/iter; left time: 96.8413s
Epoch: 7 cost time: 31.573325157165527
Epoch: 7, Steps: 318 | Train Loss: 0.0988815 Vali Loss: 0.1135409 Test Loss: 0.0851880
Validation loss decreased (0.113844 --> 0.113541).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0690848
	speed: 0.1022s/iter; left time: 87.3519s
	iters: 200, epoch: 8 | loss: 0.0826754
	speed: 0.0977s/iter; left time: 73.7623s
	iters: 300, epoch: 8 | loss: 0.1227947
	speed: 0.0980s/iter; left time: 64.2163s
Epoch: 8 cost time: 31.69674849510193
Epoch: 8, Steps: 318 | Train Loss: 0.0986214 Vali Loss: 0.1125133 Test Loss: 0.0851747
Validation loss decreased (0.113541 --> 0.112513).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1178508
	speed: 0.1011s/iter; left time: 54.2654s
	iters: 200, epoch: 9 | loss: 0.0817660
	speed: 0.0994s/iter; left time: 43.4575s
	iters: 300, epoch: 9 | loss: 0.1128345
	speed: 0.0972s/iter; left time: 32.7667s
Epoch: 9 cost time: 31.601017713546753
Epoch: 9, Steps: 318 | Train Loss: 0.0985614 Vali Loss: 0.1135338 Test Loss: 0.0850666
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0985367
	speed: 0.1008s/iter; left time: 22.0798s
	iters: 200, epoch: 10 | loss: 0.0926455
	speed: 0.0976s/iter; left time: 11.6148s
	iters: 300, epoch: 10 | loss: 0.1053227
	speed: 0.0982s/iter; left time: 1.8667s
Epoch: 10 cost time: 31.45494818687439
Epoch: 10, Steps: 318 | Train Loss: 0.0984998 Vali Loss: 0.1128482 Test Loss: 0.0850576
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1405754
	speed: 0.1172s/iter; left time: 361.2444s
	iters: 200, epoch: 1 | loss: 0.1175760
	speed: 0.1018s/iter; left time: 303.5661s
	iters: 300, epoch: 1 | loss: 0.1724619
	speed: 0.1014s/iter; left time: 292.2594s
Epoch: 1 cost time: 33.854410886764526
Epoch: 1, Steps: 318 | Train Loss: 0.2231812 Vali Loss: 0.1428521 Test Loss: 0.1071855
Validation loss decreased (inf --> 0.142852).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1239673
	speed: 0.1065s/iter; left time: 294.2900s
	iters: 200, epoch: 2 | loss: 0.1015378
	speed: 0.1025s/iter; left time: 272.8314s
	iters: 300, epoch: 2 | loss: 0.1264441
	speed: 0.1013s/iter; left time: 259.5372s
Epoch: 2 cost time: 32.9195990562439
Epoch: 2, Steps: 318 | Train Loss: 0.1224774 Vali Loss: 0.1317796 Test Loss: 0.1002923
Validation loss decreased (0.142852 --> 0.131780).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0871841
	speed: 0.1033s/iter; left time: 252.6575s
	iters: 200, epoch: 3 | loss: 0.0860101
	speed: 0.0999s/iter; left time: 234.1530s
	iters: 300, epoch: 3 | loss: 0.1058587
	speed: 0.1022s/iter; left time: 229.5190s
Epoch: 3 cost time: 32.416279554367065
Epoch: 3, Steps: 318 | Train Loss: 0.1145293 Vali Loss: 0.1302958 Test Loss: 0.0977388
Validation loss decreased (0.131780 --> 0.130296).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1061399
	speed: 0.1050s/iter; left time: 223.4099s
	iters: 200, epoch: 4 | loss: 0.0995843
	speed: 0.1043s/iter; left time: 211.3163s
	iters: 300, epoch: 4 | loss: 0.1037378
	speed: 0.1006s/iter; left time: 193.7716s
Epoch: 4 cost time: 32.87635612487793
Epoch: 4, Steps: 318 | Train Loss: 0.1103740 Vali Loss: 0.1311385 Test Loss: 0.0986760
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1087078
	speed: 0.0995s/iter; left time: 179.9388s
	iters: 200, epoch: 5 | loss: 0.1219071
	speed: 0.1020s/iter; left time: 174.3216s
	iters: 300, epoch: 5 | loss: 0.1553507
	speed: 0.1020s/iter; left time: 164.0454s
Epoch: 5 cost time: 32.297555923461914
Epoch: 5, Steps: 318 | Train Loss: 0.1087116 Vali Loss: 0.1284192 Test Loss: 0.0973219
Validation loss decreased (0.130296 --> 0.128419).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1058547
	speed: 0.1078s/iter; left time: 160.6924s
	iters: 200, epoch: 6 | loss: 0.1072573
	speed: 0.1018s/iter; left time: 141.5398s
	iters: 300, epoch: 6 | loss: 0.1228952
	speed: 0.1027s/iter; left time: 132.5938s
Epoch: 6 cost time: 33.126250982284546
Epoch: 6, Steps: 318 | Train Loss: 0.1081515 Vali Loss: 0.1272520 Test Loss: 0.0967825
Validation loss decreased (0.128419 --> 0.127252).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0886049
	speed: 0.1145s/iter; left time: 134.2782s
	iters: 200, epoch: 7 | loss: 0.0778277
	speed: 0.1001s/iter; left time: 107.4272s
	iters: 300, epoch: 7 | loss: 0.1156899
	speed: 0.0994s/iter; left time: 96.7618s
Epoch: 7 cost time: 33.005757093429565
Epoch: 7, Steps: 318 | Train Loss: 0.1076127 Vali Loss: 0.1272581 Test Loss: 0.0973032
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0818140
	speed: 0.1058s/iter; left time: 90.4668s
	iters: 200, epoch: 8 | loss: 0.1077357
	speed: 0.0959s/iter; left time: 72.4359s
	iters: 300, epoch: 8 | loss: 0.1095012
	speed: 0.1013s/iter; left time: 66.3200s
Epoch: 8 cost time: 32.307071685791016
Epoch: 8, Steps: 318 | Train Loss: 0.1073453 Vali Loss: 0.1274093 Test Loss: 0.0969557
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1356900
	speed: 0.1070s/iter; left time: 57.4812s
	iters: 200, epoch: 9 | loss: 0.0971131
	speed: 0.1035s/iter; left time: 45.2319s
	iters: 300, epoch: 9 | loss: 0.0959361
	speed: 0.1013s/iter; left time: 34.1492s
Epoch: 9 cost time: 33.13729524612427
Epoch: 9, Steps: 318 | Train Loss: 0.1073678 Vali Loss: 0.1279641 Test Loss: 0.0969145
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1573291
	speed: 0.1141s/iter; left time: 351.6823s
	iters: 200, epoch: 1 | loss: 0.1417344
	speed: 0.0960s/iter; left time: 286.2956s
	iters: 300, epoch: 1 | loss: 0.1102011
	speed: 0.0951s/iter; left time: 274.0945s
Epoch: 1 cost time: 32.28947138786316
Epoch: 1, Steps: 318 | Train Loss: 0.2366624 Vali Loss: 0.1505976 Test Loss: 0.1157219
Validation loss decreased (inf --> 0.150598).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1405847
	speed: 0.0986s/iter; left time: 272.5345s
	iters: 200, epoch: 2 | loss: 0.1531509
	speed: 0.0946s/iter; left time: 252.0202s
	iters: 300, epoch: 2 | loss: 0.1303861
	speed: 0.0929s/iter; left time: 238.1570s
Epoch: 2 cost time: 30.344713926315308
Epoch: 2, Steps: 318 | Train Loss: 0.1286381 Vali Loss: 0.1394859 Test Loss: 0.1042762
Validation loss decreased (0.150598 --> 0.139486).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1541780
	speed: 0.1000s/iter; left time: 244.6155s
	iters: 200, epoch: 3 | loss: 0.1265344
	speed: 0.0970s/iter; left time: 227.4621s
	iters: 300, epoch: 3 | loss: 0.1255483
	speed: 0.0967s/iter; left time: 217.0539s
Epoch: 3 cost time: 31.103540420532227
Epoch: 3, Steps: 318 | Train Loss: 0.1215738 Vali Loss: 0.1385995 Test Loss: 0.1018796
Validation loss decreased (0.139486 --> 0.138600).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1263994
	speed: 0.0993s/iter; left time: 211.1984s
	iters: 200, epoch: 4 | loss: 0.1184393
	speed: 0.0959s/iter; left time: 194.3438s
	iters: 300, epoch: 4 | loss: 0.1181491
	speed: 0.0980s/iter; left time: 188.7619s
Epoch: 4 cost time: 31.076401233673096
Epoch: 4, Steps: 318 | Train Loss: 0.1179888 Vali Loss: 0.1349438 Test Loss: 0.1002286
Validation loss decreased (0.138600 --> 0.134944).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0930023
	speed: 0.1008s/iter; left time: 182.3573s
	iters: 200, epoch: 5 | loss: 0.1421161
	speed: 0.0976s/iter; left time: 166.8144s
	iters: 300, epoch: 5 | loss: 0.1481451
	speed: 0.1000s/iter; left time: 160.9518s
Epoch: 5 cost time: 31.587034463882446
Epoch: 5, Steps: 318 | Train Loss: 0.1160805 Vali Loss: 0.1351588 Test Loss: 0.1012592
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1227808
	speed: 0.1032s/iter; left time: 153.8628s
	iters: 200, epoch: 6 | loss: 0.0934200
	speed: 0.0981s/iter; left time: 136.3911s
	iters: 300, epoch: 6 | loss: 0.1269558
	speed: 0.0990s/iter; left time: 127.7726s
Epoch: 6 cost time: 31.875864505767822
Epoch: 6, Steps: 318 | Train Loss: 0.1152440 Vali Loss: 0.1335441 Test Loss: 0.0993886
Validation loss decreased (0.134944 --> 0.133544).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0939189
	speed: 0.1034s/iter; left time: 121.2734s
	iters: 200, epoch: 7 | loss: 0.1105361
	speed: 0.0953s/iter; left time: 102.2626s
	iters: 300, epoch: 7 | loss: 0.0892560
	speed: 0.0921s/iter; left time: 89.6434s
Epoch: 7 cost time: 30.982850313186646
Epoch: 7, Steps: 318 | Train Loss: 0.1148631 Vali Loss: 0.1335735 Test Loss: 0.0989127
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1369243
	speed: 0.1011s/iter; left time: 86.4045s
	iters: 200, epoch: 8 | loss: 0.1164957
	speed: 0.0973s/iter; left time: 73.4296s
	iters: 300, epoch: 8 | loss: 0.1155733
	speed: 0.0996s/iter; left time: 65.2448s
Epoch: 8 cost time: 31.585455417633057
Epoch: 8, Steps: 318 | Train Loss: 0.1144686 Vali Loss: 0.1333060 Test Loss: 0.0991170
Validation loss decreased (0.133544 --> 0.133306).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1073932
	speed: 0.1013s/iter; left time: 54.3857s
	iters: 200, epoch: 9 | loss: 0.1175033
	speed: 0.0986s/iter; left time: 43.0902s
	iters: 300, epoch: 9 | loss: 0.0967938
	speed: 0.0976s/iter; left time: 32.9059s
Epoch: 9 cost time: 31.496026039123535
Epoch: 9, Steps: 318 | Train Loss: 0.1144300 Vali Loss: 0.1334461 Test Loss: 0.0988790
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0935498
	speed: 0.1009s/iter; left time: 22.0877s
	iters: 200, epoch: 10 | loss: 0.1239666
	speed: 0.0965s/iter; left time: 11.4892s
	iters: 300, epoch: 10 | loss: 0.1309103
	speed: 0.0985s/iter; left time: 1.8720s
Epoch: 10 cost time: 31.420560359954834
Epoch: 10, Steps: 318 | Train Loss: 0.1143720 Vali Loss: 0.1333399 Test Loss: 0.0988817
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1972840
	speed: 0.1201s/iter; left time: 368.9621s
	iters: 200, epoch: 1 | loss: 0.1376890
	speed: 0.1069s/iter; left time: 317.4785s
	iters: 300, epoch: 1 | loss: 0.1461900
	speed: 0.1054s/iter; left time: 302.6866s
Epoch: 1 cost time: 34.97163391113281
Epoch: 1, Steps: 317 | Train Loss: 0.2219907 Vali Loss: 0.1465310 Test Loss: 0.1134784
Validation loss decreased (inf --> 0.146531).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1269869
	speed: 0.1074s/iter; left time: 295.7725s
	iters: 200, epoch: 2 | loss: 0.1255359
	speed: 0.1016s/iter; left time: 269.6934s
	iters: 300, epoch: 2 | loss: 0.1385213
	speed: 0.1043s/iter; left time: 266.3670s
Epoch: 2 cost time: 32.540056467056274
Epoch: 2, Steps: 317 | Train Loss: 0.1287302 Vali Loss: 0.1463578 Test Loss: 0.1084683
Validation loss decreased (0.146531 --> 0.146358).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1613363
	speed: 0.1008s/iter; left time: 245.6439s
	iters: 200, epoch: 3 | loss: 0.1243673
	speed: 0.1010s/iter; left time: 236.0140s
	iters: 300, epoch: 3 | loss: 0.1203163
	speed: 0.1051s/iter; left time: 235.2092s
Epoch: 3 cost time: 32.54309797286987
Epoch: 3, Steps: 317 | Train Loss: 0.1234309 Vali Loss: 0.1392496 Test Loss: 0.1026165
Validation loss decreased (0.146358 --> 0.139250).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1365347
	speed: 0.1074s/iter; left time: 227.6392s
	iters: 200, epoch: 4 | loss: 0.1382572
	speed: 0.1020s/iter; left time: 205.9500s
	iters: 300, epoch: 4 | loss: 0.1108668
	speed: 0.1031s/iter; left time: 198.0306s
Epoch: 4 cost time: 33.05857563018799
Epoch: 4, Steps: 317 | Train Loss: 0.1199461 Vali Loss: 0.1403733 Test Loss: 0.1061692
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1325264
	speed: 0.1077s/iter; left time: 194.1833s
	iters: 200, epoch: 5 | loss: 0.1270605
	speed: 0.1044s/iter; left time: 177.7939s
	iters: 300, epoch: 5 | loss: 0.1320436
	speed: 0.1051s/iter; left time: 168.4058s
Epoch: 5 cost time: 33.595237493515015
Epoch: 5, Steps: 317 | Train Loss: 0.1181787 Vali Loss: 0.1371469 Test Loss: 0.1021276
Validation loss decreased (0.139250 --> 0.137147).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1240435
	speed: 0.1093s/iter; left time: 162.4836s
	iters: 200, epoch: 6 | loss: 0.1177312
	speed: 0.1038s/iter; left time: 143.9005s
	iters: 300, epoch: 6 | loss: 0.1348663
	speed: 0.1040s/iter; left time: 133.7336s
Epoch: 6 cost time: 33.54786825180054
Epoch: 6, Steps: 317 | Train Loss: 0.1175294 Vali Loss: 0.1359928 Test Loss: 0.1009394
Validation loss decreased (0.137147 --> 0.135993).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1067156
	speed: 0.1109s/iter; left time: 129.6939s
	iters: 200, epoch: 7 | loss: 0.1205025
	speed: 0.1051s/iter; left time: 112.3077s
	iters: 300, epoch: 7 | loss: 0.1096930
	speed: 0.1045s/iter; left time: 101.2174s
Epoch: 7 cost time: 33.93990707397461
Epoch: 7, Steps: 317 | Train Loss: 0.1170172 Vali Loss: 0.1363543 Test Loss: 0.1009419
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1318021
	speed: 0.1106s/iter; left time: 94.2511s
	iters: 200, epoch: 8 | loss: 0.1224172
	speed: 0.1065s/iter; left time: 80.1095s
	iters: 300, epoch: 8 | loss: 0.1228747
	speed: 0.1063s/iter; left time: 69.3343s
Epoch: 8 cost time: 34.23387622833252
Epoch: 8, Steps: 317 | Train Loss: 0.1166567 Vali Loss: 0.1356341 Test Loss: 0.1008231
Validation loss decreased (0.135993 --> 0.135634).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1018033
	speed: 0.1102s/iter; left time: 58.9520s
	iters: 200, epoch: 9 | loss: 0.1523310
	speed: 0.1066s/iter; left time: 46.3706s
	iters: 300, epoch: 9 | loss: 0.0985687
	speed: 0.1069s/iter; left time: 35.8118s
Epoch: 9 cost time: 34.26651453971863
Epoch: 9, Steps: 317 | Train Loss: 0.1167833 Vali Loss: 0.1358248 Test Loss: 0.1008898
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1158751
	speed: 0.1097s/iter; left time: 23.9041s
	iters: 200, epoch: 10 | loss: 0.1312656
	speed: 0.1044s/iter; left time: 12.3240s
	iters: 300, epoch: 10 | loss: 0.1125266
	speed: 0.1062s/iter; left time: 1.9114s
Epoch: 10 cost time: 33.89467215538025
Epoch: 10, Steps: 317 | Train Loss: 0.1165118 Vali Loss: 0.1353426 Test Loss: 0.1009730
Validation loss decreased (0.135634 --> 0.135343).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2260834
	speed: 0.1318s/iter; left time: 406.1186s
	iters: 200, epoch: 1 | loss: 0.1534584
	speed: 0.1127s/iter; left time: 335.8839s
	iters: 300, epoch: 1 | loss: 0.1321782
	speed: 0.1137s/iter; left time: 327.5249s
Epoch: 1 cost time: 37.84341788291931
Epoch: 1, Steps: 318 | Train Loss: 0.2209027 Vali Loss: 0.1443292 Test Loss: 0.1092272
Validation loss decreased (inf --> 0.144329).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1382186
	speed: 0.1170s/iter; left time: 323.3095s
	iters: 200, epoch: 2 | loss: 0.1048326
	speed: 0.1105s/iter; left time: 294.3176s
	iters: 300, epoch: 2 | loss: 0.1052316
	speed: 0.1128s/iter; left time: 288.9849s
Epoch: 2 cost time: 36.0946581363678
Epoch: 2, Steps: 318 | Train Loss: 0.1202601 Vali Loss: 0.1227057 Test Loss: 0.0926425
Validation loss decreased (0.144329 --> 0.122706).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0787467
	speed: 0.1013s/iter; left time: 247.6432s
	iters: 200, epoch: 3 | loss: 0.1199327
	speed: 0.1097s/iter; left time: 257.1350s
	iters: 300, epoch: 3 | loss: 0.0934667
	speed: 0.1156s/iter; left time: 259.6107s
Epoch: 3 cost time: 34.84763979911804
Epoch: 3, Steps: 318 | Train Loss: 0.1045133 Vali Loss: 0.1168804 Test Loss: 0.0843129
Validation loss decreased (0.122706 --> 0.116880).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0896369
	speed: 0.1190s/iter; left time: 253.1225s
	iters: 200, epoch: 4 | loss: 0.0981864
	speed: 0.1159s/iter; left time: 234.9165s
	iters: 300, epoch: 4 | loss: 0.1263288
	speed: 0.1166s/iter; left time: 224.6323s
Epoch: 4 cost time: 37.38230347633362
Epoch: 4, Steps: 318 | Train Loss: 0.0997222 Vali Loss: 0.1096045 Test Loss: 0.0806056
Validation loss decreased (0.116880 --> 0.109604).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0996323
	speed: 0.1141s/iter; left time: 206.4494s
	iters: 200, epoch: 5 | loss: 0.0958128
	speed: 0.1180s/iter; left time: 201.6006s
	iters: 300, epoch: 5 | loss: 0.0950962
	speed: 0.1166s/iter; left time: 187.5650s
Epoch: 5 cost time: 37.08610248565674
Epoch: 5, Steps: 318 | Train Loss: 0.0969398 Vali Loss: 0.1094151 Test Loss: 0.0795906
Validation loss decreased (0.109604 --> 0.109415).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0958782
	speed: 0.1233s/iter; left time: 183.8231s
	iters: 200, epoch: 6 | loss: 0.1096959
	speed: 0.1156s/iter; left time: 160.8345s
	iters: 300, epoch: 6 | loss: 0.1081700
	speed: 0.1200s/iter; left time: 154.8575s
Epoch: 6 cost time: 37.9546639919281
Epoch: 6, Steps: 318 | Train Loss: 0.0956836 Vali Loss: 0.1078371 Test Loss: 0.0791593
Validation loss decreased (0.109415 --> 0.107837).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0822038
	speed: 0.1214s/iter; left time: 142.3833s
	iters: 200, epoch: 7 | loss: 0.0790069
	speed: 0.1193s/iter; left time: 127.9624s
	iters: 300, epoch: 7 | loss: 0.1131949
	speed: 0.1140s/iter; left time: 110.8847s
Epoch: 7 cost time: 37.5802960395813
Epoch: 7, Steps: 318 | Train Loss: 0.0952182 Vali Loss: 0.1069756 Test Loss: 0.0790006
Validation loss decreased (0.107837 --> 0.106976).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1027958
	speed: 0.1194s/iter; left time: 102.0730s
	iters: 200, epoch: 8 | loss: 0.0681560
	speed: 0.1171s/iter; left time: 88.3967s
	iters: 300, epoch: 8 | loss: 0.0736373
	speed: 0.1205s/iter; left time: 78.9033s
Epoch: 8 cost time: 37.856908321380615
Epoch: 8, Steps: 318 | Train Loss: 0.0953927 Vali Loss: 0.1078520 Test Loss: 0.0790826
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0734145
	speed: 0.1200s/iter; left time: 64.4444s
	iters: 200, epoch: 9 | loss: 0.1018940
	speed: 0.1187s/iter; left time: 51.8519s
	iters: 300, epoch: 9 | loss: 0.0870723
	speed: 0.1171s/iter; left time: 39.4731s
Epoch: 9 cost time: 37.75292348861694
Epoch: 9, Steps: 318 | Train Loss: 0.0952175 Vali Loss: 0.1085756 Test Loss: 0.0790237
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0840436
	speed: 0.1164s/iter; left time: 25.4889s
	iters: 200, epoch: 10 | loss: 0.1136756
	speed: 0.1168s/iter; left time: 13.9017s
	iters: 300, epoch: 10 | loss: 0.1041374
	speed: 0.1192s/iter; left time: 2.2649s
Epoch: 10 cost time: 37.52342176437378
Epoch: 10, Steps: 318 | Train Loss: 0.0945981 Vali Loss: 0.1073436 Test Loss: 0.0790222
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1033969
	speed: 0.1193s/iter; left time: 367.5205s
	iters: 200, epoch: 1 | loss: 0.1319636
	speed: 0.1130s/iter; left time: 336.9721s
	iters: 300, epoch: 1 | loss: 0.1278542
	speed: 0.1167s/iter; left time: 336.2324s
Epoch: 1 cost time: 37.1742057800293
Epoch: 1, Steps: 318 | Train Loss: 0.2239519 Vali Loss: 0.1401501 Test Loss: 0.1025737
Validation loss decreased (inf --> 0.140150).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1269407
	speed: 0.1184s/iter; left time: 327.1254s
	iters: 200, epoch: 2 | loss: 0.1313124
	speed: 0.1171s/iter; left time: 311.9222s
	iters: 300, epoch: 2 | loss: 0.0863225
	speed: 0.1160s/iter; left time: 297.2781s
Epoch: 2 cost time: 37.29878807067871
Epoch: 2, Steps: 318 | Train Loss: 0.1146056 Vali Loss: 0.1208544 Test Loss: 0.0899637
Validation loss decreased (0.140150 --> 0.120854).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1446428
	speed: 0.1221s/iter; left time: 298.6027s
	iters: 200, epoch: 3 | loss: 0.0919933
	speed: 0.1163s/iter; left time: 272.6515s
	iters: 300, epoch: 3 | loss: 0.0812086
	speed: 0.1175s/iter; left time: 263.7028s
Epoch: 3 cost time: 37.61249232292175
Epoch: 3, Steps: 318 | Train Loss: 0.1053049 Vali Loss: 0.1152106 Test Loss: 0.0864225
Validation loss decreased (0.120854 --> 0.115211).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1097817
	speed: 0.1205s/iter; left time: 256.2536s
	iters: 200, epoch: 4 | loss: 0.0985187
	speed: 0.1148s/iter; left time: 232.7081s
	iters: 300, epoch: 4 | loss: 0.1271110
	speed: 0.1158s/iter; left time: 223.0593s
Epoch: 4 cost time: 37.385915994644165
Epoch: 4, Steps: 318 | Train Loss: 0.1014418 Vali Loss: 0.1134908 Test Loss: 0.0856276
Validation loss decreased (0.115211 --> 0.113491).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0853974
	speed: 0.1023s/iter; left time: 185.1037s
	iters: 200, epoch: 5 | loss: 0.1107606
	speed: 0.0959s/iter; left time: 163.8614s
	iters: 300, epoch: 5 | loss: 0.1144214
	speed: 0.1004s/iter; left time: 161.5847s
Epoch: 5 cost time: 31.563808917999268
Epoch: 5, Steps: 318 | Train Loss: 0.0999203 Vali Loss: 0.1128064 Test Loss: 0.0846568
Validation loss decreased (0.113491 --> 0.112806).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1145817
	speed: 0.1141s/iter; left time: 170.0890s
	iters: 200, epoch: 6 | loss: 0.0913424
	speed: 0.1202s/iter; left time: 167.2239s
	iters: 300, epoch: 6 | loss: 0.1258433
	speed: 0.1183s/iter; left time: 152.6828s
Epoch: 6 cost time: 37.450833559036255
Epoch: 6, Steps: 318 | Train Loss: 0.0989176 Vali Loss: 0.1130993 Test Loss: 0.0853285
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0949119
	speed: 0.1217s/iter; left time: 142.7517s
	iters: 200, epoch: 7 | loss: 0.0803556
	speed: 0.1176s/iter; left time: 126.1398s
	iters: 300, epoch: 7 | loss: 0.1104895
	speed: 0.1182s/iter; left time: 114.9658s
Epoch: 7 cost time: 38.00996470451355
Epoch: 7, Steps: 318 | Train Loss: 0.0987828 Vali Loss: 0.1124514 Test Loss: 0.0847220
Validation loss decreased (0.112806 --> 0.112451).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0692960
	speed: 0.1222s/iter; left time: 104.4686s
	iters: 200, epoch: 8 | loss: 0.0812048
	speed: 0.1192s/iter; left time: 90.0035s
	iters: 300, epoch: 8 | loss: 0.1235771
	speed: 0.1168s/iter; left time: 76.4855s
Epoch: 8 cost time: 38.1036491394043
Epoch: 8, Steps: 318 | Train Loss: 0.0985210 Vali Loss: 0.1114543 Test Loss: 0.0846726
Validation loss decreased (0.112451 --> 0.111454).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1168808
	speed: 0.1193s/iter; left time: 64.0627s
	iters: 200, epoch: 9 | loss: 0.0821568
	speed: 0.1114s/iter; left time: 48.6756s
	iters: 300, epoch: 9 | loss: 0.1135679
	speed: 0.1170s/iter; left time: 39.4408s
Epoch: 9 cost time: 36.92980098724365
Epoch: 9, Steps: 318 | Train Loss: 0.0984793 Vali Loss: 0.1124700 Test Loss: 0.0845810
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0998941
	speed: 0.1201s/iter; left time: 26.3009s
	iters: 200, epoch: 10 | loss: 0.0926412
	speed: 0.1163s/iter; left time: 13.8455s
	iters: 300, epoch: 10 | loss: 0.1047298
	speed: 0.1176s/iter; left time: 2.2348s
Epoch: 10 cost time: 37.754467248916626
Epoch: 10, Steps: 318 | Train Loss: 0.0984047 Vali Loss: 0.1117860 Test Loss: 0.0845711
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1530386
	speed: 0.1313s/iter; left time: 404.5468s
	iters: 200, epoch: 1 | loss: 0.1235449
	speed: 0.1078s/iter; left time: 321.4158s
	iters: 300, epoch: 1 | loss: 0.1668410
	speed: 0.1185s/iter; left time: 341.2787s
Epoch: 1 cost time: 37.86357879638672
Epoch: 1, Steps: 318 | Train Loss: 0.2335529 Vali Loss: 0.1431616 Test Loss: 0.1072597
Validation loss decreased (inf --> 0.143162).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1211950
	speed: 0.1207s/iter; left time: 333.3931s
	iters: 200, epoch: 2 | loss: 0.0961419
	speed: 0.1156s/iter; left time: 307.8968s
	iters: 300, epoch: 2 | loss: 0.1110659
	speed: 0.1165s/iter; left time: 298.4793s
Epoch: 2 cost time: 37.57768177986145
Epoch: 2, Steps: 318 | Train Loss: 0.1232589 Vali Loss: 0.1328314 Test Loss: 0.0983850
Validation loss decreased (0.143162 --> 0.132831).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0896685
	speed: 0.1244s/iter; left time: 304.1785s
	iters: 200, epoch: 3 | loss: 0.0842334
	speed: 0.1216s/iter; left time: 285.0792s
	iters: 300, epoch: 3 | loss: 0.1169713
	speed: 0.1180s/iter; left time: 264.9163s
Epoch: 3 cost time: 38.64528179168701
Epoch: 3, Steps: 318 | Train Loss: 0.1147145 Vali Loss: 0.1306274 Test Loss: 0.0949108
Validation loss decreased (0.132831 --> 0.130627).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1023371
	speed: 0.1251s/iter; left time: 266.0200s
	iters: 200, epoch: 4 | loss: 0.1098286
	speed: 0.1228s/iter; left time: 248.8563s
	iters: 300, epoch: 4 | loss: 0.1051030
	speed: 0.1224s/iter; left time: 235.9076s
Epoch: 4 cost time: 39.3251588344574
Epoch: 4, Steps: 318 | Train Loss: 0.1105609 Vali Loss: 0.1289136 Test Loss: 0.0921076
Validation loss decreased (0.130627 --> 0.128914).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1019029
	speed: 0.1254s/iter; left time: 226.8532s
	iters: 200, epoch: 5 | loss: 0.1216991
	speed: 0.1227s/iter; left time: 209.7602s
	iters: 300, epoch: 5 | loss: 0.1510775
	speed: 0.1230s/iter; left time: 197.9142s
Epoch: 5 cost time: 39.375967025756836
Epoch: 5, Steps: 318 | Train Loss: 0.1090392 Vali Loss: 0.1267000 Test Loss: 0.0918720
Validation loss decreased (0.128914 --> 0.126700).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1063044
	speed: 0.1273s/iter; left time: 189.8572s
	iters: 200, epoch: 6 | loss: 0.1045308
	speed: 0.1150s/iter; left time: 159.9026s
	iters: 300, epoch: 6 | loss: 0.1215521
	speed: 0.1237s/iter; left time: 159.6542s
Epoch: 6 cost time: 38.49444913864136
Epoch: 6, Steps: 318 | Train Loss: 0.1086033 Vali Loss: 0.1257271 Test Loss: 0.0915123
Validation loss decreased (0.126700 --> 0.125727).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0937128
	speed: 0.1247s/iter; left time: 146.2974s
	iters: 200, epoch: 7 | loss: 0.0860768
	speed: 0.1193s/iter; left time: 128.0278s
	iters: 300, epoch: 7 | loss: 0.1070744
	speed: 0.1212s/iter; left time: 117.9530s
Epoch: 7 cost time: 38.77097034454346
Epoch: 7, Steps: 318 | Train Loss: 0.1080163 Vali Loss: 0.1256326 Test Loss: 0.0923177
Validation loss decreased (0.125727 --> 0.125633).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0884510
	speed: 0.1279s/iter; left time: 109.3665s
	iters: 200, epoch: 8 | loss: 0.1092433
	speed: 0.1230s/iter; left time: 92.8856s
	iters: 300, epoch: 8 | loss: 0.1112512
	speed: 0.1243s/iter; left time: 81.4419s
Epoch: 8 cost time: 39.82455587387085
Epoch: 8, Steps: 318 | Train Loss: 0.1078338 Vali Loss: 0.1258355 Test Loss: 0.0916973
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1328198
	speed: 0.1266s/iter; left time: 67.9612s
	iters: 200, epoch: 9 | loss: 0.0981044
	speed: 0.1224s/iter; left time: 53.4730s
	iters: 300, epoch: 9 | loss: 0.0953961
	speed: 0.1234s/iter; left time: 41.5842s
Epoch: 9 cost time: 39.51939034461975
Epoch: 9, Steps: 318 | Train Loss: 0.1079434 Vali Loss: 0.1264016 Test Loss: 0.0916302
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1124149
	speed: 0.1275s/iter; left time: 27.9232s
	iters: 200, epoch: 10 | loss: 0.1217507
	speed: 0.1248s/iter; left time: 14.8501s
	iters: 300, epoch: 10 | loss: 0.0921271
	speed: 0.1232s/iter; left time: 2.3402s
Epoch: 10 cost time: 39.8547203540802
Epoch: 10, Steps: 318 | Train Loss: 0.1075356 Vali Loss: 0.1260405 Test Loss: 0.0916187
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1654228
	speed: 0.1315s/iter; left time: 405.1737s
	iters: 200, epoch: 1 | loss: 0.1447612
	speed: 0.1139s/iter; left time: 339.6506s
	iters: 300, epoch: 1 | loss: 0.1156697
	speed: 0.1141s/iter; left time: 328.6082s
Epoch: 1 cost time: 38.07031202316284
Epoch: 1, Steps: 318 | Train Loss: 0.2357537 Vali Loss: 0.1503785 Test Loss: 0.1151733
Validation loss decreased (inf --> 0.150378).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1436538
	speed: 0.1111s/iter; left time: 307.0972s
	iters: 200, epoch: 2 | loss: 0.1485404
	speed: 0.1135s/iter; left time: 302.3488s
	iters: 300, epoch: 2 | loss: 0.1275789
	speed: 0.1141s/iter; left time: 292.5203s
Epoch: 2 cost time: 35.98417901992798
Epoch: 2, Steps: 318 | Train Loss: 0.1272485 Vali Loss: 0.1427310 Test Loss: 0.1046917
Validation loss decreased (0.150378 --> 0.142731).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1508643
	speed: 0.1180s/iter; left time: 288.4931s
	iters: 200, epoch: 3 | loss: 0.1211450
	speed: 0.1132s/iter; left time: 265.3565s
	iters: 300, epoch: 3 | loss: 0.1239655
	speed: 0.1148s/iter; left time: 257.6601s
Epoch: 3 cost time: 36.6540641784668
Epoch: 3, Steps: 318 | Train Loss: 0.1191160 Vali Loss: 0.1435885 Test Loss: 0.1027305
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1302449
	speed: 0.1202s/iter; left time: 255.6504s
	iters: 200, epoch: 4 | loss: 0.1292269
	speed: 0.1145s/iter; left time: 232.0204s
	iters: 300, epoch: 4 | loss: 0.1282248
	speed: 0.1132s/iter; left time: 218.0563s
Epoch: 4 cost time: 36.894941329956055
Epoch: 4, Steps: 318 | Train Loss: 0.1153418 Vali Loss: 0.1421896 Test Loss: 0.1033766
Validation loss decreased (0.142731 --> 0.142190).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0977022
	speed: 0.1108s/iter; left time: 200.4692s
	iters: 200, epoch: 5 | loss: 0.1376855
	speed: 0.1161s/iter; left time: 198.4115s
	iters: 300, epoch: 5 | loss: 0.1455286
	speed: 0.1129s/iter; left time: 181.7197s
Epoch: 5 cost time: 36.04781413078308
Epoch: 5, Steps: 318 | Train Loss: 0.1132342 Vali Loss: 0.1416164 Test Loss: 0.1029214
Validation loss decreased (0.142190 --> 0.141616).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1255132
	speed: 0.1225s/iter; left time: 182.6746s
	iters: 200, epoch: 6 | loss: 0.0974578
	speed: 0.1162s/iter; left time: 161.6419s
	iters: 300, epoch: 6 | loss: 0.1236579
	speed: 0.1132s/iter; left time: 146.1911s
Epoch: 6 cost time: 37.45196270942688
Epoch: 6, Steps: 318 | Train Loss: 0.1122940 Vali Loss: 0.1411052 Test Loss: 0.1022522
Validation loss decreased (0.141616 --> 0.141105).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0925246
	speed: 0.1191s/iter; left time: 139.6749s
	iters: 200, epoch: 7 | loss: 0.1105334
	speed: 0.1142s/iter; left time: 122.5141s
	iters: 300, epoch: 7 | loss: 0.0893945
	speed: 0.1145s/iter; left time: 111.4369s
Epoch: 7 cost time: 36.93742394447327
Epoch: 7, Steps: 318 | Train Loss: 0.1118336 Vali Loss: 0.1408559 Test Loss: 0.1017749
Validation loss decreased (0.141105 --> 0.140856).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1341258
	speed: 0.0911s/iter; left time: 77.8645s
	iters: 200, epoch: 8 | loss: 0.1197359
	speed: 0.0867s/iter; left time: 65.4430s
	iters: 300, epoch: 8 | loss: 0.1134545
	speed: 0.0889s/iter; left time: 58.2165s
Epoch: 8 cost time: 28.322930574417114
Epoch: 8, Steps: 318 | Train Loss: 0.1113311 Vali Loss: 0.1405215 Test Loss: 0.1018771
Validation loss decreased (0.140856 --> 0.140521).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1033701
	speed: 0.1167s/iter; left time: 62.6733s
	iters: 200, epoch: 9 | loss: 0.1018149
	speed: 0.1148s/iter; left time: 50.1652s
	iters: 300, epoch: 9 | loss: 0.0955970
	speed: 0.1064s/iter; left time: 35.8652s
Epoch: 9 cost time: 35.80388903617859
Epoch: 9, Steps: 318 | Train Loss: 0.1114749 Vali Loss: 0.1408518 Test Loss: 0.1017726
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0976686
	speed: 0.1156s/iter; left time: 25.3191s
	iters: 200, epoch: 10 | loss: 0.1252300
	speed: 0.1132s/iter; left time: 13.4694s
	iters: 300, epoch: 10 | loss: 0.1269432
	speed: 0.1133s/iter; left time: 2.1528s
Epoch: 10 cost time: 36.36476278305054
Epoch: 10, Steps: 318 | Train Loss: 0.1111506 Vali Loss: 0.1408001 Test Loss: 0.1017150
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=2, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1975625
	speed: 0.1396s/iter; left time: 428.5946s
	iters: 200, epoch: 1 | loss: 0.1374977
	speed: 0.1308s/iter; left time: 388.6201s
	iters: 300, epoch: 1 | loss: 0.1470141
	speed: 0.1302s/iter; left time: 373.8741s
Epoch: 1 cost time: 42.29911828041077
Epoch: 1, Steps: 317 | Train Loss: 0.2242724 Vali Loss: 0.1461460 Test Loss: 0.1131581
Validation loss decreased (inf --> 0.146146).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1271801
	speed: 0.1328s/iter; left time: 365.7398s
	iters: 200, epoch: 2 | loss: 0.1250944
	speed: 0.1316s/iter; left time: 349.3305s
	iters: 300, epoch: 2 | loss: 0.1370509
	speed: 0.1289s/iter; left time: 329.1705s
Epoch: 2 cost time: 41.634910345077515
Epoch: 2, Steps: 317 | Train Loss: 0.1284532 Vali Loss: 0.1455155 Test Loss: 0.1079967
Validation loss decreased (0.146146 --> 0.145516).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1599356
	speed: 0.1345s/iter; left time: 327.8294s
	iters: 200, epoch: 3 | loss: 0.1235516
	speed: 0.1214s/iter; left time: 283.6063s
	iters: 300, epoch: 3 | loss: 0.1202425
	speed: 0.1280s/iter; left time: 286.4060s
Epoch: 3 cost time: 40.744487047195435
Epoch: 3, Steps: 317 | Train Loss: 0.1229931 Vali Loss: 0.1384132 Test Loss: 0.1017458
Validation loss decreased (0.145516 --> 0.138413).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1364480
	speed: 0.1327s/iter; left time: 281.2885s
	iters: 200, epoch: 4 | loss: 0.1374710
	speed: 0.1306s/iter; left time: 263.7307s
	iters: 300, epoch: 4 | loss: 0.1096494
	speed: 0.1301s/iter; left time: 249.8878s
Epoch: 4 cost time: 41.676880836486816
Epoch: 4, Steps: 317 | Train Loss: 0.1193924 Vali Loss: 0.1396056 Test Loss: 0.1054776
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1326242
	speed: 0.1330s/iter; left time: 239.8458s
	iters: 200, epoch: 5 | loss: 0.1260557
	speed: 0.1276s/iter; left time: 217.3337s
	iters: 300, epoch: 5 | loss: 0.1302842
	speed: 0.1301s/iter; left time: 208.5411s
Epoch: 5 cost time: 41.23591232299805
Epoch: 5, Steps: 317 | Train Loss: 0.1175725 Vali Loss: 0.1361471 Test Loss: 0.1012228
Validation loss decreased (0.138413 --> 0.136147).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1238964
	speed: 0.1339s/iter; left time: 198.9213s
	iters: 200, epoch: 6 | loss: 0.1178652
	speed: 0.1289s/iter; left time: 178.6240s
	iters: 300, epoch: 6 | loss: 0.1343397
	speed: 0.1295s/iter; left time: 166.5935s
Epoch: 6 cost time: 41.46468997001648
Epoch: 6, Steps: 317 | Train Loss: 0.1167548 Vali Loss: 0.1350046 Test Loss: 0.0999707
Validation loss decreased (0.136147 --> 0.135005).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1059505
	speed: 0.1346s/iter; left time: 157.3089s
	iters: 200, epoch: 7 | loss: 0.1209223
	speed: 0.1310s/iter; left time: 139.9938s
	iters: 300, epoch: 7 | loss: 0.1104276
	speed: 0.1280s/iter; left time: 124.0762s
Epoch: 7 cost time: 41.71590733528137
Epoch: 7, Steps: 317 | Train Loss: 0.1162288 Vali Loss: 0.1353483 Test Loss: 0.0999640
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1314416
	speed: 0.1326s/iter; left time: 112.9743s
	iters: 200, epoch: 8 | loss: 0.1237347
	speed: 0.1221s/iter; left time: 91.8306s
	iters: 300, epoch: 8 | loss: 0.1223003
	speed: 0.1311s/iter; left time: 85.4455s
Epoch: 8 cost time: 40.87510657310486
Epoch: 8, Steps: 317 | Train Loss: 0.1158439 Vali Loss: 0.1346927 Test Loss: 0.0999212
Validation loss decreased (0.135005 --> 0.134693).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1012782
	speed: 0.1158s/iter; left time: 61.9346s
	iters: 200, epoch: 9 | loss: 0.1503016
	speed: 0.1203s/iter; left time: 52.3328s
	iters: 300, epoch: 9 | loss: 0.0958777
	speed: 0.1308s/iter; left time: 43.8203s
Epoch: 9 cost time: 38.801549673080444
Epoch: 9, Steps: 317 | Train Loss: 0.1160154 Vali Loss: 0.1348586 Test Loss: 0.0999500
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1154169
	speed: 0.1410s/iter; left time: 30.7421s
	iters: 200, epoch: 10 | loss: 0.1303068
	speed: 0.1302s/iter; left time: 15.3624s
	iters: 300, epoch: 10 | loss: 0.1116998
	speed: 0.1286s/iter; left time: 2.3153s
Epoch: 10 cost time: 42.306042432785034
Epoch: 10, Steps: 317 | Train Loss: 0.1156880 Vali Loss: 0.1344059 Test Loss: 0.1000459
Validation loss decreased (0.134693 --> 0.134406).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1442247
	speed: 0.0639s/iter; left time: 196.9405s
	iters: 200, epoch: 1 | loss: 0.0872648
	speed: 0.0462s/iter; left time: 137.8243s
	iters: 300, epoch: 1 | loss: 0.1002913
	speed: 0.0497s/iter; left time: 143.1613s
Epoch: 1 cost time: 17.03109049797058
Epoch: 1, Steps: 318 | Train Loss: 0.2011737 Vali Loss: 0.1497545 Test Loss: 0.1440166
Validation loss decreased (inf --> 0.149755).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1684315
	speed: 0.0609s/iter; left time: 168.3769s
	iters: 200, epoch: 2 | loss: 0.0936413
	speed: 0.0565s/iter; left time: 150.4354s
	iters: 300, epoch: 2 | loss: 0.1656661
	speed: 0.0677s/iter; left time: 173.4011s
Epoch: 2 cost time: 19.736109733581543
Epoch: 2, Steps: 318 | Train Loss: 0.1279923 Vali Loss: 0.1295368 Test Loss: 0.0965204
Validation loss decreased (0.149755 --> 0.129537).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1202750
	speed: 0.0752s/iter; left time: 183.8955s
	iters: 200, epoch: 3 | loss: 0.1268837
	speed: 0.0706s/iter; left time: 165.5826s
	iters: 300, epoch: 3 | loss: 0.1058346
	speed: 0.0704s/iter; left time: 158.0880s
Epoch: 3 cost time: 22.994513511657715
Epoch: 3, Steps: 318 | Train Loss: 0.1107172 Vali Loss: 0.1172896 Test Loss: 0.0930129
Validation loss decreased (0.129537 --> 0.117290).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1345615
	speed: 0.0746s/iter; left time: 158.7475s
	iters: 200, epoch: 4 | loss: 0.0975773
	speed: 0.0741s/iter; left time: 150.2575s
	iters: 300, epoch: 4 | loss: 0.0812628
	speed: 0.0717s/iter; left time: 138.2238s
Epoch: 4 cost time: 23.446582078933716
Epoch: 4, Steps: 318 | Train Loss: 0.1015018 Vali Loss: 0.1095146 Test Loss: 0.0868224
Validation loss decreased (0.117290 --> 0.109515).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1198764
	speed: 0.0679s/iter; left time: 122.9088s
	iters: 200, epoch: 5 | loss: 0.1217793
	speed: 0.0692s/iter; left time: 118.3369s
	iters: 300, epoch: 5 | loss: 0.0880138
	speed: 0.0705s/iter; left time: 113.3707s
Epoch: 5 cost time: 22.047626733779907
Epoch: 5, Steps: 318 | Train Loss: 0.0983854 Vali Loss: 0.1072779 Test Loss: 0.0861754
Validation loss decreased (0.109515 --> 0.107278).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1284155
	speed: 0.0747s/iter; left time: 111.4161s
	iters: 200, epoch: 6 | loss: 0.0984071
	speed: 0.0732s/iter; left time: 101.8154s
	iters: 300, epoch: 6 | loss: 0.1056037
	speed: 0.0716s/iter; left time: 92.4856s
Epoch: 6 cost time: 23.222955226898193
Epoch: 6, Steps: 318 | Train Loss: 0.0970722 Vali Loss: 0.1056765 Test Loss: 0.0866564
Validation loss decreased (0.107278 --> 0.105676).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0988694
	speed: 0.0751s/iter; left time: 88.1412s
	iters: 200, epoch: 7 | loss: 0.0934007
	speed: 0.0722s/iter; left time: 77.4531s
	iters: 300, epoch: 7 | loss: 0.1064788
	speed: 0.0719s/iter; left time: 70.0054s
Epoch: 7 cost time: 23.296316385269165
Epoch: 7, Steps: 318 | Train Loss: 0.0962589 Vali Loss: 0.1069140 Test Loss: 0.0867317
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0978109
	speed: 0.0756s/iter; left time: 64.6573s
	iters: 200, epoch: 8 | loss: 0.0998966
	speed: 0.0714s/iter; left time: 53.8710s
	iters: 300, epoch: 8 | loss: 0.0902531
	speed: 0.0715s/iter; left time: 46.8354s
Epoch: 8 cost time: 23.10887336730957
Epoch: 8, Steps: 318 | Train Loss: 0.0957332 Vali Loss: 0.1055148 Test Loss: 0.0862901
Validation loss decreased (0.105676 --> 0.105515).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0767762
	speed: 0.0719s/iter; left time: 38.5855s
	iters: 200, epoch: 9 | loss: 0.0863889
	speed: 0.0711s/iter; left time: 31.0635s
	iters: 300, epoch: 9 | loss: 0.0947798
	speed: 0.0710s/iter; left time: 23.9262s
Epoch: 9 cost time: 22.720674753189087
Epoch: 9, Steps: 318 | Train Loss: 0.0959419 Vali Loss: 0.1053164 Test Loss: 0.0862917
Validation loss decreased (0.105515 --> 0.105316).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1044407
	speed: 0.0742s/iter; left time: 16.2530s
	iters: 200, epoch: 10 | loss: 0.0502529
	speed: 0.0698s/iter; left time: 8.3054s
	iters: 300, epoch: 10 | loss: 0.0962672
	speed: 0.0699s/iter; left time: 1.3278s
Epoch: 10 cost time: 22.641400814056396
Epoch: 10, Steps: 318 | Train Loss: 0.0954068 Vali Loss: 0.1067793 Test Loss: 0.0863221
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1799754
	speed: 0.0665s/iter; left time: 204.7774s
	iters: 200, epoch: 1 | loss: 0.1952581
	speed: 0.0540s/iter; left time: 161.0731s
	iters: 300, epoch: 1 | loss: 0.1191684
	speed: 0.0617s/iter; left time: 177.8449s
Epoch: 1 cost time: 19.299416065216064
Epoch: 1, Steps: 318 | Train Loss: 0.2308449 Vali Loss: 0.1517029 Test Loss: 0.2442804
Validation loss decreased (inf --> 0.151703).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1301050
	speed: 0.0650s/iter; left time: 179.6772s
	iters: 200, epoch: 2 | loss: 0.1962361
	speed: 0.0610s/iter; left time: 162.3506s
	iters: 300, epoch: 2 | loss: 0.1190306
	speed: 0.0588s/iter; left time: 150.7034s
Epoch: 2 cost time: 19.507218837738037
Epoch: 2, Steps: 318 | Train Loss: 0.1361980 Vali Loss: 0.1399683 Test Loss: 0.2054997
Validation loss decreased (0.151703 --> 0.139968).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1210686
	speed: 0.0640s/iter; left time: 156.5332s
	iters: 200, epoch: 3 | loss: 0.1060452
	speed: 0.0530s/iter; left time: 124.3007s
	iters: 300, epoch: 3 | loss: 0.1288974
	speed: 0.0601s/iter; left time: 134.9046s
Epoch: 3 cost time: 18.861446857452393
Epoch: 3, Steps: 318 | Train Loss: 0.1290764 Vali Loss: 0.1372113 Test Loss: 0.1950788
Validation loss decreased (0.139968 --> 0.137211).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1468232
	speed: 0.0651s/iter; left time: 138.5374s
	iters: 200, epoch: 4 | loss: 0.1459925
	speed: 0.0579s/iter; left time: 117.4235s
	iters: 300, epoch: 4 | loss: 0.0898127
	speed: 0.0612s/iter; left time: 117.9681s
Epoch: 4 cost time: 19.47316312789917
Epoch: 4, Steps: 318 | Train Loss: 0.1255587 Vali Loss: 0.1360648 Test Loss: 0.1888810
Validation loss decreased (0.137211 --> 0.136065).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1057225
	speed: 0.0642s/iter; left time: 116.2197s
	iters: 200, epoch: 5 | loss: 0.1219544
	speed: 0.0611s/iter; left time: 104.3969s
	iters: 300, epoch: 5 | loss: 0.0946022
	speed: 0.0586s/iter; left time: 94.3113s
Epoch: 5 cost time: 19.517961502075195
Epoch: 5, Steps: 318 | Train Loss: 0.1242126 Vali Loss: 0.1345474 Test Loss: 0.1837711
Validation loss decreased (0.136065 --> 0.134547).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1174670
	speed: 0.0654s/iter; left time: 97.4964s
	iters: 200, epoch: 6 | loss: 0.1084251
	speed: 0.0602s/iter; left time: 83.6843s
	iters: 300, epoch: 6 | loss: 0.1027857
	speed: 0.0604s/iter; left time: 78.0216s
Epoch: 6 cost time: 19.8688383102417
Epoch: 6, Steps: 318 | Train Loss: 0.1236529 Vali Loss: 0.1336147 Test Loss: 0.1851917
Validation loss decreased (0.134547 --> 0.133615).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1276457
	speed: 0.0639s/iter; left time: 74.9942s
	iters: 200, epoch: 7 | loss: 0.0956369
	speed: 0.0609s/iter; left time: 65.3695s
	iters: 300, epoch: 7 | loss: 0.1218701
	speed: 0.0591s/iter; left time: 57.4937s
Epoch: 7 cost time: 19.441924333572388
Epoch: 7, Steps: 318 | Train Loss: 0.1233912 Vali Loss: 0.1325685 Test Loss: 0.1839164
Validation loss decreased (0.133615 --> 0.132569).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1160407
	speed: 0.0696s/iter; left time: 59.4663s
	iters: 200, epoch: 8 | loss: 0.0960978
	speed: 0.0599s/iter; left time: 45.2015s
	iters: 300, epoch: 8 | loss: 0.1681996
	speed: 0.0625s/iter; left time: 40.9576s
Epoch: 8 cost time: 20.3080050945282
Epoch: 8, Steps: 318 | Train Loss: 0.1229797 Vali Loss: 0.1341435 Test Loss: 0.1833274
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1004012
	speed: 0.0618s/iter; left time: 33.1853s
	iters: 200, epoch: 9 | loss: 0.1059591
	speed: 0.0594s/iter; left time: 25.9721s
	iters: 300, epoch: 9 | loss: 0.1081467
	speed: 0.0603s/iter; left time: 20.3293s
Epoch: 9 cost time: 19.338100910186768
Epoch: 9, Steps: 318 | Train Loss: 0.1230753 Vali Loss: 0.1338500 Test Loss: 0.1834047
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1257732
	speed: 0.0688s/iter; left time: 15.0630s
	iters: 200, epoch: 10 | loss: 0.0999073
	speed: 0.0580s/iter; left time: 6.9029s
	iters: 300, epoch: 10 | loss: 0.1412859
	speed: 0.0605s/iter; left time: 1.1498s
Epoch: 10 cost time: 19.914875984191895
Epoch: 10, Steps: 318 | Train Loss: 0.1229845 Vali Loss: 0.1344095 Test Loss: 0.1834764
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1766438
	speed: 0.0631s/iter; left time: 194.3307s
	iters: 200, epoch: 1 | loss: 0.1388933
	speed: 0.0461s/iter; left time: 137.5449s
	iters: 300, epoch: 1 | loss: 0.1011552
	speed: 0.0487s/iter; left time: 140.2657s
Epoch: 1 cost time: 16.71299982070923
Epoch: 1, Steps: 318 | Train Loss: 0.1879277 Vali Loss: 0.1309190 Test Loss: 0.2018340
Validation loss decreased (inf --> 0.130919).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1196185
	speed: 0.0533s/iter; left time: 147.3168s
	iters: 200, epoch: 2 | loss: 0.0842676
	speed: 0.0506s/iter; left time: 134.7407s
	iters: 300, epoch: 2 | loss: 0.1176215
	speed: 0.0470s/iter; left time: 120.4058s
Epoch: 2 cost time: 16.087120056152344
Epoch: 2, Steps: 318 | Train Loss: 0.1117886 Vali Loss: 0.1250142 Test Loss: 0.2173344
Validation loss decreased (0.130919 --> 0.125014).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0933504
	speed: 0.0519s/iter; left time: 126.8385s
	iters: 200, epoch: 3 | loss: 0.0928841
	speed: 0.0493s/iter; left time: 115.6954s
	iters: 300, epoch: 3 | loss: 0.0995422
	speed: 0.0472s/iter; left time: 105.8839s
Epoch: 3 cost time: 15.71829605102539
Epoch: 3, Steps: 318 | Train Loss: 0.1017755 Vali Loss: 0.1261417 Test Loss: 0.2266795
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0880802
	speed: 0.0531s/iter; left time: 112.9237s
	iters: 200, epoch: 4 | loss: 0.1221603
	speed: 0.0575s/iter; left time: 116.5493s
	iters: 300, epoch: 4 | loss: 0.1249734
	speed: 0.0612s/iter; left time: 117.9165s
Epoch: 4 cost time: 18.259105682373047
Epoch: 4, Steps: 318 | Train Loss: 0.1306433 Vali Loss: 0.1382385 Test Loss: 0.1423985
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1425342
	speed: 0.0664s/iter; left time: 120.1145s
	iters: 200, epoch: 5 | loss: 0.1122426
	speed: 0.0603s/iter; left time: 103.0308s
	iters: 300, epoch: 5 | loss: 0.1196810
	speed: 0.0634s/iter; left time: 102.0611s
Epoch: 5 cost time: 20.215065002441406
Epoch: 5, Steps: 318 | Train Loss: 0.1228937 Vali Loss: 0.1351133 Test Loss: 0.1406915
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000031
Early stopping
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1474471
	speed: 0.0673s/iter; left time: 207.3397s
	iters: 200, epoch: 1 | loss: 0.1679178
	speed: 0.0502s/iter; left time: 149.7437s
	iters: 300, epoch: 1 | loss: 0.1369458
	speed: 0.0547s/iter; left time: 157.6652s
Epoch: 1 cost time: 18.143648386001587
Epoch: 1, Steps: 318 | Train Loss: 0.1878494 Vali Loss: 0.1630799 Test Loss: 0.2660713
Validation loss decreased (inf --> 0.163080).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1262059
	speed: 0.0579s/iter; left time: 160.0932s
	iters: 200, epoch: 2 | loss: 0.1066299
	speed: 0.0550s/iter; left time: 146.3732s
	iters: 300, epoch: 2 | loss: 0.1344893
	speed: 0.0520s/iter; left time: 133.3023s
Epoch: 2 cost time: 17.60665512084961
Epoch: 2, Steps: 318 | Train Loss: 0.1348641 Vali Loss: 0.1727860 Test Loss: 0.1288266
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1094144
	speed: 0.0649s/iter; left time: 158.7749s
	iters: 200, epoch: 3 | loss: 0.1486491
	speed: 0.0620s/iter; left time: 145.2880s
	iters: 300, epoch: 3 | loss: 0.1423764
	speed: 0.0585s/iter; left time: 131.3684s
Epoch: 3 cost time: 19.699312925338745
Epoch: 3, Steps: 318 | Train Loss: 0.1414571 Vali Loss: 0.1517979 Test Loss: 0.1205403
Validation loss decreased (0.163080 --> 0.151798).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1432648
	speed: 0.0665s/iter; left time: 141.4530s
	iters: 200, epoch: 4 | loss: 0.1285042
	speed: 0.0604s/iter; left time: 122.4572s
	iters: 300, epoch: 4 | loss: 0.1261178
	speed: 0.0603s/iter; left time: 116.1994s
Epoch: 4 cost time: 19.84564185142517
Epoch: 4, Steps: 318 | Train Loss: 0.1328050 Vali Loss: 0.1496450 Test Loss: 0.1190002
Validation loss decreased (0.151798 --> 0.149645).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1027778
	speed: 0.0660s/iter; left time: 119.3523s
	iters: 200, epoch: 5 | loss: 0.1454678
	speed: 0.0618s/iter; left time: 105.5626s
	iters: 300, epoch: 5 | loss: 0.1253441
	speed: 0.0626s/iter; left time: 100.7969s
Epoch: 5 cost time: 20.237571239471436
Epoch: 5, Steps: 318 | Train Loss: 0.1309151 Vali Loss: 0.1491899 Test Loss: 0.1184930
Validation loss decreased (0.149645 --> 0.149190).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1239468
	speed: 0.0644s/iter; left time: 95.9898s
	iters: 200, epoch: 6 | loss: 0.1480075
	speed: 0.0604s/iter; left time: 83.9823s
	iters: 300, epoch: 6 | loss: 0.1436059
	speed: 0.0617s/iter; left time: 79.7187s
Epoch: 6 cost time: 19.844420194625854
Epoch: 6, Steps: 318 | Train Loss: 0.1297453 Vali Loss: 0.1493511 Test Loss: 0.1180677
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1091752
	speed: 0.0656s/iter; left time: 76.9315s
	iters: 200, epoch: 7 | loss: 0.1272095
	speed: 0.0598s/iter; left time: 64.1348s
	iters: 300, epoch: 7 | loss: 0.1416776
	speed: 0.0629s/iter; left time: 61.1556s
Epoch: 7 cost time: 19.968435287475586
Epoch: 7, Steps: 318 | Train Loss: 0.1292171 Vali Loss: 0.1485888 Test Loss: 0.1177930
Validation loss decreased (0.149190 --> 0.148589).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1318188
	speed: 0.0634s/iter; left time: 54.2360s
	iters: 200, epoch: 8 | loss: 0.1346469
	speed: 0.0627s/iter; left time: 47.3465s
	iters: 300, epoch: 8 | loss: 0.1250427
	speed: 0.0616s/iter; left time: 40.3314s
Epoch: 8 cost time: 19.914673566818237
Epoch: 8, Steps: 318 | Train Loss: 0.1290628 Vali Loss: 0.1492680 Test Loss: 0.1179851
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1365682
	speed: 0.0650s/iter; left time: 34.9089s
	iters: 200, epoch: 9 | loss: 0.1315622
	speed: 0.0606s/iter; left time: 26.4769s
	iters: 300, epoch: 9 | loss: 0.1380045
	speed: 0.0551s/iter; left time: 18.5564s
Epoch: 9 cost time: 19.180031538009644
Epoch: 9, Steps: 318 | Train Loss: 0.1288364 Vali Loss: 0.1484062 Test Loss: 0.1178594
Validation loss decreased (0.148589 --> 0.148406).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1207447
	speed: 0.0647s/iter; left time: 14.1677s
	iters: 200, epoch: 10 | loss: 0.1093172
	speed: 0.0616s/iter; left time: 7.3287s
	iters: 300, epoch: 10 | loss: 0.1164111
	speed: 0.0616s/iter; left time: 1.1713s
Epoch: 10 cost time: 19.993736505508423
Epoch: 10, Steps: 318 | Train Loss: 0.1287370 Vali Loss: 0.1483289 Test Loss: 0.1178450
Validation loss decreased (0.148406 --> 0.148329).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1939125
	speed: 0.0795s/iter; left time: 244.1975s
	iters: 200, epoch: 1 | loss: 0.1978198
	speed: 0.0626s/iter; left time: 185.8612s
	iters: 300, epoch: 1 | loss: 0.1648682
	speed: 0.0628s/iter; left time: 180.2527s
Epoch: 1 cost time: 21.480489015579224
Epoch: 1, Steps: 317 | Train Loss: 0.2619175 Vali Loss: 0.1636668 Test Loss: 0.2564692
Validation loss decreased (inf --> 0.163667).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1468232
	speed: 0.0619s/iter; left time: 170.4920s
	iters: 200, epoch: 2 | loss: 0.1165105
	speed: 0.0623s/iter; left time: 165.3537s
	iters: 300, epoch: 2 | loss: 0.1244344
	speed: 0.0626s/iter; left time: 159.9042s
Epoch: 2 cost time: 19.69737982749939
Epoch: 2, Steps: 317 | Train Loss: 0.1384358 Vali Loss: 0.1517912 Test Loss: 0.2444980
Validation loss decreased (0.163667 --> 0.151791).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1319929
	speed: 0.0654s/iter; left time: 159.3307s
	iters: 200, epoch: 3 | loss: 0.1423452
	speed: 0.0610s/iter; left time: 142.5657s
	iters: 300, epoch: 3 | loss: 0.1435863
	speed: 0.0637s/iter; left time: 142.3886s
Epoch: 3 cost time: 20.175132989883423
Epoch: 3, Steps: 317 | Train Loss: 0.1329796 Vali Loss: 0.1531124 Test Loss: 0.2321141
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1332937
	speed: 0.0635s/iter; left time: 134.5657s
	iters: 200, epoch: 4 | loss: 0.1329828
	speed: 0.0627s/iter; left time: 126.7339s
	iters: 300, epoch: 4 | loss: 0.1408719
	speed: 0.0585s/iter; left time: 112.2731s
Epoch: 4 cost time: 19.464178800582886
Epoch: 4, Steps: 317 | Train Loss: 0.1306018 Vali Loss: 0.1513192 Test Loss: 0.2238310
Validation loss decreased (0.151791 --> 0.151319).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1429035
	speed: 0.0646s/iter; left time: 116.5550s
	iters: 200, epoch: 5 | loss: 0.1358566
	speed: 0.0627s/iter; left time: 106.8558s
	iters: 300, epoch: 5 | loss: 0.1041558
	speed: 0.0632s/iter; left time: 101.2616s
Epoch: 5 cost time: 20.19079852104187
Epoch: 5, Steps: 317 | Train Loss: 0.1294028 Vali Loss: 0.1503286 Test Loss: 0.2231668
Validation loss decreased (0.151319 --> 0.150329).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1417220
	speed: 0.0660s/iter; left time: 98.0042s
	iters: 200, epoch: 6 | loss: 0.0964104
	speed: 0.0621s/iter; left time: 86.0970s
	iters: 300, epoch: 6 | loss: 0.1117314
	speed: 0.0606s/iter; left time: 77.9116s
Epoch: 6 cost time: 19.932239055633545
Epoch: 6, Steps: 317 | Train Loss: 0.1289548 Vali Loss: 0.1508902 Test Loss: 0.2219492
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1098403
	speed: 0.0658s/iter; left time: 76.8669s
	iters: 200, epoch: 7 | loss: 0.1095524
	speed: 0.0623s/iter; left time: 66.5467s
	iters: 300, epoch: 7 | loss: 0.1439621
	speed: 0.0613s/iter; left time: 59.3736s
Epoch: 7 cost time: 20.02030611038208
Epoch: 7, Steps: 317 | Train Loss: 0.1286955 Vali Loss: 0.1506225 Test Loss: 0.2200387
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0907267
	speed: 0.0647s/iter; left time: 55.0839s
	iters: 200, epoch: 8 | loss: 0.1464486
	speed: 0.0648s/iter; left time: 48.7265s
	iters: 300, epoch: 8 | loss: 0.1043149
	speed: 0.0638s/iter; left time: 41.5911s
Epoch: 8 cost time: 20.35101079940796
Epoch: 8, Steps: 317 | Train Loss: 0.1282331 Vali Loss: 0.1509288 Test Loss: 0.2204531
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1487278
	speed: 0.0996s/iter; left time: 306.7993s
	iters: 200, epoch: 1 | loss: 0.0909519
	speed: 0.0831s/iter; left time: 247.6365s
	iters: 300, epoch: 1 | loss: 0.0927320
	speed: 0.0830s/iter; left time: 238.9827s
Epoch: 1 cost time: 28.098408699035645
Epoch: 1, Steps: 318 | Train Loss: 0.1867384 Vali Loss: 0.1297257 Test Loss: 0.1022001
Validation loss decreased (inf --> 0.129726).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1161189
	speed: 0.0862s/iter; left time: 238.1640s
	iters: 200, epoch: 2 | loss: 0.0964223
	speed: 0.0848s/iter; left time: 225.8026s
	iters: 300, epoch: 2 | loss: 0.1333128
	speed: 0.0827s/iter; left time: 211.8325s
Epoch: 2 cost time: 26.87041449546814
Epoch: 2, Steps: 318 | Train Loss: 0.1024374 Vali Loss: 0.1134576 Test Loss: 0.0863802
Validation loss decreased (0.129726 --> 0.113458).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0977586
	speed: 0.0863s/iter; left time: 210.9866s
	iters: 200, epoch: 3 | loss: 0.1121470
	speed: 0.0839s/iter; left time: 196.7237s
	iters: 300, epoch: 3 | loss: 0.0878239
	speed: 0.0813s/iter; left time: 182.5462s
Epoch: 3 cost time: 26.666626453399658
Epoch: 3, Steps: 318 | Train Loss: 0.0903079 Vali Loss: 0.1138481 Test Loss: 0.0867734
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1143778
	speed: 0.0809s/iter; left time: 172.1487s
	iters: 200, epoch: 4 | loss: 0.0810526
	speed: 0.0820s/iter; left time: 166.1211s
	iters: 300, epoch: 4 | loss: 0.0724379
	speed: 0.0837s/iter; left time: 161.3126s
Epoch: 4 cost time: 26.255375862121582
Epoch: 4, Steps: 318 | Train Loss: 0.0855471 Vali Loss: 0.1109276 Test Loss: 0.0817160
Validation loss decreased (0.113458 --> 0.110928).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1105769
	speed: 0.0870s/iter; left time: 157.3721s
	iters: 200, epoch: 5 | loss: 0.1009247
	speed: 0.0835s/iter; left time: 142.6685s
	iters: 300, epoch: 5 | loss: 0.0814949
	speed: 0.0736s/iter; left time: 118.3443s
Epoch: 5 cost time: 25.949450254440308
Epoch: 5, Steps: 318 | Train Loss: 0.0830984 Vali Loss: 0.1087435 Test Loss: 0.0808172
Validation loss decreased (0.110928 --> 0.108744).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1126945
	speed: 0.0880s/iter; left time: 131.1740s
	iters: 200, epoch: 6 | loss: 0.0899950
	speed: 0.0819s/iter; left time: 113.8666s
	iters: 300, epoch: 6 | loss: 0.0906791
	speed: 0.0822s/iter; left time: 106.0564s
Epoch: 6 cost time: 26.683255434036255
Epoch: 6, Steps: 318 | Train Loss: 0.0818756 Vali Loss: 0.1071248 Test Loss: 0.0813105
Validation loss decreased (0.108744 --> 0.107125).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0797929
	speed: 0.0868s/iter; left time: 101.7826s
	iters: 200, epoch: 7 | loss: 0.0848980
	speed: 0.0837s/iter; left time: 89.8383s
	iters: 300, epoch: 7 | loss: 0.0891162
	speed: 0.0834s/iter; left time: 81.1027s
Epoch: 7 cost time: 26.78009009361267
Epoch: 7, Steps: 318 | Train Loss: 0.0809451 Vali Loss: 0.1084680 Test Loss: 0.0812585
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0776180
	speed: 0.0866s/iter; left time: 74.0518s
	iters: 200, epoch: 8 | loss: 0.0729801
	speed: 0.0828s/iter; left time: 62.5245s
	iters: 300, epoch: 8 | loss: 0.0798544
	speed: 0.0838s/iter; left time: 54.9213s
Epoch: 8 cost time: 26.838904857635498
Epoch: 8, Steps: 318 | Train Loss: 0.0810341 Vali Loss: 0.1075034 Test Loss: 0.0807156
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0615359
	speed: 0.0860s/iter; left time: 46.1573s
	iters: 200, epoch: 9 | loss: 0.0775130
	speed: 0.0819s/iter; left time: 35.8095s
	iters: 300, epoch: 9 | loss: 0.0734546
	speed: 0.0821s/iter; left time: 27.6713s
Epoch: 9 cost time: 26.541414260864258
Epoch: 9, Steps: 318 | Train Loss: 0.0805899 Vali Loss: 0.1074320 Test Loss: 0.0807518
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1932110
	speed: 0.1077s/iter; left time: 331.8647s
	iters: 200, epoch: 1 | loss: 0.1279945
	speed: 0.0920s/iter; left time: 274.1630s
	iters: 300, epoch: 1 | loss: 0.1124703
	speed: 0.0898s/iter; left time: 258.8187s
Epoch: 1 cost time: 30.60637903213501
Epoch: 1, Steps: 318 | Train Loss: 0.1897163 Vali Loss: 0.1333080 Test Loss: 0.0999465
Validation loss decreased (inf --> 0.133308).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.0990171
	speed: 0.0959s/iter; left time: 264.8688s
	iters: 200, epoch: 2 | loss: 0.1679968
	speed: 0.0914s/iter; left time: 243.4107s
	iters: 300, epoch: 2 | loss: 0.0943331
	speed: 0.0945s/iter; left time: 242.1020s
Epoch: 2 cost time: 29.81060028076172
Epoch: 2, Steps: 318 | Train Loss: 0.1070824 Vali Loss: 0.1203900 Test Loss: 0.0888965
Validation loss decreased (0.133308 --> 0.120390).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0817885
	speed: 0.1000s/iter; left time: 244.5145s
	iters: 200, epoch: 3 | loss: 0.0865973
	speed: 0.0925s/iter; left time: 216.7988s
	iters: 300, epoch: 3 | loss: 0.0836589
	speed: 0.0922s/iter; left time: 207.0004s
Epoch: 3 cost time: 30.180444478988647
Epoch: 3, Steps: 318 | Train Loss: 0.0971761 Vali Loss: 0.1148147 Test Loss: 0.0856110
Validation loss decreased (0.120390 --> 0.114815).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1157230
	speed: 0.0950s/iter; left time: 202.0024s
	iters: 200, epoch: 4 | loss: 0.1020467
	speed: 0.0934s/iter; left time: 189.4073s
	iters: 300, epoch: 4 | loss: 0.0679428
	speed: 0.0927s/iter; left time: 178.6754s
Epoch: 4 cost time: 29.823128700256348
Epoch: 4, Steps: 318 | Train Loss: 0.0929608 Vali Loss: 0.1161202 Test Loss: 0.0858789
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0683800
	speed: 0.0785s/iter; left time: 142.0303s
	iters: 200, epoch: 5 | loss: 0.0904095
	speed: 0.1007s/iter; left time: 172.1547s
	iters: 300, epoch: 5 | loss: 0.0804896
	speed: 0.0921s/iter; left time: 148.1346s
Epoch: 5 cost time: 28.71532392501831
Epoch: 5, Steps: 318 | Train Loss: 0.0907900 Vali Loss: 0.1162712 Test Loss: 0.0868564
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0846302
	speed: 0.1141s/iter; left time: 170.1817s
	iters: 200, epoch: 6 | loss: 0.0833715
	speed: 0.0972s/iter; left time: 135.2046s
	iters: 300, epoch: 6 | loss: 0.0761737
	speed: 0.0957s/iter; left time: 123.5300s
Epoch: 6 cost time: 32.67374062538147
Epoch: 6, Steps: 318 | Train Loss: 0.0898789 Vali Loss: 0.1140858 Test Loss: 0.0848368
Validation loss decreased (0.114815 --> 0.114086).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0847290
	speed: 0.0950s/iter; left time: 111.4431s
	iters: 200, epoch: 7 | loss: 0.0783925
	speed: 0.0942s/iter; left time: 101.1098s
	iters: 300, epoch: 7 | loss: 0.0902571
	speed: 0.0918s/iter; left time: 89.3130s
Epoch: 7 cost time: 29.79955816268921
Epoch: 7, Steps: 318 | Train Loss: 0.0891158 Vali Loss: 0.1142323 Test Loss: 0.0855553
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0809254
	speed: 0.0972s/iter; left time: 83.1131s
	iters: 200, epoch: 8 | loss: 0.0740571
	speed: 0.0915s/iter; left time: 69.0741s
	iters: 300, epoch: 8 | loss: 0.1306183
	speed: 0.0926s/iter; left time: 60.6395s
Epoch: 8 cost time: 29.83798909187317
Epoch: 8, Steps: 318 | Train Loss: 0.0888103 Vali Loss: 0.1149959 Test Loss: 0.0849528
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0717675
	speed: 0.0966s/iter; left time: 51.8526s
	iters: 200, epoch: 9 | loss: 0.0798941
	speed: 0.0941s/iter; left time: 41.1261s
	iters: 300, epoch: 9 | loss: 0.0802566
	speed: 0.0916s/iter; left time: 30.8721s
Epoch: 9 cost time: 29.93517804145813
Epoch: 9, Steps: 318 | Train Loss: 0.0887783 Vali Loss: 0.1147410 Test Loss: 0.0849353
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1740645
	speed: 0.1000s/iter; left time: 308.1714s
	iters: 200, epoch: 1 | loss: 0.1323139
	speed: 0.0829s/iter; left time: 247.1292s
	iters: 300, epoch: 1 | loss: 0.0976569
	speed: 0.0854s/iter; left time: 246.0028s
Epoch: 1 cost time: 28.428196668624878
Epoch: 1, Steps: 318 | Train Loss: 0.1841110 Vali Loss: 0.1325375 Test Loss: 0.1017030
Validation loss decreased (inf --> 0.132537).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1247441
	speed: 0.0872s/iter; left time: 240.9028s
	iters: 200, epoch: 2 | loss: 0.0848603
	speed: 0.0850s/iter; left time: 226.2647s
	iters: 300, epoch: 2 | loss: 0.1228114
	speed: 0.0833s/iter; left time: 213.6233s
Epoch: 2 cost time: 27.07242774963379
Epoch: 2, Steps: 318 | Train Loss: 0.1126586 Vali Loss: 0.1264419 Test Loss: 0.0960544
Validation loss decreased (0.132537 --> 0.126442).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0966643
	speed: 0.0810s/iter; left time: 198.0276s
	iters: 200, epoch: 3 | loss: 0.0934843
	speed: 0.0843s/iter; left time: 197.7227s
	iters: 300, epoch: 3 | loss: 0.0994551
	speed: 0.0848s/iter; left time: 190.3049s
Epoch: 3 cost time: 26.58301568031311
Epoch: 3, Steps: 318 | Train Loss: 0.1021958 Vali Loss: 0.1266180 Test Loss: 0.0964588
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0877569
	speed: 0.0872s/iter; left time: 185.5507s
	iters: 200, epoch: 4 | loss: 0.0833660
	speed: 0.0875s/iter; left time: 177.4029s
	iters: 300, epoch: 4 | loss: 0.1121370
	speed: 0.0783s/iter; left time: 150.9279s
Epoch: 4 cost time: 26.954824924468994
Epoch: 4, Steps: 318 | Train Loss: 0.0981821 Vali Loss: 0.1244138 Test Loss: 0.0930337
Validation loss decreased (0.126442 --> 0.124414).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1141603
	speed: 0.0898s/iter; left time: 162.4243s
	iters: 200, epoch: 5 | loss: 0.0902198
	speed: 0.0848s/iter; left time: 144.9641s
	iters: 300, epoch: 5 | loss: 0.0964405
	speed: 0.0839s/iter; left time: 135.0695s
Epoch: 5 cost time: 27.413108825683594
Epoch: 5, Steps: 318 | Train Loss: 0.0956758 Vali Loss: 0.1243947 Test Loss: 0.0934668
Validation loss decreased (0.124414 --> 0.124395).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0827060
	speed: 0.0891s/iter; left time: 132.9159s
	iters: 200, epoch: 6 | loss: 0.1276463
	speed: 0.0855s/iter; left time: 118.9112s
	iters: 300, epoch: 6 | loss: 0.1045094
	speed: 0.0859s/iter; left time: 110.8351s
Epoch: 6 cost time: 27.69909167289734
Epoch: 6, Steps: 318 | Train Loss: 0.0944357 Vali Loss: 0.1250666 Test Loss: 0.0935871
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1015172
	speed: 0.0880s/iter; left time: 103.2057s
	iters: 200, epoch: 7 | loss: 0.1163886
	speed: 0.0855s/iter; left time: 91.7425s
	iters: 300, epoch: 7 | loss: 0.1066689
	speed: 0.0845s/iter; left time: 82.2225s
Epoch: 7 cost time: 27.352191925048828
Epoch: 7, Steps: 318 | Train Loss: 0.0938586 Vali Loss: 0.1245502 Test Loss: 0.0941063
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1122911
	speed: 0.0895s/iter; left time: 76.4962s
	iters: 200, epoch: 8 | loss: 0.0809386
	speed: 0.0852s/iter; left time: 64.3506s
	iters: 300, epoch: 8 | loss: 0.1057579
	speed: 0.0839s/iter; left time: 54.9329s
Epoch: 8 cost time: 27.440914154052734
Epoch: 8, Steps: 318 | Train Loss: 0.0936585 Vali Loss: 0.1240704 Test Loss: 0.0938026
Validation loss decreased (0.124395 --> 0.124070).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1000509
	speed: 0.0878s/iter; left time: 47.1656s
	iters: 200, epoch: 9 | loss: 0.0832433
	speed: 0.0846s/iter; left time: 36.9573s
	iters: 300, epoch: 9 | loss: 0.1093321
	speed: 0.0851s/iter; left time: 28.6804s
Epoch: 9 cost time: 27.322279453277588
Epoch: 9, Steps: 318 | Train Loss: 0.0931931 Vali Loss: 0.1234180 Test Loss: 0.0936837
Validation loss decreased (0.124070 --> 0.123418).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1053880
	speed: 0.0877s/iter; left time: 19.2058s
	iters: 200, epoch: 10 | loss: 0.0909540
	speed: 0.0866s/iter; left time: 10.3081s
	iters: 300, epoch: 10 | loss: 0.0759746
	speed: 0.0836s/iter; left time: 1.5889s
Epoch: 10 cost time: 27.41014575958252
Epoch: 10, Steps: 318 | Train Loss: 0.0935726 Vali Loss: 0.1248017 Test Loss: 0.0938053
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1474117
	speed: 0.1046s/iter; left time: 322.2375s
	iters: 200, epoch: 1 | loss: 0.1684619
	speed: 0.0913s/iter; left time: 272.1354s
	iters: 300, epoch: 1 | loss: 0.1296057
	speed: 0.0916s/iter; left time: 263.8658s
Epoch: 1 cost time: 30.464720964431763
Epoch: 1, Steps: 318 | Train Loss: 0.1788854 Vali Loss: 0.1449534 Test Loss: 0.1112191
Validation loss decreased (inf --> 0.144953).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1225823
	speed: 0.0992s/iter; left time: 274.0206s
	iters: 200, epoch: 2 | loss: 0.0982943
	speed: 0.0928s/iter; left time: 247.0074s
	iters: 300, epoch: 2 | loss: 0.1267416
	speed: 0.0944s/iter; left time: 241.9791s
Epoch: 2 cost time: 30.372904539108276
Epoch: 2, Steps: 318 | Train Loss: 0.1163073 Vali Loss: 0.1333755 Test Loss: 0.0992729
Validation loss decreased (0.144953 --> 0.133376).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0913954
	speed: 0.0835s/iter; left time: 204.0887s
	iters: 200, epoch: 3 | loss: 0.1168251
	speed: 0.0738s/iter; left time: 172.9461s
	iters: 300, epoch: 3 | loss: 0.1049394
	speed: 0.0757s/iter; left time: 169.9182s
Epoch: 3 cost time: 24.946080207824707
Epoch: 3, Steps: 318 | Train Loss: 0.1066606 Vali Loss: 0.1322879 Test Loss: 0.0986561
Validation loss decreased (0.133376 --> 0.132288).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1155917
	speed: 0.0835s/iter; left time: 177.6137s
	iters: 200, epoch: 4 | loss: 0.1082564
	speed: 0.0852s/iter; left time: 172.5999s
	iters: 300, epoch: 4 | loss: 0.1006689
	speed: 0.0780s/iter; left time: 150.2795s
Epoch: 4 cost time: 26.139540433883667
Epoch: 4, Steps: 318 | Train Loss: 0.1027443 Vali Loss: 0.1300813 Test Loss: 0.0984331
Validation loss decreased (0.132288 --> 0.130081).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0800024
	speed: 0.0967s/iter; left time: 174.9007s
	iters: 200, epoch: 5 | loss: 0.1097575
	speed: 0.0913s/iter; left time: 156.0274s
	iters: 300, epoch: 5 | loss: 0.0947777
	speed: 0.0932s/iter; left time: 149.9488s
Epoch: 5 cost time: 29.87918186187744
Epoch: 5, Steps: 318 | Train Loss: 0.1008788 Vali Loss: 0.1308140 Test Loss: 0.0976598
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0983957
	speed: 0.0962s/iter; left time: 143.4828s
	iters: 200, epoch: 6 | loss: 0.1086546
	speed: 0.0930s/iter; left time: 129.4009s
	iters: 300, epoch: 6 | loss: 0.1127725
	speed: 0.0925s/iter; left time: 119.4519s
Epoch: 6 cost time: 29.93265175819397
Epoch: 6, Steps: 318 | Train Loss: 0.0997310 Vali Loss: 0.1326491 Test Loss: 0.0980975
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0941659
	speed: 0.0934s/iter; left time: 109.5120s
	iters: 200, epoch: 7 | loss: 0.1011958
	speed: 0.0925s/iter; left time: 99.2745s
	iters: 300, epoch: 7 | loss: 0.1029209
	speed: 0.0935s/iter; left time: 91.0232s
Epoch: 7 cost time: 29.64065647125244
Epoch: 7, Steps: 318 | Train Loss: 0.0991442 Vali Loss: 0.1304969 Test Loss: 0.0972204
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1535616
	speed: 0.1144s/iter; left time: 351.2236s
	iters: 200, epoch: 1 | loss: 0.1815898
	speed: 0.1030s/iter; left time: 305.9695s
	iters: 300, epoch: 1 | loss: 0.1525128
	speed: 0.0960s/iter; left time: 275.6060s
Epoch: 1 cost time: 33.09491491317749
Epoch: 1, Steps: 317 | Train Loss: 0.2025148 Vali Loss: 0.1500239 Test Loss: 0.1113056
Validation loss decreased (inf --> 0.150024).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1240321
	speed: 0.1002s/iter; left time: 275.9021s
	iters: 200, epoch: 2 | loss: 0.1149454
	speed: 0.0961s/iter; left time: 255.1422s
	iters: 300, epoch: 2 | loss: 0.1156248
	speed: 0.0975s/iter; left time: 248.9461s
Epoch: 2 cost time: 31.10685682296753
Epoch: 2, Steps: 317 | Train Loss: 0.1209646 Vali Loss: 0.1384230 Test Loss: 0.1048037
Validation loss decreased (0.150024 --> 0.138423).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1041053
	speed: 0.1002s/iter; left time: 244.2400s
	iters: 200, epoch: 3 | loss: 0.1073321
	speed: 0.0983s/iter; left time: 229.7991s
	iters: 300, epoch: 3 | loss: 0.1269338
	speed: 0.0967s/iter; left time: 216.3540s
Epoch: 3 cost time: 31.31938672065735
Epoch: 3, Steps: 317 | Train Loss: 0.1125545 Vali Loss: 0.1400795 Test Loss: 0.1103689
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1177189
	speed: 0.0929s/iter; left time: 196.9559s
	iters: 200, epoch: 4 | loss: 0.1025057
	speed: 0.0973s/iter; left time: 196.4826s
	iters: 300, epoch: 4 | loss: 0.1219047
	speed: 0.0973s/iter; left time: 186.7313s
Epoch: 4 cost time: 30.468051195144653
Epoch: 4, Steps: 317 | Train Loss: 0.1081940 Vali Loss: 0.1369487 Test Loss: 0.1054756
Validation loss decreased (0.138423 --> 0.136949).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1301957
	speed: 0.1012s/iter; left time: 182.3984s
	iters: 200, epoch: 5 | loss: 0.1020894
	speed: 0.0904s/iter; left time: 153.8861s
	iters: 300, epoch: 5 | loss: 0.0849213
	speed: 0.0975s/iter; left time: 156.3219s
Epoch: 5 cost time: 30.64097762107849
Epoch: 5, Steps: 317 | Train Loss: 0.1057766 Vali Loss: 0.1367435 Test Loss: 0.1055719
Validation loss decreased (0.136949 --> 0.136743).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1094935
	speed: 0.1032s/iter; left time: 153.2974s
	iters: 200, epoch: 6 | loss: 0.0810449
	speed: 0.0948s/iter; left time: 131.4052s
	iters: 300, epoch: 6 | loss: 0.1010928
	speed: 0.0909s/iter; left time: 116.9576s
Epoch: 6 cost time: 30.601025342941284
Epoch: 6, Steps: 317 | Train Loss: 0.1046402 Vali Loss: 0.1365989 Test Loss: 0.1053599
Validation loss decreased (0.136743 --> 0.136599).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0979363
	speed: 0.1005s/iter; left time: 117.5242s
	iters: 200, epoch: 7 | loss: 0.0877611
	speed: 0.0961s/iter; left time: 102.7359s
	iters: 300, epoch: 7 | loss: 0.1057661
	speed: 0.0983s/iter; left time: 95.2122s
Epoch: 7 cost time: 30.70210337638855
Epoch: 7, Steps: 317 | Train Loss: 0.1042136 Vali Loss: 0.1364815 Test Loss: 0.1048270
Validation loss decreased (0.136599 --> 0.136481).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0844955
	speed: 0.1021s/iter; left time: 86.9581s
	iters: 200, epoch: 8 | loss: 0.1022422
	speed: 0.0981s/iter; left time: 73.7737s
	iters: 300, epoch: 8 | loss: 0.0804219
	speed: 0.0961s/iter; left time: 62.6817s
Epoch: 8 cost time: 31.348355293273926
Epoch: 8, Steps: 317 | Train Loss: 0.1035975 Vali Loss: 0.1362637 Test Loss: 0.1054035
Validation loss decreased (0.136481 --> 0.136264).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0870961
	speed: 0.1017s/iter; left time: 54.3877s
	iters: 200, epoch: 9 | loss: 0.0904170
	speed: 0.0976s/iter; left time: 42.4774s
	iters: 300, epoch: 9 | loss: 0.1068332
	speed: 0.0964s/iter; left time: 32.3044s
Epoch: 9 cost time: 31.284908533096313
Epoch: 9, Steps: 317 | Train Loss: 0.1035768 Vali Loss: 0.1363162 Test Loss: 0.1055843
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0902079
	speed: 0.1005s/iter; left time: 21.9000s
	iters: 200, epoch: 10 | loss: 0.1231275
	speed: 0.0985s/iter; left time: 11.6185s
	iters: 300, epoch: 10 | loss: 0.0815509
	speed: 0.0977s/iter; left time: 1.7591s
Epoch: 10 cost time: 31.393937349319458
Epoch: 10, Steps: 317 | Train Loss: 0.1034338 Vali Loss: 0.1362534 Test Loss: 0.1053808
Validation loss decreased (0.136264 --> 0.136253).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1468460
	speed: 0.1348s/iter; left time: 415.2693s
	iters: 200, epoch: 1 | loss: 0.0919062
	speed: 0.1166s/iter; left time: 347.5240s
	iters: 300, epoch: 1 | loss: 0.0915985
	speed: 0.1177s/iter; left time: 339.1695s
Epoch: 1 cost time: 39.12506031990051
Epoch: 1, Steps: 318 | Train Loss: 0.1862421 Vali Loss: 0.1292136 Test Loss: 0.1017902
Validation loss decreased (inf --> 0.129214).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1156227
	speed: 0.1199s/iter; left time: 331.2281s
	iters: 200, epoch: 2 | loss: 0.0960514
	speed: 0.1193s/iter; left time: 317.8123s
	iters: 300, epoch: 2 | loss: 0.1329554
	speed: 0.0949s/iter; left time: 243.2241s
Epoch: 2 cost time: 35.41356372833252
Epoch: 2, Steps: 318 | Train Loss: 0.1020020 Vali Loss: 0.1132275 Test Loss: 0.0859808
Validation loss decreased (0.129214 --> 0.113228).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0970139
	speed: 0.1236s/iter; left time: 302.2864s
	iters: 200, epoch: 3 | loss: 0.1111089
	speed: 0.1036s/iter; left time: 242.8298s
	iters: 300, epoch: 3 | loss: 0.0868468
	speed: 0.1118s/iter; left time: 250.9241s
Epoch: 3 cost time: 36.176177978515625
Epoch: 3, Steps: 318 | Train Loss: 0.0900267 Vali Loss: 0.1135723 Test Loss: 0.0863971
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1141334
	speed: 0.1209s/iter; left time: 257.1943s
	iters: 200, epoch: 4 | loss: 0.0806116
	speed: 0.1171s/iter; left time: 237.3130s
	iters: 300, epoch: 4 | loss: 0.0718901
	speed: 0.1172s/iter; left time: 225.8745s
Epoch: 4 cost time: 37.70335602760315
Epoch: 4, Steps: 318 | Train Loss: 0.0853060 Vali Loss: 0.1108935 Test Loss: 0.0812755
Validation loss decreased (0.113228 --> 0.110894).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1100049
	speed: 0.1214s/iter; left time: 219.6264s
	iters: 200, epoch: 5 | loss: 0.1002777
	speed: 0.1190s/iter; left time: 203.3263s
	iters: 300, epoch: 5 | loss: 0.0804590
	speed: 0.1181s/iter; left time: 189.9675s
Epoch: 5 cost time: 38.03982925415039
Epoch: 5, Steps: 318 | Train Loss: 0.0828856 Vali Loss: 0.1086345 Test Loss: 0.0803893
Validation loss decreased (0.110894 --> 0.108634).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1123958
	speed: 0.1229s/iter; left time: 183.2398s
	iters: 200, epoch: 6 | loss: 0.0906299
	speed: 0.1186s/iter; left time: 164.9567s
	iters: 300, epoch: 6 | loss: 0.0902321
	speed: 0.1182s/iter; left time: 152.5979s
Epoch: 6 cost time: 38.11891317367554
Epoch: 6, Steps: 318 | Train Loss: 0.0817016 Vali Loss: 0.1069454 Test Loss: 0.0808447
Validation loss decreased (0.108634 --> 0.106945).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0797506
	speed: 0.1224s/iter; left time: 143.6105s
	iters: 200, epoch: 7 | loss: 0.0853743
	speed: 0.1183s/iter; left time: 126.9042s
	iters: 300, epoch: 7 | loss: 0.0886657
	speed: 0.1184s/iter; left time: 115.1715s
Epoch: 7 cost time: 38.17113542556763
Epoch: 7, Steps: 318 | Train Loss: 0.0807606 Vali Loss: 0.1082806 Test Loss: 0.0807944
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0770080
	speed: 0.1221s/iter; left time: 104.3716s
	iters: 200, epoch: 8 | loss: 0.0726264
	speed: 0.1177s/iter; left time: 88.8499s
	iters: 300, epoch: 8 | loss: 0.0794502
	speed: 0.1163s/iter; left time: 76.1949s
Epoch: 8 cost time: 37.859111309051514
Epoch: 8, Steps: 318 | Train Loss: 0.0808415 Vali Loss: 0.1074304 Test Loss: 0.0802862
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0614891
	speed: 0.0660s/iter; left time: 35.4655s
	iters: 200, epoch: 9 | loss: 0.0780530
	speed: 0.0634s/iter; left time: 27.6873s
	iters: 300, epoch: 9 | loss: 0.0730682
	speed: 0.0635s/iter; left time: 21.3845s
Epoch: 9 cost time: 20.45894193649292
Epoch: 9, Steps: 318 | Train Loss: 0.0803735 Vali Loss: 0.1073429 Test Loss: 0.0803222
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1938523
	speed: 0.1258s/iter; left time: 387.4827s
	iters: 200, epoch: 1 | loss: 0.1253059
	speed: 0.1165s/iter; left time: 347.4158s
	iters: 300, epoch: 1 | loss: 0.1138244
	speed: 0.1059s/iter; left time: 304.9776s
Epoch: 1 cost time: 36.96667528152466
Epoch: 1, Steps: 318 | Train Loss: 0.1895642 Vali Loss: 0.1325867 Test Loss: 0.0995987
Validation loss decreased (inf --> 0.132587).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.0984067
	speed: 0.0776s/iter; left time: 214.3833s
	iters: 200, epoch: 2 | loss: 0.1678713
	speed: 0.0737s/iter; left time: 196.2636s
	iters: 300, epoch: 2 | loss: 0.0918336
	speed: 0.0749s/iter; left time: 191.9158s
Epoch: 2 cost time: 23.997021675109863
Epoch: 2, Steps: 318 | Train Loss: 0.1060850 Vali Loss: 0.1193750 Test Loss: 0.0883392
Validation loss decreased (0.132587 --> 0.119375).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0819935
	speed: 0.0771s/iter; left time: 188.4711s
	iters: 200, epoch: 3 | loss: 0.0858846
	speed: 0.0741s/iter; left time: 173.7751s
	iters: 300, epoch: 3 | loss: 0.0836348
	speed: 0.0750s/iter; left time: 168.2646s
Epoch: 3 cost time: 24.033672332763672
Epoch: 3, Steps: 318 | Train Loss: 0.0965853 Vali Loss: 0.1140531 Test Loss: 0.0849328
Validation loss decreased (0.119375 --> 0.114053).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1151748
	speed: 0.0785s/iter; left time: 166.8832s
	iters: 200, epoch: 4 | loss: 0.1019602
	speed: 0.0742s/iter; left time: 150.4282s
	iters: 300, epoch: 4 | loss: 0.0683130
	speed: 0.0732s/iter; left time: 140.9901s
Epoch: 4 cost time: 23.957783460617065
Epoch: 4, Steps: 318 | Train Loss: 0.0925341 Vali Loss: 0.1154642 Test Loss: 0.0852789
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0687859
	speed: 0.0769s/iter; left time: 139.0996s
	iters: 200, epoch: 5 | loss: 0.0903948
	speed: 0.0738s/iter; left time: 126.0723s
	iters: 300, epoch: 5 | loss: 0.0795657
	speed: 0.0737s/iter; left time: 118.5993s
Epoch: 5 cost time: 23.822720289230347
Epoch: 5, Steps: 318 | Train Loss: 0.0904421 Vali Loss: 0.1155310 Test Loss: 0.0862293
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0840629
	speed: 0.0771s/iter; left time: 114.9426s
	iters: 200, epoch: 6 | loss: 0.0829567
	speed: 0.0748s/iter; left time: 104.0353s
	iters: 300, epoch: 6 | loss: 0.0759919
	speed: 0.0741s/iter; left time: 95.6011s
Epoch: 6 cost time: 23.97158193588257
Epoch: 6, Steps: 318 | Train Loss: 0.0895585 Vali Loss: 0.1135171 Test Loss: 0.0842758
Validation loss decreased (0.114053 --> 0.113517).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0844470
	speed: 0.0770s/iter; left time: 90.2937s
	iters: 200, epoch: 7 | loss: 0.0782647
	speed: 0.0729s/iter; left time: 78.1903s
	iters: 300, epoch: 7 | loss: 0.0889364
	speed: 0.0731s/iter; left time: 71.0966s
Epoch: 7 cost time: 23.657084941864014
Epoch: 7, Steps: 318 | Train Loss: 0.0888286 Vali Loss: 0.1136905 Test Loss: 0.0849936
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0802722
	speed: 0.0767s/iter; left time: 65.5642s
	iters: 200, epoch: 8 | loss: 0.0736461
	speed: 0.0753s/iter; left time: 56.8147s
	iters: 300, epoch: 8 | loss: 0.1289507
	speed: 0.0780s/iter; left time: 51.0716s
Epoch: 8 cost time: 24.36785054206848
Epoch: 8, Steps: 318 | Train Loss: 0.0885092 Vali Loss: 0.1144549 Test Loss: 0.0843880
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0710614
	speed: 0.0790s/iter; left time: 42.4159s
	iters: 200, epoch: 9 | loss: 0.0798863
	speed: 0.0745s/iter; left time: 32.5635s
	iters: 300, epoch: 9 | loss: 0.0799759
	speed: 0.0730s/iter; left time: 24.6023s
Epoch: 9 cost time: 24.028074264526367
Epoch: 9, Steps: 318 | Train Loss: 0.0884779 Vali Loss: 0.1142037 Test Loss: 0.0843833
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1729826
	speed: 0.0821s/iter; left time: 253.0781s
	iters: 200, epoch: 1 | loss: 0.1314626
	speed: 0.0652s/iter; left time: 194.4770s
	iters: 300, epoch: 1 | loss: 0.0965819
	speed: 0.0642s/iter; left time: 184.9810s
Epoch: 1 cost time: 22.33862829208374
Epoch: 1, Steps: 318 | Train Loss: 0.1840427 Vali Loss: 0.1323879 Test Loss: 0.1012007
Validation loss decreased (inf --> 0.132388).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1264354
	speed: 0.0682s/iter; left time: 188.3994s
	iters: 200, epoch: 2 | loss: 0.0852810
	speed: 0.0644s/iter; left time: 171.4811s
	iters: 300, epoch: 2 | loss: 0.1239919
	speed: 0.0647s/iter; left time: 165.8249s
Epoch: 2 cost time: 20.94020104408264
Epoch: 2, Steps: 318 | Train Loss: 0.1126852 Vali Loss: 0.1265457 Test Loss: 0.0960772
Validation loss decreased (0.132388 --> 0.126546).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0983135
	speed: 0.0684s/iter; left time: 167.3330s
	iters: 200, epoch: 3 | loss: 0.0941459
	speed: 0.0646s/iter; left time: 151.5015s
	iters: 300, epoch: 3 | loss: 0.1001758
	speed: 0.0659s/iter; left time: 147.9988s
Epoch: 3 cost time: 21.138365507125854
Epoch: 3, Steps: 318 | Train Loss: 0.1022147 Vali Loss: 0.1265842 Test Loss: 0.0962034
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0884679
	speed: 0.0692s/iter; left time: 147.2420s
	iters: 200, epoch: 4 | loss: 0.0832210
	speed: 0.0661s/iter; left time: 133.9454s
	iters: 300, epoch: 4 | loss: 0.1120024
	speed: 0.0659s/iter; left time: 127.0760s
Epoch: 4 cost time: 21.381165027618408
Epoch: 4, Steps: 318 | Train Loss: 0.0981811 Vali Loss: 0.1243495 Test Loss: 0.0928753
Validation loss decreased (0.126546 --> 0.124349).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1142943
	speed: 0.0713s/iter; left time: 128.9953s
	iters: 200, epoch: 5 | loss: 0.0891048
	speed: 0.0665s/iter; left time: 113.6607s
	iters: 300, epoch: 5 | loss: 0.0968856
	speed: 0.0668s/iter; left time: 107.5393s
Epoch: 5 cost time: 21.714475631713867
Epoch: 5, Steps: 318 | Train Loss: 0.0956663 Vali Loss: 0.1241185 Test Loss: 0.0932978
Validation loss decreased (0.124349 --> 0.124118).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0830114
	speed: 0.0694s/iter; left time: 103.5003s
	iters: 200, epoch: 6 | loss: 0.1272462
	speed: 0.0660s/iter; left time: 91.8201s
	iters: 300, epoch: 6 | loss: 0.1048814
	speed: 0.0661s/iter; left time: 85.3327s
Epoch: 6 cost time: 21.48652195930481
Epoch: 6, Steps: 318 | Train Loss: 0.0944511 Vali Loss: 0.1247901 Test Loss: 0.0934022
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1013620
	speed: 0.0699s/iter; left time: 81.9397s
	iters: 200, epoch: 7 | loss: 0.1144390
	speed: 0.0660s/iter; left time: 70.8664s
	iters: 300, epoch: 7 | loss: 0.1066082
	speed: 0.0666s/iter; left time: 64.7829s
Epoch: 7 cost time: 21.49225950241089
Epoch: 7, Steps: 318 | Train Loss: 0.0938564 Vali Loss: 0.1242593 Test Loss: 0.0939356
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1097933
	speed: 0.0700s/iter; left time: 59.8429s
	iters: 200, epoch: 8 | loss: 0.0798857
	speed: 0.0663s/iter; left time: 50.0414s
	iters: 300, epoch: 8 | loss: 0.1057478
	speed: 0.0666s/iter; left time: 43.6116s
Epoch: 8 cost time: 21.55824589729309
Epoch: 8, Steps: 318 | Train Loss: 0.0936901 Vali Loss: 0.1237791 Test Loss: 0.0936185
Validation loss decreased (0.124118 --> 0.123779).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1005738
	speed: 0.0701s/iter; left time: 37.6415s
	iters: 200, epoch: 9 | loss: 0.0841517
	speed: 0.0661s/iter; left time: 28.8880s
	iters: 300, epoch: 9 | loss: 0.1105146
	speed: 0.0665s/iter; left time: 22.4086s
Epoch: 9 cost time: 21.527217149734497
Epoch: 9, Steps: 318 | Train Loss: 0.0931860 Vali Loss: 0.1231818 Test Loss: 0.0934887
Validation loss decreased (0.123779 --> 0.123182).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1049171
	speed: 0.0699s/iter; left time: 15.3085s
	iters: 200, epoch: 10 | loss: 0.0919933
	speed: 0.0663s/iter; left time: 7.8886s
	iters: 300, epoch: 10 | loss: 0.0755255
	speed: 0.0664s/iter; left time: 1.2613s
Epoch: 10 cost time: 21.499105215072632
Epoch: 10, Steps: 318 | Train Loss: 0.0935636 Vali Loss: 0.1245149 Test Loss: 0.0936185
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1473202
	speed: 0.0824s/iter; left time: 253.9580s
	iters: 200, epoch: 1 | loss: 0.1684701
	speed: 0.0696s/iter; left time: 207.4425s
	iters: 300, epoch: 1 | loss: 0.1288862
	speed: 0.0700s/iter; left time: 201.6442s
Epoch: 1 cost time: 23.495166301727295
Epoch: 1, Steps: 318 | Train Loss: 0.1796306 Vali Loss: 0.1455936 Test Loss: 0.1115354
Validation loss decreased (inf --> 0.145594).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1231211
	speed: 0.0754s/iter; left time: 208.3417s
	iters: 200, epoch: 2 | loss: 0.0988116
	speed: 0.0704s/iter; left time: 187.4284s
	iters: 300, epoch: 2 | loss: 0.1274431
	speed: 0.0705s/iter; left time: 180.7417s
Epoch: 2 cost time: 22.95887804031372
Epoch: 2, Steps: 318 | Train Loss: 0.1167155 Vali Loss: 0.1338713 Test Loss: 0.0995409
Validation loss decreased (0.145594 --> 0.133871).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0918997
	speed: 0.0737s/iter; left time: 180.1534s
	iters: 200, epoch: 3 | loss: 0.1175849
	speed: 0.0707s/iter; left time: 165.8952s
	iters: 300, epoch: 3 | loss: 0.1052827
	speed: 0.0705s/iter; left time: 158.3008s
Epoch: 3 cost time: 22.819748878479004
Epoch: 3, Steps: 318 | Train Loss: 0.1069852 Vali Loss: 0.1325858 Test Loss: 0.0987015
Validation loss decreased (0.133871 --> 0.132586).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1158267
	speed: 0.0735s/iter; left time: 156.2284s
	iters: 200, epoch: 4 | loss: 0.1081533
	speed: 0.0707s/iter; left time: 143.2468s
	iters: 300, epoch: 4 | loss: 0.1012929
	speed: 0.0708s/iter; left time: 136.5076s
Epoch: 4 cost time: 22.817617654800415
Epoch: 4, Steps: 318 | Train Loss: 0.1030425 Vali Loss: 0.1304664 Test Loss: 0.0985726
Validation loss decreased (0.132586 --> 0.130466).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0799805
	speed: 0.0735s/iter; left time: 133.0153s
	iters: 200, epoch: 5 | loss: 0.1100047
	speed: 0.0713s/iter; left time: 121.7956s
	iters: 300, epoch: 5 | loss: 0.0951628
	speed: 0.0707s/iter; left time: 113.7788s
Epoch: 5 cost time: 22.880858659744263
Epoch: 5, Steps: 318 | Train Loss: 0.1011358 Vali Loss: 0.1312621 Test Loss: 0.0978441
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0986282
	speed: 0.0736s/iter; left time: 109.7145s
	iters: 200, epoch: 6 | loss: 0.1090369
	speed: 0.0707s/iter; left time: 98.3854s
	iters: 300, epoch: 6 | loss: 0.1137677
	speed: 0.0703s/iter; left time: 90.7876s
Epoch: 6 cost time: 22.800945520401
Epoch: 6, Steps: 318 | Train Loss: 0.0999859 Vali Loss: 0.1330091 Test Loss: 0.0981917
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0935609
	speed: 0.0734s/iter; left time: 86.0838s
	iters: 200, epoch: 7 | loss: 0.1008927
	speed: 0.0702s/iter; left time: 75.3255s
	iters: 300, epoch: 7 | loss: 0.1031080
	speed: 0.0703s/iter; left time: 68.3662s
Epoch: 7 cost time: 22.702741622924805
Epoch: 7, Steps: 318 | Train Loss: 0.0993952 Vali Loss: 0.1308212 Test Loss: 0.0972874
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1650191
	speed: 0.0858s/iter; left time: 263.3848s
	iters: 200, epoch: 1 | loss: 0.1881557
	speed: 0.0683s/iter; left time: 202.8125s
	iters: 300, epoch: 1 | loss: 0.1390330
	speed: 0.0683s/iter; left time: 196.1420s
Epoch: 1 cost time: 23.42242121696472
Epoch: 1, Steps: 317 | Train Loss: 0.2076901 Vali Loss: 0.1492946 Test Loss: 0.1107450
Validation loss decreased (inf --> 0.149295).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1281264
	speed: 0.0744s/iter; left time: 204.8802s
	iters: 200, epoch: 2 | loss: 0.1092489
	speed: 0.0689s/iter; left time: 182.8319s
	iters: 300, epoch: 2 | loss: 0.1135121
	speed: 0.0711s/iter; left time: 181.5633s
Epoch: 2 cost time: 22.689494132995605
Epoch: 2, Steps: 317 | Train Loss: 0.1225953 Vali Loss: 0.1399681 Test Loss: 0.1073920
Validation loss decreased (0.149295 --> 0.139968).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1030872
	speed: 0.0730s/iter; left time: 177.9151s
	iters: 200, epoch: 3 | loss: 0.1076397
	speed: 0.0701s/iter; left time: 163.7309s
	iters: 300, epoch: 3 | loss: 0.1310308
	speed: 0.0703s/iter; left time: 157.2797s
Epoch: 3 cost time: 22.58464813232422
Epoch: 3, Steps: 317 | Train Loss: 0.1148163 Vali Loss: 0.1383853 Test Loss: 0.1106974
Validation loss decreased (0.139968 --> 0.138385).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1103891
	speed: 0.0731s/iter; left time: 155.0322s
	iters: 200, epoch: 4 | loss: 0.1045825
	speed: 0.0708s/iter; left time: 142.9721s
	iters: 300, epoch: 4 | loss: 0.1269983
	speed: 0.0701s/iter; left time: 134.6695s
Epoch: 4 cost time: 22.6725172996521
Epoch: 4, Steps: 317 | Train Loss: 0.1097614 Vali Loss: 0.1368301 Test Loss: 0.1089445
Validation loss decreased (0.138385 --> 0.136830).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1299933
	speed: 0.0737s/iter; left time: 132.8110s
	iters: 200, epoch: 5 | loss: 0.1039595
	speed: 0.0700s/iter; left time: 119.1267s
	iters: 300, epoch: 5 | loss: 0.0843861
	speed: 0.0703s/iter; left time: 112.7311s
Epoch: 5 cost time: 22.63576340675354
Epoch: 5, Steps: 317 | Train Loss: 0.1071110 Vali Loss: 0.1364379 Test Loss: 0.1095546
Validation loss decreased (0.136830 --> 0.136438).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1055774
	speed: 0.0731s/iter; left time: 108.5656s
	iters: 200, epoch: 6 | loss: 0.0807213
	speed: 0.0697s/iter; left time: 96.6332s
	iters: 300, epoch: 6 | loss: 0.0990502
	speed: 0.0706s/iter; left time: 90.7308s
Epoch: 6 cost time: 22.574894189834595
Epoch: 6, Steps: 317 | Train Loss: 0.1058976 Vali Loss: 0.1359486 Test Loss: 0.1092953
Validation loss decreased (0.136438 --> 0.135949).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0992850
	speed: 0.0726s/iter; left time: 84.8723s
	iters: 200, epoch: 7 | loss: 0.0893042
	speed: 0.0696s/iter; left time: 74.4078s
	iters: 300, epoch: 7 | loss: 0.1053945
	speed: 0.0696s/iter; left time: 67.4722s
Epoch: 7 cost time: 22.435361623764038
Epoch: 7, Steps: 317 | Train Loss: 0.1054632 Vali Loss: 0.1355859 Test Loss: 0.1092321
Validation loss decreased (0.135949 --> 0.135586).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0867351
	speed: 0.0732s/iter; left time: 62.3522s
	iters: 200, epoch: 8 | loss: 0.1063114
	speed: 0.0698s/iter; left time: 52.4542s
	iters: 300, epoch: 8 | loss: 0.0741389
	speed: 0.0700s/iter; left time: 45.6226s
Epoch: 8 cost time: 22.5216646194458
Epoch: 8, Steps: 317 | Train Loss: 0.1048472 Vali Loss: 0.1357365 Test Loss: 0.1096620
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0921121
	speed: 0.0739s/iter; left time: 39.5113s
	iters: 200, epoch: 9 | loss: 0.0897120
	speed: 0.0695s/iter; left time: 30.2236s
	iters: 300, epoch: 9 | loss: 0.0998120
	speed: 0.0694s/iter; left time: 23.2459s
Epoch: 9 cost time: 22.50835418701172
Epoch: 9, Steps: 317 | Train Loss: 0.1047566 Vali Loss: 0.1356571 Test Loss: 0.1097572
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1009910
	speed: 0.0737s/iter; left time: 16.0721s
	iters: 200, epoch: 10 | loss: 0.1172246
	speed: 0.0704s/iter; left time: 8.3090s
	iters: 300, epoch: 10 | loss: 0.0790367
	speed: 0.0698s/iter; left time: 1.2571s
Epoch: 10 cost time: 22.643296241760254
Epoch: 10, Steps: 317 | Train Loss: 0.1047295 Vali Loss: 0.1355685 Test Loss: 0.1096051
Validation loss decreased (0.135586 --> 0.135569).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1391063
	speed: 0.0980s/iter; left time: 301.8264s
	iters: 200, epoch: 1 | loss: 0.0952290
	speed: 0.0815s/iter; left time: 242.8394s
	iters: 300, epoch: 1 | loss: 0.0875272
	speed: 0.0826s/iter; left time: 237.9030s
Epoch: 1 cost time: 27.74820613861084
Epoch: 1, Steps: 318 | Train Loss: 0.1861869 Vali Loss: 0.1267806 Test Loss: 0.0992293
Validation loss decreased (inf --> 0.126781).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1111313
	speed: 0.0846s/iter; left time: 233.8347s
	iters: 200, epoch: 2 | loss: 0.0893077
	speed: 0.0819s/iter; left time: 218.1554s
	iters: 300, epoch: 2 | loss: 0.1226607
	speed: 0.0815s/iter; left time: 208.8707s
Epoch: 2 cost time: 26.358604431152344
Epoch: 2, Steps: 318 | Train Loss: 0.0989200 Vali Loss: 0.1088941 Test Loss: 0.0840231
Validation loss decreased (0.126781 --> 0.108894).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0981141
	speed: 0.0847s/iter; left time: 207.0360s
	iters: 200, epoch: 3 | loss: 0.1099380
	speed: 0.0824s/iter; left time: 193.1803s
	iters: 300, epoch: 3 | loss: 0.0884522
	speed: 0.0818s/iter; left time: 183.7258s
Epoch: 3 cost time: 26.429722785949707
Epoch: 3, Steps: 318 | Train Loss: 0.0871022 Vali Loss: 0.1099927 Test Loss: 0.0843312
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1070744
	speed: 0.0851s/iter; left time: 181.1075s
	iters: 200, epoch: 4 | loss: 0.0731815
	speed: 0.0820s/iter; left time: 166.3066s
	iters: 300, epoch: 4 | loss: 0.0697838
	speed: 0.0823s/iter; left time: 158.5575s
Epoch: 4 cost time: 26.48941445350647
Epoch: 4, Steps: 318 | Train Loss: 0.0822969 Vali Loss: 0.1066459 Test Loss: 0.0803949
Validation loss decreased (0.108894 --> 0.106646).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1106229
	speed: 0.0857s/iter; left time: 155.1002s
	iters: 200, epoch: 5 | loss: 0.0920585
	speed: 0.0826s/iter; left time: 141.1690s
	iters: 300, epoch: 5 | loss: 0.0781370
	speed: 0.0822s/iter; left time: 132.2415s
Epoch: 5 cost time: 26.606200456619263
Epoch: 5, Steps: 318 | Train Loss: 0.0799273 Vali Loss: 0.1036303 Test Loss: 0.0786098
Validation loss decreased (0.106646 --> 0.103630).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1100591
	speed: 0.0853s/iter; left time: 127.1995s
	iters: 200, epoch: 6 | loss: 0.0903675
	speed: 0.0824s/iter; left time: 114.6373s
	iters: 300, epoch: 6 | loss: 0.0843379
	speed: 0.0828s/iter; left time: 106.8641s
Epoch: 6 cost time: 26.596291065216064
Epoch: 6, Steps: 318 | Train Loss: 0.0786716 Vali Loss: 0.1026536 Test Loss: 0.0788889
Validation loss decreased (0.103630 --> 0.102654).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0783089
	speed: 0.0846s/iter; left time: 99.2917s
	iters: 200, epoch: 7 | loss: 0.0835867
	speed: 0.0825s/iter; left time: 88.5240s
	iters: 300, epoch: 7 | loss: 0.0855459
	speed: 0.0819s/iter; left time: 79.6987s
Epoch: 7 cost time: 26.456804513931274
Epoch: 7, Steps: 318 | Train Loss: 0.0778224 Vali Loss: 0.1040319 Test Loss: 0.0790864
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0817870
	speed: 0.0859s/iter; left time: 73.4718s
	iters: 200, epoch: 8 | loss: 0.0731883
	speed: 0.0830s/iter; left time: 62.6958s
	iters: 300, epoch: 8 | loss: 0.0753478
	speed: 0.0822s/iter; left time: 53.8275s
Epoch: 8 cost time: 26.674142122268677
Epoch: 8, Steps: 318 | Train Loss: 0.0773687 Vali Loss: 0.1030004 Test Loss: 0.0785871
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0586346
	speed: 0.0861s/iter; left time: 46.2151s
	iters: 200, epoch: 9 | loss: 0.0757492
	speed: 0.0826s/iter; left time: 36.0856s
	iters: 300, epoch: 9 | loss: 0.0660059
	speed: 0.0835s/iter; left time: 28.1272s
Epoch: 9 cost time: 26.75063920021057
Epoch: 9, Steps: 318 | Train Loss: 0.0771224 Vali Loss: 0.1031283 Test Loss: 0.0786187
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1933766
	speed: 0.1082s/iter; left time: 333.2467s
	iters: 200, epoch: 1 | loss: 0.1247689
	speed: 0.0935s/iter; left time: 278.7149s
	iters: 300, epoch: 1 | loss: 0.1126141
	speed: 0.0929s/iter; left time: 267.7086s
Epoch: 1 cost time: 31.16699767112732
Epoch: 1, Steps: 318 | Train Loss: 0.1909085 Vali Loss: 0.1321258 Test Loss: 0.0988476
Validation loss decreased (inf --> 0.132126).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.0985222
	speed: 0.0961s/iter; left time: 265.4759s
	iters: 200, epoch: 2 | loss: 0.1692082
	speed: 0.0929s/iter; left time: 247.2632s
	iters: 300, epoch: 2 | loss: 0.0929810
	speed: 0.0926s/iter; left time: 237.2168s
Epoch: 2 cost time: 29.88464617729187
Epoch: 2, Steps: 318 | Train Loss: 0.1065505 Vali Loss: 0.1199822 Test Loss: 0.0890121
Validation loss decreased (0.132126 --> 0.119982).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0829552
	speed: 0.0965s/iter; left time: 235.8671s
	iters: 200, epoch: 3 | loss: 0.0855382
	speed: 0.0951s/iter; left time: 223.0883s
	iters: 300, epoch: 3 | loss: 0.0851094
	speed: 0.0931s/iter; left time: 209.1080s
Epoch: 3 cost time: 30.211191654205322
Epoch: 3, Steps: 318 | Train Loss: 0.0970679 Vali Loss: 0.1143047 Test Loss: 0.0852234
Validation loss decreased (0.119982 --> 0.114305).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1158973
	speed: 0.0950s/iter; left time: 202.0958s
	iters: 200, epoch: 4 | loss: 0.1028734
	speed: 0.0925s/iter; left time: 187.5798s
	iters: 300, epoch: 4 | loss: 0.0684215
	speed: 0.0925s/iter; left time: 178.2756s
Epoch: 4 cost time: 29.733861923217773
Epoch: 4, Steps: 318 | Train Loss: 0.0929391 Vali Loss: 0.1157151 Test Loss: 0.0856367
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0687526
	speed: 0.0947s/iter; left time: 171.3979s
	iters: 200, epoch: 5 | loss: 0.0905783
	speed: 0.0923s/iter; left time: 157.8252s
	iters: 300, epoch: 5 | loss: 0.0799179
	speed: 0.0931s/iter; left time: 149.8442s
Epoch: 5 cost time: 29.744147777557373
Epoch: 5, Steps: 318 | Train Loss: 0.0907997 Vali Loss: 0.1157373 Test Loss: 0.0865613
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0842866
	speed: 0.0955s/iter; left time: 142.3682s
	iters: 200, epoch: 6 | loss: 0.0835981
	speed: 0.0932s/iter; left time: 129.6735s
	iters: 300, epoch: 6 | loss: 0.0756068
	speed: 0.0930s/iter; left time: 120.0193s
Epoch: 6 cost time: 29.903210401535034
Epoch: 6, Steps: 318 | Train Loss: 0.0898951 Vali Loss: 0.1136348 Test Loss: 0.0845659
Validation loss decreased (0.114305 --> 0.113635).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0845186
	speed: 0.0958s/iter; left time: 112.3766s
	iters: 200, epoch: 7 | loss: 0.0776241
	speed: 0.0927s/iter; left time: 99.4276s
	iters: 300, epoch: 7 | loss: 0.0892244
	speed: 0.0928s/iter; left time: 90.3220s
Epoch: 7 cost time: 29.86106586456299
Epoch: 7, Steps: 318 | Train Loss: 0.0891889 Vali Loss: 0.1137745 Test Loss: 0.0852416
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0811133
	speed: 0.0961s/iter; left time: 82.1977s
	iters: 200, epoch: 8 | loss: 0.0737671
	speed: 0.0935s/iter; left time: 70.6021s
	iters: 300, epoch: 8 | loss: 0.1291381
	speed: 0.0931s/iter; left time: 60.9814s
Epoch: 8 cost time: 30.002705335617065
Epoch: 8, Steps: 318 | Train Loss: 0.0888570 Vali Loss: 0.1145548 Test Loss: 0.0846654
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0715099
	speed: 0.0968s/iter; left time: 52.0024s
	iters: 200, epoch: 9 | loss: 0.0803628
	speed: 0.0933s/iter; left time: 40.7836s
	iters: 300, epoch: 9 | loss: 0.0807654
	speed: 0.0932s/iter; left time: 31.4148s
Epoch: 9 cost time: 30.079729318618774
Epoch: 9, Steps: 318 | Train Loss: 0.0888218 Vali Loss: 0.1142953 Test Loss: 0.0846528
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1734541
	speed: 0.0998s/iter; left time: 307.4625s
	iters: 200, epoch: 1 | loss: 0.1327592
	speed: 0.0842s/iter; left time: 250.9724s
	iters: 300, epoch: 1 | loss: 0.0955832
	speed: 0.0867s/iter; left time: 249.7711s
Epoch: 1 cost time: 28.67738914489746
Epoch: 1, Steps: 318 | Train Loss: 0.1854229 Vali Loss: 0.1323797 Test Loss: 0.1010527
Validation loss decreased (inf --> 0.132380).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1264243
	speed: 0.0898s/iter; left time: 248.0791s
	iters: 200, epoch: 2 | loss: 0.0851018
	speed: 0.0862s/iter; left time: 229.6802s
	iters: 300, epoch: 2 | loss: 0.1248618
	speed: 0.0862s/iter; left time: 220.9115s
Epoch: 2 cost time: 27.841623067855835
Epoch: 2, Steps: 318 | Train Loss: 0.1127466 Vali Loss: 0.1263163 Test Loss: 0.0957263
Validation loss decreased (0.132380 --> 0.126316).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0991951
	speed: 0.0900s/iter; left time: 219.9866s
	iters: 200, epoch: 3 | loss: 0.0952156
	speed: 0.0868s/iter; left time: 203.6248s
	iters: 300, epoch: 3 | loss: 0.1013260
	speed: 0.0875s/iter; left time: 196.4875s
Epoch: 3 cost time: 28.068089246749878
Epoch: 3, Steps: 318 | Train Loss: 0.1022493 Vali Loss: 0.1263346 Test Loss: 0.0956629
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0885885
	speed: 0.0899s/iter; left time: 191.2307s
	iters: 200, epoch: 4 | loss: 0.0833685
	speed: 0.0873s/iter; left time: 176.8899s
	iters: 300, epoch: 4 | loss: 0.1126547
	speed: 0.0869s/iter; left time: 167.4311s
Epoch: 4 cost time: 28.049256324768066
Epoch: 4, Steps: 318 | Train Loss: 0.0982161 Vali Loss: 0.1239779 Test Loss: 0.0924861
Validation loss decreased (0.126316 --> 0.123978).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1150678
	speed: 0.0913s/iter; left time: 165.1294s
	iters: 200, epoch: 5 | loss: 0.0891607
	speed: 0.0875s/iter; left time: 149.5161s
	iters: 300, epoch: 5 | loss: 0.0960315
	speed: 0.0872s/iter; left time: 140.3247s
Epoch: 5 cost time: 28.329184770584106
Epoch: 5, Steps: 318 | Train Loss: 0.0957396 Vali Loss: 0.1236874 Test Loss: 0.0928885
Validation loss decreased (0.123978 --> 0.123687).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0827821
	speed: 0.0913s/iter; left time: 136.0672s
	iters: 200, epoch: 6 | loss: 0.1280720
	speed: 0.0883s/iter; left time: 122.8213s
	iters: 300, epoch: 6 | loss: 0.1050730
	speed: 0.0883s/iter; left time: 113.9569s
Epoch: 6 cost time: 28.42287826538086
Epoch: 6, Steps: 318 | Train Loss: 0.0945565 Vali Loss: 0.1243621 Test Loss: 0.0929882
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1016492
	speed: 0.0913s/iter; left time: 107.1090s
	iters: 200, epoch: 7 | loss: 0.1138275
	speed: 0.0877s/iter; left time: 94.1528s
	iters: 300, epoch: 7 | loss: 0.1082158
	speed: 0.0874s/iter; left time: 85.0267s
Epoch: 7 cost time: 28.295811414718628
Epoch: 7, Steps: 318 | Train Loss: 0.0939609 Vali Loss: 0.1238877 Test Loss: 0.0934819
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1083460
	speed: 0.0924s/iter; left time: 79.0214s
	iters: 200, epoch: 8 | loss: 0.0791036
	speed: 0.0883s/iter; left time: 66.6600s
	iters: 300, epoch: 8 | loss: 0.1053624
	speed: 0.0882s/iter; left time: 57.7526s
Epoch: 8 cost time: 28.54748558998108
Epoch: 8, Steps: 318 | Train Loss: 0.0938044 Vali Loss: 0.1233581 Test Loss: 0.0931800
Validation loss decreased (0.123687 --> 0.123358).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1008413
	speed: 0.0915s/iter; left time: 49.1100s
	iters: 200, epoch: 9 | loss: 0.0841303
	speed: 0.0885s/iter; left time: 38.6689s
	iters: 300, epoch: 9 | loss: 0.1098913
	speed: 0.0880s/iter; left time: 29.6657s
Epoch: 9 cost time: 28.440271139144897
Epoch: 9, Steps: 318 | Train Loss: 0.0933062 Vali Loss: 0.1227794 Test Loss: 0.0930541
Validation loss decreased (0.123358 --> 0.122779).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1053438
	speed: 0.0915s/iter; left time: 20.0334s
	iters: 200, epoch: 10 | loss: 0.0915312
	speed: 0.0876s/iter; left time: 10.4201s
	iters: 300, epoch: 10 | loss: 0.0759641
	speed: 0.0882s/iter; left time: 1.6753s
Epoch: 10 cost time: 28.370790719985962
Epoch: 10, Steps: 318 | Train Loss: 0.0936471 Vali Loss: 0.1240989 Test Loss: 0.0931754
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1506869
	speed: 0.1040s/iter; left time: 320.4357s
	iters: 200, epoch: 1 | loss: 0.1667717
	speed: 0.0902s/iter; left time: 268.9761s
	iters: 300, epoch: 1 | loss: 0.1279146
	speed: 0.0908s/iter; left time: 261.6793s
Epoch: 1 cost time: 30.20880699157715
Epoch: 1, Steps: 318 | Train Loss: 0.1802197 Vali Loss: 0.1453796 Test Loss: 0.1111614
Validation loss decreased (inf --> 0.145380).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1232555
	speed: 0.0957s/iter; left time: 264.3946s
	iters: 200, epoch: 2 | loss: 0.1010459
	speed: 0.0940s/iter; left time: 250.1941s
	iters: 300, epoch: 2 | loss: 0.1275635
	speed: 0.0934s/iter; left time: 239.4898s
Epoch: 2 cost time: 30.053505420684814
Epoch: 2, Steps: 318 | Train Loss: 0.1177013 Vali Loss: 0.1344828 Test Loss: 0.1001923
Validation loss decreased (0.145380 --> 0.134483).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0916493
	speed: 0.0952s/iter; left time: 232.8106s
	iters: 200, epoch: 3 | loss: 0.1213592
	speed: 0.0922s/iter; left time: 216.2634s
	iters: 300, epoch: 3 | loss: 0.1067775
	speed: 0.0924s/iter; left time: 207.3335s
Epoch: 3 cost time: 29.704176425933838
Epoch: 3, Steps: 318 | Train Loss: 0.1083037 Vali Loss: 0.1322366 Test Loss: 0.0991448
Validation loss decreased (0.134483 --> 0.132237).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1173589
	speed: 0.0953s/iter; left time: 202.7114s
	iters: 200, epoch: 4 | loss: 0.1081976
	speed: 0.0933s/iter; left time: 189.0710s
	iters: 300, epoch: 4 | loss: 0.1037976
	speed: 0.0934s/iter; left time: 179.9147s
Epoch: 4 cost time: 29.92046546936035
Epoch: 4, Steps: 318 | Train Loss: 0.1043751 Vali Loss: 0.1302173 Test Loss: 0.0990850
Validation loss decreased (0.132237 --> 0.130217).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0822238
	speed: 0.0960s/iter; left time: 173.7392s
	iters: 200, epoch: 5 | loss: 0.1107859
	speed: 0.0933s/iter; left time: 159.4968s
	iters: 300, epoch: 5 | loss: 0.0978996
	speed: 0.0924s/iter; left time: 148.6015s
Epoch: 5 cost time: 29.913979291915894
Epoch: 5, Steps: 318 | Train Loss: 0.1024160 Vali Loss: 0.1312857 Test Loss: 0.0983892
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0998005
	speed: 0.0966s/iter; left time: 144.0471s
	iters: 200, epoch: 6 | loss: 0.1095923
	speed: 0.0935s/iter; left time: 130.1060s
	iters: 300, epoch: 6 | loss: 0.1165228
	speed: 0.0931s/iter; left time: 120.2073s
Epoch: 6 cost time: 30.063847303390503
Epoch: 6, Steps: 318 | Train Loss: 0.1012619 Vali Loss: 0.1329051 Test Loss: 0.0985221
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0929705
	speed: 0.0962s/iter; left time: 112.8054s
	iters: 200, epoch: 7 | loss: 0.1006450
	speed: 0.0930s/iter; left time: 99.8131s
	iters: 300, epoch: 7 | loss: 0.1035905
	speed: 0.0930s/iter; left time: 90.4637s
Epoch: 7 cost time: 29.951900243759155
Epoch: 7, Steps: 318 | Train Loss: 0.1007282 Vali Loss: 0.1306885 Test Loss: 0.0976341
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1610183
	speed: 0.1130s/iter; left time: 347.1591s
	iters: 200, epoch: 1 | loss: 0.1855318
	speed: 0.0986s/iter; left time: 293.0248s
	iters: 300, epoch: 1 | loss: 0.1416577
	speed: 0.0991s/iter; left time: 284.4619s
Epoch: 1 cost time: 32.809818506240845
Epoch: 1, Steps: 317 | Train Loss: 0.2077175 Vali Loss: 0.1500665 Test Loss: 0.1105853
Validation loss decreased (inf --> 0.150067).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1265651
	speed: 0.1031s/iter; left time: 283.9543s
	iters: 200, epoch: 2 | loss: 0.1061609
	speed: 0.1003s/iter; left time: 266.2601s
	iters: 300, epoch: 2 | loss: 0.1120877
	speed: 0.1010s/iter; left time: 258.0535s
Epoch: 2 cost time: 32.23615384101868
Epoch: 2, Steps: 317 | Train Loss: 0.1222543 Vali Loss: 0.1403573 Test Loss: 0.1084857
Validation loss decreased (0.150067 --> 0.140357).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1011123
	speed: 0.1025s/iter; left time: 249.8303s
	iters: 200, epoch: 3 | loss: 0.1081649
	speed: 0.1004s/iter; left time: 234.6859s
	iters: 300, epoch: 3 | loss: 0.1320487
	speed: 0.1002s/iter; left time: 224.1018s
Epoch: 3 cost time: 32.06576681137085
Epoch: 3, Steps: 317 | Train Loss: 0.1140021 Vali Loss: 0.1391151 Test Loss: 0.1109992
Validation loss decreased (0.140357 --> 0.139115).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1116127
	speed: 0.1040s/iter; left time: 220.4647s
	iters: 200, epoch: 4 | loss: 0.1024271
	speed: 0.1010s/iter; left time: 204.0714s
	iters: 300, epoch: 4 | loss: 0.1258723
	speed: 0.1005s/iter; left time: 193.0008s
Epoch: 4 cost time: 32.341891050338745
Epoch: 4, Steps: 317 | Train Loss: 0.1087798 Vali Loss: 0.1371216 Test Loss: 0.1083978
Validation loss decreased (0.139115 --> 0.137122).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1304069
	speed: 0.1030s/iter; left time: 185.6267s
	iters: 200, epoch: 5 | loss: 0.1026950
	speed: 0.1008s/iter; left time: 171.6235s
	iters: 300, epoch: 5 | loss: 0.0837186
	speed: 0.1003s/iter; left time: 160.8335s
Epoch: 5 cost time: 32.16858196258545
Epoch: 5, Steps: 317 | Train Loss: 0.1060457 Vali Loss: 0.1374394 Test Loss: 0.1106707
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1025474
	speed: 0.1032s/iter; left time: 153.3569s
	iters: 200, epoch: 6 | loss: 0.0819438
	speed: 0.1010s/iter; left time: 140.0287s
	iters: 300, epoch: 6 | loss: 0.0993060
	speed: 0.1006s/iter; left time: 129.4264s
Epoch: 6 cost time: 32.27200531959534
Epoch: 6, Steps: 317 | Train Loss: 0.1048314 Vali Loss: 0.1372774 Test Loss: 0.1102086
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0986905
	speed: 0.1017s/iter; left time: 118.9329s
	iters: 200, epoch: 7 | loss: 0.0887430
	speed: 0.1001s/iter; left time: 106.9809s
	iters: 300, epoch: 7 | loss: 0.1051968
	speed: 0.1000s/iter; left time: 96.9263s
Epoch: 7 cost time: 31.95184087753296
Epoch: 7, Steps: 317 | Train Loss: 0.1043943 Vali Loss: 0.1366435 Test Loss: 0.1100538
Validation loss decreased (0.137122 --> 0.136644).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0891345
	speed: 0.1029s/iter; left time: 87.6507s
	iters: 200, epoch: 8 | loss: 0.1040170
	speed: 0.1014s/iter; left time: 76.2502s
	iters: 300, epoch: 8 | loss: 0.0728723
	speed: 0.1012s/iter; left time: 65.9997s
Epoch: 8 cost time: 32.33653974533081
Epoch: 8, Steps: 317 | Train Loss: 0.1038153 Vali Loss: 0.1366522 Test Loss: 0.1103240
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0918325
	speed: 0.1038s/iter; left time: 55.5269s
	iters: 200, epoch: 9 | loss: 0.0890449
	speed: 0.1010s/iter; left time: 43.9248s
	iters: 300, epoch: 9 | loss: 0.0998814
	speed: 0.1003s/iter; left time: 33.6150s
Epoch: 9 cost time: 32.28028082847595
Epoch: 9, Steps: 317 | Train Loss: 0.1036895 Vali Loss: 0.1367196 Test Loss: 0.1104790
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1003618
	speed: 0.1033s/iter; left time: 22.5300s
	iters: 200, epoch: 10 | loss: 0.1164229
	speed: 0.0999s/iter; left time: 11.7933s
	iters: 300, epoch: 10 | loss: 0.0756326
	speed: 0.1001s/iter; left time: 1.8022s
Epoch: 10 cost time: 32.11043739318848
Epoch: 10, Steps: 317 | Train Loss: 0.1037587 Vali Loss: 0.1366557 Test Loss: 0.1104106
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1257680
	speed: 0.1173s/iter; left time: 361.3485s
	iters: 200, epoch: 1 | loss: 0.0927872
	speed: 0.1017s/iter; left time: 303.2572s
	iters: 300, epoch: 1 | loss: 0.0898122
	speed: 0.1017s/iter; left time: 292.8843s
Epoch: 1 cost time: 33.951605796813965
Epoch: 1, Steps: 318 | Train Loss: 0.1852244 Vali Loss: 0.1256445 Test Loss: 0.0978523
Validation loss decreased (inf --> 0.125644).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1127013
	speed: 0.1041s/iter; left time: 287.4937s
	iters: 200, epoch: 2 | loss: 0.0875894
	speed: 0.1009s/iter; left time: 268.7397s
	iters: 300, epoch: 2 | loss: 0.1243606
	speed: 0.1021s/iter; left time: 261.6198s
Epoch: 2 cost time: 32.60202169418335
Epoch: 2, Steps: 318 | Train Loss: 0.0987661 Vali Loss: 0.1089794 Test Loss: 0.0838216
Validation loss decreased (0.125644 --> 0.108979).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0976045
	speed: 0.1047s/iter; left time: 255.9495s
	iters: 200, epoch: 3 | loss: 0.1098718
	speed: 0.1022s/iter; left time: 239.7141s
	iters: 300, epoch: 3 | loss: 0.0885973
	speed: 0.1030s/iter; left time: 231.2347s
Epoch: 3 cost time: 32.89219117164612
Epoch: 3, Steps: 318 | Train Loss: 0.0869376 Vali Loss: 0.1097524 Test Loss: 0.0839429
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1059315
	speed: 0.1049s/iter; left time: 223.0292s
	iters: 200, epoch: 4 | loss: 0.0729742
	speed: 0.1020s/iter; left time: 206.6640s
	iters: 300, epoch: 4 | loss: 0.0689074
	speed: 0.1019s/iter; left time: 196.3602s
Epoch: 4 cost time: 32.77356696128845
Epoch: 4, Steps: 318 | Train Loss: 0.0821151 Vali Loss: 0.1070457 Test Loss: 0.0805537
Validation loss decreased (0.108979 --> 0.107046).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1092576
	speed: 0.1050s/iter; left time: 189.9224s
	iters: 200, epoch: 5 | loss: 0.0917606
	speed: 0.1023s/iter; left time: 174.8156s
	iters: 300, epoch: 5 | loss: 0.0791483
	speed: 0.1018s/iter; left time: 163.8231s
Epoch: 5 cost time: 32.8551070690155
Epoch: 5, Steps: 318 | Train Loss: 0.0797211 Vali Loss: 0.1040570 Test Loss: 0.0787851
Validation loss decreased (0.107046 --> 0.104057).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1084108
	speed: 0.1055s/iter; left time: 157.3350s
	iters: 200, epoch: 6 | loss: 0.0909170
	speed: 0.1026s/iter; left time: 142.7380s
	iters: 300, epoch: 6 | loss: 0.0845231
	speed: 0.1032s/iter; left time: 133.2472s
Epoch: 6 cost time: 33.08475923538208
Epoch: 6, Steps: 318 | Train Loss: 0.0784857 Vali Loss: 0.1030584 Test Loss: 0.0790401
Validation loss decreased (0.104057 --> 0.103058).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0781374
	speed: 0.1059s/iter; left time: 124.1817s
	iters: 200, epoch: 7 | loss: 0.0830515
	speed: 0.1030s/iter; left time: 110.4713s
	iters: 300, epoch: 7 | loss: 0.0862474
	speed: 0.1019s/iter; left time: 99.1296s
Epoch: 7 cost time: 32.982861280441284
Epoch: 7, Steps: 318 | Train Loss: 0.0776226 Vali Loss: 0.1044545 Test Loss: 0.0792503
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0809012
	speed: 0.1050s/iter; left time: 89.8106s
	iters: 200, epoch: 8 | loss: 0.0729827
	speed: 0.1027s/iter; left time: 77.5190s
	iters: 300, epoch: 8 | loss: 0.0758922
	speed: 0.1025s/iter; left time: 67.1614s
Epoch: 8 cost time: 32.93366551399231
Epoch: 8, Steps: 318 | Train Loss: 0.0771655 Vali Loss: 0.1033846 Test Loss: 0.0787391
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0590342
	speed: 0.1058s/iter; left time: 56.8252s
	iters: 200, epoch: 9 | loss: 0.0770182
	speed: 0.1024s/iter; left time: 44.7464s
	iters: 300, epoch: 9 | loss: 0.0660381
	speed: 0.1033s/iter; left time: 34.8148s
Epoch: 9 cost time: 33.06810975074768
Epoch: 9, Steps: 318 | Train Loss: 0.0768806 Vali Loss: 0.1035088 Test Loss: 0.0787660
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1942782
	speed: 0.1277s/iter; left time: 393.5919s
	iters: 200, epoch: 1 | loss: 0.1288162
	speed: 0.1133s/iter; left time: 337.7352s
	iters: 300, epoch: 1 | loss: 0.1114131
	speed: 0.1138s/iter; left time: 327.9445s
Epoch: 1 cost time: 37.59434938430786
Epoch: 1, Steps: 318 | Train Loss: 0.1953631 Vali Loss: 0.1316886 Test Loss: 0.0993352
Validation loss decreased (inf --> 0.131689).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1006387
	speed: 0.1166s/iter; left time: 322.1703s
	iters: 200, epoch: 2 | loss: 0.1694137
	speed: 0.1138s/iter; left time: 303.0103s
	iters: 300, epoch: 2 | loss: 0.0974342
	speed: 0.1145s/iter; left time: 293.4912s
Epoch: 2 cost time: 36.63600564002991
Epoch: 2, Steps: 318 | Train Loss: 0.1066849 Vali Loss: 0.1218995 Test Loss: 0.0923836
Validation loss decreased (0.131689 --> 0.121900).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0861017
	speed: 0.1164s/iter; left time: 284.6256s
	iters: 200, epoch: 3 | loss: 0.0854416
	speed: 0.1145s/iter; left time: 268.4872s
	iters: 300, epoch: 3 | loss: 0.0779510
	speed: 0.1145s/iter; left time: 256.9482s
Epoch: 3 cost time: 36.663817167282104
Epoch: 3, Steps: 318 | Train Loss: 0.0971357 Vali Loss: 0.1167797 Test Loss: 0.0880681
Validation loss decreased (0.121900 --> 0.116780).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1130286
	speed: 0.1164s/iter; left time: 247.5922s
	iters: 200, epoch: 4 | loss: 0.1077085
	speed: 0.1153s/iter; left time: 233.7066s
	iters: 300, epoch: 4 | loss: 0.0669971
	speed: 0.1152s/iter; left time: 221.9040s
Epoch: 4 cost time: 36.82797169685364
Epoch: 4, Steps: 318 | Train Loss: 0.0925747 Vali Loss: 0.1174503 Test Loss: 0.0877576
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0687443
	speed: 0.1167s/iter; left time: 211.0375s
	iters: 200, epoch: 5 | loss: 0.0833898
	speed: 0.1135s/iter; left time: 193.9859s
	iters: 300, epoch: 5 | loss: 0.0753016
	speed: 0.1140s/iter; left time: 183.4716s
Epoch: 5 cost time: 36.54981184005737
Epoch: 5, Steps: 318 | Train Loss: 0.0901183 Vali Loss: 0.1171694 Test Loss: 0.0883557
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0833203
	speed: 0.1173s/iter; left time: 174.8739s
	iters: 200, epoch: 6 | loss: 0.0785356
	speed: 0.1137s/iter; left time: 158.1335s
	iters: 300, epoch: 6 | loss: 0.0738093
	speed: 0.1142s/iter; left time: 147.4500s
Epoch: 6 cost time: 36.64218878746033
Epoch: 6, Steps: 318 | Train Loss: 0.0891567 Vali Loss: 0.1154985 Test Loss: 0.0871139
Validation loss decreased (0.116780 --> 0.115498).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0824626
	speed: 0.1178s/iter; left time: 138.2310s
	iters: 200, epoch: 7 | loss: 0.0801629
	speed: 0.1145s/iter; left time: 122.8373s
	iters: 300, epoch: 7 | loss: 0.0881995
	speed: 0.1150s/iter; left time: 111.9373s
Epoch: 7 cost time: 36.864529848098755
Epoch: 7, Steps: 318 | Train Loss: 0.0885179 Vali Loss: 0.1154385 Test Loss: 0.0873652
Validation loss decreased (0.115498 --> 0.115438).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0771725
	speed: 0.1168s/iter; left time: 99.8560s
	iters: 200, epoch: 8 | loss: 0.0762824
	speed: 0.1134s/iter; left time: 85.5909s
	iters: 300, epoch: 8 | loss: 0.1109309
	speed: 0.1140s/iter; left time: 74.6652s
Epoch: 8 cost time: 36.53356909751892
Epoch: 8, Steps: 318 | Train Loss: 0.0880368 Vali Loss: 0.1162754 Test Loss: 0.0870885
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0713460
	speed: 0.1150s/iter; left time: 61.7579s
	iters: 200, epoch: 9 | loss: 0.0786982
	speed: 0.1125s/iter; left time: 49.1744s
	iters: 300, epoch: 9 | loss: 0.0815219
	speed: 0.1130s/iter; left time: 38.0788s
Epoch: 9 cost time: 36.167725563049316
Epoch: 9, Steps: 318 | Train Loss: 0.0879710 Vali Loss: 0.1160451 Test Loss: 0.0870363
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0795176
	speed: 0.1164s/iter; left time: 25.4913s
	iters: 200, epoch: 10 | loss: 0.0732637
	speed: 0.1139s/iter; left time: 13.5508s
	iters: 300, epoch: 10 | loss: 0.0914385
	speed: 0.1144s/iter; left time: 2.1744s
Epoch: 10 cost time: 36.63460087776184
Epoch: 10, Steps: 318 | Train Loss: 0.0878033 Vali Loss: 0.1164483 Test Loss: 0.0870523
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1730271
	speed: 0.1206s/iter; left time: 371.5320s
	iters: 200, epoch: 1 | loss: 0.1325597
	speed: 0.1083s/iter; left time: 322.8464s
	iters: 300, epoch: 1 | loss: 0.0954429
	speed: 0.1095s/iter; left time: 315.3943s
Epoch: 1 cost time: 35.8563117980957
Epoch: 1, Steps: 318 | Train Loss: 0.1856799 Vali Loss: 0.1325244 Test Loss: 0.1011408
Validation loss decreased (inf --> 0.132524).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1260510
	speed: 0.1142s/iter; left time: 315.6535s
	iters: 200, epoch: 2 | loss: 0.0848749
	speed: 0.1103s/iter; left time: 293.7789s
	iters: 300, epoch: 2 | loss: 0.1248612
	speed: 0.1107s/iter; left time: 283.7565s
Epoch: 2 cost time: 35.602389097213745
Epoch: 2, Steps: 318 | Train Loss: 0.1127061 Vali Loss: 0.1262978 Test Loss: 0.0957497
Validation loss decreased (0.132524 --> 0.126298).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0989610
	speed: 0.1147s/iter; left time: 280.3744s
	iters: 200, epoch: 3 | loss: 0.0952850
	speed: 0.1108s/iter; left time: 259.8182s
	iters: 300, epoch: 3 | loss: 0.1014950
	speed: 0.1105s/iter; left time: 248.0998s
Epoch: 3 cost time: 35.68766260147095
Epoch: 3, Steps: 318 | Train Loss: 0.1022250 Vali Loss: 0.1262296 Test Loss: 0.0956570
Validation loss decreased (0.126298 --> 0.126230).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0884680
	speed: 0.1149s/iter; left time: 244.3574s
	iters: 200, epoch: 4 | loss: 0.0835417
	speed: 0.1104s/iter; left time: 223.7051s
	iters: 300, epoch: 4 | loss: 0.1128219
	speed: 0.1105s/iter; left time: 212.9250s
Epoch: 4 cost time: 35.646109104156494
Epoch: 4, Steps: 318 | Train Loss: 0.0981942 Vali Loss: 0.1238607 Test Loss: 0.0924887
Validation loss decreased (0.126230 --> 0.123861).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1146194
	speed: 0.1134s/iter; left time: 205.1962s
	iters: 200, epoch: 5 | loss: 0.0891232
	speed: 0.1106s/iter; left time: 188.9925s
	iters: 300, epoch: 5 | loss: 0.0960267
	speed: 0.1103s/iter; left time: 177.4495s
Epoch: 5 cost time: 35.489264249801636
Epoch: 5, Steps: 318 | Train Loss: 0.0957227 Vali Loss: 0.1235943 Test Loss: 0.0929413
Validation loss decreased (0.123861 --> 0.123594).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0827236
	speed: 0.1135s/iter; left time: 169.2221s
	iters: 200, epoch: 6 | loss: 0.1285736
	speed: 0.1103s/iter; left time: 153.3843s
	iters: 300, epoch: 6 | loss: 0.1052519
	speed: 0.1106s/iter; left time: 142.8469s
Epoch: 6 cost time: 35.493537187576294
Epoch: 6, Steps: 318 | Train Loss: 0.0945474 Vali Loss: 0.1242785 Test Loss: 0.0930506
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1015926
	speed: 0.1126s/iter; left time: 132.0528s
	iters: 200, epoch: 7 | loss: 0.1135048
	speed: 0.1105s/iter; left time: 118.5426s
	iters: 300, epoch: 7 | loss: 0.1083944
	speed: 0.1102s/iter; left time: 107.2103s
Epoch: 7 cost time: 35.39345860481262
Epoch: 7, Steps: 318 | Train Loss: 0.0939479 Vali Loss: 0.1238146 Test Loss: 0.0935546
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1081114
	speed: 0.1125s/iter; left time: 96.1456s
	iters: 200, epoch: 8 | loss: 0.0792176
	speed: 0.1101s/iter; left time: 83.1257s
	iters: 300, epoch: 8 | loss: 0.1052194
	speed: 0.1096s/iter; left time: 71.7897s
Epoch: 8 cost time: 35.28809833526611
Epoch: 8, Steps: 318 | Train Loss: 0.0937988 Vali Loss: 0.1232686 Test Loss: 0.0932448
Validation loss decreased (0.123594 --> 0.123269).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1006565
	speed: 0.1126s/iter; left time: 60.4582s
	iters: 200, epoch: 9 | loss: 0.0840326
	speed: 0.1095s/iter; left time: 47.8696s
	iters: 300, epoch: 9 | loss: 0.1098801
	speed: 0.1094s/iter; left time: 36.8746s
Epoch: 9 cost time: 35.20411825180054
Epoch: 9, Steps: 318 | Train Loss: 0.0933015 Vali Loss: 0.1226773 Test Loss: 0.0931149
Validation loss decreased (0.123269 --> 0.122677).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1052483
	speed: 0.1126s/iter; left time: 24.6655s
	iters: 200, epoch: 10 | loss: 0.0912644
	speed: 0.1100s/iter; left time: 13.0853s
	iters: 300, epoch: 10 | loss: 0.0759102
	speed: 0.1109s/iter; left time: 2.1074s
Epoch: 10 cost time: 35.4263916015625
Epoch: 10, Steps: 318 | Train Loss: 0.0936327 Vali Loss: 0.1240103 Test Loss: 0.0932407
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1511801
	speed: 0.1252s/iter; left time: 385.7927s
	iters: 200, epoch: 1 | loss: 0.1671216
	speed: 0.1138s/iter; left time: 339.2348s
	iters: 300, epoch: 1 | loss: 0.1279344
	speed: 0.1165s/iter; left time: 335.5699s
Epoch: 1 cost time: 37.70410227775574
Epoch: 1, Steps: 318 | Train Loss: 0.1801819 Vali Loss: 0.1453186 Test Loss: 0.1116654
Validation loss decreased (inf --> 0.145319).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1228217
	speed: 0.1196s/iter; left time: 330.5318s
	iters: 200, epoch: 2 | loss: 0.1006745
	speed: 0.1173s/iter; left time: 312.2606s
	iters: 300, epoch: 2 | loss: 0.1278552
	speed: 0.1172s/iter; left time: 300.3614s
Epoch: 2 cost time: 37.610464096069336
Epoch: 2, Steps: 318 | Train Loss: 0.1175850 Vali Loss: 0.1344411 Test Loss: 0.1001659
Validation loss decreased (0.145319 --> 0.134441).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0917217
	speed: 0.1200s/iter; left time: 293.4961s
	iters: 200, epoch: 3 | loss: 0.1212956
	speed: 0.1173s/iter; left time: 275.1320s
	iters: 300, epoch: 3 | loss: 0.1068022
	speed: 0.1176s/iter; left time: 263.9825s
Epoch: 3 cost time: 37.69463086128235
Epoch: 3, Steps: 318 | Train Loss: 0.1082774 Vali Loss: 0.1322769 Test Loss: 0.0992061
Validation loss decreased (0.134441 --> 0.132277).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1173320
	speed: 0.1202s/iter; left time: 255.6292s
	iters: 200, epoch: 4 | loss: 0.1081980
	speed: 0.1172s/iter; left time: 237.5932s
	iters: 300, epoch: 4 | loss: 0.1033613
	speed: 0.1178s/iter; left time: 226.9501s
Epoch: 4 cost time: 37.73869323730469
Epoch: 4, Steps: 318 | Train Loss: 0.1043698 Vali Loss: 0.1302088 Test Loss: 0.0991432
Validation loss decreased (0.132277 --> 0.130209).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0822149
	speed: 0.1206s/iter; left time: 218.0761s
	iters: 200, epoch: 5 | loss: 0.1109780
	speed: 0.1170s/iter; left time: 200.0271s
	iters: 300, epoch: 5 | loss: 0.0978034
	speed: 0.1179s/iter; left time: 189.6981s
Epoch: 5 cost time: 37.750630378723145
Epoch: 5, Steps: 318 | Train Loss: 0.1024215 Vali Loss: 0.1312969 Test Loss: 0.0984224
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0999773
	speed: 0.1216s/iter; left time: 181.2956s
	iters: 200, epoch: 6 | loss: 0.1095474
	speed: 0.1173s/iter; left time: 163.1020s
	iters: 300, epoch: 6 | loss: 0.1164731
	speed: 0.1180s/iter; left time: 152.3827s
Epoch: 6 cost time: 37.91674184799194
Epoch: 6, Steps: 318 | Train Loss: 0.1012684 Vali Loss: 0.1329165 Test Loss: 0.0985326
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0930674
	speed: 0.1210s/iter; left time: 141.8984s
	iters: 200, epoch: 7 | loss: 0.1005693
	speed: 0.1183s/iter; left time: 126.9381s
	iters: 300, epoch: 7 | loss: 0.1036865
	speed: 0.1190s/iter; left time: 115.8120s
Epoch: 7 cost time: 38.05333638191223
Epoch: 7, Steps: 318 | Train Loss: 0.1007389 Vali Loss: 0.1306797 Test Loss: 0.0976378
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=2, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1528460
	speed: 0.1219s/iter; left time: 374.3657s
	iters: 200, epoch: 1 | loss: 0.1849896
	speed: 0.1102s/iter; left time: 327.4058s
	iters: 300, epoch: 1 | loss: 0.1420146
	speed: 0.1114s/iter; left time: 319.8744s
Epoch: 1 cost time: 36.31135058403015
Epoch: 1, Steps: 317 | Train Loss: 0.1993988 Vali Loss: 0.1524372 Test Loss: 0.1120866
Validation loss decreased (inf --> 0.152437).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1236238
	speed: 0.1150s/iter; left time: 316.6415s
	iters: 200, epoch: 2 | loss: 0.1053145
	speed: 0.1122s/iter; left time: 297.8707s
	iters: 300, epoch: 2 | loss: 0.1073680
	speed: 0.1131s/iter; left time: 288.7665s
Epoch: 2 cost time: 36.04462480545044
Epoch: 2, Steps: 317 | Train Loss: 0.1211556 Vali Loss: 0.1403995 Test Loss: 0.1082044
Validation loss decreased (0.152437 --> 0.140399).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1037673
	speed: 0.1151s/iter; left time: 280.4526s
	iters: 200, epoch: 3 | loss: 0.1110298
	speed: 0.1140s/iter; left time: 266.3938s
	iters: 300, epoch: 3 | loss: 0.1265877
	speed: 0.1129s/iter; left time: 252.4965s
Epoch: 3 cost time: 36.19201612472534
Epoch: 3, Steps: 317 | Train Loss: 0.1137116 Vali Loss: 0.1388474 Test Loss: 0.1088717
Validation loss decreased (0.140399 --> 0.138847).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1145720
	speed: 0.1158s/iter; left time: 245.4241s
	iters: 200, epoch: 4 | loss: 0.1009433
	speed: 0.1122s/iter; left time: 226.6612s
	iters: 300, epoch: 4 | loss: 0.1291286
	speed: 0.1119s/iter; left time: 214.8017s
Epoch: 4 cost time: 35.97328305244446
Epoch: 4, Steps: 317 | Train Loss: 0.1096936 Vali Loss: 0.1352968 Test Loss: 0.1038272
Validation loss decreased (0.138847 --> 0.135297).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1354699
	speed: 0.1152s/iter; left time: 207.7749s
	iters: 200, epoch: 5 | loss: 0.1032442
	speed: 0.1119s/iter; left time: 190.5420s
	iters: 300, epoch: 5 | loss: 0.0866047
	speed: 0.1126s/iter; left time: 180.5628s
Epoch: 5 cost time: 35.989694595336914
Epoch: 5, Steps: 317 | Train Loss: 0.1075186 Vali Loss: 0.1360341 Test Loss: 0.1044608
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1114814
	speed: 0.1150s/iter; left time: 170.8582s
	iters: 200, epoch: 6 | loss: 0.0817792
	speed: 0.1117s/iter; left time: 154.7579s
	iters: 300, epoch: 6 | loss: 0.1030234
	speed: 0.1122s/iter; left time: 144.2497s
Epoch: 6 cost time: 35.845911264419556
Epoch: 6, Steps: 317 | Train Loss: 0.1062533 Vali Loss: 0.1356899 Test Loss: 0.1046819
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0973180
	speed: 0.1154s/iter; left time: 134.9508s
	iters: 200, epoch: 7 | loss: 0.0862896
	speed: 0.1118s/iter; left time: 119.5097s
	iters: 300, epoch: 7 | loss: 0.1056169
	speed: 0.1123s/iter; left time: 108.8241s
Epoch: 7 cost time: 35.9335355758667
Epoch: 7, Steps: 317 | Train Loss: 0.1060033 Vali Loss: 0.1350741 Test Loss: 0.1038586
Validation loss decreased (0.135297 --> 0.135074).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0851765
	speed: 0.1146s/iter; left time: 97.6517s
	iters: 200, epoch: 8 | loss: 0.1077686
	speed: 0.1126s/iter; left time: 84.6828s
	iters: 300, epoch: 8 | loss: 0.0765646
	speed: 0.1138s/iter; left time: 74.2140s
Epoch: 8 cost time: 36.10187220573425
Epoch: 8, Steps: 317 | Train Loss: 0.1053760 Vali Loss: 0.1352229 Test Loss: 0.1043226
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0843961
	speed: 0.1143s/iter; left time: 61.1584s
	iters: 200, epoch: 9 | loss: 0.0975668
	speed: 0.1122s/iter; left time: 48.8202s
	iters: 300, epoch: 9 | loss: 0.1067040
	speed: 0.1115s/iter; left time: 37.3647s
Epoch: 9 cost time: 35.785319328308105
Epoch: 9, Steps: 317 | Train Loss: 0.1052375 Vali Loss: 0.1350295 Test Loss: 0.1044547
Validation loss decreased (0.135074 --> 0.135030).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0921446
	speed: 0.1142s/iter; left time: 24.8987s
	iters: 200, epoch: 10 | loss: 0.1158611
	speed: 0.1108s/iter; left time: 13.0802s
	iters: 300, epoch: 10 | loss: 0.0780671
	speed: 0.1123s/iter; left time: 2.0214s
Epoch: 10 cost time: 35.74469995498657
Epoch: 10, Steps: 317 | Train Loss: 0.1052428 Vali Loss: 0.1350715 Test Loss: 0.1042448
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el2_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2669131
	speed: 0.0367s/iter; left time: 113.2146s
	iters: 200, epoch: 1 | loss: 0.1382019
	speed: 0.0217s/iter; left time: 64.5650s
	iters: 300, epoch: 1 | loss: 0.1371737
	speed: 0.0215s/iter; left time: 61.8503s
Epoch: 1 cost time: 8.466217041015625
Epoch: 1, Steps: 318 | Train Loss: 0.3003713 Vali Loss: 0.1481337 Test Loss: 0.1486403
Validation loss decreased (inf --> 0.148134).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1467154
	speed: 0.0240s/iter; left time: 66.3567s
	iters: 200, epoch: 2 | loss: 0.1148885
	speed: 0.0214s/iter; left time: 57.0808s
	iters: 300, epoch: 2 | loss: 0.0993868
	speed: 0.0213s/iter; left time: 54.6261s
Epoch: 2 cost time: 7.095986366271973
Epoch: 2, Steps: 318 | Train Loss: 0.1299119 Vali Loss: 0.1320839 Test Loss: 0.1469134
Validation loss decreased (0.148134 --> 0.132084).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1244434
	speed: 0.0275s/iter; left time: 67.1369s
	iters: 200, epoch: 3 | loss: 0.1260646
	speed: 0.0226s/iter; left time: 53.0607s
	iters: 300, epoch: 3 | loss: 0.1119319
	speed: 0.0225s/iter; left time: 50.4299s
Epoch: 3 cost time: 7.753561496734619
Epoch: 3, Steps: 318 | Train Loss: 0.1273372 Vali Loss: 0.1323113 Test Loss: 0.1101567
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1040608
	speed: 0.0276s/iter; left time: 58.6368s
	iters: 200, epoch: 4 | loss: 0.1141240
	speed: 0.0211s/iter; left time: 42.8416s
	iters: 300, epoch: 4 | loss: 0.1405590
	speed: 0.0222s/iter; left time: 42.8576s
Epoch: 4 cost time: 7.514320135116577
Epoch: 4, Steps: 318 | Train Loss: 0.1209505 Vali Loss: 0.1244518 Test Loss: 0.0999686
Validation loss decreased (0.132084 --> 0.124452).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0929190
	speed: 0.0273s/iter; left time: 49.3969s
	iters: 200, epoch: 5 | loss: 0.1462791
	speed: 0.0218s/iter; left time: 37.2525s
	iters: 300, epoch: 5 | loss: 0.0969562
	speed: 0.0215s/iter; left time: 34.6257s
Epoch: 5 cost time: 7.489818811416626
Epoch: 5, Steps: 318 | Train Loss: 0.1157207 Vali Loss: 0.1217335 Test Loss: 0.0991821
Validation loss decreased (0.124452 --> 0.121733).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1237597
	speed: 0.0258s/iter; left time: 38.3945s
	iters: 200, epoch: 6 | loss: 0.1018509
	speed: 0.0217s/iter; left time: 30.1837s
	iters: 300, epoch: 6 | loss: 0.1082035
	speed: 0.0211s/iter; left time: 27.2700s
Epoch: 6 cost time: 7.288634777069092
Epoch: 6, Steps: 318 | Train Loss: 0.1142998 Vali Loss: 0.1220196 Test Loss: 0.0969942
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0873398
	speed: 0.0250s/iter; left time: 29.3086s
	iters: 200, epoch: 7 | loss: 0.1497479
	speed: 0.0220s/iter; left time: 23.5922s
	iters: 300, epoch: 7 | loss: 0.1237688
	speed: 0.0230s/iter; left time: 22.3711s
Epoch: 7 cost time: 10.006960153579712
Epoch: 7, Steps: 318 | Train Loss: 0.1132215 Vali Loss: 0.1207023 Test Loss: 0.0970955
Validation loss decreased (0.121733 --> 0.120702).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1222685
	speed: 0.0253s/iter; left time: 21.5937s
	iters: 200, epoch: 8 | loss: 0.1073913
	speed: 0.0222s/iter; left time: 16.7236s
	iters: 300, epoch: 8 | loss: 0.1115728
	speed: 0.0226s/iter; left time: 14.7731s
Epoch: 8 cost time: 7.422724723815918
Epoch: 8, Steps: 318 | Train Loss: 0.1130936 Vali Loss: 0.1221290 Test Loss: 0.0967180
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0980959
	speed: 0.0251s/iter; left time: 13.4957s
	iters: 200, epoch: 9 | loss: 0.1374768
	speed: 0.0214s/iter; left time: 9.3579s
	iters: 300, epoch: 9 | loss: 0.0891544
	speed: 0.0227s/iter; left time: 7.6554s
Epoch: 9 cost time: 7.353738307952881
Epoch: 9, Steps: 318 | Train Loss: 0.1127577 Vali Loss: 0.1206547 Test Loss: 0.0967142
Validation loss decreased (0.120702 --> 0.120655).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0996862
	speed: 0.0251s/iter; left time: 5.4998s
	iters: 200, epoch: 10 | loss: 0.1007307
	speed: 0.0226s/iter; left time: 2.6880s
	iters: 300, epoch: 10 | loss: 0.1450093
	speed: 0.0229s/iter; left time: 0.4349s
Epoch: 10 cost time: 7.485714673995972
Epoch: 10, Steps: 318 | Train Loss: 0.1126633 Vali Loss: 0.1209668 Test Loss: 0.0966067
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2016031
	speed: 0.0370s/iter; left time: 114.0264s
	iters: 200, epoch: 1 | loss: 0.1998113
	speed: 0.0205s/iter; left time: 61.1100s
	iters: 300, epoch: 1 | loss: 0.1322400
	speed: 0.0209s/iter; left time: 60.1621s
Epoch: 1 cost time: 8.222791910171509
Epoch: 1, Steps: 318 | Train Loss: 0.2760805 Vali Loss: 0.1464334 Test Loss: 0.1591506
Validation loss decreased (inf --> 0.146433).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1220310
	speed: 0.0231s/iter; left time: 63.7265s
	iters: 200, epoch: 2 | loss: 0.1199645
	speed: 0.0205s/iter; left time: 54.5310s
	iters: 300, epoch: 2 | loss: 0.1046920
	speed: 0.0191s/iter; left time: 48.9145s
Epoch: 2 cost time: 6.656466245651245
Epoch: 2, Steps: 318 | Train Loss: 0.1315244 Vali Loss: 0.1404621 Test Loss: 0.1582438
Validation loss decreased (0.146433 --> 0.140462).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1274811
	speed: 0.0241s/iter; left time: 59.0461s
	iters: 200, epoch: 3 | loss: 0.1254259
	speed: 0.0212s/iter; left time: 49.7039s
	iters: 300, epoch: 3 | loss: 0.1180155
	speed: 0.0216s/iter; left time: 48.3949s
Epoch: 3 cost time: 7.131434917449951
Epoch: 3, Steps: 318 | Train Loss: 0.1248930 Vali Loss: 0.1385678 Test Loss: 0.1584239
Validation loss decreased (0.140462 --> 0.138568).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0929136
	speed: 0.0267s/iter; left time: 56.7626s
	iters: 200, epoch: 4 | loss: 0.1183324
	speed: 0.0229s/iter; left time: 46.4555s
	iters: 300, epoch: 4 | loss: 0.1126767
	speed: 0.0217s/iter; left time: 41.8704s
Epoch: 4 cost time: 7.5305092334747314
Epoch: 4, Steps: 318 | Train Loss: 0.1222624 Vali Loss: 0.1355107 Test Loss: 0.1585351
Validation loss decreased (0.138568 --> 0.135511).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1096805
	speed: 0.0236s/iter; left time: 42.7554s
	iters: 200, epoch: 5 | loss: 0.1337322
	speed: 0.0216s/iter; left time: 36.9174s
	iters: 300, epoch: 5 | loss: 0.1453172
	speed: 0.0213s/iter; left time: 34.2592s
Epoch: 5 cost time: 7.087625980377197
Epoch: 5, Steps: 318 | Train Loss: 0.1206808 Vali Loss: 0.1371678 Test Loss: 0.1600024
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1515107
	speed: 0.0258s/iter; left time: 38.5153s
	iters: 200, epoch: 6 | loss: 0.1076970
	speed: 0.0207s/iter; left time: 28.7895s
	iters: 300, epoch: 6 | loss: 0.1033257
	speed: 0.0204s/iter; left time: 26.2768s
Epoch: 6 cost time: 7.096136569976807
Epoch: 6, Steps: 318 | Train Loss: 0.1200658 Vali Loss: 0.1352811 Test Loss: 0.1606114
Validation loss decreased (0.135511 --> 0.135281).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1207953
	speed: 0.0244s/iter; left time: 28.6358s
	iters: 200, epoch: 7 | loss: 0.1235758
	speed: 0.0208s/iter; left time: 22.2959s
	iters: 300, epoch: 7 | loss: 0.1180039
	speed: 0.0200s/iter; left time: 19.4824s
Epoch: 7 cost time: 6.933386325836182
Epoch: 7, Steps: 318 | Train Loss: 0.1197617 Vali Loss: 0.1362052 Test Loss: 0.1608234
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1401543
	speed: 0.0256s/iter; left time: 21.8857s
	iters: 200, epoch: 8 | loss: 0.1431667
	speed: 0.0220s/iter; left time: 16.5801s
	iters: 300, epoch: 8 | loss: 0.1104007
	speed: 0.0218s/iter; left time: 14.2467s
Epoch: 8 cost time: 7.349174976348877
Epoch: 8, Steps: 318 | Train Loss: 0.1195062 Vali Loss: 0.1356498 Test Loss: 0.1610489
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1228299
	speed: 0.0271s/iter; left time: 14.5681s
	iters: 200, epoch: 9 | loss: 0.1511963
	speed: 0.0210s/iter; left time: 9.1600s
	iters: 300, epoch: 9 | loss: 0.1415761
	speed: 0.0215s/iter; left time: 7.2419s
Epoch: 9 cost time: 7.362872123718262
Epoch: 9, Steps: 318 | Train Loss: 0.1192672 Vali Loss: 0.1362184 Test Loss: 0.1611810
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2768343
	speed: 0.0355s/iter; left time: 109.3311s
	iters: 200, epoch: 1 | loss: 0.1462969
	speed: 0.0202s/iter; left time: 60.3290s
	iters: 300, epoch: 1 | loss: 0.1400630
	speed: 0.0221s/iter; left time: 63.5943s
Epoch: 1 cost time: 8.183297157287598
Epoch: 1, Steps: 318 | Train Loss: 0.3110987 Vali Loss: 0.1514972 Test Loss: 0.1217416
Validation loss decreased (inf --> 0.151497).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1315384
	speed: 0.0243s/iter; left time: 67.0905s
	iters: 200, epoch: 2 | loss: 0.1239934
	speed: 0.0223s/iter; left time: 59.3810s
	iters: 300, epoch: 2 | loss: 0.1359424
	speed: 0.0199s/iter; left time: 50.9255s
Epoch: 2 cost time: 7.051332235336304
Epoch: 2, Steps: 318 | Train Loss: 0.1351039 Vali Loss: 0.1428136 Test Loss: 0.1090078
Validation loss decreased (0.151497 --> 0.142814).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1492982
	speed: 0.0231s/iter; left time: 56.4573s
	iters: 200, epoch: 3 | loss: 0.1033472
	speed: 0.0195s/iter; left time: 45.7765s
	iters: 300, epoch: 3 | loss: 0.1031398
	speed: 0.0205s/iter; left time: 45.9977s
Epoch: 3 cost time: 6.701977491378784
Epoch: 3, Steps: 318 | Train Loss: 0.1297564 Vali Loss: 0.1412110 Test Loss: 0.1079186
Validation loss decreased (0.142814 --> 0.141211).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1071437
	speed: 0.0234s/iter; left time: 49.7590s
	iters: 200, epoch: 4 | loss: 0.1028807
	speed: 0.0198s/iter; left time: 40.1841s
	iters: 300, epoch: 4 | loss: 0.1451332
	speed: 0.0200s/iter; left time: 38.6191s
Epoch: 4 cost time: 6.756964683532715
Epoch: 4, Steps: 318 | Train Loss: 0.1272557 Vali Loss: 0.1412476 Test Loss: 0.1071138
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1394681
	speed: 0.0238s/iter; left time: 43.0218s
	iters: 200, epoch: 5 | loss: 0.1313451
	speed: 0.0196s/iter; left time: 33.5031s
	iters: 300, epoch: 5 | loss: 0.1178857
	speed: 0.0222s/iter; left time: 35.6814s
Epoch: 5 cost time: 6.975661516189575
Epoch: 5, Steps: 318 | Train Loss: 0.1263020 Vali Loss: 0.1409133 Test Loss: 0.1060993
Validation loss decreased (0.141211 --> 0.140913).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1362847
	speed: 0.0245s/iter; left time: 36.6005s
	iters: 200, epoch: 6 | loss: 0.1254991
	speed: 0.0203s/iter; left time: 28.3028s
	iters: 300, epoch: 6 | loss: 0.1508391
	speed: 0.0204s/iter; left time: 26.3116s
Epoch: 6 cost time: 6.9189064502716064
Epoch: 6, Steps: 318 | Train Loss: 0.1256660 Vali Loss: 0.1402518 Test Loss: 0.1062699
Validation loss decreased (0.140913 --> 0.140252).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1603613
	speed: 0.0242s/iter; left time: 28.4442s
	iters: 200, epoch: 7 | loss: 0.1306110
	speed: 0.0213s/iter; left time: 22.8274s
	iters: 300, epoch: 7 | loss: 0.1112329
	speed: 0.0197s/iter; left time: 19.1909s
Epoch: 7 cost time: 6.954540967941284
Epoch: 7, Steps: 318 | Train Loss: 0.1253930 Vali Loss: 0.1392052 Test Loss: 0.1061845
Validation loss decreased (0.140252 --> 0.139205).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1090785
	speed: 0.0237s/iter; left time: 20.3022s
	iters: 200, epoch: 8 | loss: 0.1154615
	speed: 0.0194s/iter; left time: 14.6427s
	iters: 300, epoch: 8 | loss: 0.1620915
	speed: 0.0208s/iter; left time: 13.6451s
Epoch: 8 cost time: 6.832218647003174
Epoch: 8, Steps: 318 | Train Loss: 0.1251684 Vali Loss: 0.1399944 Test Loss: 0.1060415
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0904488
	speed: 0.0247s/iter; left time: 13.2803s
	iters: 200, epoch: 9 | loss: 0.1328000
	speed: 0.0203s/iter; left time: 8.8505s
	iters: 300, epoch: 9 | loss: 0.1111860
	speed: 0.0193s/iter; left time: 6.4888s
Epoch: 9 cost time: 6.839813232421875
Epoch: 9, Steps: 318 | Train Loss: 0.1250283 Vali Loss: 0.1399188 Test Loss: 0.1059900
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1101666
	speed: 0.0228s/iter; left time: 5.0011s
	iters: 200, epoch: 10 | loss: 0.1359100
	speed: 0.0195s/iter; left time: 2.3167s
	iters: 300, epoch: 10 | loss: 0.1147751
	speed: 0.0194s/iter; left time: 0.3686s
Epoch: 10 cost time: 6.5820019245147705
Epoch: 10, Steps: 318 | Train Loss: 0.1250437 Vali Loss: 0.1404251 Test Loss: 0.1059744
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2079258
	speed: 0.0361s/iter; left time: 111.3282s
	iters: 200, epoch: 1 | loss: 0.1712759
	speed: 0.0228s/iter; left time: 68.0731s
	iters: 300, epoch: 1 | loss: 0.1182129
	speed: 0.0219s/iter; left time: 63.0197s
Epoch: 1 cost time: 8.496469974517822
Epoch: 1, Steps: 318 | Train Loss: 0.2685259 Vali Loss: 0.1508617 Test Loss: 0.1291142
Validation loss decreased (inf --> 0.150862).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1235641
	speed: 0.0250s/iter; left time: 68.9919s
	iters: 200, epoch: 2 | loss: 0.1341895
	speed: 0.0192s/iter; left time: 51.1568s
	iters: 300, epoch: 2 | loss: 0.1072341
	speed: 0.0219s/iter; left time: 56.2167s
Epoch: 2 cost time: 7.072532892227173
Epoch: 2, Steps: 318 | Train Loss: 0.1340713 Vali Loss: 0.1425111 Test Loss: 0.1249794
Validation loss decreased (0.150862 --> 0.142511).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1228581
	speed: 0.0259s/iter; left time: 63.4019s
	iters: 200, epoch: 3 | loss: 0.1002783
	speed: 0.0211s/iter; left time: 49.4875s
	iters: 300, epoch: 3 | loss: 0.0943791
	speed: 0.0225s/iter; left time: 50.5985s
Epoch: 3 cost time: 7.334721326828003
Epoch: 3, Steps: 318 | Train Loss: 0.1270820 Vali Loss: 0.1396155 Test Loss: 0.1223093
Validation loss decreased (0.142511 --> 0.139616).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1467483
	speed: 0.0271s/iter; left time: 57.6621s
	iters: 200, epoch: 4 | loss: 0.1036045
	speed: 0.0208s/iter; left time: 42.1243s
	iters: 300, epoch: 4 | loss: 0.0976714
	speed: 0.0220s/iter; left time: 42.4776s
Epoch: 4 cost time: 7.405454874038696
Epoch: 4, Steps: 318 | Train Loss: 0.1245104 Vali Loss: 0.1381705 Test Loss: 0.1231025
Validation loss decreased (0.139616 --> 0.138171).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1262990
	speed: 0.0260s/iter; left time: 47.0659s
	iters: 200, epoch: 5 | loss: 0.1297454
	speed: 0.0236s/iter; left time: 40.2555s
	iters: 300, epoch: 5 | loss: 0.0959947
	speed: 0.0212s/iter; left time: 34.1645s
Epoch: 5 cost time: 7.558055639266968
Epoch: 5, Steps: 318 | Train Loss: 0.1234539 Vali Loss: 0.1388210 Test Loss: 0.1237941
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0941475
	speed: 0.0258s/iter; left time: 38.5192s
	iters: 200, epoch: 6 | loss: 0.0903553
	speed: 0.0229s/iter; left time: 31.8793s
	iters: 300, epoch: 6 | loss: 0.1107025
	speed: 0.0198s/iter; left time: 25.5981s
Epoch: 6 cost time: 7.2419233322143555
Epoch: 6, Steps: 318 | Train Loss: 0.1228748 Vali Loss: 0.1380846 Test Loss: 0.1233909
Validation loss decreased (0.138171 --> 0.138085).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1318285
	speed: 0.0270s/iter; left time: 31.7001s
	iters: 200, epoch: 7 | loss: 0.1574155
	speed: 0.0204s/iter; left time: 21.9170s
	iters: 300, epoch: 7 | loss: 0.1361473
	speed: 0.0223s/iter; left time: 21.7031s
Epoch: 7 cost time: 7.375355005264282
Epoch: 7, Steps: 318 | Train Loss: 0.1225838 Vali Loss: 0.1387720 Test Loss: 0.1235014
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1171062
	speed: 0.0238s/iter; left time: 20.3896s
	iters: 200, epoch: 8 | loss: 0.1516284
	speed: 0.0209s/iter; left time: 15.7455s
	iters: 300, epoch: 8 | loss: 0.1185410
	speed: 0.0235s/iter; left time: 15.3762s
Epoch: 8 cost time: 7.254256725311279
Epoch: 8, Steps: 318 | Train Loss: 0.1224048 Vali Loss: 0.1381663 Test Loss: 0.1235119
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1349425
	speed: 0.0253s/iter; left time: 13.5898s
	iters: 200, epoch: 9 | loss: 0.1433173
	speed: 0.0218s/iter; left time: 9.5153s
	iters: 300, epoch: 9 | loss: 0.1166069
	speed: 0.0192s/iter; left time: 6.4729s
Epoch: 9 cost time: 7.0040833950042725
Epoch: 9, Steps: 318 | Train Loss: 0.1223840 Vali Loss: 0.1378170 Test Loss: 0.1235028
Validation loss decreased (0.138085 --> 0.137817).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0838746
	speed: 0.0274s/iter; left time: 6.0018s
	iters: 200, epoch: 10 | loss: 0.1334967
	speed: 0.0217s/iter; left time: 2.5769s
	iters: 300, epoch: 10 | loss: 0.1312357
	speed: 0.0202s/iter; left time: 0.3844s
Epoch: 10 cost time: 7.43079948425293
Epoch: 10, Steps: 318 | Train Loss: 0.1223012 Vali Loss: 0.1375549 Test Loss: 0.1234965
Validation loss decreased (0.137817 --> 0.137555).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2606059
	speed: 0.0404s/iter; left time: 123.9355s
	iters: 200, epoch: 1 | loss: 0.1906741
	speed: 0.0231s/iter; left time: 68.6324s
	iters: 300, epoch: 1 | loss: 0.1423086
	speed: 0.0227s/iter; left time: 65.2143s
Epoch: 1 cost time: 9.000398635864258
Epoch: 1, Steps: 317 | Train Loss: 0.3310962 Vali Loss: 0.1674120 Test Loss: 0.1532851
Validation loss decreased (inf --> 0.167412).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1671063
	speed: 0.0252s/iter; left time: 69.3978s
	iters: 200, epoch: 2 | loss: 0.2043578
	speed: 0.0216s/iter; left time: 57.4192s
	iters: 300, epoch: 2 | loss: 0.1754581
	speed: 0.0228s/iter; left time: 58.1726s
Epoch: 2 cost time: 7.3861613273620605
Epoch: 2, Steps: 317 | Train Loss: 0.1603872 Vali Loss: 0.1569031 Test Loss: 0.1370983
Validation loss decreased (0.167412 --> 0.156903).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1499065
	speed: 0.0303s/iter; left time: 73.7325s
	iters: 200, epoch: 3 | loss: 0.1531385
	speed: 0.0219s/iter; left time: 51.1975s
	iters: 300, epoch: 3 | loss: 0.1598326
	speed: 0.0203s/iter; left time: 45.5037s
Epoch: 3 cost time: 7.644176483154297
Epoch: 3, Steps: 317 | Train Loss: 0.1496447 Vali Loss: 0.1547881 Test Loss: 0.1307011
Validation loss decreased (0.156903 --> 0.154788).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1394557
	speed: 0.0261s/iter; left time: 55.3656s
	iters: 200, epoch: 4 | loss: 0.1145113
	speed: 0.0231s/iter; left time: 46.5972s
	iters: 300, epoch: 4 | loss: 0.1053965
	speed: 0.0227s/iter; left time: 43.5083s
Epoch: 4 cost time: 7.597554922103882
Epoch: 4, Steps: 317 | Train Loss: 0.1463529 Vali Loss: 0.1542631 Test Loss: 0.1284554
Validation loss decreased (0.154788 --> 0.154263).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1231739
	speed: 0.0272s/iter; left time: 49.0068s
	iters: 200, epoch: 5 | loss: 0.1771730
	speed: 0.0230s/iter; left time: 39.2268s
	iters: 300, epoch: 5 | loss: 0.1594617
	speed: 0.0218s/iter; left time: 35.0191s
Epoch: 5 cost time: 7.641406297683716
Epoch: 5, Steps: 317 | Train Loss: 0.1443750 Vali Loss: 0.1536875 Test Loss: 0.1274162
Validation loss decreased (0.154263 --> 0.153688).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1485440
	speed: 0.0263s/iter; left time: 39.0221s
	iters: 200, epoch: 6 | loss: 0.1507355
	speed: 0.0227s/iter; left time: 31.4154s
	iters: 300, epoch: 6 | loss: 0.1149080
	speed: 0.0233s/iter; left time: 29.9569s
Epoch: 6 cost time: 7.629881381988525
Epoch: 6, Steps: 317 | Train Loss: 0.1437969 Vali Loss: 0.1538246 Test Loss: 0.1261482
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1115071
	speed: 0.0251s/iter; left time: 29.3927s
	iters: 200, epoch: 7 | loss: 0.1419253
	speed: 0.0235s/iter; left time: 25.1021s
	iters: 300, epoch: 7 | loss: 0.1650976
	speed: 0.0239s/iter; left time: 23.1768s
Epoch: 7 cost time: 7.651623487472534
Epoch: 7, Steps: 317 | Train Loss: 0.1435317 Vali Loss: 0.1540787 Test Loss: 0.1263723
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1255330
	speed: 0.0262s/iter; left time: 22.2809s
	iters: 200, epoch: 8 | loss: 0.1371256
	speed: 0.0214s/iter; left time: 16.0561s
	iters: 300, epoch: 8 | loss: 0.1305237
	speed: 0.0222s/iter; left time: 14.4687s
Epoch: 8 cost time: 7.381439685821533
Epoch: 8, Steps: 317 | Train Loss: 0.1433021 Vali Loss: 0.1541302 Test Loss: 0.1262401
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2832839
	speed: 0.0493s/iter; left time: 151.9081s
	iters: 200, epoch: 1 | loss: 0.1394016
	speed: 0.0314s/iter; left time: 93.6800s
	iters: 300, epoch: 1 | loss: 0.1327437
	speed: 0.0303s/iter; left time: 87.1828s
Epoch: 1 cost time: 11.705216646194458
Epoch: 1, Steps: 318 | Train Loss: 0.2914118 Vali Loss: 0.1504092 Test Loss: 0.1276101
Validation loss decreased (inf --> 0.150409).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1667941
	speed: 0.0354s/iter; left time: 97.8211s
	iters: 200, epoch: 2 | loss: 0.1142163
	speed: 0.0317s/iter; left time: 84.3092s
	iters: 300, epoch: 2 | loss: 0.1016953
	speed: 0.0315s/iter; left time: 80.8374s
Epoch: 2 cost time: 10.456324338912964
Epoch: 2, Steps: 318 | Train Loss: 0.1338911 Vali Loss: 0.1362132 Test Loss: 0.1131502
Validation loss decreased (0.150409 --> 0.136213).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1174001
	speed: 0.0350s/iter; left time: 85.5955s
	iters: 200, epoch: 3 | loss: 0.1116591
	speed: 0.0306s/iter; left time: 71.6484s
	iters: 300, epoch: 3 | loss: 0.1142314
	speed: 0.0304s/iter; left time: 68.3527s
Epoch: 3 cost time: 10.194644927978516
Epoch: 3, Steps: 318 | Train Loss: 0.1241304 Vali Loss: 0.1333142 Test Loss: 0.1079611
Validation loss decreased (0.136213 --> 0.133314).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1071258
	speed: 0.0352s/iter; left time: 74.9042s
	iters: 200, epoch: 4 | loss: 0.1117521
	speed: 0.0320s/iter; left time: 64.8440s
	iters: 300, epoch: 4 | loss: 0.1468142
	speed: 0.0337s/iter; left time: 64.9645s
Epoch: 4 cost time: 10.667959928512573
Epoch: 4, Steps: 318 | Train Loss: 0.1201425 Vali Loss: 0.1293187 Test Loss: 0.1040840
Validation loss decreased (0.133314 --> 0.129319).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1054347
	speed: 0.0352s/iter; left time: 63.6766s
	iters: 200, epoch: 5 | loss: 0.1429383
	speed: 0.0329s/iter; left time: 56.1788s
	iters: 300, epoch: 5 | loss: 0.1034899
	speed: 0.0341s/iter; left time: 54.9183s
Epoch: 5 cost time: 10.80000376701355
Epoch: 5, Steps: 318 | Train Loss: 0.1174791 Vali Loss: 0.1277968 Test Loss: 0.1037378
Validation loss decreased (0.129319 --> 0.127797).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1290947
	speed: 0.0473s/iter; left time: 70.5495s
	iters: 200, epoch: 6 | loss: 0.1007799
	speed: 0.0324s/iter; left time: 45.0243s
	iters: 300, epoch: 6 | loss: 0.1070460
	speed: 0.0472s/iter; left time: 60.8931s
Epoch: 6 cost time: 13.681751728057861
Epoch: 6, Steps: 318 | Train Loss: 0.1168578 Vali Loss: 0.1286864 Test Loss: 0.1023677
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0955215
	speed: 0.0581s/iter; left time: 68.1214s
	iters: 200, epoch: 7 | loss: 0.1448751
	speed: 0.0541s/iter; left time: 58.0171s
	iters: 300, epoch: 7 | loss: 0.1222211
	speed: 0.0525s/iter; left time: 51.0395s
Epoch: 7 cost time: 17.47110867500305
Epoch: 7, Steps: 318 | Train Loss: 0.1159621 Vali Loss: 0.1275085 Test Loss: 0.1019498
Validation loss decreased (0.127797 --> 0.127509).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1102419
	speed: 0.0630s/iter; left time: 53.9002s
	iters: 200, epoch: 8 | loss: 0.1174805
	speed: 0.0548s/iter; left time: 41.4091s
	iters: 300, epoch: 8 | loss: 0.1087322
	speed: 0.0537s/iter; left time: 35.1761s
Epoch: 8 cost time: 18.217294216156006
Epoch: 8, Steps: 318 | Train Loss: 0.1158454 Vali Loss: 0.1290987 Test Loss: 0.1019081
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1045919
	speed: 0.0593s/iter; left time: 31.8401s
	iters: 200, epoch: 9 | loss: 0.1415426
	speed: 0.0552s/iter; left time: 24.1006s
	iters: 300, epoch: 9 | loss: 0.0859575
	speed: 0.0557s/iter; left time: 18.7786s
Epoch: 9 cost time: 18.047301292419434
Epoch: 9, Steps: 318 | Train Loss: 0.1157640 Vali Loss: 0.1273021 Test Loss: 0.1018410
Validation loss decreased (0.127509 --> 0.127302).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1047520
	speed: 0.0592s/iter; left time: 12.9732s
	iters: 200, epoch: 10 | loss: 0.0950168
	speed: 0.0547s/iter; left time: 6.5043s
	iters: 300, epoch: 10 | loss: 0.1404227
	speed: 0.0539s/iter; left time: 1.0244s
Epoch: 10 cost time: 17.79904055595398
Epoch: 10, Steps: 318 | Train Loss: 0.1154568 Vali Loss: 0.1279299 Test Loss: 0.1018041
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2096054
	speed: 0.0766s/iter; left time: 236.1420s
	iters: 200, epoch: 1 | loss: 0.1997126
	speed: 0.0543s/iter; left time: 162.0125s
	iters: 300, epoch: 1 | loss: 0.1645042
	speed: 0.0578s/iter; left time: 166.4374s
Epoch: 1 cost time: 19.87570309638977
Epoch: 1, Steps: 318 | Train Loss: 0.2829245 Vali Loss: 0.1577511 Test Loss: 0.1275208
Validation loss decreased (inf --> 0.157751).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1398963
	speed: 0.0612s/iter; left time: 169.1064s
	iters: 200, epoch: 2 | loss: 0.1288353
	speed: 0.0549s/iter; left time: 146.2641s
	iters: 300, epoch: 2 | loss: 0.1117845
	speed: 0.0520s/iter; left time: 133.3795s
Epoch: 2 cost time: 17.821170806884766
Epoch: 2, Steps: 318 | Train Loss: 0.1464177 Vali Loss: 0.1478095 Test Loss: 0.1161440
Validation loss decreased (0.157751 --> 0.147809).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1497469
	speed: 0.0582s/iter; left time: 142.1940s
	iters: 200, epoch: 3 | loss: 0.1443325
	speed: 0.0532s/iter; left time: 124.6794s
	iters: 300, epoch: 3 | loss: 0.1296979
	speed: 0.0533s/iter; left time: 119.6037s
Epoch: 3 cost time: 17.47461771965027
Epoch: 3, Steps: 318 | Train Loss: 0.1372761 Vali Loss: 0.1432223 Test Loss: 0.1120581
Validation loss decreased (0.147809 --> 0.143222).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0978508
	speed: 0.0406s/iter; left time: 86.3073s
	iters: 200, epoch: 4 | loss: 0.1457988
	speed: 0.0318s/iter; left time: 64.4175s
	iters: 300, epoch: 4 | loss: 0.1156052
	speed: 0.0314s/iter; left time: 60.4673s
Epoch: 4 cost time: 10.942927360534668
Epoch: 4, Steps: 318 | Train Loss: 0.1336672 Vali Loss: 0.1416894 Test Loss: 0.1100500
Validation loss decreased (0.143222 --> 0.141689).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1234683
	speed: 0.0364s/iter; left time: 65.8765s
	iters: 200, epoch: 5 | loss: 0.1446735
	speed: 0.0307s/iter; left time: 52.4351s
	iters: 300, epoch: 5 | loss: 0.1579364
	speed: 0.0323s/iter; left time: 51.9785s
Epoch: 5 cost time: 10.579988241195679
Epoch: 5, Steps: 318 | Train Loss: 0.1321834 Vali Loss: 0.1420666 Test Loss: 0.1101377
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1671520
	speed: 0.0571s/iter; left time: 85.1174s
	iters: 200, epoch: 6 | loss: 0.1073628
	speed: 0.0523s/iter; left time: 72.8104s
	iters: 300, epoch: 6 | loss: 0.1173482
	speed: 0.0537s/iter; left time: 69.2988s
Epoch: 6 cost time: 17.382840871810913
Epoch: 6, Steps: 318 | Train Loss: 0.1314778 Vali Loss: 0.1407488 Test Loss: 0.1095044
Validation loss decreased (0.141689 --> 0.140749).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1235206
	speed: 0.0593s/iter; left time: 69.5772s
	iters: 200, epoch: 7 | loss: 0.1492593
	speed: 0.0535s/iter; left time: 57.4261s
	iters: 300, epoch: 7 | loss: 0.1294651
	speed: 0.0548s/iter; left time: 53.3492s
Epoch: 7 cost time: 17.76803231239319
Epoch: 7, Steps: 318 | Train Loss: 0.1310158 Vali Loss: 0.1417778 Test Loss: 0.1095577
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1425776
	speed: 0.0417s/iter; left time: 35.6669s
	iters: 200, epoch: 8 | loss: 0.1466336
	speed: 0.0467s/iter; left time: 35.2803s
	iters: 300, epoch: 8 | loss: 0.1320468
	speed: 0.0321s/iter; left time: 21.0543s
Epoch: 8 cost time: 12.697594404220581
Epoch: 8, Steps: 318 | Train Loss: 0.1305450 Vali Loss: 0.1407164 Test Loss: 0.1095283
Validation loss decreased (0.140749 --> 0.140716).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1291325
	speed: 0.0484s/iter; left time: 25.9916s
	iters: 200, epoch: 9 | loss: 0.1699330
	speed: 0.0354s/iter; left time: 15.4819s
	iters: 300, epoch: 9 | loss: 0.1559072
	speed: 0.0462s/iter; left time: 15.5609s
Epoch: 9 cost time: 13.932760238647461
Epoch: 9, Steps: 318 | Train Loss: 0.1302010 Vali Loss: 0.1417587 Test Loss: 0.1095219
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1438291
	speed: 0.0559s/iter; left time: 12.2364s
	iters: 200, epoch: 10 | loss: 0.1196179
	speed: 0.0527s/iter; left time: 6.2705s
	iters: 300, epoch: 10 | loss: 0.1092437
	speed: 0.0563s/iter; left time: 1.0694s
Epoch: 10 cost time: 17.505958557128906
Epoch: 10, Steps: 318 | Train Loss: 0.1304444 Vali Loss: 0.1409083 Test Loss: 0.1094899
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2701840
	speed: 0.0728s/iter; left time: 224.4214s
	iters: 200, epoch: 1 | loss: 0.1375974
	speed: 0.0562s/iter; left time: 167.5068s
	iters: 300, epoch: 1 | loss: 0.1364812
	speed: 0.0566s/iter; left time: 162.9786s
Epoch: 1 cost time: 19.501744985580444
Epoch: 1, Steps: 318 | Train Loss: 0.2932044 Vali Loss: 0.1498249 Test Loss: 0.1175794
Validation loss decreased (inf --> 0.149825).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1245600
	speed: 0.0565s/iter; left time: 156.0783s
	iters: 200, epoch: 2 | loss: 0.1182400
	speed: 0.0543s/iter; left time: 144.5326s
	iters: 300, epoch: 2 | loss: 0.1236233
	speed: 0.0517s/iter; left time: 132.6223s
Epoch: 2 cost time: 17.296634435653687
Epoch: 2, Steps: 318 | Train Loss: 0.1317840 Vali Loss: 0.1440192 Test Loss: 0.1077979
Validation loss decreased (0.149825 --> 0.144019).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1423020
	speed: 0.0596s/iter; left time: 145.7820s
	iters: 200, epoch: 3 | loss: 0.0997369
	speed: 0.0587s/iter; left time: 137.5802s
	iters: 300, epoch: 3 | loss: 0.1060809
	speed: 0.0596s/iter; left time: 133.7191s
Epoch: 3 cost time: 18.721211910247803
Epoch: 3, Steps: 318 | Train Loss: 0.1264344 Vali Loss: 0.1425792 Test Loss: 0.1069073
Validation loss decreased (0.144019 --> 0.142579).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0990571
	speed: 0.0622s/iter; left time: 132.4049s
	iters: 200, epoch: 4 | loss: 0.1027055
	speed: 0.0549s/iter; left time: 111.2058s
	iters: 300, epoch: 4 | loss: 0.1457849
	speed: 0.0587s/iter; left time: 113.0611s
Epoch: 4 cost time: 18.531033039093018
Epoch: 4, Steps: 318 | Train Loss: 0.1239820 Vali Loss: 0.1427240 Test Loss: 0.1060807
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1371199
	speed: 0.0592s/iter; left time: 107.1084s
	iters: 200, epoch: 5 | loss: 0.1289608
	speed: 0.0540s/iter; left time: 92.2534s
	iters: 300, epoch: 5 | loss: 0.1136686
	speed: 0.0579s/iter; left time: 93.1917s
Epoch: 5 cost time: 18.191848516464233
Epoch: 5, Steps: 318 | Train Loss: 0.1229381 Vali Loss: 0.1431456 Test Loss: 0.1060129
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1361673
	speed: 0.0602s/iter; left time: 89.7698s
	iters: 200, epoch: 6 | loss: 0.1167916
	speed: 0.0575s/iter; left time: 79.9943s
	iters: 300, epoch: 6 | loss: 0.1437732
	speed: 0.0584s/iter; left time: 75.3421s
Epoch: 6 cost time: 18.580528259277344
Epoch: 6, Steps: 318 | Train Loss: 0.1221512 Vali Loss: 0.1424209 Test Loss: 0.1063095
Validation loss decreased (0.142579 --> 0.142421).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1610191
	speed: 0.0607s/iter; left time: 71.1498s
	iters: 200, epoch: 7 | loss: 0.1285686
	speed: 0.0576s/iter; left time: 61.8074s
	iters: 300, epoch: 7 | loss: 0.1075966
	speed: 0.0576s/iter; left time: 56.0822s
Epoch: 7 cost time: 18.593671560287476
Epoch: 7, Steps: 318 | Train Loss: 0.1218008 Vali Loss: 0.1413561 Test Loss: 0.1061102
Validation loss decreased (0.142421 --> 0.141356).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1083144
	speed: 0.0592s/iter; left time: 50.5822s
	iters: 200, epoch: 8 | loss: 0.1106121
	speed: 0.0511s/iter; left time: 38.5502s
	iters: 300, epoch: 8 | loss: 0.1682350
	speed: 0.0544s/iter; left time: 35.6108s
Epoch: 8 cost time: 17.506566524505615
Epoch: 8, Steps: 318 | Train Loss: 0.1218777 Vali Loss: 0.1421876 Test Loss: 0.1061039
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0852272
	speed: 0.0592s/iter; left time: 31.7892s
	iters: 200, epoch: 9 | loss: 0.1312589
	speed: 0.0550s/iter; left time: 24.0198s
	iters: 300, epoch: 9 | loss: 0.1016680
	speed: 0.0581s/iter; left time: 19.5664s
Epoch: 9 cost time: 18.209712505340576
Epoch: 9, Steps: 318 | Train Loss: 0.1215175 Vali Loss: 0.1424829 Test Loss: 0.1061086
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1117646
	speed: 0.0569s/iter; left time: 12.4650s
	iters: 200, epoch: 10 | loss: 0.1344666
	speed: 0.0598s/iter; left time: 7.1152s
	iters: 300, epoch: 10 | loss: 0.1089739
	speed: 0.0641s/iter; left time: 1.2176s
Epoch: 10 cost time: 19.122877836227417
Epoch: 10, Steps: 318 | Train Loss: 0.1212197 Vali Loss: 0.1426413 Test Loss: 0.1060963
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1971842
	speed: 0.0770s/iter; left time: 237.2593s
	iters: 200, epoch: 1 | loss: 0.1760077
	speed: 0.0544s/iter; left time: 162.2429s
	iters: 300, epoch: 1 | loss: 0.1269222
	speed: 0.0540s/iter; left time: 155.4879s
Epoch: 1 cost time: 19.45994997024536
Epoch: 1, Steps: 318 | Train Loss: 0.2554409 Vali Loss: 0.1522764 Test Loss: 0.1189752
Validation loss decreased (inf --> 0.152276).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1201773
	speed: 0.0586s/iter; left time: 161.7928s
	iters: 200, epoch: 2 | loss: 0.1322949
	speed: 0.0532s/iter; left time: 141.6596s
	iters: 300, epoch: 2 | loss: 0.1107396
	speed: 0.0516s/iter; left time: 132.3078s
Epoch: 2 cost time: 17.38860273361206
Epoch: 2, Steps: 318 | Train Loss: 0.1366809 Vali Loss: 0.1445340 Test Loss: 0.1127176
Validation loss decreased (0.152276 --> 0.144534).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1185121
	speed: 0.0579s/iter; left time: 141.4575s
	iters: 200, epoch: 3 | loss: 0.1007059
	speed: 0.0561s/iter; left time: 131.5771s
	iters: 300, epoch: 3 | loss: 0.0945381
	speed: 0.0526s/iter; left time: 117.9872s
Epoch: 3 cost time: 17.66225266456604
Epoch: 3, Steps: 318 | Train Loss: 0.1294728 Vali Loss: 0.1413434 Test Loss: 0.1086041
Validation loss decreased (0.144534 --> 0.141343).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1501582
	speed: 0.0619s/iter; left time: 131.6707s
	iters: 200, epoch: 4 | loss: 0.1107407
	speed: 0.0526s/iter; left time: 106.6733s
	iters: 300, epoch: 4 | loss: 0.1011047
	speed: 0.0515s/iter; left time: 99.2201s
Epoch: 4 cost time: 17.591553926467896
Epoch: 4, Steps: 318 | Train Loss: 0.1269220 Vali Loss: 0.1391757 Test Loss: 0.1075166
Validation loss decreased (0.141343 --> 0.139176).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1308555
	speed: 0.0689s/iter; left time: 124.6852s
	iters: 200, epoch: 5 | loss: 0.1284582
	speed: 0.0724s/iter; left time: 123.8148s
	iters: 300, epoch: 5 | loss: 0.1010105
	speed: 0.0632s/iter; left time: 101.7081s
Epoch: 5 cost time: 21.435853958129883
Epoch: 5, Steps: 318 | Train Loss: 0.1255719 Vali Loss: 0.1398798 Test Loss: 0.1077030
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1007354
	speed: 0.0719s/iter; left time: 107.1750s
	iters: 200, epoch: 6 | loss: 0.0927014
	speed: 0.0664s/iter; left time: 92.4108s
	iters: 300, epoch: 6 | loss: 0.1109088
	speed: 0.0624s/iter; left time: 80.5102s
Epoch: 6 cost time: 21.23223066329956
Epoch: 6, Steps: 318 | Train Loss: 0.1249097 Vali Loss: 0.1392807 Test Loss: 0.1073300
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1392960
	speed: 0.0573s/iter; left time: 67.2074s
	iters: 200, epoch: 7 | loss: 0.1653250
	speed: 0.0527s/iter; left time: 56.5615s
	iters: 300, epoch: 7 | loss: 0.1353070
	speed: 0.0532s/iter; left time: 51.7344s
Epoch: 7 cost time: 17.30106520652771
Epoch: 7, Steps: 318 | Train Loss: 0.1246693 Vali Loss: 0.1400076 Test Loss: 0.1073602
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2004245
	speed: 0.0764s/iter; left time: 234.6368s
	iters: 200, epoch: 1 | loss: 0.1589566
	speed: 0.0590s/iter; left time: 175.3579s
	iters: 300, epoch: 1 | loss: 0.1334304
	speed: 0.0606s/iter; left time: 173.9728s
Epoch: 1 cost time: 20.57104802131653
Epoch: 1, Steps: 317 | Train Loss: 0.2749971 Vali Loss: 0.1548303 Test Loss: 0.1201732
Validation loss decreased (inf --> 0.154830).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1526113
	speed: 0.0687s/iter; left time: 189.1031s
	iters: 200, epoch: 2 | loss: 0.1809220
	speed: 0.0630s/iter; left time: 167.1323s
	iters: 300, epoch: 2 | loss: 0.1542276
	speed: 0.0596s/iter; left time: 152.1260s
Epoch: 2 cost time: 20.09069323539734
Epoch: 2, Steps: 317 | Train Loss: 0.1347467 Vali Loss: 0.1472062 Test Loss: 0.1138400
Validation loss decreased (0.154830 --> 0.147206).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1325921
	speed: 0.0651s/iter; left time: 158.6390s
	iters: 200, epoch: 3 | loss: 0.1171044
	speed: 0.0560s/iter; left time: 130.9759s
	iters: 300, epoch: 3 | loss: 0.1232862
	speed: 0.0583s/iter; left time: 130.4408s
Epoch: 3 cost time: 18.92596960067749
Epoch: 3, Steps: 317 | Train Loss: 0.1283315 Vali Loss: 0.1443658 Test Loss: 0.1115566
Validation loss decreased (0.147206 --> 0.144366).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1255343
	speed: 0.0580s/iter; left time: 122.8764s
	iters: 200, epoch: 4 | loss: 0.0973550
	speed: 0.0545s/iter; left time: 110.1467s
	iters: 300, epoch: 4 | loss: 0.0965932
	speed: 0.0573s/iter; left time: 109.9466s
Epoch: 4 cost time: 17.972889184951782
Epoch: 4, Steps: 317 | Train Loss: 0.1259972 Vali Loss: 0.1439756 Test Loss: 0.1106733
Validation loss decreased (0.144366 --> 0.143976).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0977882
	speed: 0.0591s/iter; left time: 106.5962s
	iters: 200, epoch: 5 | loss: 0.1644675
	speed: 0.0536s/iter; left time: 91.3407s
	iters: 300, epoch: 5 | loss: 0.1354002
	speed: 0.0540s/iter; left time: 86.5471s
Epoch: 5 cost time: 17.589511394500732
Epoch: 5, Steps: 317 | Train Loss: 0.1244763 Vali Loss: 0.1437565 Test Loss: 0.1106603
Validation loss decreased (0.143976 --> 0.143756).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1262766
	speed: 0.0572s/iter; left time: 84.9864s
	iters: 200, epoch: 6 | loss: 0.1487379
	speed: 0.0550s/iter; left time: 76.2022s
	iters: 300, epoch: 6 | loss: 0.1061909
	speed: 0.0529s/iter; left time: 68.0377s
Epoch: 6 cost time: 17.45022439956665
Epoch: 6, Steps: 317 | Train Loss: 0.1235541 Vali Loss: 0.1434511 Test Loss: 0.1102616
Validation loss decreased (0.143756 --> 0.143451).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0899643
	speed: 0.0560s/iter; left time: 65.4278s
	iters: 200, epoch: 7 | loss: 0.1323102
	speed: 0.0518s/iter; left time: 55.3679s
	iters: 300, epoch: 7 | loss: 0.1400644
	speed: 0.0545s/iter; left time: 52.7795s
Epoch: 7 cost time: 17.217337608337402
Epoch: 7, Steps: 317 | Train Loss: 0.1233176 Vali Loss: 0.1434664 Test Loss: 0.1102613
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1040627
	speed: 0.0565s/iter; left time: 48.1277s
	iters: 200, epoch: 8 | loss: 0.1281359
	speed: 0.0525s/iter; left time: 39.5087s
	iters: 300, epoch: 8 | loss: 0.1021285
	speed: 0.0555s/iter; left time: 36.2107s
Epoch: 8 cost time: 17.407970905303955
Epoch: 8, Steps: 317 | Train Loss: 0.1231450 Vali Loss: 0.1435969 Test Loss: 0.1102332
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1516544
	speed: 0.0584s/iter; left time: 31.2371s
	iters: 200, epoch: 9 | loss: 0.1569886
	speed: 0.0537s/iter; left time: 23.3534s
	iters: 300, epoch: 9 | loss: 0.1193448
	speed: 0.0551s/iter; left time: 18.4693s
Epoch: 9 cost time: 17.61441707611084
Epoch: 9, Steps: 317 | Train Loss: 0.1229891 Vali Loss: 0.1435579 Test Loss: 0.1102296
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2833419
	speed: 0.0818s/iter; left time: 252.1613s
	iters: 200, epoch: 1 | loss: 0.1399874
	speed: 0.0685s/iter; left time: 204.2113s
	iters: 300, epoch: 1 | loss: 0.1331285
	speed: 0.0698s/iter; left time: 201.0814s
Epoch: 1 cost time: 23.209409713745117
Epoch: 1, Steps: 318 | Train Loss: 0.2914899 Vali Loss: 0.1507144 Test Loss: 0.1277246
Validation loss decreased (inf --> 0.150714).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1680546
	speed: 0.0735s/iter; left time: 203.0094s
	iters: 200, epoch: 2 | loss: 0.1140530
	speed: 0.0664s/iter; left time: 176.8064s
	iters: 300, epoch: 2 | loss: 0.1015319
	speed: 0.0639s/iter; left time: 163.7452s
Epoch: 2 cost time: 21.63727831840515
Epoch: 2, Steps: 318 | Train Loss: 0.1338754 Vali Loss: 0.1364695 Test Loss: 0.1132362
Validation loss decreased (0.150714 --> 0.136470).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1173855
	speed: 0.0705s/iter; left time: 172.4774s
	iters: 200, epoch: 3 | loss: 0.1116800
	speed: 0.0652s/iter; left time: 152.9787s
	iters: 300, epoch: 3 | loss: 0.1141425
	speed: 0.0650s/iter; left time: 145.9068s
Epoch: 3 cost time: 21.201048851013184
Epoch: 3, Steps: 318 | Train Loss: 0.1240932 Vali Loss: 0.1335641 Test Loss: 0.1079995
Validation loss decreased (0.136470 --> 0.133564).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1072949
	speed: 0.0722s/iter; left time: 153.6180s
	iters: 200, epoch: 4 | loss: 0.1115384
	speed: 0.0694s/iter; left time: 140.7613s
	iters: 300, epoch: 4 | loss: 0.1462759
	speed: 0.0661s/iter; left time: 127.3537s
Epoch: 4 cost time: 21.950161457061768
Epoch: 4, Steps: 318 | Train Loss: 0.1200947 Vali Loss: 0.1295791 Test Loss: 0.1041821
Validation loss decreased (0.133564 --> 0.129579).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1051650
	speed: 0.0755s/iter; left time: 136.6101s
	iters: 200, epoch: 5 | loss: 0.1428281
	speed: 0.0660s/iter; left time: 112.7729s
	iters: 300, epoch: 5 | loss: 0.1036980
	speed: 0.0649s/iter; left time: 104.3904s
Epoch: 5 cost time: 21.81907296180725
Epoch: 5, Steps: 318 | Train Loss: 0.1174889 Vali Loss: 0.1279548 Test Loss: 0.1037461
Validation loss decreased (0.129579 --> 0.127955).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1291424
	speed: 0.0753s/iter; left time: 112.2159s
	iters: 200, epoch: 6 | loss: 0.1006961
	speed: 0.0679s/iter; left time: 94.4525s
	iters: 300, epoch: 6 | loss: 0.1103311
	speed: 0.0715s/iter; left time: 92.2952s
Epoch: 6 cost time: 22.81706690788269
Epoch: 6, Steps: 318 | Train Loss: 0.1169772 Vali Loss: 0.1284166 Test Loss: 0.1023339
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0954835
	speed: 0.0735s/iter; left time: 86.2383s
	iters: 200, epoch: 7 | loss: 0.1444649
	speed: 0.0707s/iter; left time: 75.8668s
	iters: 300, epoch: 7 | loss: 0.1206151
	speed: 0.0662s/iter; left time: 64.4256s
Epoch: 7 cost time: 22.2846941947937
Epoch: 7, Steps: 318 | Train Loss: 0.1161698 Vali Loss: 0.1272457 Test Loss: 0.1019544
Validation loss decreased (0.127955 --> 0.127246).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1106090
	speed: 0.0695s/iter; left time: 59.4406s
	iters: 200, epoch: 8 | loss: 0.1131839
	speed: 0.0647s/iter; left time: 48.8371s
	iters: 300, epoch: 8 | loss: 0.1034454
	speed: 0.0689s/iter; left time: 45.1615s
Epoch: 8 cost time: 21.515575408935547
Epoch: 8, Steps: 318 | Train Loss: 0.1160411 Vali Loss: 0.1288217 Test Loss: 0.1019110
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1046219
	speed: 0.0688s/iter; left time: 36.9333s
	iters: 200, epoch: 9 | loss: 0.1402099
	speed: 0.0654s/iter; left time: 28.5701s
	iters: 300, epoch: 9 | loss: 0.0909494
	speed: 0.0652s/iter; left time: 21.9820s
Epoch: 9 cost time: 21.166146516799927
Epoch: 9, Steps: 318 | Train Loss: 0.1159595 Vali Loss: 0.1269957 Test Loss: 0.1018293
Validation loss decreased (0.127246 --> 0.126996).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1042274
	speed: 0.0723s/iter; left time: 15.8269s
	iters: 200, epoch: 10 | loss: 0.0947023
	speed: 0.0664s/iter; left time: 7.9072s
	iters: 300, epoch: 10 | loss: 0.1403219
	speed: 0.0662s/iter; left time: 1.2584s
Epoch: 10 cost time: 21.736018657684326
Epoch: 10, Steps: 318 | Train Loss: 0.1156735 Vali Loss: 0.1276045 Test Loss: 0.1017870
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2092184
	speed: 0.0913s/iter; left time: 281.4051s
	iters: 200, epoch: 1 | loss: 0.2105099
	speed: 0.0742s/iter; left time: 221.0638s
	iters: 300, epoch: 1 | loss: 0.1642489
	speed: 0.0712s/iter; left time: 205.2598s
Epoch: 1 cost time: 25.081013679504395
Epoch: 1, Steps: 318 | Train Loss: 0.2824447 Vali Loss: 0.1574306 Test Loss: 0.1271006
Validation loss decreased (inf --> 0.157431).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1395014
	speed: 0.0760s/iter; left time: 210.0243s
	iters: 200, epoch: 2 | loss: 0.1285405
	speed: 0.0712s/iter; left time: 189.4944s
	iters: 300, epoch: 2 | loss: 0.1114292
	speed: 0.0734s/iter; left time: 188.2027s
Epoch: 2 cost time: 23.2571382522583
Epoch: 2, Steps: 318 | Train Loss: 0.1460382 Vali Loss: 0.1474635 Test Loss: 0.1157428
Validation loss decreased (0.157431 --> 0.147463).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1495323
	speed: 0.0711s/iter; left time: 173.8229s
	iters: 200, epoch: 3 | loss: 0.1439957
	speed: 0.0680s/iter; left time: 159.4270s
	iters: 300, epoch: 3 | loss: 0.1292586
	speed: 0.0675s/iter; left time: 151.4821s
Epoch: 3 cost time: 21.91940665245056
Epoch: 3, Steps: 318 | Train Loss: 0.1369933 Vali Loss: 0.1428705 Test Loss: 0.1116847
Validation loss decreased (0.147463 --> 0.142871).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0974229
	speed: 0.0716s/iter; left time: 152.3840s
	iters: 200, epoch: 4 | loss: 0.1454613
	speed: 0.0686s/iter; left time: 139.0355s
	iters: 300, epoch: 4 | loss: 0.1153735
	speed: 0.0706s/iter; left time: 135.9837s
Epoch: 4 cost time: 22.387376308441162
Epoch: 4, Steps: 318 | Train Loss: 0.1334406 Vali Loss: 0.1413337 Test Loss: 0.1096031
Validation loss decreased (0.142871 --> 0.141334).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1228894
	speed: 0.0730s/iter; left time: 131.9895s
	iters: 200, epoch: 5 | loss: 0.1441260
	speed: 0.0711s/iter; left time: 121.5807s
	iters: 300, epoch: 5 | loss: 0.1578506
	speed: 0.0695s/iter; left time: 111.7952s
Epoch: 5 cost time: 22.652003288269043
Epoch: 5, Steps: 318 | Train Loss: 0.1319972 Vali Loss: 0.1417407 Test Loss: 0.1096646
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1668127
	speed: 0.0725s/iter; left time: 108.1359s
	iters: 200, epoch: 6 | loss: 0.1069174
	speed: 0.0688s/iter; left time: 95.7071s
	iters: 300, epoch: 6 | loss: 0.1172239
	speed: 0.0682s/iter; left time: 88.0718s
Epoch: 6 cost time: 22.284122705459595
Epoch: 6, Steps: 318 | Train Loss: 0.1312840 Vali Loss: 0.1403849 Test Loss: 0.1090011
Validation loss decreased (0.141334 --> 0.140385).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1232157
	speed: 0.0776s/iter; left time: 90.9947s
	iters: 200, epoch: 7 | loss: 0.1484528
	speed: 0.0714s/iter; left time: 76.5888s
	iters: 300, epoch: 7 | loss: 0.1289627
	speed: 0.0688s/iter; left time: 66.9115s
Epoch: 7 cost time: 23.033396244049072
Epoch: 7, Steps: 318 | Train Loss: 0.1308386 Vali Loss: 0.1414036 Test Loss: 0.1089648
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1418071
	speed: 0.0749s/iter; left time: 64.0217s
	iters: 200, epoch: 8 | loss: 0.1465406
	speed: 0.0722s/iter; left time: 54.5428s
	iters: 300, epoch: 8 | loss: 0.1315135
	speed: 0.0681s/iter; left time: 44.6069s
Epoch: 8 cost time: 22.89039969444275
Epoch: 8, Steps: 318 | Train Loss: 0.1303790 Vali Loss: 0.1403208 Test Loss: 0.1088787
Validation loss decreased (0.140385 --> 0.140321).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1354098
	speed: 0.0742s/iter; left time: 39.8514s
	iters: 200, epoch: 9 | loss: 0.1692898
	speed: 0.0687s/iter; left time: 30.0103s
	iters: 300, epoch: 9 | loss: 0.1558567
	speed: 0.0688s/iter; left time: 23.1974s
Epoch: 9 cost time: 22.43176245689392
Epoch: 9, Steps: 318 | Train Loss: 0.1300011 Vali Loss: 0.1413318 Test Loss: 0.1088273
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1433503
	speed: 0.0620s/iter; left time: 13.5831s
	iters: 200, epoch: 10 | loss: 0.1208886
	speed: 0.0715s/iter; left time: 8.5072s
	iters: 300, epoch: 10 | loss: 0.1084901
	speed: 0.0698s/iter; left time: 1.3264s
Epoch: 10 cost time: 21.769165515899658
Epoch: 10, Steps: 318 | Train Loss: 0.1302439 Vali Loss: 0.1405032 Test Loss: 0.1088256
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.3013996
	speed: 0.0910s/iter; left time: 280.3735s
	iters: 200, epoch: 1 | loss: 0.1586630
	speed: 0.0753s/iter; left time: 224.4988s
	iters: 300, epoch: 1 | loss: 0.1423440
	speed: 0.0755s/iter; left time: 217.5928s
Epoch: 1 cost time: 25.497751235961914
Epoch: 1, Steps: 318 | Train Loss: 0.3149078 Vali Loss: 0.1573213 Test Loss: 0.1281470
Validation loss decreased (inf --> 0.157321).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1317795
	speed: 0.0801s/iter; left time: 221.3332s
	iters: 200, epoch: 2 | loss: 0.1230793
	speed: 0.0786s/iter; left time: 209.3816s
	iters: 300, epoch: 2 | loss: 0.1234975
	speed: 0.0777s/iter; left time: 199.1728s
Epoch: 2 cost time: 25.02638578414917
Epoch: 2, Steps: 318 | Train Loss: 0.1368138 Vali Loss: 0.1458016 Test Loss: 0.1125156
Validation loss decreased (0.157321 --> 0.145802).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1602222
	speed: 0.0834s/iter; left time: 203.8869s
	iters: 200, epoch: 3 | loss: 0.1129277
	speed: 0.0801s/iter; left time: 187.8632s
	iters: 300, epoch: 3 | loss: 0.1044858
	speed: 0.0737s/iter; left time: 165.5316s
Epoch: 3 cost time: 25.11382484436035
Epoch: 3, Steps: 318 | Train Loss: 0.1306641 Vali Loss: 0.1439577 Test Loss: 0.1115776
Validation loss decreased (0.145802 --> 0.143958).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1118942
	speed: 0.0791s/iter; left time: 168.1860s
	iters: 200, epoch: 4 | loss: 0.1059837
	speed: 0.0778s/iter; left time: 157.7675s
	iters: 300, epoch: 4 | loss: 0.1489335
	speed: 0.0754s/iter; left time: 145.3256s
Epoch: 4 cost time: 24.61967635154724
Epoch: 4, Steps: 318 | Train Loss: 0.1279356 Vali Loss: 0.1441105 Test Loss: 0.1111322
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1433842
	speed: 0.0829s/iter; left time: 150.0111s
	iters: 200, epoch: 5 | loss: 0.1209671
	speed: 0.0789s/iter; left time: 134.8699s
	iters: 300, epoch: 5 | loss: 0.1076101
	speed: 0.0787s/iter; left time: 126.7007s
Epoch: 5 cost time: 25.52502417564392
Epoch: 5, Steps: 318 | Train Loss: 0.1267345 Vali Loss: 0.1439652 Test Loss: 0.1099460
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1280780
	speed: 0.0799s/iter; left time: 119.0903s
	iters: 200, epoch: 6 | loss: 0.1250194
	speed: 0.0795s/iter; left time: 110.5847s
	iters: 300, epoch: 6 | loss: 0.1356056
	speed: 0.0785s/iter; left time: 101.3736s
Epoch: 6 cost time: 25.159769535064697
Epoch: 6, Steps: 318 | Train Loss: 0.1258926 Vali Loss: 0.1430611 Test Loss: 0.1101502
Validation loss decreased (0.143958 --> 0.143061).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1658080
	speed: 0.0823s/iter; left time: 96.5073s
	iters: 200, epoch: 7 | loss: 0.1284342
	speed: 0.0768s/iter; left time: 82.4421s
	iters: 300, epoch: 7 | loss: 0.1040702
	speed: 0.0775s/iter; left time: 75.3941s
Epoch: 7 cost time: 25.127504110336304
Epoch: 7, Steps: 318 | Train Loss: 0.1255766 Vali Loss: 0.1419779 Test Loss: 0.1100597
Validation loss decreased (0.143061 --> 0.141978).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1084709
	speed: 0.0836s/iter; left time: 71.5110s
	iters: 200, epoch: 8 | loss: 0.1138494
	speed: 0.0786s/iter; left time: 59.3167s
	iters: 300, epoch: 8 | loss: 0.1624417
	speed: 0.0794s/iter; left time: 52.0339s
Epoch: 8 cost time: 25.56994318962097
Epoch: 8, Steps: 318 | Train Loss: 0.1254996 Vali Loss: 0.1426326 Test Loss: 0.1099752
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0941234
	speed: 0.0800s/iter; left time: 42.9706s
	iters: 200, epoch: 9 | loss: 0.1362051
	speed: 0.0786s/iter; left time: 34.3322s
	iters: 300, epoch: 9 | loss: 0.1090035
	speed: 0.0760s/iter; left time: 25.6225s
Epoch: 9 cost time: 24.933111906051636
Epoch: 9, Steps: 318 | Train Loss: 0.1253349 Vali Loss: 0.1425895 Test Loss: 0.1099479
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1115181
	speed: 0.0810s/iter; left time: 17.7448s
	iters: 200, epoch: 10 | loss: 0.1377242
	speed: 0.0767s/iter; left time: 9.1239s
	iters: 300, epoch: 10 | loss: 0.1015540
	speed: 0.0753s/iter; left time: 1.4302s
Epoch: 10 cost time: 24.81859016418457
Epoch: 10, Steps: 318 | Train Loss: 0.1252653 Vali Loss: 0.1431186 Test Loss: 0.1099505
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1920047
	speed: 0.0956s/iter; left time: 294.5272s
	iters: 200, epoch: 1 | loss: 0.1708332
	speed: 0.0706s/iter; left time: 210.4263s
	iters: 300, epoch: 1 | loss: 0.1347150
	speed: 0.0749s/iter; left time: 215.8547s
Epoch: 1 cost time: 25.555988073349
Epoch: 1, Steps: 318 | Train Loss: 0.2608271 Vali Loss: 0.1558429 Test Loss: 0.1166015
Validation loss decreased (inf --> 0.155843).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1238453
	speed: 0.0778s/iter; left time: 214.9945s
	iters: 200, epoch: 2 | loss: 0.1326042
	speed: 0.0734s/iter; left time: 195.4211s
	iters: 300, epoch: 2 | loss: 0.1137410
	speed: 0.0733s/iter; left time: 187.7778s
Epoch: 2 cost time: 23.849494695663452
Epoch: 2, Steps: 318 | Train Loss: 0.1373147 Vali Loss: 0.1474606 Test Loss: 0.1117139
Validation loss decreased (0.155843 --> 0.147461).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1227169
	speed: 0.0763s/iter; left time: 186.4454s
	iters: 200, epoch: 3 | loss: 0.1029455
	speed: 0.0725s/iter; left time: 169.8966s
	iters: 300, epoch: 3 | loss: 0.0966076
	speed: 0.0701s/iter; left time: 157.4501s
Epoch: 3 cost time: 23.21441388130188
Epoch: 3, Steps: 318 | Train Loss: 0.1303312 Vali Loss: 0.1455672 Test Loss: 0.1077227
Validation loss decreased (0.147461 --> 0.145567).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1503646
	speed: 0.0771s/iter; left time: 163.8904s
	iters: 200, epoch: 4 | loss: 0.1109722
	speed: 0.0740s/iter; left time: 150.0813s
	iters: 300, epoch: 4 | loss: 0.0996253
	speed: 0.0706s/iter; left time: 135.9971s
Epoch: 4 cost time: 23.49989652633667
Epoch: 4, Steps: 318 | Train Loss: 0.1272615 Vali Loss: 0.1445850 Test Loss: 0.1061658
Validation loss decreased (0.145567 --> 0.144585).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1272756
	speed: 0.0773s/iter; left time: 139.7844s
	iters: 200, epoch: 5 | loss: 0.1319857
	speed: 0.0755s/iter; left time: 129.0072s
	iters: 300, epoch: 5 | loss: 0.1113151
	speed: 0.0733s/iter; left time: 117.9872s
Epoch: 5 cost time: 23.984379529953003
Epoch: 5, Steps: 318 | Train Loss: 0.1258836 Vali Loss: 0.1433410 Test Loss: 0.1063332
Validation loss decreased (0.144585 --> 0.143341).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1089670
	speed: 0.0768s/iter; left time: 114.5476s
	iters: 200, epoch: 6 | loss: 0.0911193
	speed: 0.0722s/iter; left time: 100.3706s
	iters: 300, epoch: 6 | loss: 0.1117341
	speed: 0.0720s/iter; left time: 92.9732s
Epoch: 6 cost time: 23.52859592437744
Epoch: 6, Steps: 318 | Train Loss: 0.1251969 Vali Loss: 0.1427325 Test Loss: 0.1058150
Validation loss decreased (0.143341 --> 0.142732).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1291995
	speed: 0.0764s/iter; left time: 89.6608s
	iters: 200, epoch: 7 | loss: 0.1597209
	speed: 0.0723s/iter; left time: 77.5701s
	iters: 300, epoch: 7 | loss: 0.1400830
	speed: 0.0751s/iter; left time: 73.1050s
Epoch: 7 cost time: 23.74656057357788
Epoch: 7, Steps: 318 | Train Loss: 0.1247807 Vali Loss: 0.1434985 Test Loss: 0.1057675
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1248235
	speed: 0.0810s/iter; left time: 69.2929s
	iters: 200, epoch: 8 | loss: 0.1480213
	speed: 0.0735s/iter; left time: 55.5025s
	iters: 300, epoch: 8 | loss: 0.1181716
	speed: 0.0722s/iter; left time: 47.3010s
Epoch: 8 cost time: 24.095147848129272
Epoch: 8, Steps: 318 | Train Loss: 0.1247479 Vali Loss: 0.1433281 Test Loss: 0.1057643
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1403538
	speed: 0.0772s/iter; left time: 41.4682s
	iters: 200, epoch: 9 | loss: 0.1388444
	speed: 0.0730s/iter; left time: 31.8958s
	iters: 300, epoch: 9 | loss: 0.1111509
	speed: 0.0735s/iter; left time: 24.7612s
Epoch: 9 cost time: 23.765049934387207
Epoch: 9, Steps: 318 | Train Loss: 0.1246910 Vali Loss: 0.1430790 Test Loss: 0.1057392
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1983210
	speed: 0.0971s/iter; left time: 298.2784s
	iters: 200, epoch: 1 | loss: 0.1591809
	speed: 0.0780s/iter; left time: 231.7755s
	iters: 300, epoch: 1 | loss: 0.1325184
	speed: 0.0755s/iter; left time: 216.8892s
Epoch: 1 cost time: 26.549559831619263
Epoch: 1, Steps: 317 | Train Loss: 0.2766013 Vali Loss: 0.1551688 Test Loss: 0.1206490
Validation loss decreased (inf --> 0.155169).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1491417
	speed: 0.0838s/iter; left time: 230.7761s
	iters: 200, epoch: 2 | loss: 0.1803607
	speed: 0.0818s/iter; left time: 217.0413s
	iters: 300, epoch: 2 | loss: 0.1549378
	speed: 0.0814s/iter; left time: 207.9701s
Epoch: 2 cost time: 26.02877974510193
Epoch: 2, Steps: 317 | Train Loss: 0.1345884 Vali Loss: 0.1474250 Test Loss: 0.1137629
Validation loss decreased (0.155169 --> 0.147425).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1325245
	speed: 0.0853s/iter; left time: 207.8519s
	iters: 200, epoch: 3 | loss: 0.1216903
	speed: 0.0845s/iter; left time: 197.5118s
	iters: 300, epoch: 3 | loss: 0.1219782
	speed: 0.0805s/iter; left time: 179.9897s
Epoch: 3 cost time: 26.481138467788696
Epoch: 3, Steps: 317 | Train Loss: 0.1280925 Vali Loss: 0.1447463 Test Loss: 0.1110861
Validation loss decreased (0.147425 --> 0.144746).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1248300
	speed: 0.0853s/iter; left time: 180.8094s
	iters: 200, epoch: 4 | loss: 0.0977877
	speed: 0.0788s/iter; left time: 159.2248s
	iters: 300, epoch: 4 | loss: 0.0971421
	speed: 0.0801s/iter; left time: 153.8836s
Epoch: 4 cost time: 25.674705266952515
Epoch: 4, Steps: 317 | Train Loss: 0.1254440 Vali Loss: 0.1442346 Test Loss: 0.1099040
Validation loss decreased (0.144746 --> 0.144235).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0975244
	speed: 0.0825s/iter; left time: 148.7451s
	iters: 200, epoch: 5 | loss: 0.1630779
	speed: 0.0828s/iter; left time: 140.9475s
	iters: 300, epoch: 5 | loss: 0.1350117
	speed: 0.0788s/iter; left time: 126.2455s
Epoch: 5 cost time: 25.69332480430603
Epoch: 5, Steps: 317 | Train Loss: 0.1239115 Vali Loss: 0.1439138 Test Loss: 0.1096946
Validation loss decreased (0.144235 --> 0.143914).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1263959
	speed: 0.0810s/iter; left time: 120.3552s
	iters: 200, epoch: 6 | loss: 0.1515345
	speed: 0.0762s/iter; left time: 105.5768s
	iters: 300, epoch: 6 | loss: 0.1044405
	speed: 0.0789s/iter; left time: 101.4470s
Epoch: 6 cost time: 24.868425369262695
Epoch: 6, Steps: 317 | Train Loss: 0.1230770 Vali Loss: 0.1437964 Test Loss: 0.1093332
Validation loss decreased (0.143914 --> 0.143796).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0894872
	speed: 0.0819s/iter; left time: 95.7887s
	iters: 200, epoch: 7 | loss: 0.1318366
	speed: 0.0775s/iter; left time: 82.8017s
	iters: 300, epoch: 7 | loss: 0.1383357
	speed: 0.0758s/iter; left time: 73.4493s
Epoch: 7 cost time: 24.81787109375
Epoch: 7, Steps: 317 | Train Loss: 0.1228343 Vali Loss: 0.1438017 Test Loss: 0.1092695
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1042098
	speed: 0.1131s/iter; left time: 96.3215s
	iters: 200, epoch: 8 | loss: 0.1252995
	speed: 0.1148s/iter; left time: 86.3030s
	iters: 300, epoch: 8 | loss: 0.1045752
	speed: 0.1256s/iter; left time: 81.8932s
Epoch: 8 cost time: 36.89267873764038
Epoch: 8, Steps: 317 | Train Loss: 0.1226740 Vali Loss: 0.1438828 Test Loss: 0.1092395
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1514288
	speed: 0.1425s/iter; left time: 76.2166s
	iters: 200, epoch: 9 | loss: 0.1563009
	speed: 0.0910s/iter; left time: 39.5979s
	iters: 300, epoch: 9 | loss: 0.1192042
	speed: 0.0803s/iter; left time: 26.8900s
Epoch: 9 cost time: 32.66443991661072
Epoch: 9, Steps: 317 | Train Loss: 0.1225000 Vali Loss: 0.1438594 Test Loss: 0.1092109
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2598159
	speed: 0.1085s/iter; left time: 334.1436s
	iters: 200, epoch: 1 | loss: 0.1281575
	speed: 0.0935s/iter; left time: 278.8584s
	iters: 300, epoch: 1 | loss: 0.1139176
	speed: 0.0908s/iter; left time: 261.6194s
Epoch: 1 cost time: 31.01931405067444
Epoch: 1, Steps: 318 | Train Loss: 0.2684757 Vali Loss: 0.1394325 Test Loss: 0.1163890
Validation loss decreased (inf --> 0.139432).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1328055
	speed: 0.0995s/iter; left time: 274.8080s
	iters: 200, epoch: 2 | loss: 0.1031381
	speed: 0.0948s/iter; left time: 252.5324s
	iters: 300, epoch: 2 | loss: 0.0960603
	speed: 0.0971s/iter; left time: 248.9824s
Epoch: 2 cost time: 30.964614868164062
Epoch: 2, Steps: 318 | Train Loss: 0.1215840 Vali Loss: 0.1233767 Test Loss: 0.0970986
Validation loss decreased (0.139432 --> 0.123377).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1156973
	speed: 0.0995s/iter; left time: 243.1558s
	iters: 200, epoch: 3 | loss: 0.1024786
	speed: 0.0918s/iter; left time: 215.1937s
	iters: 300, epoch: 3 | loss: 0.1054392
	speed: 0.0946s/iter; left time: 212.3125s
Epoch: 3 cost time: 30.263600826263428
Epoch: 3, Steps: 318 | Train Loss: 0.1129648 Vali Loss: 0.1211357 Test Loss: 0.0944108
Validation loss decreased (0.123377 --> 0.121136).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0954555
	speed: 0.0973s/iter; left time: 206.8981s
	iters: 200, epoch: 4 | loss: 0.1009945
	speed: 0.0935s/iter; left time: 189.5246s
	iters: 300, epoch: 4 | loss: 0.1286710
	speed: 0.0954s/iter; left time: 183.8552s
Epoch: 4 cost time: 30.456204414367676
Epoch: 4, Steps: 318 | Train Loss: 0.1098157 Vali Loss: 0.1183275 Test Loss: 0.0917079
Validation loss decreased (0.121136 --> 0.118327).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0920684
	speed: 0.0992s/iter; left time: 179.4076s
	iters: 200, epoch: 5 | loss: 0.1354201
	speed: 0.0954s/iter; left time: 162.9724s
	iters: 300, epoch: 5 | loss: 0.0907935
	speed: 0.0948s/iter; left time: 152.5071s
Epoch: 5 cost time: 30.653716802597046
Epoch: 5, Steps: 318 | Train Loss: 0.1074021 Vali Loss: 0.1167357 Test Loss: 0.0914494
Validation loss decreased (0.118327 --> 0.116736).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1278501
	speed: 0.0991s/iter; left time: 147.8037s
	iters: 200, epoch: 6 | loss: 0.0931818
	speed: 0.0942s/iter; left time: 131.0040s
	iters: 300, epoch: 6 | loss: 0.1110863
	speed: 0.0928s/iter; left time: 119.8313s
Epoch: 6 cost time: 30.463050842285156
Epoch: 6, Steps: 318 | Train Loss: 0.1063414 Vali Loss: 0.1176721 Test Loss: 0.0901070
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0827645
	speed: 0.0972s/iter; left time: 114.0078s
	iters: 200, epoch: 7 | loss: 0.1268501
	speed: 0.0964s/iter; left time: 103.4060s
	iters: 300, epoch: 7 | loss: 0.1265349
	speed: 0.0947s/iter; left time: 92.1010s
Epoch: 7 cost time: 30.634192943572998
Epoch: 7, Steps: 318 | Train Loss: 0.1063796 Vali Loss: 0.1165895 Test Loss: 0.0899566
Validation loss decreased (0.116736 --> 0.116590).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1009067
	speed: 0.0966s/iter; left time: 82.5969s
	iters: 200, epoch: 8 | loss: 0.0980247
	speed: 0.0944s/iter; left time: 71.2644s
	iters: 300, epoch: 8 | loss: 0.1053917
	speed: 0.0951s/iter; left time: 62.2959s
Epoch: 8 cost time: 30.315393686294556
Epoch: 8, Steps: 318 | Train Loss: 0.1057387 Vali Loss: 0.1180751 Test Loss: 0.0898582
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0896084
	speed: 0.0954s/iter; left time: 51.2102s
	iters: 200, epoch: 9 | loss: 0.1216582
	speed: 0.0935s/iter; left time: 40.8766s
	iters: 300, epoch: 9 | loss: 0.0850079
	speed: 0.0924s/iter; left time: 31.1356s
Epoch: 9 cost time: 29.86994481086731
Epoch: 9, Steps: 318 | Train Loss: 0.1059526 Vali Loss: 0.1167728 Test Loss: 0.0898220
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0958516
	speed: 0.0976s/iter; left time: 21.3786s
	iters: 200, epoch: 10 | loss: 0.0935660
	speed: 0.0944s/iter; left time: 11.2315s
	iters: 300, epoch: 10 | loss: 0.1256097
	speed: 0.0948s/iter; left time: 1.8006s
Epoch: 10 cost time: 30.454004287719727
Epoch: 10, Steps: 318 | Train Loss: 0.1057696 Vali Loss: 0.1166538 Test Loss: 0.0897746
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2104176
	speed: 0.1093s/iter; left time: 336.7376s
	iters: 200, epoch: 1 | loss: 0.2094314
	speed: 0.0908s/iter; left time: 270.6846s
	iters: 300, epoch: 1 | loss: 0.1624994
	speed: 0.0910s/iter; left time: 262.2258s
Epoch: 1 cost time: 30.733142614364624
Epoch: 1, Steps: 318 | Train Loss: 0.2832154 Vali Loss: 0.1583856 Test Loss: 0.1282608
Validation loss decreased (inf --> 0.158386).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1334442
	speed: 0.0947s/iter; left time: 261.6319s
	iters: 200, epoch: 2 | loss: 0.1326853
	speed: 0.0887s/iter; left time: 236.1189s
	iters: 300, epoch: 2 | loss: 0.1113955
	speed: 0.0826s/iter; left time: 211.5991s
Epoch: 2 cost time: 28.17212700843811
Epoch: 2, Steps: 318 | Train Loss: 0.1454274 Vali Loss: 0.1469115 Test Loss: 0.1151103
Validation loss decreased (0.158386 --> 0.146912).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1437899
	speed: 0.0898s/iter; left time: 219.5769s
	iters: 200, epoch: 3 | loss: 0.1326366
	speed: 0.0837s/iter; left time: 196.2116s
	iters: 300, epoch: 3 | loss: 0.1305237
	speed: 0.0855s/iter; left time: 191.9540s
Epoch: 3 cost time: 27.50437879562378
Epoch: 3, Steps: 318 | Train Loss: 0.1338070 Vali Loss: 0.1432132 Test Loss: 0.1112761
Validation loss decreased (0.146912 --> 0.143213).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0959294
	speed: 0.0895s/iter; left time: 190.4279s
	iters: 200, epoch: 4 | loss: 0.1389560
	speed: 0.0885s/iter; left time: 179.4725s
	iters: 300, epoch: 4 | loss: 0.1176640
	speed: 0.0869s/iter; left time: 167.4868s
Epoch: 4 cost time: 28.134140014648438
Epoch: 4, Steps: 318 | Train Loss: 0.1289724 Vali Loss: 0.1421882 Test Loss: 0.1089619
Validation loss decreased (0.143213 --> 0.142188).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1258512
	speed: 0.0937s/iter; left time: 169.4668s
	iters: 200, epoch: 5 | loss: 0.1441698
	speed: 0.0892s/iter; left time: 152.3724s
	iters: 300, epoch: 5 | loss: 0.1332695
	speed: 0.0843s/iter; left time: 135.5613s
Epoch: 5 cost time: 28.285257816314697
Epoch: 5, Steps: 318 | Train Loss: 0.1263861 Vali Loss: 0.1435241 Test Loss: 0.1095181
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1482615
	speed: 0.0912s/iter; left time: 136.0060s
	iters: 200, epoch: 6 | loss: 0.1082954
	speed: 0.0843s/iter; left time: 117.2516s
	iters: 300, epoch: 6 | loss: 0.1156786
	speed: 0.0879s/iter; left time: 113.5249s
Epoch: 6 cost time: 28.03356671333313
Epoch: 6, Steps: 318 | Train Loss: 0.1254578 Vali Loss: 0.1422934 Test Loss: 0.1088615
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1103624
	speed: 0.0917s/iter; left time: 107.6185s
	iters: 200, epoch: 7 | loss: 0.1410905
	speed: 0.0866s/iter; left time: 92.9337s
	iters: 300, epoch: 7 | loss: 0.1122173
	speed: 0.0899s/iter; left time: 87.4906s
Epoch: 7 cost time: 28.354083776474
Epoch: 7, Steps: 318 | Train Loss: 0.1246906 Vali Loss: 0.1426398 Test Loss: 0.1088126
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2640982
	speed: 0.1127s/iter; left time: 347.1946s
	iters: 200, epoch: 1 | loss: 0.1392104
	speed: 0.0928s/iter; left time: 276.4980s
	iters: 300, epoch: 1 | loss: 0.1349009
	speed: 0.0929s/iter; left time: 267.7837s
Epoch: 1 cost time: 31.50411558151245
Epoch: 1, Steps: 318 | Train Loss: 0.2918864 Vali Loss: 0.1490668 Test Loss: 0.1198975
Validation loss decreased (inf --> 0.149067).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1235300
	speed: 0.0968s/iter; left time: 267.5125s
	iters: 200, epoch: 2 | loss: 0.1171097
	speed: 0.0920s/iter; left time: 245.1144s
	iters: 300, epoch: 2 | loss: 0.1228361
	speed: 0.0910s/iter; left time: 233.2322s
Epoch: 2 cost time: 29.66860866546631
Epoch: 2, Steps: 318 | Train Loss: 0.1317352 Vali Loss: 0.1429709 Test Loss: 0.1086087
Validation loss decreased (0.149067 --> 0.142971).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1425143
	speed: 0.0961s/iter; left time: 234.9484s
	iters: 200, epoch: 3 | loss: 0.0988787
	speed: 0.0946s/iter; left time: 221.9420s
	iters: 300, epoch: 3 | loss: 0.1036187
	speed: 0.0944s/iter; left time: 211.8585s
Epoch: 3 cost time: 30.24397873878479
Epoch: 3, Steps: 318 | Train Loss: 0.1259127 Vali Loss: 0.1410134 Test Loss: 0.1077970
Validation loss decreased (0.142971 --> 0.141013).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0967819
	speed: 0.0968s/iter; left time: 205.9162s
	iters: 200, epoch: 4 | loss: 0.1009651
	speed: 0.0932s/iter; left time: 188.8855s
	iters: 300, epoch: 4 | loss: 0.1445753
	speed: 0.0909s/iter; left time: 175.1533s
Epoch: 4 cost time: 29.819570779800415
Epoch: 4, Steps: 318 | Train Loss: 0.1231781 Vali Loss: 0.1408409 Test Loss: 0.1073302
Validation loss decreased (0.141013 --> 0.140841).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1331864
	speed: 0.0983s/iter; left time: 177.8735s
	iters: 200, epoch: 5 | loss: 0.1251618
	speed: 0.0927s/iter; left time: 158.5033s
	iters: 300, epoch: 5 | loss: 0.1053597
	speed: 0.0933s/iter; left time: 150.1060s
Epoch: 5 cost time: 30.140716791152954
Epoch: 5, Steps: 318 | Train Loss: 0.1219291 Vali Loss: 0.1410914 Test Loss: 0.1069397
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1285022
	speed: 0.1336s/iter; left time: 199.1858s
	iters: 200, epoch: 6 | loss: 0.1157318
	speed: 0.1280s/iter; left time: 177.9953s
	iters: 300, epoch: 6 | loss: 0.1384374
	speed: 0.1259s/iter; left time: 162.5321s
Epoch: 6 cost time: 41.192984104156494
Epoch: 6, Steps: 318 | Train Loss: 0.1209962 Vali Loss: 0.1400979 Test Loss: 0.1072322
Validation loss decreased (0.140841 --> 0.140098).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1623017
	speed: 0.0962s/iter; left time: 112.8864s
	iters: 200, epoch: 7 | loss: 0.1225621
	speed: 0.0953s/iter; left time: 102.2688s
	iters: 300, epoch: 7 | loss: 0.1016310
	speed: 0.0953s/iter; left time: 92.7198s
Epoch: 7 cost time: 30.47105050086975
Epoch: 7, Steps: 318 | Train Loss: 0.1208047 Vali Loss: 0.1393342 Test Loss: 0.1070426
Validation loss decreased (0.140098 --> 0.139334).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1110075
	speed: 0.0961s/iter; left time: 82.1911s
	iters: 200, epoch: 8 | loss: 0.1088000
	speed: 0.0965s/iter; left time: 72.8268s
	iters: 300, epoch: 8 | loss: 0.1716577
	speed: 0.0914s/iter; left time: 59.8623s
Epoch: 8 cost time: 30.134552717208862
Epoch: 8, Steps: 318 | Train Loss: 0.1206996 Vali Loss: 0.1399095 Test Loss: 0.1069357
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0832036
	speed: 0.0983s/iter; left time: 52.7917s
	iters: 200, epoch: 9 | loss: 0.1297245
	speed: 0.0944s/iter; left time: 41.2353s
	iters: 300, epoch: 9 | loss: 0.0989273
	speed: 0.0949s/iter; left time: 31.9959s
Epoch: 9 cost time: 30.562467575073242
Epoch: 9, Steps: 318 | Train Loss: 0.1203613 Vali Loss: 0.1401337 Test Loss: 0.1069433
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1114810
	speed: 0.0977s/iter; left time: 21.3881s
	iters: 200, epoch: 10 | loss: 0.1396337
	speed: 0.0956s/iter; left time: 11.3794s
	iters: 300, epoch: 10 | loss: 0.1098097
	speed: 0.0947s/iter; left time: 1.8001s
Epoch: 10 cost time: 30.618722438812256
Epoch: 10, Steps: 318 | Train Loss: 0.1200930 Vali Loss: 0.1402729 Test Loss: 0.1069497
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1912680
	speed: 0.1121s/iter; left time: 345.4657s
	iters: 200, epoch: 1 | loss: 0.1671302
	speed: 0.0906s/iter; left time: 270.1876s
	iters: 300, epoch: 1 | loss: 0.1209965
	speed: 0.0875s/iter; left time: 252.0757s
Epoch: 1 cost time: 30.605830192565918
Epoch: 1, Steps: 318 | Train Loss: 0.2419635 Vali Loss: 0.1515627 Test Loss: 0.1163843
Validation loss decreased (inf --> 0.151563).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1221142
	speed: 0.0949s/iter; left time: 262.2693s
	iters: 200, epoch: 2 | loss: 0.1296787
	speed: 0.0891s/iter; left time: 237.3773s
	iters: 300, epoch: 2 | loss: 0.1053710
	speed: 0.0883s/iter; left time: 226.2248s
Epoch: 2 cost time: 28.904074668884277
Epoch: 2, Steps: 318 | Train Loss: 0.1322634 Vali Loss: 0.1425733 Test Loss: 0.1119101
Validation loss decreased (0.151563 --> 0.142573).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1151332
	speed: 0.0923s/iter; left time: 225.6706s
	iters: 200, epoch: 3 | loss: 0.1223176
	speed: 0.0883s/iter; left time: 207.0717s
	iters: 300, epoch: 3 | loss: 0.1038480
	speed: 0.0895s/iter; left time: 200.9819s
Epoch: 3 cost time: 28.785532474517822
Epoch: 3, Steps: 318 | Train Loss: 0.1246629 Vali Loss: 0.1403779 Test Loss: 0.1086640
Validation loss decreased (0.142573 --> 0.140378).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1526523
	speed: 0.0890s/iter; left time: 189.4018s
	iters: 200, epoch: 4 | loss: 0.1075016
	speed: 0.0882s/iter; left time: 178.8028s
	iters: 300, epoch: 4 | loss: 0.0987713
	speed: 0.0883s/iter; left time: 170.0901s
Epoch: 4 cost time: 28.19410991668701
Epoch: 4, Steps: 318 | Train Loss: 0.1213388 Vali Loss: 0.1384402 Test Loss: 0.1085553
Validation loss decreased (0.140378 --> 0.138440).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1278198
	speed: 0.0897s/iter; left time: 162.3370s
	iters: 200, epoch: 5 | loss: 0.1201128
	speed: 0.0864s/iter; left time: 147.6948s
	iters: 300, epoch: 5 | loss: 0.0948652
	speed: 0.0865s/iter; left time: 139.2465s
Epoch: 5 cost time: 27.92520761489868
Epoch: 5, Steps: 318 | Train Loss: 0.1193603 Vali Loss: 0.1389843 Test Loss: 0.1086060
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0932767
	speed: 0.0874s/iter; left time: 130.2620s
	iters: 200, epoch: 6 | loss: 0.0910209
	speed: 0.0850s/iter; left time: 118.2790s
	iters: 300, epoch: 6 | loss: 0.1103366
	speed: 0.0855s/iter; left time: 110.3510s
Epoch: 6 cost time: 27.407382488250732
Epoch: 6, Steps: 318 | Train Loss: 0.1183946 Vali Loss: 0.1385988 Test Loss: 0.1085967
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1244103
	speed: 0.0944s/iter; left time: 110.7619s
	iters: 200, epoch: 7 | loss: 0.1531544
	speed: 0.0880s/iter; left time: 94.4069s
	iters: 300, epoch: 7 | loss: 0.1190703
	speed: 0.0883s/iter; left time: 85.9641s
Epoch: 7 cost time: 28.807923078536987
Epoch: 7, Steps: 318 | Train Loss: 0.1178560 Vali Loss: 0.1392221 Test Loss: 0.1088890
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1977883
	speed: 0.1156s/iter; left time: 354.8663s
	iters: 200, epoch: 1 | loss: 0.1587711
	speed: 0.0944s/iter; left time: 280.4061s
	iters: 300, epoch: 1 | loss: 0.1332079
	speed: 0.0940s/iter; left time: 269.9225s
Epoch: 1 cost time: 32.18646693229675
Epoch: 1, Steps: 317 | Train Loss: 0.2773201 Vali Loss: 0.1550594 Test Loss: 0.1206934
Validation loss decreased (inf --> 0.155059).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1490591
	speed: 0.1016s/iter; left time: 279.8630s
	iters: 200, epoch: 2 | loss: 0.1802861
	speed: 0.1006s/iter; left time: 266.8924s
	iters: 300, epoch: 2 | loss: 0.1551410
	speed: 0.0972s/iter; left time: 248.3058s
Epoch: 2 cost time: 31.646262168884277
Epoch: 2, Steps: 317 | Train Loss: 0.1346842 Vali Loss: 0.1475298 Test Loss: 0.1140287
Validation loss decreased (0.155059 --> 0.147530).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1329319
	speed: 0.1021s/iter; left time: 248.7963s
	iters: 200, epoch: 3 | loss: 0.1218612
	speed: 0.0967s/iter; left time: 225.9489s
	iters: 300, epoch: 3 | loss: 0.1223650
	speed: 0.0957s/iter; left time: 214.0113s
Epoch: 3 cost time: 31.09375500679016
Epoch: 3, Steps: 317 | Train Loss: 0.1283063 Vali Loss: 0.1447185 Test Loss: 0.1114317
Validation loss decreased (0.147530 --> 0.144719).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1263174
	speed: 0.1012s/iter; left time: 214.5076s
	iters: 200, epoch: 4 | loss: 0.0975956
	speed: 0.0953s/iter; left time: 192.4092s
	iters: 300, epoch: 4 | loss: 0.0970461
	speed: 0.0930s/iter; left time: 178.5725s
Epoch: 4 cost time: 30.75688600540161
Epoch: 4, Steps: 317 | Train Loss: 0.1257491 Vali Loss: 0.1442604 Test Loss: 0.1104556
Validation loss decreased (0.144719 --> 0.144260).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0980687
	speed: 0.0970s/iter; left time: 174.8339s
	iters: 200, epoch: 5 | loss: 0.1635078
	speed: 0.0942s/iter; left time: 160.3391s
	iters: 300, epoch: 5 | loss: 0.1305091
	speed: 0.0951s/iter; left time: 152.5184s
Epoch: 5 cost time: 30.282642364501953
Epoch: 5, Steps: 317 | Train Loss: 0.1241168 Vali Loss: 0.1439236 Test Loss: 0.1102868
Validation loss decreased (0.144260 --> 0.143924).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1266739
	speed: 0.0991s/iter; left time: 147.2939s
	iters: 200, epoch: 6 | loss: 0.1518733
	speed: 0.0938s/iter; left time: 130.0256s
	iters: 300, epoch: 6 | loss: 0.1056644
	speed: 0.0947s/iter; left time: 121.7927s
Epoch: 6 cost time: 30.460070371627808
Epoch: 6, Steps: 317 | Train Loss: 0.1232574 Vali Loss: 0.1437463 Test Loss: 0.1099614
Validation loss decreased (0.143924 --> 0.143746).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0899166
	speed: 0.0990s/iter; left time: 115.7588s
	iters: 200, epoch: 7 | loss: 0.1316297
	speed: 0.0967s/iter; left time: 103.4018s
	iters: 300, epoch: 7 | loss: 0.1387286
	speed: 0.0963s/iter; left time: 93.3618s
Epoch: 7 cost time: 30.89749574661255
Epoch: 7, Steps: 317 | Train Loss: 0.1229898 Vali Loss: 0.1438302 Test Loss: 0.1099083
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1043817
	speed: 0.1001s/iter; left time: 85.2951s
	iters: 200, epoch: 8 | loss: 0.1259270
	speed: 0.0977s/iter; left time: 73.4473s
	iters: 300, epoch: 8 | loss: 0.1024905
	speed: 0.0952s/iter; left time: 62.0634s
Epoch: 8 cost time: 30.995854139328003
Epoch: 8, Steps: 317 | Train Loss: 0.1228000 Vali Loss: 0.1439518 Test Loss: 0.1099155
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1518180
	speed: 0.0995s/iter; left time: 53.2186s
	iters: 200, epoch: 9 | loss: 0.1569912
	speed: 0.0981s/iter; left time: 42.6614s
	iters: 300, epoch: 9 | loss: 0.1193657
	speed: 0.0988s/iter; left time: 33.0853s
Epoch: 9 cost time: 31.333847522735596
Epoch: 9, Steps: 317 | Train Loss: 0.1226533 Vali Loss: 0.1439489 Test Loss: 0.1098873
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2598256
	speed: 0.1223s/iter; left time: 376.7974s
	iters: 200, epoch: 1 | loss: 0.1281000
	speed: 0.1102s/iter; left time: 328.4131s
	iters: 300, epoch: 1 | loss: 0.1140030
	speed: 0.1089s/iter; left time: 313.8146s
Epoch: 1 cost time: 36.17472958564758
Epoch: 1, Steps: 318 | Train Loss: 0.2681340 Vali Loss: 0.1396956 Test Loss: 0.1165634
Validation loss decreased (inf --> 0.139696).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1332224
	speed: 0.1254s/iter; left time: 346.3639s
	iters: 200, epoch: 2 | loss: 0.1047388
	speed: 0.1062s/iter; left time: 282.7300s
	iters: 300, epoch: 2 | loss: 0.0958880
	speed: 0.1168s/iter; left time: 299.3640s
Epoch: 2 cost time: 37.108802318573
Epoch: 2, Steps: 318 | Train Loss: 0.1217821 Vali Loss: 0.1236585 Test Loss: 0.0972031
Validation loss decreased (0.139696 --> 0.123659).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1157290
	speed: 0.1170s/iter; left time: 285.9752s
	iters: 200, epoch: 3 | loss: 0.1027573
	speed: 0.1139s/iter; left time: 267.1067s
	iters: 300, epoch: 3 | loss: 0.1054437
	speed: 0.1117s/iter; left time: 250.7332s
Epoch: 3 cost time: 36.336899518966675
Epoch: 3, Steps: 318 | Train Loss: 0.1131064 Vali Loss: 0.1213192 Test Loss: 0.0944771
Validation loss decreased (0.123659 --> 0.121319).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0970318
	speed: 0.1100s/iter; left time: 234.0133s
	iters: 200, epoch: 4 | loss: 0.1018124
	speed: 0.1125s/iter; left time: 228.0748s
	iters: 300, epoch: 4 | loss: 0.1295721
	speed: 0.1092s/iter; left time: 210.3375s
Epoch: 4 cost time: 35.09159874916077
Epoch: 4, Steps: 318 | Train Loss: 0.1099338 Vali Loss: 0.1185766 Test Loss: 0.0916364
Validation loss decreased (0.121319 --> 0.118577).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0919188
	speed: 0.1128s/iter; left time: 204.1428s
	iters: 200, epoch: 5 | loss: 0.1357600
	speed: 0.1098s/iter; left time: 187.6181s
	iters: 300, epoch: 5 | loss: 0.0913434
	speed: 0.1098s/iter; left time: 176.6711s
Epoch: 5 cost time: 35.25177979469299
Epoch: 5, Steps: 318 | Train Loss: 0.1076141 Vali Loss: 0.1169802 Test Loss: 0.0913461
Validation loss decreased (0.118577 --> 0.116980).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1272170
	speed: 0.1135s/iter; left time: 169.2545s
	iters: 200, epoch: 6 | loss: 0.0933266
	speed: 0.1102s/iter; left time: 153.2469s
	iters: 300, epoch: 6 | loss: 0.1110707
	speed: 0.1144s/iter; left time: 147.6472s
Epoch: 6 cost time: 35.83964729309082
Epoch: 6, Steps: 318 | Train Loss: 0.1066456 Vali Loss: 0.1179528 Test Loss: 0.0900167
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0832651
	speed: 0.1164s/iter; left time: 136.5186s
	iters: 200, epoch: 7 | loss: 0.1271462
	speed: 0.1097s/iter; left time: 117.6702s
	iters: 300, epoch: 7 | loss: 0.1261886
	speed: 0.1089s/iter; left time: 105.9565s
Epoch: 7 cost time: 35.551307916641235
Epoch: 7, Steps: 318 | Train Loss: 0.1066801 Vali Loss: 0.1168118 Test Loss: 0.0898746
Validation loss decreased (0.116980 --> 0.116812).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1013621
	speed: 0.1209s/iter; left time: 103.3319s
	iters: 200, epoch: 8 | loss: 0.0986525
	speed: 0.1104s/iter; left time: 83.3456s
	iters: 300, epoch: 8 | loss: 0.1055956
	speed: 0.1113s/iter; left time: 72.9080s
Epoch: 8 cost time: 36.28799390792847
Epoch: 8, Steps: 318 | Train Loss: 0.1061106 Vali Loss: 0.1183525 Test Loss: 0.0897736
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0902720
	speed: 0.1149s/iter; left time: 61.7037s
	iters: 200, epoch: 9 | loss: 0.1217118
	speed: 0.1112s/iter; left time: 48.5768s
	iters: 300, epoch: 9 | loss: 0.0850763
	speed: 0.1102s/iter; left time: 37.1250s
Epoch: 9 cost time: 35.54778790473938
Epoch: 9, Steps: 318 | Train Loss: 0.1063487 Vali Loss: 0.1170430 Test Loss: 0.0897307
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0953915
	speed: 0.1152s/iter; left time: 25.2188s
	iters: 200, epoch: 10 | loss: 0.0932343
	speed: 0.1138s/iter; left time: 13.5401s
	iters: 300, epoch: 10 | loss: 0.1290911
	speed: 0.1114s/iter; left time: 2.1157s
Epoch: 10 cost time: 36.130722761154175
Epoch: 10, Steps: 318 | Train Loss: 0.1061283 Vali Loss: 0.1168999 Test Loss: 0.0896793
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2098179
	speed: 0.1325s/iter; left time: 408.1566s
	iters: 200, epoch: 1 | loss: 0.2106599
	speed: 0.1116s/iter; left time: 332.6670s
	iters: 300, epoch: 1 | loss: 0.1642577
	speed: 0.1131s/iter; left time: 325.7916s
Epoch: 1 cost time: 37.72155547142029
Epoch: 1, Steps: 318 | Train Loss: 0.2828134 Vali Loss: 0.1574628 Test Loss: 0.1271055
Validation loss decreased (inf --> 0.157463).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1394427
	speed: 0.1185s/iter; left time: 327.3381s
	iters: 200, epoch: 2 | loss: 0.1280431
	speed: 0.1134s/iter; left time: 302.0050s
	iters: 300, epoch: 2 | loss: 0.1116189
	speed: 0.1033s/iter; left time: 264.8836s
Epoch: 2 cost time: 35.376848220825195
Epoch: 2, Steps: 318 | Train Loss: 0.1461122 Vali Loss: 0.1475322 Test Loss: 0.1157651
Validation loss decreased (0.157463 --> 0.147532).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1499605
	speed: 0.1244s/iter; left time: 304.2126s
	iters: 200, epoch: 3 | loss: 0.1439268
	speed: 0.1280s/iter; left time: 300.1985s
	iters: 300, epoch: 3 | loss: 0.1295352
	speed: 0.1073s/iter; left time: 240.8954s
Epoch: 3 cost time: 38.19457697868347
Epoch: 3, Steps: 318 | Train Loss: 0.1371026 Vali Loss: 0.1429680 Test Loss: 0.1117031
Validation loss decreased (0.147532 --> 0.142968).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0974032
	speed: 0.1164s/iter; left time: 247.6336s
	iters: 200, epoch: 4 | loss: 0.1458525
	speed: 0.1129s/iter; left time: 228.8789s
	iters: 300, epoch: 4 | loss: 0.1155901
	speed: 0.1139s/iter; left time: 219.4568s
Epoch: 4 cost time: 36.515981674194336
Epoch: 4, Steps: 318 | Train Loss: 0.1335742 Vali Loss: 0.1414789 Test Loss: 0.1096301
Validation loss decreased (0.142968 --> 0.141479).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1227458
	speed: 0.1111s/iter; left time: 201.0217s
	iters: 200, epoch: 5 | loss: 0.1436434
	speed: 0.1123s/iter; left time: 191.8799s
	iters: 300, epoch: 5 | loss: 0.1574875
	speed: 0.1119s/iter; left time: 180.0101s
Epoch: 5 cost time: 35.63565111160278
Epoch: 5, Steps: 318 | Train Loss: 0.1321752 Vali Loss: 0.1419090 Test Loss: 0.1096976
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1670723
	speed: 0.1194s/iter; left time: 177.9922s
	iters: 200, epoch: 6 | loss: 0.1070603
	speed: 0.1158s/iter; left time: 161.1264s
	iters: 300, epoch: 6 | loss: 0.1172593
	speed: 0.1125s/iter; left time: 145.2539s
Epoch: 6 cost time: 36.849419355392456
Epoch: 6, Steps: 318 | Train Loss: 0.1315125 Vali Loss: 0.1405683 Test Loss: 0.1090468
Validation loss decreased (0.141479 --> 0.140568).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1233579
	speed: 0.1158s/iter; left time: 135.8699s
	iters: 200, epoch: 7 | loss: 0.1487997
	speed: 0.1122s/iter; left time: 120.3796s
	iters: 300, epoch: 7 | loss: 0.1291317
	speed: 0.1117s/iter; left time: 108.6980s
Epoch: 7 cost time: 36.090153217315674
Epoch: 7, Steps: 318 | Train Loss: 0.1311032 Vali Loss: 0.1415998 Test Loss: 0.1089846
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1419053
	speed: 0.1139s/iter; left time: 97.3485s
	iters: 200, epoch: 8 | loss: 0.1460692
	speed: 0.1101s/iter; left time: 83.1195s
	iters: 300, epoch: 8 | loss: 0.1320518
	speed: 0.1122s/iter; left time: 73.5021s
Epoch: 8 cost time: 35.68905782699585
Epoch: 8, Steps: 318 | Train Loss: 0.1306776 Vali Loss: 0.1405050 Test Loss: 0.1088936
Validation loss decreased (0.140568 --> 0.140505).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1375404
	speed: 0.1187s/iter; left time: 63.7467s
	iters: 200, epoch: 9 | loss: 0.1700567
	speed: 0.1135s/iter; left time: 49.5967s
	iters: 300, epoch: 9 | loss: 0.1563531
	speed: 0.1146s/iter; left time: 38.6098s
Epoch: 9 cost time: 36.8596727848053
Epoch: 9, Steps: 318 | Train Loss: 0.1303254 Vali Loss: 0.1415339 Test Loss: 0.1088365
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1438953
	speed: 0.1185s/iter; left time: 25.9475s
	iters: 200, epoch: 10 | loss: 0.1250342
	speed: 0.1099s/iter; left time: 13.0821s
	iters: 300, epoch: 10 | loss: 0.1091328
	speed: 0.1107s/iter; left time: 2.1040s
Epoch: 10 cost time: 35.94656252861023
Epoch: 10, Steps: 318 | Train Loss: 0.1305476 Vali Loss: 0.1406954 Test Loss: 0.1088280
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2635113
	speed: 0.1317s/iter; left time: 405.8363s
	iters: 200, epoch: 1 | loss: 0.1388149
	speed: 0.1112s/iter; left time: 331.6169s
	iters: 300, epoch: 1 | loss: 0.1339406
	speed: 0.1095s/iter; left time: 315.4075s
Epoch: 1 cost time: 37.26302623748779
Epoch: 1, Steps: 318 | Train Loss: 0.2897221 Vali Loss: 0.1483876 Test Loss: 0.1181230
Validation loss decreased (inf --> 0.148388).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1210675
	speed: 0.1165s/iter; left time: 321.8328s
	iters: 200, epoch: 2 | loss: 0.1170553
	speed: 0.1134s/iter; left time: 302.0641s
	iters: 300, epoch: 2 | loss: 0.1203919
	speed: 0.1142s/iter; left time: 292.7996s
Epoch: 2 cost time: 36.52395439147949
Epoch: 2, Steps: 318 | Train Loss: 0.1312241 Vali Loss: 0.1425154 Test Loss: 0.1081940
Validation loss decreased (0.148388 --> 0.142515).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1420038
	speed: 0.1761s/iter; left time: 430.6240s
	iters: 200, epoch: 3 | loss: 0.0977610
	speed: 0.1743s/iter; left time: 408.8190s
	iters: 300, epoch: 3 | loss: 0.1040593
	speed: 0.1854s/iter; left time: 416.2855s
Epoch: 3 cost time: 56.20235776901245
Epoch: 3, Steps: 318 | Train Loss: 0.1256127 Vali Loss: 0.1409317 Test Loss: 0.1077120
Validation loss decreased (0.142515 --> 0.140932).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0975518
	speed: 0.1198s/iter; left time: 254.8181s
	iters: 200, epoch: 4 | loss: 0.1008620
	speed: 0.1135s/iter; left time: 230.0392s
	iters: 300, epoch: 4 | loss: 0.1443214
	speed: 0.1128s/iter; left time: 217.4180s
Epoch: 4 cost time: 36.752607345581055
Epoch: 4, Steps: 318 | Train Loss: 0.1229596 Vali Loss: 0.1410415 Test Loss: 0.1070247
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1346287
	speed: 0.1157s/iter; left time: 209.3394s
	iters: 200, epoch: 5 | loss: 0.1252023
	speed: 0.1114s/iter; left time: 190.3416s
	iters: 300, epoch: 5 | loss: 0.1065510
	speed: 0.1115s/iter; left time: 179.4739s
Epoch: 5 cost time: 36.01727294921875
Epoch: 5, Steps: 318 | Train Loss: 0.1217837 Vali Loss: 0.1412663 Test Loss: 0.1067194
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1259277
	speed: 0.1193s/iter; left time: 177.9065s
	iters: 200, epoch: 6 | loss: 0.1157065
	speed: 0.1120s/iter; left time: 155.8173s
	iters: 300, epoch: 6 | loss: 0.1403829
	speed: 0.1116s/iter; left time: 144.1251s
Epoch: 6 cost time: 36.360111951828
Epoch: 6, Steps: 318 | Train Loss: 0.1208357 Vali Loss: 0.1404627 Test Loss: 0.1070036
Validation loss decreased (0.140932 --> 0.140463).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1614036
	speed: 0.1206s/iter; left time: 141.4146s
	iters: 200, epoch: 7 | loss: 0.1242856
	speed: 0.1131s/iter; left time: 121.3733s
	iters: 300, epoch: 7 | loss: 0.1030310
	speed: 0.1090s/iter; left time: 106.0226s
Epoch: 7 cost time: 36.49664831161499
Epoch: 7, Steps: 318 | Train Loss: 0.1206493 Vali Loss: 0.1395274 Test Loss: 0.1068160
Validation loss decreased (0.140463 --> 0.139527).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1102330
	speed: 0.1167s/iter; left time: 99.7432s
	iters: 200, epoch: 8 | loss: 0.0991653
	speed: 0.1133s/iter; left time: 85.5734s
	iters: 300, epoch: 8 | loss: 0.1714393
	speed: 0.1113s/iter; left time: 72.9167s
Epoch: 8 cost time: 36.25872564315796
Epoch: 8, Steps: 318 | Train Loss: 0.1205210 Vali Loss: 0.1402729 Test Loss: 0.1067393
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0824471
	speed: 0.1180s/iter; left time: 63.3503s
	iters: 200, epoch: 9 | loss: 0.1302134
	speed: 0.1125s/iter; left time: 49.1463s
	iters: 300, epoch: 9 | loss: 0.0993599
	speed: 0.1111s/iter; left time: 37.4492s
Epoch: 9 cost time: 36.3200409412384
Epoch: 9, Steps: 318 | Train Loss: 0.1202117 Vali Loss: 0.1404985 Test Loss: 0.1067601
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1122622
	speed: 0.1166s/iter; left time: 25.5423s
	iters: 200, epoch: 10 | loss: 0.1359730
	speed: 0.1135s/iter; left time: 13.5016s
	iters: 300, epoch: 10 | loss: 0.1097411
	speed: 0.1153s/iter; left time: 2.1898s
Epoch: 10 cost time: 36.60069561004639
Epoch: 10, Steps: 318 | Train Loss: 0.1199955 Vali Loss: 0.1406054 Test Loss: 0.1067771
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1885708
	speed: 0.1276s/iter; left time: 393.2654s
	iters: 200, epoch: 1 | loss: 0.1622752
	speed: 0.1055s/iter; left time: 314.3605s
	iters: 300, epoch: 1 | loss: 0.1153629
	speed: 0.1043s/iter; left time: 300.5614s
Epoch: 1 cost time: 35.558385610580444
Epoch: 1, Steps: 318 | Train Loss: 0.2301143 Vali Loss: 0.1495740 Test Loss: 0.1163801
Validation loss decreased (inf --> 0.149574).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1265946
	speed: 0.1042s/iter; left time: 287.9487s
	iters: 200, epoch: 2 | loss: 0.1246595
	speed: 0.0987s/iter; left time: 262.7599s
	iters: 300, epoch: 2 | loss: 0.1036721
	speed: 0.0995s/iter; left time: 254.9430s
Epoch: 2 cost time: 32.10572934150696
Epoch: 2, Steps: 318 | Train Loss: 0.1300722 Vali Loss: 0.1418530 Test Loss: 0.1127986
Validation loss decreased (0.149574 --> 0.141853).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1190223
	speed: 0.0961s/iter; left time: 234.8497s
	iters: 200, epoch: 3 | loss: 0.0928312
	speed: 0.0898s/iter; left time: 210.6871s
	iters: 300, epoch: 3 | loss: 0.0944993
	speed: 0.0857s/iter; left time: 192.3138s
Epoch: 3 cost time: 29.067389011383057
Epoch: 3, Steps: 318 | Train Loss: 0.1232856 Vali Loss: 0.1385176 Test Loss: 0.1087614
Validation loss decreased (0.141853 --> 0.138518).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1488760
	speed: 0.1038s/iter; left time: 220.7387s
	iters: 200, epoch: 4 | loss: 0.1050093
	speed: 0.0995s/iter; left time: 201.7639s
	iters: 300, epoch: 4 | loss: 0.1011757
	speed: 0.0996s/iter; left time: 191.8547s
Epoch: 4 cost time: 32.13369584083557
Epoch: 4, Steps: 318 | Train Loss: 0.1204402 Vali Loss: 0.1369267 Test Loss: 0.1081877
Validation loss decreased (0.138518 --> 0.136927).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1251215
	speed: 0.1103s/iter; left time: 199.4956s
	iters: 200, epoch: 5 | loss: 0.1209311
	speed: 0.1019s/iter; left time: 174.1528s
	iters: 300, epoch: 5 | loss: 0.0965800
	speed: 0.1060s/iter; left time: 170.5667s
Epoch: 5 cost time: 33.69667291641235
Epoch: 5, Steps: 318 | Train Loss: 0.1188233 Vali Loss: 0.1371511 Test Loss: 0.1079737
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0929067
	speed: 0.1044s/iter; left time: 155.6138s
	iters: 200, epoch: 6 | loss: 0.0864189
	speed: 0.0976s/iter; left time: 135.7254s
	iters: 300, epoch: 6 | loss: 0.1117671
	speed: 0.0992s/iter; left time: 128.0175s
Epoch: 6 cost time: 32.097954988479614
Epoch: 6, Steps: 318 | Train Loss: 0.1182981 Vali Loss: 0.1364993 Test Loss: 0.1075651
Validation loss decreased (0.136927 --> 0.136499).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1222221
	speed: 0.1117s/iter; left time: 131.0533s
	iters: 200, epoch: 7 | loss: 0.1589118
	speed: 0.1054s/iter; left time: 113.1035s
	iters: 300, epoch: 7 | loss: 0.1214013
	speed: 0.1048s/iter; left time: 101.9606s
Epoch: 7 cost time: 34.19858384132385
Epoch: 7, Steps: 318 | Train Loss: 0.1177447 Vali Loss: 0.1372187 Test Loss: 0.1077572
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1170255
	speed: 0.1050s/iter; left time: 89.7867s
	iters: 200, epoch: 8 | loss: 0.1444179
	speed: 0.1008s/iter; left time: 76.0828s
	iters: 300, epoch: 8 | loss: 0.1103127
	speed: 0.1011s/iter; left time: 66.2247s
Epoch: 8 cost time: 32.589879512786865
Epoch: 8, Steps: 318 | Train Loss: 0.1175022 Vali Loss: 0.1364941 Test Loss: 0.1077651
Validation loss decreased (0.136499 --> 0.136494).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1250841
	speed: 0.1001s/iter; left time: 53.7390s
	iters: 200, epoch: 9 | loss: 0.1319432
	speed: 0.1018s/iter; left time: 44.4686s
	iters: 300, epoch: 9 | loss: 0.1147197
	speed: 0.1008s/iter; left time: 33.9553s
Epoch: 9 cost time: 32.14636969566345
Epoch: 9, Steps: 318 | Train Loss: 0.1173995 Vali Loss: 0.1363539 Test Loss: 0.1077723
Validation loss decreased (0.136494 --> 0.136354).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0799312
	speed: 0.1076s/iter; left time: 23.5676s
	iters: 200, epoch: 10 | loss: 0.1224663
	speed: 0.1028s/iter; left time: 12.2333s
	iters: 300, epoch: 10 | loss: 0.1323223
	speed: 0.1049s/iter; left time: 1.9932s
Epoch: 10 cost time: 33.49574041366577
Epoch: 10, Steps: 318 | Train Loss: 0.1172932 Vali Loss: 0.1361135 Test Loss: 0.1077631
Validation loss decreased (0.136354 --> 0.136113).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=16, n_heads=8, e_layers=3, d_layers=1, d_ff=16, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1978654
	speed: 0.1327s/iter; left time: 407.3747s
	iters: 200, epoch: 1 | loss: 0.1591096
	speed: 0.1159s/iter; left time: 344.3201s
	iters: 300, epoch: 1 | loss: 0.1338731
	speed: 0.1172s/iter; left time: 336.5119s
Epoch: 1 cost time: 38.616495847702026
Epoch: 1, Steps: 317 | Train Loss: 0.2756918 Vali Loss: 0.1548227 Test Loss: 0.1204457
Validation loss decreased (inf --> 0.154823).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1492150
	speed: 0.1181s/iter; left time: 325.2417s
	iters: 200, epoch: 2 | loss: 0.1799113
	speed: 0.1155s/iter; left time: 306.6486s
	iters: 300, epoch: 2 | loss: 0.1553526
	speed: 0.1167s/iter; left time: 298.1499s
Epoch: 2 cost time: 37.24798917770386
Epoch: 2, Steps: 317 | Train Loss: 0.1343722 Vali Loss: 0.1471041 Test Loss: 0.1137258
Validation loss decreased (0.154823 --> 0.147104).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1324889
	speed: 0.1228s/iter; left time: 299.3543s
	iters: 200, epoch: 3 | loss: 0.1210105
	speed: 0.1174s/iter; left time: 274.4098s
	iters: 300, epoch: 3 | loss: 0.1211808
	speed: 0.1189s/iter; left time: 266.0863s
Epoch: 3 cost time: 38.047234535217285
Epoch: 3, Steps: 317 | Train Loss: 0.1279632 Vali Loss: 0.1442910 Test Loss: 0.1111019
Validation loss decreased (0.147104 --> 0.144291).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1259302
	speed: 0.1611s/iter; left time: 341.5549s
	iters: 200, epoch: 4 | loss: 0.0972850
	speed: 0.1718s/iter; left time: 347.0643s
	iters: 300, epoch: 4 | loss: 0.0963670
	speed: 0.1905s/iter; left time: 365.7837s
Epoch: 4 cost time: 54.81932806968689
Epoch: 4, Steps: 317 | Train Loss: 0.1250564 Vali Loss: 0.1437295 Test Loss: 0.1098874
Validation loss decreased (0.144291 --> 0.143729).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0975845
	speed: 0.1233s/iter; left time: 222.2808s
	iters: 200, epoch: 5 | loss: 0.1630567
	speed: 0.1160s/iter; left time: 197.4794s
	iters: 300, epoch: 5 | loss: 0.1261028
	speed: 0.1154s/iter; left time: 185.0077s
Epoch: 5 cost time: 37.44942808151245
Epoch: 5, Steps: 317 | Train Loss: 0.1234304 Vali Loss: 0.1438588 Test Loss: 0.1097772
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1268184
	speed: 0.1166s/iter; left time: 173.3204s
	iters: 200, epoch: 6 | loss: 0.1510542
	speed: 0.1180s/iter; left time: 163.5991s
	iters: 300, epoch: 6 | loss: 0.1078641
	speed: 0.1178s/iter; left time: 151.4393s
Epoch: 6 cost time: 37.338433504104614
Epoch: 6, Steps: 317 | Train Loss: 0.1225430 Vali Loss: 0.1435059 Test Loss: 0.1093271
Validation loss decreased (0.143729 --> 0.143506).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0890774
	speed: 0.1209s/iter; left time: 141.3117s
	iters: 200, epoch: 7 | loss: 0.1319001
	speed: 0.1157s/iter; left time: 123.6386s
	iters: 300, epoch: 7 | loss: 0.1386145
	speed: 0.1160s/iter; left time: 112.3813s
Epoch: 7 cost time: 37.344558000564575
Epoch: 7, Steps: 317 | Train Loss: 0.1223393 Vali Loss: 0.1436217 Test Loss: 0.1092799
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1043264
	speed: 0.1182s/iter; left time: 100.7432s
	iters: 200, epoch: 8 | loss: 0.1264535
	speed: 0.1159s/iter; left time: 87.1502s
	iters: 300, epoch: 8 | loss: 0.0980362
	speed: 0.1161s/iter; left time: 75.7043s
Epoch: 8 cost time: 37.11823892593384
Epoch: 8, Steps: 317 | Train Loss: 0.1221802 Vali Loss: 0.1438336 Test Loss: 0.1093029
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1471140
	speed: 0.1194s/iter; left time: 63.8638s
	iters: 200, epoch: 9 | loss: 0.1565631
	speed: 0.1161s/iter; left time: 50.5091s
	iters: 300, epoch: 9 | loss: 0.1196772
	speed: 0.1179s/iter; left time: 39.5008s
Epoch: 9 cost time: 37.38771605491638
Epoch: 9, Steps: 317 | Train Loss: 0.1220323 Vali Loss: 0.1438735 Test Loss: 0.1092997
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm16_nh8_el3_dl1_df16_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1934354
	speed: 0.0611s/iter; left time: 188.1472s
	iters: 200, epoch: 1 | loss: 0.1526539
	speed: 0.0426s/iter; left time: 127.1053s
	iters: 300, epoch: 1 | loss: 0.1355976
	speed: 0.0448s/iter; left time: 129.0473s
Epoch: 1 cost time: 15.639977216720581
Epoch: 1, Steps: 318 | Train Loss: 0.2569068 Vali Loss: 0.1324053 Test Loss: 0.1791978
Validation loss decreased (inf --> 0.132405).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1487767
	speed: 0.0494s/iter; left time: 136.5686s
	iters: 200, epoch: 2 | loss: 0.0901194
	speed: 0.0461s/iter; left time: 122.8834s
	iters: 300, epoch: 2 | loss: 0.0795566
	speed: 0.0430s/iter; left time: 110.1125s
Epoch: 2 cost time: 14.67868685722351
Epoch: 2, Steps: 318 | Train Loss: 0.1166098 Vali Loss: 0.1167003 Test Loss: 0.1574061
Validation loss decreased (0.132405 --> 0.116700).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1178840
	speed: 0.0476s/iter; left time: 116.4678s
	iters: 200, epoch: 3 | loss: 0.1124895
	speed: 0.0450s/iter; left time: 105.6274s
	iters: 300, epoch: 3 | loss: 0.0996796
	speed: 0.0422s/iter; left time: 94.8472s
Epoch: 3 cost time: 14.31116509437561
Epoch: 3, Steps: 318 | Train Loss: 0.1042199 Vali Loss: 0.1095311 Test Loss: 0.1742583
Validation loss decreased (0.116700 --> 0.109531).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1090495
	speed: 0.0489s/iter; left time: 104.0404s
	iters: 200, epoch: 4 | loss: 0.0972373
	speed: 0.0444s/iter; left time: 89.9724s
	iters: 300, epoch: 4 | loss: 0.1196522
	speed: 0.0455s/iter; left time: 87.7400s
Epoch: 4 cost time: 14.670297384262085
Epoch: 4, Steps: 318 | Train Loss: 0.0992513 Vali Loss: 0.1087093 Test Loss: 0.1811716
Validation loss decreased (0.109531 --> 0.108709).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1287456
	speed: 0.0477s/iter; left time: 86.2532s
	iters: 200, epoch: 5 | loss: 0.0769499
	speed: 0.0435s/iter; left time: 74.3559s
	iters: 300, epoch: 5 | loss: 0.1196200
	speed: 0.0442s/iter; left time: 71.0761s
Epoch: 5 cost time: 14.248192548751831
Epoch: 5, Steps: 318 | Train Loss: 0.0968472 Vali Loss: 0.1055902 Test Loss: 0.1842460
Validation loss decreased (0.108709 --> 0.105590).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1433657
	speed: 0.0473s/iter; left time: 70.5803s
	iters: 200, epoch: 6 | loss: 0.0933704
	speed: 0.0468s/iter; left time: 65.0884s
	iters: 300, epoch: 6 | loss: 0.0953313
	speed: 0.0447s/iter; left time: 57.6552s
Epoch: 6 cost time: 14.688109874725342
Epoch: 6, Steps: 318 | Train Loss: 0.0960593 Vali Loss: 0.1053507 Test Loss: 0.1859756
Validation loss decreased (0.105590 --> 0.105351).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0861076
	speed: 0.0530s/iter; left time: 62.1574s
	iters: 200, epoch: 7 | loss: 0.0700400
	speed: 0.0469s/iter; left time: 50.3301s
	iters: 300, epoch: 7 | loss: 0.1338145
	speed: 0.0469s/iter; left time: 45.6357s
Epoch: 7 cost time: 15.526914596557617
Epoch: 7, Steps: 318 | Train Loss: 0.0949925 Vali Loss: 0.1051220 Test Loss: 0.1874787
Validation loss decreased (0.105351 --> 0.105122).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0700339
	speed: 0.0503s/iter; left time: 42.9778s
	iters: 200, epoch: 8 | loss: 0.0912639
	speed: 0.0441s/iter; left time: 33.2825s
	iters: 300, epoch: 8 | loss: 0.0942854
	speed: 0.0424s/iter; left time: 27.7838s
Epoch: 8 cost time: 14.52344274520874
Epoch: 8, Steps: 318 | Train Loss: 0.0948036 Vali Loss: 0.1052953 Test Loss: 0.1879724
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1154213
	speed: 0.0480s/iter; left time: 25.7608s
	iters: 200, epoch: 9 | loss: 0.0941672
	speed: 0.0536s/iter; left time: 23.4264s
	iters: 300, epoch: 9 | loss: 0.1017782
	speed: 0.0585s/iter; left time: 19.6997s
Epoch: 9 cost time: 17.07803463935852
Epoch: 9, Steps: 318 | Train Loss: 0.0946589 Vali Loss: 0.1055910 Test Loss: 0.1874268
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0833538
	speed: 0.0607s/iter; left time: 13.2911s
	iters: 200, epoch: 10 | loss: 0.0884618
	speed: 0.0550s/iter; left time: 6.5452s
	iters: 300, epoch: 10 | loss: 0.0933828
	speed: 0.0552s/iter; left time: 1.0487s
Epoch: 10 cost time: 18.307048559188843
Epoch: 10, Steps: 318 | Train Loss: 0.0949829 Vali Loss: 0.1055228 Test Loss: 0.1871813
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1394188
	speed: 0.0674s/iter; left time: 207.6616s
	iters: 200, epoch: 1 | loss: 0.1135532
	speed: 0.0506s/iter; left time: 150.9659s
	iters: 300, epoch: 1 | loss: 0.0895133
	speed: 0.0498s/iter; left time: 143.6076s
Epoch: 1 cost time: 17.756460666656494
Epoch: 1, Steps: 318 | Train Loss: 0.2431799 Vali Loss: 0.1438291 Test Loss: 0.1119328
Validation loss decreased (inf --> 0.143829).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1438347
	speed: 0.0556s/iter; left time: 153.7069s
	iters: 200, epoch: 2 | loss: 0.1075076
	speed: 0.0517s/iter; left time: 137.6571s
	iters: 300, epoch: 2 | loss: 0.1493733
	speed: 0.0544s/iter; left time: 139.4976s
Epoch: 2 cost time: 17.084542512893677
Epoch: 2, Steps: 318 | Train Loss: 0.1250665 Vali Loss: 0.1267921 Test Loss: 0.1021616
Validation loss decreased (0.143829 --> 0.126792).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1364917
	speed: 0.0438s/iter; left time: 107.0230s
	iters: 200, epoch: 3 | loss: 0.1199986
	speed: 0.0509s/iter; left time: 119.4394s
	iters: 300, epoch: 3 | loss: 0.1037821
	speed: 0.0532s/iter; left time: 119.5087s
Epoch: 3 cost time: 15.689881324768066
Epoch: 3, Steps: 318 | Train Loss: 0.1135143 Vali Loss: 0.1222190 Test Loss: 0.1007789
Validation loss decreased (0.126792 --> 0.122219).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0988226
	speed: 0.0539s/iter; left time: 114.7053s
	iters: 200, epoch: 4 | loss: 0.1277040
	speed: 0.0519s/iter; left time: 105.2662s
	iters: 300, epoch: 4 | loss: 0.0911416
	speed: 0.0510s/iter; left time: 98.3416s
Epoch: 4 cost time: 16.686553955078125
Epoch: 4, Steps: 318 | Train Loss: 0.1081998 Vali Loss: 0.1193058 Test Loss: 0.0990905
Validation loss decreased (0.122219 --> 0.119306).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0973081
	speed: 0.0574s/iter; left time: 103.7687s
	iters: 200, epoch: 5 | loss: 0.0922589
	speed: 0.0512s/iter; left time: 87.4774s
	iters: 300, epoch: 5 | loss: 0.0885791
	speed: 0.0508s/iter; left time: 81.8008s
Epoch: 5 cost time: 16.89117980003357
Epoch: 5, Steps: 318 | Train Loss: 0.1058643 Vali Loss: 0.1171329 Test Loss: 0.0994788
Validation loss decreased (0.119306 --> 0.117133).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0968551
	speed: 0.0569s/iter; left time: 84.8839s
	iters: 200, epoch: 6 | loss: 0.1299389
	speed: 0.0510s/iter; left time: 70.8792s
	iters: 300, epoch: 6 | loss: 0.0784816
	speed: 0.0492s/iter; left time: 63.4790s
Epoch: 6 cost time: 16.8151638507843
Epoch: 6, Steps: 318 | Train Loss: 0.1047457 Vali Loss: 0.1173098 Test Loss: 0.0993404
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0830160
	speed: 0.0486s/iter; left time: 57.0312s
	iters: 200, epoch: 7 | loss: 0.1016888
	speed: 0.0524s/iter; left time: 56.2623s
	iters: 300, epoch: 7 | loss: 0.0969020
	speed: 0.0524s/iter; left time: 50.9965s
Epoch: 7 cost time: 16.24952006340027
Epoch: 7, Steps: 318 | Train Loss: 0.1043250 Vali Loss: 0.1163161 Test Loss: 0.0987017
Validation loss decreased (0.117133 --> 0.116316).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1029080
	speed: 0.0540s/iter; left time: 46.2055s
	iters: 200, epoch: 8 | loss: 0.1265518
	speed: 0.0572s/iter; left time: 43.1623s
	iters: 300, epoch: 8 | loss: 0.0972916
	speed: 0.0511s/iter; left time: 33.4599s
Epoch: 8 cost time: 17.271350622177124
Epoch: 8, Steps: 318 | Train Loss: 0.1043017 Vali Loss: 0.1176267 Test Loss: 0.0985836
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1375236
	speed: 0.0557s/iter; left time: 29.9193s
	iters: 200, epoch: 9 | loss: 0.0862930
	speed: 0.0533s/iter; left time: 23.3053s
	iters: 300, epoch: 9 | loss: 0.1165137
	speed: 0.0530s/iter; left time: 17.8590s
Epoch: 9 cost time: 17.119096279144287
Epoch: 9, Steps: 318 | Train Loss: 0.1037750 Vali Loss: 0.1174566 Test Loss: 0.0984038
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0825901
	speed: 0.0568s/iter; left time: 12.4342s
	iters: 200, epoch: 10 | loss: 0.1108469
	speed: 0.0540s/iter; left time: 6.4272s
	iters: 300, epoch: 10 | loss: 0.1171655
	speed: 0.0503s/iter; left time: 0.9558s
Epoch: 10 cost time: 17.049368619918823
Epoch: 10, Steps: 318 | Train Loss: 0.1042676 Vali Loss: 0.1167256 Test Loss: 0.0984644
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1848427
	speed: 0.0664s/iter; left time: 204.6713s
	iters: 200, epoch: 1 | loss: 0.1484319
	speed: 0.0515s/iter; left time: 153.6284s
	iters: 300, epoch: 1 | loss: 0.1475235
	speed: 0.0504s/iter; left time: 145.1036s
Epoch: 1 cost time: 17.80401039123535
Epoch: 1, Steps: 318 | Train Loss: 0.2220085 Vali Loss: 0.1470314 Test Loss: 0.1102749
Validation loss decreased (inf --> 0.147031).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1402558
	speed: 0.0579s/iter; left time: 159.9655s
	iters: 200, epoch: 2 | loss: 0.1173183
	speed: 0.0523s/iter; left time: 139.3976s
	iters: 300, epoch: 2 | loss: 0.1068172
	speed: 0.0508s/iter; left time: 130.1563s
Epoch: 2 cost time: 17.116263151168823
Epoch: 2, Steps: 318 | Train Loss: 0.1267059 Vali Loss: 0.1343592 Test Loss: 0.1022785
Validation loss decreased (0.147031 --> 0.134359).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1298555
	speed: 0.0537s/iter; left time: 131.4070s
	iters: 200, epoch: 3 | loss: 0.0981414
	speed: 0.0508s/iter; left time: 119.0159s
	iters: 300, epoch: 3 | loss: 0.0864000
	speed: 0.0522s/iter; left time: 117.2826s
Epoch: 3 cost time: 16.574259281158447
Epoch: 3, Steps: 318 | Train Loss: 0.1176574 Vali Loss: 0.1325383 Test Loss: 0.1071897
Validation loss decreased (0.134359 --> 0.132538).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1005128
	speed: 0.0553s/iter; left time: 117.6222s
	iters: 200, epoch: 4 | loss: 0.1145315
	speed: 0.0527s/iter; left time: 106.7814s
	iters: 300, epoch: 4 | loss: 0.1168660
	speed: 0.0525s/iter; left time: 101.1776s
Epoch: 4 cost time: 17.021113872528076
Epoch: 4, Steps: 318 | Train Loss: 0.1135257 Vali Loss: 0.1290407 Test Loss: 0.1065813
Validation loss decreased (0.132538 --> 0.129041).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0991207
	speed: 0.0540s/iter; left time: 97.6706s
	iters: 200, epoch: 5 | loss: 0.1035056
	speed: 0.0420s/iter; left time: 71.7496s
	iters: 300, epoch: 5 | loss: 0.1061288
	speed: 0.0523s/iter; left time: 84.2084s
Epoch: 5 cost time: 15.725269079208374
Epoch: 5, Steps: 318 | Train Loss: 0.1111288 Vali Loss: 0.1267396 Test Loss: 0.1054005
Validation loss decreased (0.129041 --> 0.126740).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0899880
	speed: 0.0541s/iter; left time: 80.7203s
	iters: 200, epoch: 6 | loss: 0.1092520
	speed: 0.0512s/iter; left time: 71.2732s
	iters: 300, epoch: 6 | loss: 0.1032754
	speed: 0.0521s/iter; left time: 67.2345s
Epoch: 6 cost time: 16.75595498085022
Epoch: 6, Steps: 318 | Train Loss: 0.1101650 Vali Loss: 0.1255297 Test Loss: 0.1060751
Validation loss decreased (0.126740 --> 0.125530).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1230221
	speed: 0.0576s/iter; left time: 67.5165s
	iters: 200, epoch: 7 | loss: 0.0960723
	speed: 0.0499s/iter; left time: 53.4910s
	iters: 300, epoch: 7 | loss: 0.1566753
	speed: 0.0552s/iter; left time: 53.7315s
Epoch: 7 cost time: 17.212627410888672
Epoch: 7, Steps: 318 | Train Loss: 0.1096626 Vali Loss: 0.1258122 Test Loss: 0.1059942
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0789434
	speed: 0.0586s/iter; left time: 50.1407s
	iters: 200, epoch: 8 | loss: 0.0789167
	speed: 0.0502s/iter; left time: 37.8656s
	iters: 300, epoch: 8 | loss: 0.1229681
	speed: 0.0500s/iter; left time: 32.7678s
Epoch: 8 cost time: 16.865769863128662
Epoch: 8, Steps: 318 | Train Loss: 0.1096871 Vali Loss: 0.1262128 Test Loss: 0.1066110
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1265047
	speed: 0.0553s/iter; left time: 29.6957s
	iters: 200, epoch: 9 | loss: 0.1116916
	speed: 0.0524s/iter; left time: 22.9045s
	iters: 300, epoch: 9 | loss: 0.1026501
	speed: 0.0494s/iter; left time: 16.6422s
Epoch: 9 cost time: 16.76148819923401
Epoch: 9, Steps: 318 | Train Loss: 0.1091266 Vali Loss: 0.1262396 Test Loss: 0.1067440
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1344502
	speed: 0.0720s/iter; left time: 221.8497s
	iters: 200, epoch: 1 | loss: 0.1344945
	speed: 0.0646s/iter; left time: 192.5890s
	iters: 300, epoch: 1 | loss: 0.1205254
	speed: 0.0638s/iter; left time: 183.7942s
Epoch: 1 cost time: 21.20251154899597
Epoch: 1, Steps: 318 | Train Loss: 0.2361911 Vali Loss: 0.1484903 Test Loss: 0.1125223
Validation loss decreased (inf --> 0.148490).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1692492
	speed: 0.0706s/iter; left time: 195.1273s
	iters: 200, epoch: 2 | loss: 0.1126996
	speed: 0.0663s/iter; left time: 176.6112s
	iters: 300, epoch: 2 | loss: 0.1176839
	speed: 0.0550s/iter; left time: 140.9585s
Epoch: 2 cost time: 20.065991640090942
Epoch: 2, Steps: 318 | Train Loss: 0.1324291 Vali Loss: 0.1411465 Test Loss: 0.1075026
Validation loss decreased (0.148490 --> 0.141146).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1257888
	speed: 0.0468s/iter; left time: 114.5127s
	iters: 200, epoch: 3 | loss: 0.0960309
	speed: 0.0450s/iter; left time: 105.4561s
	iters: 300, epoch: 3 | loss: 0.1203358
	speed: 0.0457s/iter; left time: 102.6688s
Epoch: 3 cost time: 14.551994562149048
Epoch: 3, Steps: 318 | Train Loss: 0.1234079 Vali Loss: 0.1382309 Test Loss: 0.1075823
Validation loss decreased (0.141146 --> 0.138231).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1286267
	speed: 0.0530s/iter; left time: 112.6922s
	iters: 200, epoch: 4 | loss: 0.1144229
	speed: 0.0443s/iter; left time: 89.7428s
	iters: 300, epoch: 4 | loss: 0.1524071
	speed: 0.0452s/iter; left time: 87.0570s
Epoch: 4 cost time: 15.0697762966156
Epoch: 4, Steps: 318 | Train Loss: 0.1196616 Vali Loss: 0.1352893 Test Loss: 0.1019040
Validation loss decreased (0.138231 --> 0.135289).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1184999
	speed: 0.0526s/iter; left time: 95.1038s
	iters: 200, epoch: 5 | loss: 0.1049922
	speed: 0.0460s/iter; left time: 78.6462s
	iters: 300, epoch: 5 | loss: 0.1145131
	speed: 0.0482s/iter; left time: 77.5381s
Epoch: 5 cost time: 15.553639650344849
Epoch: 5, Steps: 318 | Train Loss: 0.1175673 Vali Loss: 0.1340888 Test Loss: 0.1020259
Validation loss decreased (0.135289 --> 0.134089).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1435216
	speed: 0.0496s/iter; left time: 73.9133s
	iters: 200, epoch: 6 | loss: 0.1237518
	speed: 0.0444s/iter; left time: 61.7001s
	iters: 300, epoch: 6 | loss: 0.1101415
	speed: 0.0452s/iter; left time: 58.3067s
Epoch: 6 cost time: 14.74634313583374
Epoch: 6, Steps: 318 | Train Loss: 0.1164821 Vali Loss: 0.1336936 Test Loss: 0.1010988
Validation loss decreased (0.134089 --> 0.133694).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1335998
	speed: 0.0477s/iter; left time: 55.9335s
	iters: 200, epoch: 7 | loss: 0.1135921
	speed: 0.0469s/iter; left time: 50.3299s
	iters: 300, epoch: 7 | loss: 0.1107077
	speed: 0.0462s/iter; left time: 44.9313s
Epoch: 7 cost time: 14.872554779052734
Epoch: 7, Steps: 318 | Train Loss: 0.1162748 Vali Loss: 0.1338423 Test Loss: 0.1006837
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1086866
	speed: 0.0475s/iter; left time: 40.6219s
	iters: 200, epoch: 8 | loss: 0.1222151
	speed: 0.0471s/iter; left time: 35.5938s
	iters: 300, epoch: 8 | loss: 0.0960111
	speed: 0.0481s/iter; left time: 31.5380s
Epoch: 8 cost time: 15.146743059158325
Epoch: 8, Steps: 318 | Train Loss: 0.1159287 Vali Loss: 0.1334064 Test Loss: 0.1005541
Validation loss decreased (0.133694 --> 0.133406).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0908196
	speed: 0.0464s/iter; left time: 24.9088s
	iters: 200, epoch: 9 | loss: 0.1539274
	speed: 0.0490s/iter; left time: 21.3993s
	iters: 300, epoch: 9 | loss: 0.0975360
	speed: 0.0446s/iter; left time: 15.0205s
Epoch: 9 cost time: 14.833037853240967
Epoch: 9, Steps: 318 | Train Loss: 0.1157223 Vali Loss: 0.1327775 Test Loss: 0.1006075
Validation loss decreased (0.133406 --> 0.132778).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1716650
	speed: 0.0498s/iter; left time: 10.8964s
	iters: 200, epoch: 10 | loss: 0.1228683
	speed: 0.0448s/iter; left time: 5.3340s
	iters: 300, epoch: 10 | loss: 0.1029119
	speed: 0.0449s/iter; left time: 0.8535s
Epoch: 10 cost time: 14.772228240966797
Epoch: 10, Steps: 318 | Train Loss: 0.1156364 Vali Loss: 0.1331230 Test Loss: 0.1005959
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1427236
	speed: 0.0692s/iter; left time: 212.5357s
	iters: 200, epoch: 1 | loss: 0.1642748
	speed: 0.0541s/iter; left time: 160.8421s
	iters: 300, epoch: 1 | loss: 0.1272231
	speed: 0.0478s/iter; left time: 137.3531s
Epoch: 1 cost time: 17.768600940704346
Epoch: 1, Steps: 317 | Train Loss: 0.2271320 Vali Loss: 0.1542283 Test Loss: 0.1189647
Validation loss decreased (inf --> 0.154228).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1318025
	speed: 0.0570s/iter; left time: 157.0379s
	iters: 200, epoch: 2 | loss: 0.1587307
	speed: 0.0520s/iter; left time: 138.1307s
	iters: 300, epoch: 2 | loss: 0.1677294
	speed: 0.0524s/iter; left time: 133.7631s
Epoch: 2 cost time: 16.99465012550354
Epoch: 2, Steps: 317 | Train Loss: 0.1337897 Vali Loss: 0.1465843 Test Loss: 0.1076920
Validation loss decreased (0.154228 --> 0.146584).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1422753
	speed: 0.0589s/iter; left time: 143.4482s
	iters: 200, epoch: 3 | loss: 0.1424477
	speed: 0.0519s/iter; left time: 121.3270s
	iters: 300, epoch: 3 | loss: 0.1317984
	speed: 0.0532s/iter; left time: 119.0893s
Epoch: 3 cost time: 17.29909062385559
Epoch: 3, Steps: 317 | Train Loss: 0.1267498 Vali Loss: 0.1415722 Test Loss: 0.1017831
Validation loss decreased (0.146584 --> 0.141572).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1299837
	speed: 0.0543s/iter; left time: 115.0780s
	iters: 200, epoch: 4 | loss: 0.1521300
	speed: 0.0522s/iter; left time: 105.4787s
	iters: 300, epoch: 4 | loss: 0.1301223
	speed: 0.0516s/iter; left time: 99.1077s
Epoch: 4 cost time: 16.699410915374756
Epoch: 4, Steps: 317 | Train Loss: 0.1234335 Vali Loss: 0.1407444 Test Loss: 0.1017415
Validation loss decreased (0.141572 --> 0.140744).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1152503
	speed: 0.0540s/iter; left time: 97.3411s
	iters: 200, epoch: 5 | loss: 0.1051252
	speed: 0.0529s/iter; left time: 90.0401s
	iters: 300, epoch: 5 | loss: 0.1028778
	speed: 0.0517s/iter; left time: 82.8557s
Epoch: 5 cost time: 16.742812871932983
Epoch: 5, Steps: 317 | Train Loss: 0.1216040 Vali Loss: 0.1402626 Test Loss: 0.1033600
Validation loss decreased (0.140744 --> 0.140263).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1147207
	speed: 0.0561s/iter; left time: 83.3900s
	iters: 200, epoch: 6 | loss: 0.0894627
	speed: 0.0517s/iter; left time: 71.6973s
	iters: 300, epoch: 6 | loss: 0.1070897
	speed: 0.0518s/iter; left time: 66.5985s
Epoch: 6 cost time: 16.850709199905396
Epoch: 6, Steps: 317 | Train Loss: 0.1210789 Vali Loss: 0.1388799 Test Loss: 0.1011007
Validation loss decreased (0.140263 --> 0.138880).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0822957
	speed: 0.0557s/iter; left time: 65.0935s
	iters: 200, epoch: 7 | loss: 0.0951826
	speed: 0.0539s/iter; left time: 57.5879s
	iters: 300, epoch: 7 | loss: 0.1198579
	speed: 0.0518s/iter; left time: 50.2337s
Epoch: 7 cost time: 17.05591607093811
Epoch: 7, Steps: 317 | Train Loss: 0.1206196 Vali Loss: 0.1385017 Test Loss: 0.1008776
Validation loss decreased (0.138880 --> 0.138502).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1276102
	speed: 0.0574s/iter; left time: 48.8982s
	iters: 200, epoch: 8 | loss: 0.1103710
	speed: 0.0511s/iter; left time: 38.3988s
	iters: 300, epoch: 8 | loss: 0.1639758
	speed: 0.0495s/iter; left time: 32.3014s
Epoch: 8 cost time: 16.88712239265442
Epoch: 8, Steps: 317 | Train Loss: 0.1200049 Vali Loss: 0.1391631 Test Loss: 0.1009160
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1067223
	speed: 0.0558s/iter; left time: 29.8608s
	iters: 200, epoch: 9 | loss: 0.1277116
	speed: 0.0511s/iter; left time: 22.2098s
	iters: 300, epoch: 9 | loss: 0.1343905
	speed: 0.0527s/iter; left time: 17.6612s
Epoch: 9 cost time: 16.869609117507935
Epoch: 9, Steps: 317 | Train Loss: 0.1197553 Vali Loss: 0.1387195 Test Loss: 0.1008423
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0881251
	speed: 0.0558s/iter; left time: 12.1618s
	iters: 200, epoch: 10 | loss: 0.1065066
	speed: 0.0534s/iter; left time: 6.2974s
	iters: 300, epoch: 10 | loss: 0.1487427
	speed: 0.0520s/iter; left time: 0.9361s
Epoch: 10 cost time: 17.057393074035645
Epoch: 10, Steps: 317 | Train Loss: 0.1200152 Vali Loss: 0.1384081 Test Loss: 0.1008306
Validation loss decreased (0.138502 --> 0.138408).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1810247
	speed: 0.1004s/iter; left time: 309.2631s
	iters: 200, epoch: 1 | loss: 0.1402969
	speed: 0.0824s/iter; left time: 245.5140s
	iters: 300, epoch: 1 | loss: 0.1295321
	speed: 0.0829s/iter; left time: 238.8606s
Epoch: 1 cost time: 28.028815507888794
Epoch: 1, Steps: 318 | Train Loss: 0.2121714 Vali Loss: 0.1320182 Test Loss: 0.0987782
Validation loss decreased (inf --> 0.132018).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1315225
	speed: 0.0747s/iter; left time: 206.3607s
	iters: 200, epoch: 2 | loss: 0.0805000
	speed: 0.0678s/iter; left time: 180.5452s
	iters: 300, epoch: 2 | loss: 0.0831623
	speed: 0.0615s/iter; left time: 157.6754s
Epoch: 2 cost time: 21.760388135910034
Epoch: 2, Steps: 318 | Train Loss: 0.1121493 Vali Loss: 0.1176886 Test Loss: 0.0871965
Validation loss decreased (0.132018 --> 0.117689).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1141665
	speed: 0.0732s/iter; left time: 179.0935s
	iters: 200, epoch: 3 | loss: 0.1104463
	speed: 0.0732s/iter; left time: 171.6817s
	iters: 300, epoch: 3 | loss: 0.1094503
	speed: 0.0655s/iter; left time: 147.0387s
Epoch: 3 cost time: 22.6080379486084
Epoch: 3, Steps: 318 | Train Loss: 0.0997079 Vali Loss: 0.1095051 Test Loss: 0.0816278
Validation loss decreased (0.117689 --> 0.109505).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1153603
	speed: 0.0882s/iter; left time: 187.5059s
	iters: 200, epoch: 4 | loss: 0.0942890
	speed: 0.0821s/iter; left time: 166.5101s
	iters: 300, epoch: 4 | loss: 0.1276669
	speed: 0.0831s/iter; left time: 160.1890s
Epoch: 4 cost time: 27.008228302001953
Epoch: 4, Steps: 318 | Train Loss: 0.0948366 Vali Loss: 0.1070600 Test Loss: 0.0815872
Validation loss decreased (0.109505 --> 0.107060).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1255031
	speed: 0.0847s/iter; left time: 153.3018s
	iters: 200, epoch: 5 | loss: 0.0727729
	speed: 0.0848s/iter; left time: 144.8533s
	iters: 300, epoch: 5 | loss: 0.1185892
	speed: 0.0848s/iter; left time: 136.4110s
Epoch: 5 cost time: 26.940200328826904
Epoch: 5, Steps: 318 | Train Loss: 0.0920060 Vali Loss: 0.1050549 Test Loss: 0.0794894
Validation loss decreased (0.107060 --> 0.105055).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1278564
	speed: 0.0861s/iter; left time: 128.3048s
	iters: 200, epoch: 6 | loss: 0.1021039
	speed: 0.0849s/iter; left time: 118.0269s
	iters: 300, epoch: 6 | loss: 0.1066914
	speed: 0.0813s/iter; left time: 104.9118s
Epoch: 6 cost time: 26.798226594924927
Epoch: 6, Steps: 318 | Train Loss: 0.0909040 Vali Loss: 0.1048173 Test Loss: 0.0791313
Validation loss decreased (0.105055 --> 0.104817).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0821517
	speed: 0.0847s/iter; left time: 99.3437s
	iters: 200, epoch: 7 | loss: 0.0583595
	speed: 0.0843s/iter; left time: 90.4062s
	iters: 300, epoch: 7 | loss: 0.1209182
	speed: 0.0850s/iter; left time: 82.7142s
Epoch: 7 cost time: 26.879342079162598
Epoch: 7, Steps: 318 | Train Loss: 0.0899161 Vali Loss: 0.1048461 Test Loss: 0.0785921
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0663878
	speed: 0.0865s/iter; left time: 73.9480s
	iters: 200, epoch: 8 | loss: 0.0767203
	speed: 0.0817s/iter; left time: 61.6624s
	iters: 300, epoch: 8 | loss: 0.0932753
	speed: 0.0821s/iter; left time: 53.7722s
Epoch: 8 cost time: 26.539995431900024
Epoch: 8, Steps: 318 | Train Loss: 0.0899375 Vali Loss: 0.1050312 Test Loss: 0.0790455
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1010593
	speed: 0.0853s/iter; left time: 45.7878s
	iters: 200, epoch: 9 | loss: 0.0907433
	speed: 0.0818s/iter; left time: 35.7321s
	iters: 300, epoch: 9 | loss: 0.0914400
	speed: 0.0813s/iter; left time: 27.3894s
Epoch: 9 cost time: 26.405563831329346
Epoch: 9, Steps: 318 | Train Loss: 0.0898071 Vali Loss: 0.1050891 Test Loss: 0.0790860
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1384285
	speed: 0.1014s/iter; left time: 312.4679s
	iters: 200, epoch: 1 | loss: 0.1135481
	speed: 0.0824s/iter; left time: 245.5497s
	iters: 300, epoch: 1 | loss: 0.0924958
	speed: 0.0826s/iter; left time: 238.1117s
Epoch: 1 cost time: 28.133342504501343
Epoch: 1, Steps: 318 | Train Loss: 0.2212241 Vali Loss: 0.1413892 Test Loss: 0.1061703
Validation loss decreased (inf --> 0.141389).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1426457
	speed: 0.0866s/iter; left time: 239.4004s
	iters: 200, epoch: 2 | loss: 0.1086365
	speed: 0.0815s/iter; left time: 217.1038s
	iters: 300, epoch: 2 | loss: 0.1440030
	speed: 0.0821s/iter; left time: 210.5031s
Epoch: 2 cost time: 26.493415594100952
Epoch: 2, Steps: 318 | Train Loss: 0.1242704 Vali Loss: 0.1228152 Test Loss: 0.0937235
Validation loss decreased (0.141389 --> 0.122815).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1358157
	speed: 0.0858s/iter; left time: 209.6743s
	iters: 200, epoch: 3 | loss: 0.1255460
	speed: 0.0818s/iter; left time: 191.7304s
	iters: 300, epoch: 3 | loss: 0.0947676
	speed: 0.0809s/iter; left time: 181.5950s
Epoch: 3 cost time: 26.312867164611816
Epoch: 3, Steps: 318 | Train Loss: 0.1122292 Vali Loss: 0.1187798 Test Loss: 0.0905728
Validation loss decreased (0.122815 --> 0.118780).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1017760
	speed: 0.0861s/iter; left time: 183.1061s
	iters: 200, epoch: 4 | loss: 0.1235692
	speed: 0.0806s/iter; left time: 163.3135s
	iters: 300, epoch: 4 | loss: 0.0946508
	speed: 0.0809s/iter; left time: 155.8959s
Epoch: 4 cost time: 26.214011430740356
Epoch: 4, Steps: 318 | Train Loss: 0.1070411 Vali Loss: 0.1169768 Test Loss: 0.0877622
Validation loss decreased (0.118780 --> 0.116977).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1017319
	speed: 0.0847s/iter; left time: 153.1592s
	iters: 200, epoch: 5 | loss: 0.0915857
	speed: 0.0811s/iter; left time: 138.6649s
	iters: 300, epoch: 5 | loss: 0.0885916
	speed: 0.0816s/iter; left time: 131.2553s
Epoch: 5 cost time: 26.309276819229126
Epoch: 5, Steps: 318 | Train Loss: 0.1049134 Vali Loss: 0.1143295 Test Loss: 0.0878895
Validation loss decreased (0.116977 --> 0.114329).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0983230
	speed: 0.0826s/iter; left time: 123.1768s
	iters: 200, epoch: 6 | loss: 0.1317279
	speed: 0.0821s/iter; left time: 114.1759s
	iters: 300, epoch: 6 | loss: 0.0862910
	speed: 0.0804s/iter; left time: 103.8055s
Epoch: 6 cost time: 26.140469789505005
Epoch: 6, Steps: 318 | Train Loss: 0.1036268 Vali Loss: 0.1145870 Test Loss: 0.0870708
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0889328
	speed: 0.0857s/iter; left time: 100.5167s
	iters: 200, epoch: 7 | loss: 0.0928783
	speed: 0.0818s/iter; left time: 87.7380s
	iters: 300, epoch: 7 | loss: 0.0998667
	speed: 0.0822s/iter; left time: 79.9589s
Epoch: 7 cost time: 26.48799753189087
Epoch: 7, Steps: 318 | Train Loss: 0.1034084 Vali Loss: 0.1136963 Test Loss: 0.0868095
Validation loss decreased (0.114329 --> 0.113696).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1069417
	speed: 0.0868s/iter; left time: 74.2148s
	iters: 200, epoch: 8 | loss: 0.1149443
	speed: 0.0831s/iter; left time: 62.7678s
	iters: 300, epoch: 8 | loss: 0.0944265
	speed: 0.0797s/iter; left time: 52.2212s
Epoch: 8 cost time: 26.478290796279907
Epoch: 8, Steps: 318 | Train Loss: 0.1032551 Vali Loss: 0.1148771 Test Loss: 0.0868615
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1375158
	speed: 0.0836s/iter; left time: 44.8780s
	iters: 200, epoch: 9 | loss: 0.0817467
	speed: 0.0806s/iter; left time: 35.2079s
	iters: 300, epoch: 9 | loss: 0.1199112
	speed: 0.0824s/iter; left time: 27.7697s
Epoch: 9 cost time: 26.195614337921143
Epoch: 9, Steps: 318 | Train Loss: 0.1031911 Vali Loss: 0.1149673 Test Loss: 0.0866943
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0840081
	speed: 0.0803s/iter; left time: 17.5916s
	iters: 200, epoch: 10 | loss: 0.1011617
	speed: 0.0667s/iter; left time: 7.9374s
	iters: 300, epoch: 10 | loss: 0.1124632
	speed: 0.0690s/iter; left time: 1.3107s
Epoch: 10 cost time: 22.992711305618286
Epoch: 10, Steps: 318 | Train Loss: 0.1033878 Vali Loss: 0.1142117 Test Loss: 0.0866711
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1789368
	speed: 0.1018s/iter; left time: 313.5481s
	iters: 200, epoch: 1 | loss: 0.1458501
	speed: 0.0890s/iter; left time: 265.2299s
	iters: 300, epoch: 1 | loss: 0.1426675
	speed: 0.0852s/iter; left time: 245.5095s
Epoch: 1 cost time: 29.10168480873108
Epoch: 1, Steps: 318 | Train Loss: 0.1972236 Vali Loss: 0.1453700 Test Loss: 0.1091509
Validation loss decreased (inf --> 0.145370).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1378532
	speed: 0.0860s/iter; left time: 237.7455s
	iters: 200, epoch: 2 | loss: 0.1158413
	speed: 0.0856s/iter; left time: 228.0563s
	iters: 300, epoch: 2 | loss: 0.1031761
	speed: 0.0839s/iter; left time: 215.0424s
Epoch: 2 cost time: 27.104955911636353
Epoch: 2, Steps: 318 | Train Loss: 0.1229785 Vali Loss: 0.1316048 Test Loss: 0.0975860
Validation loss decreased (0.145370 --> 0.131605).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1323805
	speed: 0.0896s/iter; left time: 219.1804s
	iters: 200, epoch: 3 | loss: 0.1009411
	speed: 0.0848s/iter; left time: 198.9062s
	iters: 300, epoch: 3 | loss: 0.0897807
	speed: 0.0847s/iter; left time: 190.2310s
Epoch: 3 cost time: 27.565614461898804
Epoch: 3, Steps: 318 | Train Loss: 0.1134926 Vali Loss: 0.1284425 Test Loss: 0.0951869
Validation loss decreased (0.131605 --> 0.128443).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0980222
	speed: 0.0901s/iter; left time: 191.5677s
	iters: 200, epoch: 4 | loss: 0.1058748
	speed: 0.0866s/iter; left time: 175.5533s
	iters: 300, epoch: 4 | loss: 0.1082291
	speed: 0.0844s/iter; left time: 162.6229s
Epoch: 4 cost time: 27.663846254348755
Epoch: 4, Steps: 318 | Train Loss: 0.1093440 Vali Loss: 0.1254303 Test Loss: 0.0930080
Validation loss decreased (0.128443 --> 0.125430).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0928656
	speed: 0.0886s/iter; left time: 160.2642s
	iters: 200, epoch: 5 | loss: 0.0986339
	speed: 0.0866s/iter; left time: 148.0411s
	iters: 300, epoch: 5 | loss: 0.1018851
	speed: 0.0858s/iter; left time: 138.0829s
Epoch: 5 cost time: 27.638164043426514
Epoch: 5, Steps: 318 | Train Loss: 0.1072111 Vali Loss: 0.1237934 Test Loss: 0.0917122
Validation loss decreased (0.125430 --> 0.123793).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0796655
	speed: 0.0900s/iter; left time: 134.1177s
	iters: 200, epoch: 6 | loss: 0.1048251
	speed: 0.0838s/iter; left time: 116.5190s
	iters: 300, epoch: 6 | loss: 0.0976220
	speed: 0.0838s/iter; left time: 108.2220s
Epoch: 6 cost time: 27.3710880279541
Epoch: 6, Steps: 318 | Train Loss: 0.1062183 Vali Loss: 0.1225227 Test Loss: 0.0914781
Validation loss decreased (0.123793 --> 0.122523).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1104543
	speed: 0.0835s/iter; left time: 97.9480s
	iters: 200, epoch: 7 | loss: 0.0922836
	speed: 0.0803s/iter; left time: 86.1353s
	iters: 300, epoch: 7 | loss: 0.1470249
	speed: 0.0830s/iter; left time: 80.7345s
Epoch: 7 cost time: 26.198331594467163
Epoch: 7, Steps: 318 | Train Loss: 0.1054163 Vali Loss: 0.1228804 Test Loss: 0.0913269
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0711857
	speed: 0.0845s/iter; left time: 72.2053s
	iters: 200, epoch: 8 | loss: 0.0886871
	speed: 0.0804s/iter; left time: 60.6869s
	iters: 300, epoch: 8 | loss: 0.1103277
	speed: 0.0820s/iter; left time: 53.7415s
Epoch: 8 cost time: 26.158267498016357
Epoch: 8, Steps: 318 | Train Loss: 0.1054142 Vali Loss: 0.1231817 Test Loss: 0.0914572
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1200435
	speed: 0.0857s/iter; left time: 46.0140s
	iters: 200, epoch: 9 | loss: 0.1012990
	speed: 0.0815s/iter; left time: 35.6270s
	iters: 300, epoch: 9 | loss: 0.1015342
	speed: 0.0806s/iter; left time: 27.1458s
Epoch: 9 cost time: 26.35975193977356
Epoch: 9, Steps: 318 | Train Loss: 0.1050793 Vali Loss: 0.1230485 Test Loss: 0.0915773
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1369028
	speed: 0.0965s/iter; left time: 297.4671s
	iters: 200, epoch: 1 | loss: 0.1381433
	speed: 0.0793s/iter; left time: 236.2809s
	iters: 300, epoch: 1 | loss: 0.1224220
	speed: 0.0708s/iter; left time: 204.0837s
Epoch: 1 cost time: 25.922263860702515
Epoch: 1, Steps: 318 | Train Loss: 0.2157626 Vali Loss: 0.1419407 Test Loss: 0.1089848
Validation loss decreased (inf --> 0.141941).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1663769
	speed: 0.0800s/iter; left time: 221.1118s
	iters: 200, epoch: 2 | loss: 0.1097498
	speed: 0.0753s/iter; left time: 200.5436s
	iters: 300, epoch: 2 | loss: 0.1207085
	speed: 0.0764s/iter; left time: 195.7284s
Epoch: 2 cost time: 24.61421012878418
Epoch: 2, Steps: 318 | Train Loss: 0.1276000 Vali Loss: 0.1337042 Test Loss: 0.1035341
Validation loss decreased (0.141941 --> 0.133704).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1190823
	speed: 0.0812s/iter; left time: 198.6049s
	iters: 200, epoch: 3 | loss: 0.0940876
	speed: 0.0771s/iter; left time: 180.8180s
	iters: 300, epoch: 3 | loss: 0.1119254
	speed: 0.0766s/iter; left time: 171.8619s
Epoch: 3 cost time: 24.791540384292603
Epoch: 3, Steps: 318 | Train Loss: 0.1189267 Vali Loss: 0.1306217 Test Loss: 0.1025358
Validation loss decreased (0.133704 --> 0.130622).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1286348
	speed: 0.0790s/iter; left time: 168.1064s
	iters: 200, epoch: 4 | loss: 0.1080812
	speed: 0.0738s/iter; left time: 149.6439s
	iters: 300, epoch: 4 | loss: 0.1459091
	speed: 0.0746s/iter; left time: 143.6815s
Epoch: 4 cost time: 24.16012144088745
Epoch: 4, Steps: 318 | Train Loss: 0.1161688 Vali Loss: 0.1277761 Test Loss: 0.0971048
Validation loss decreased (0.130622 --> 0.127776).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1080694
	speed: 0.0787s/iter; left time: 142.3986s
	iters: 200, epoch: 5 | loss: 0.1014483
	speed: 0.0760s/iter; left time: 129.8389s
	iters: 300, epoch: 5 | loss: 0.1057390
	speed: 0.0759s/iter; left time: 122.1489s
Epoch: 5 cost time: 24.410728931427002
Epoch: 5, Steps: 318 | Train Loss: 0.1143927 Vali Loss: 0.1268251 Test Loss: 0.0979834
Validation loss decreased (0.127776 --> 0.126825).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1301835
	speed: 0.0792s/iter; left time: 118.1552s
	iters: 200, epoch: 6 | loss: 0.1156985
	speed: 0.0757s/iter; left time: 105.3530s
	iters: 300, epoch: 6 | loss: 0.1035500
	speed: 0.0750s/iter; left time: 96.8012s
Epoch: 6 cost time: 24.283669471740723
Epoch: 6, Steps: 318 | Train Loss: 0.1136033 Vali Loss: 0.1262893 Test Loss: 0.0971559
Validation loss decreased (0.126825 --> 0.126289).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1279308
	speed: 0.0694s/iter; left time: 81.3602s
	iters: 200, epoch: 7 | loss: 0.1078540
	speed: 0.0638s/iter; left time: 68.4700s
	iters: 300, epoch: 7 | loss: 0.1037729
	speed: 0.0664s/iter; left time: 64.5794s
Epoch: 7 cost time: 21.29427456855774
Epoch: 7, Steps: 318 | Train Loss: 0.1135566 Vali Loss: 0.1264924 Test Loss: 0.0967226
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1100973
	speed: 0.0682s/iter; left time: 58.3173s
	iters: 200, epoch: 8 | loss: 0.1211101
	speed: 0.0659s/iter; left time: 49.7475s
	iters: 300, epoch: 8 | loss: 0.1071259
	speed: 0.0700s/iter; left time: 45.8265s
Epoch: 8 cost time: 21.643259286880493
Epoch: 8, Steps: 318 | Train Loss: 0.1131681 Vali Loss: 0.1260091 Test Loss: 0.0966498
Validation loss decreased (0.126289 --> 0.126009).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0949825
	speed: 0.0792s/iter; left time: 42.5418s
	iters: 200, epoch: 9 | loss: 0.1537595
	speed: 0.0761s/iter; left time: 33.2647s
	iters: 300, epoch: 9 | loss: 0.1062014
	speed: 0.0746s/iter; left time: 25.1238s
Epoch: 9 cost time: 24.315176963806152
Epoch: 9, Steps: 318 | Train Loss: 0.1130689 Vali Loss: 0.1253692 Test Loss: 0.0966509
Validation loss decreased (0.126009 --> 0.125369).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1725473
	speed: 0.0776s/iter; left time: 16.9924s
	iters: 200, epoch: 10 | loss: 0.1122774
	speed: 0.0755s/iter; left time: 8.9819s
	iters: 300, epoch: 10 | loss: 0.0946143
	speed: 0.0748s/iter; left time: 1.4210s
Epoch: 10 cost time: 24.23785948753357
Epoch: 10, Steps: 318 | Train Loss: 0.1128986 Vali Loss: 0.1255868 Test Loss: 0.0966487
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1391314
	speed: 0.1005s/iter; left time: 308.6456s
	iters: 200, epoch: 1 | loss: 0.1696183
	speed: 0.0823s/iter; left time: 244.4699s
	iters: 300, epoch: 1 | loss: 0.1225054
	speed: 0.0841s/iter; left time: 241.3789s
Epoch: 1 cost time: 28.054750204086304
Epoch: 1, Steps: 317 | Train Loss: 0.2055492 Vali Loss: 0.1524827 Test Loss: 0.1231678
Validation loss decreased (inf --> 0.152483).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1274005
	speed: 0.0839s/iter; left time: 231.1530s
	iters: 200, epoch: 2 | loss: 0.1470820
	speed: 0.0810s/iter; left time: 215.0568s
	iters: 300, epoch: 2 | loss: 0.1635245
	speed: 0.0789s/iter; left time: 201.5940s
Epoch: 2 cost time: 25.906383275985718
Epoch: 2, Steps: 317 | Train Loss: 0.1288217 Vali Loss: 0.1420238 Test Loss: 0.1061484
Validation loss decreased (0.152483 --> 0.142024).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1287996
	speed: 0.0814s/iter; left time: 198.2543s
	iters: 200, epoch: 3 | loss: 0.1340276
	speed: 0.0793s/iter; left time: 185.2794s
	iters: 300, epoch: 3 | loss: 0.1229157
	speed: 0.0798s/iter; left time: 178.5255s
Epoch: 3 cost time: 25.414743423461914
Epoch: 3, Steps: 317 | Train Loss: 0.1215176 Vali Loss: 0.1389983 Test Loss: 0.1011260
Validation loss decreased (0.142024 --> 0.138998).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1326786
	speed: 0.0830s/iter; left time: 175.9995s
	iters: 200, epoch: 4 | loss: 0.1428176
	speed: 0.0787s/iter; left time: 158.9483s
	iters: 300, epoch: 4 | loss: 0.1161082
	speed: 0.0801s/iter; left time: 153.7496s
Epoch: 4 cost time: 25.58641791343689
Epoch: 4, Steps: 317 | Train Loss: 0.1174503 Vali Loss: 0.1384413 Test Loss: 0.1012987
Validation loss decreased (0.138998 --> 0.138441).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1125638
	speed: 0.0827s/iter; left time: 149.1980s
	iters: 200, epoch: 5 | loss: 0.0959367
	speed: 0.0799s/iter; left time: 136.0132s
	iters: 300, epoch: 5 | loss: 0.0981477
	speed: 0.0804s/iter; left time: 128.9526s
Epoch: 5 cost time: 25.704638957977295
Epoch: 5, Steps: 317 | Train Loss: 0.1154800 Vali Loss: 0.1373524 Test Loss: 0.1037674
Validation loss decreased (0.138441 --> 0.137352).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1093271
	speed: 0.0830s/iter; left time: 123.3281s
	iters: 200, epoch: 6 | loss: 0.0906693
	speed: 0.0794s/iter; left time: 110.0237s
	iters: 300, epoch: 6 | loss: 0.0988214
	speed: 0.0793s/iter; left time: 101.9674s
Epoch: 6 cost time: 25.475125312805176
Epoch: 6, Steps: 317 | Train Loss: 0.1147959 Vali Loss: 0.1361944 Test Loss: 0.1005632
Validation loss decreased (0.137352 --> 0.136194).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0828178
	speed: 0.0833s/iter; left time: 97.4056s
	iters: 200, epoch: 7 | loss: 0.0914367
	speed: 0.0781s/iter; left time: 83.5235s
	iters: 300, epoch: 7 | loss: 0.1216309
	speed: 0.0822s/iter; left time: 79.6518s
Epoch: 7 cost time: 25.690335512161255
Epoch: 7, Steps: 317 | Train Loss: 0.1142761 Vali Loss: 0.1359726 Test Loss: 0.1009598
Validation loss decreased (0.136194 --> 0.135973).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1143415
	speed: 0.0824s/iter; left time: 70.2414s
	iters: 200, epoch: 8 | loss: 0.1054736
	speed: 0.0796s/iter; left time: 59.8250s
	iters: 300, epoch: 8 | loss: 0.1549207
	speed: 0.0820s/iter; left time: 53.4829s
Epoch: 8 cost time: 25.80747628211975
Epoch: 8, Steps: 317 | Train Loss: 0.1139303 Vali Loss: 0.1368446 Test Loss: 0.1012692
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1081305
	speed: 0.0459s/iter; left time: 24.5674s
	iters: 200, epoch: 9 | loss: 0.1182508
	speed: 0.0420s/iter; left time: 18.2573s
	iters: 300, epoch: 9 | loss: 0.1225632
	speed: 0.0429s/iter; left time: 14.3772s
Epoch: 9 cost time: 13.799335956573486
Epoch: 9, Steps: 317 | Train Loss: 0.1136826 Vali Loss: 0.1362775 Test Loss: 0.1008388
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0887851
	speed: 0.0461s/iter; left time: 10.0488s
	iters: 200, epoch: 10 | loss: 0.1015143
	speed: 0.0407s/iter; left time: 4.8080s
	iters: 300, epoch: 10 | loss: 0.1373117
	speed: 0.0413s/iter; left time: 0.7437s
Epoch: 10 cost time: 13.594751358032227
Epoch: 10, Steps: 317 | Train Loss: 0.1141671 Vali Loss: 0.1360071 Test Loss: 0.1008111
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1904818
	speed: 0.0743s/iter; left time: 228.8946s
	iters: 200, epoch: 1 | loss: 0.1512742
	speed: 0.0587s/iter; left time: 174.9576s
	iters: 300, epoch: 1 | loss: 0.1267885
	speed: 0.0569s/iter; left time: 163.7931s
Epoch: 1 cost time: 20.048718214035034
Epoch: 1, Steps: 318 | Train Loss: 0.2215443 Vali Loss: 0.1306426 Test Loss: 0.0976990
Validation loss decreased (inf --> 0.130643).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1449983
	speed: 0.0607s/iter; left time: 167.7130s
	iters: 200, epoch: 2 | loss: 0.0898096
	speed: 0.0571s/iter; left time: 152.0423s
	iters: 300, epoch: 2 | loss: 0.0877507
	speed: 0.0567s/iter; left time: 145.4041s
Epoch: 2 cost time: 18.57506561279297
Epoch: 2, Steps: 318 | Train Loss: 0.1155934 Vali Loss: 0.1176494 Test Loss: 0.0860842
Validation loss decreased (0.130643 --> 0.117649).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1236333
	speed: 0.0598s/iter; left time: 146.1991s
	iters: 200, epoch: 3 | loss: 0.1098716
	speed: 0.0562s/iter; left time: 131.6753s
	iters: 300, epoch: 3 | loss: 0.1104513
	speed: 0.0566s/iter; left time: 127.1230s
Epoch: 3 cost time: 18.331056833267212
Epoch: 3, Steps: 318 | Train Loss: 0.1036916 Vali Loss: 0.1090837 Test Loss: 0.0810079
Validation loss decreased (0.117649 --> 0.109084).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1047361
	speed: 0.0601s/iter; left time: 127.8869s
	iters: 200, epoch: 4 | loss: 0.0939271
	speed: 0.0566s/iter; left time: 114.6925s
	iters: 300, epoch: 4 | loss: 0.1353951
	speed: 0.0571s/iter; left time: 110.0153s
Epoch: 4 cost time: 18.450783014297485
Epoch: 4, Steps: 318 | Train Loss: 0.0990325 Vali Loss: 0.1069225 Test Loss: 0.0815930
Validation loss decreased (0.109084 --> 0.106922).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1242810
	speed: 0.0603s/iter; left time: 109.1363s
	iters: 200, epoch: 5 | loss: 0.0787461
	speed: 0.0558s/iter; left time: 95.3269s
	iters: 300, epoch: 5 | loss: 0.1216013
	speed: 0.0568s/iter; left time: 91.3340s
Epoch: 5 cost time: 18.344499826431274
Epoch: 5, Steps: 318 | Train Loss: 0.0967387 Vali Loss: 0.1045785 Test Loss: 0.0790424
Validation loss decreased (0.106922 --> 0.104579).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1366675
	speed: 0.0596s/iter; left time: 88.9303s
	iters: 200, epoch: 6 | loss: 0.1025444
	speed: 0.0550s/iter; left time: 76.4487s
	iters: 300, epoch: 6 | loss: 0.1053198
	speed: 0.0558s/iter; left time: 71.9790s
Epoch: 6 cost time: 18.093217134475708
Epoch: 6, Steps: 318 | Train Loss: 0.0956837 Vali Loss: 0.1045307 Test Loss: 0.0789878
Validation loss decreased (0.104579 --> 0.104531).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0919777
	speed: 0.0592s/iter; left time: 69.4771s
	iters: 200, epoch: 7 | loss: 0.0575759
	speed: 0.0552s/iter; left time: 59.2208s
	iters: 300, epoch: 7 | loss: 0.1310208
	speed: 0.0561s/iter; left time: 54.5567s
Epoch: 7 cost time: 18.1109139919281
Epoch: 7, Steps: 318 | Train Loss: 0.0947960 Vali Loss: 0.1040075 Test Loss: 0.0783760
Validation loss decreased (0.104531 --> 0.104008).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0731311
	speed: 0.0589s/iter; left time: 50.3749s
	iters: 200, epoch: 8 | loss: 0.0791790
	speed: 0.0559s/iter; left time: 42.2091s
	iters: 300, epoch: 8 | loss: 0.0962953
	speed: 0.0560s/iter; left time: 36.7054s
Epoch: 8 cost time: 18.144652843475342
Epoch: 8, Steps: 318 | Train Loss: 0.0947821 Vali Loss: 0.1042441 Test Loss: 0.0787026
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1048232
	speed: 0.0596s/iter; left time: 32.0081s
	iters: 200, epoch: 9 | loss: 0.0971232
	speed: 0.0550s/iter; left time: 24.0143s
	iters: 300, epoch: 9 | loss: 0.1004191
	speed: 0.0553s/iter; left time: 18.6226s
Epoch: 9 cost time: 18.045381546020508
Epoch: 9, Steps: 318 | Train Loss: 0.0946302 Vali Loss: 0.1045425 Test Loss: 0.0787162
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0872985
	speed: 0.0588s/iter; left time: 12.8745s
	iters: 200, epoch: 10 | loss: 0.0920268
	speed: 0.0561s/iter; left time: 6.6700s
	iters: 300, epoch: 10 | loss: 0.0946529
	speed: 0.0556s/iter; left time: 1.0560s
Epoch: 10 cost time: 18.098073482513428
Epoch: 10, Steps: 318 | Train Loss: 0.0951027 Vali Loss: 0.1043486 Test Loss: 0.0787146
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1379604
	speed: 0.0771s/iter; left time: 237.4325s
	iters: 200, epoch: 1 | loss: 0.1132771
	speed: 0.0622s/iter; left time: 185.4075s
	iters: 300, epoch: 1 | loss: 0.0923032
	speed: 0.0620s/iter; left time: 178.6415s
Epoch: 1 cost time: 21.275169134140015
Epoch: 1, Steps: 318 | Train Loss: 0.2214153 Vali Loss: 0.1413647 Test Loss: 0.1064565
Validation loss decreased (inf --> 0.141365).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1423488
	speed: 0.0652s/iter; left time: 180.0932s
	iters: 200, epoch: 2 | loss: 0.1100521
	speed: 0.0620s/iter; left time: 165.1789s
	iters: 300, epoch: 2 | loss: 0.1452637
	speed: 0.0605s/iter; left time: 155.1273s
Epoch: 2 cost time: 19.931241035461426
Epoch: 2, Steps: 318 | Train Loss: 0.1246485 Vali Loss: 0.1233572 Test Loss: 0.0942580
Validation loss decreased (0.141365 --> 0.123357).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1370282
	speed: 0.0651s/iter; left time: 159.1531s
	iters: 200, epoch: 3 | loss: 0.1257167
	speed: 0.0603s/iter; left time: 141.3055s
	iters: 300, epoch: 3 | loss: 0.0951387
	speed: 0.0602s/iter; left time: 135.2234s
Epoch: 3 cost time: 19.694884300231934
Epoch: 3, Steps: 318 | Train Loss: 0.1127771 Vali Loss: 0.1189796 Test Loss: 0.0908503
Validation loss decreased (0.123357 --> 0.118980).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1020250
	speed: 0.0664s/iter; left time: 141.2593s
	iters: 200, epoch: 4 | loss: 0.1232991
	speed: 0.0611s/iter; left time: 123.8789s
	iters: 300, epoch: 4 | loss: 0.0943802
	speed: 0.0607s/iter; left time: 116.9333s
Epoch: 4 cost time: 19.985386610031128
Epoch: 4, Steps: 318 | Train Loss: 0.1074835 Vali Loss: 0.1172780 Test Loss: 0.0881806
Validation loss decreased (0.118980 --> 0.117278).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1014685
	speed: 0.0651s/iter; left time: 117.7413s
	iters: 200, epoch: 5 | loss: 0.0915522
	speed: 0.0615s/iter; left time: 105.0216s
	iters: 300, epoch: 5 | loss: 0.0885333
	speed: 0.0610s/iter; left time: 98.1013s
Epoch: 5 cost time: 19.919560194015503
Epoch: 5, Steps: 318 | Train Loss: 0.1053566 Vali Loss: 0.1144471 Test Loss: 0.0881950
Validation loss decreased (0.117278 --> 0.114447).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0980306
	speed: 0.0637s/iter; left time: 94.9333s
	iters: 200, epoch: 6 | loss: 0.1322118
	speed: 0.0618s/iter; left time: 85.9005s
	iters: 300, epoch: 6 | loss: 0.0865996
	speed: 0.0623s/iter; left time: 80.4715s
Epoch: 6 cost time: 19.91408681869507
Epoch: 6, Steps: 318 | Train Loss: 0.1039962 Vali Loss: 0.1147076 Test Loss: 0.0873778
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0890096
	speed: 0.0655s/iter; left time: 76.8430s
	iters: 200, epoch: 7 | loss: 0.0940502
	speed: 0.0600s/iter; left time: 64.4285s
	iters: 300, epoch: 7 | loss: 0.1012570
	speed: 0.0600s/iter; left time: 58.3653s
Epoch: 7 cost time: 19.700682163238525
Epoch: 7, Steps: 318 | Train Loss: 0.1037903 Vali Loss: 0.1137840 Test Loss: 0.0870906
Validation loss decreased (0.114447 --> 0.113784).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1082916
	speed: 0.0653s/iter; left time: 55.8264s
	iters: 200, epoch: 8 | loss: 0.1147222
	speed: 0.0613s/iter; left time: 46.3131s
	iters: 300, epoch: 8 | loss: 0.0941955
	speed: 0.0618s/iter; left time: 40.4776s
Epoch: 8 cost time: 19.97368049621582
Epoch: 8, Steps: 318 | Train Loss: 0.1036257 Vali Loss: 0.1149488 Test Loss: 0.0871373
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1364882
	speed: 0.0648s/iter; left time: 34.7791s
	iters: 200, epoch: 9 | loss: 0.0821700
	speed: 0.0618s/iter; left time: 27.0128s
	iters: 300, epoch: 9 | loss: 0.1207945
	speed: 0.0613s/iter; left time: 20.6569s
Epoch: 9 cost time: 19.923766613006592
Epoch: 9, Steps: 318 | Train Loss: 0.1035374 Vali Loss: 0.1150523 Test Loss: 0.0869751
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0846639
	speed: 0.0643s/iter; left time: 14.0865s
	iters: 200, epoch: 10 | loss: 0.1017674
	speed: 0.0602s/iter; left time: 7.1602s
	iters: 300, epoch: 10 | loss: 0.1142768
	speed: 0.0609s/iter; left time: 1.1564s
Epoch: 10 cost time: 19.681039810180664
Epoch: 10, Steps: 318 | Train Loss: 0.1037520 Vali Loss: 0.1142882 Test Loss: 0.0869508
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1776451
	speed: 0.0783s/iter; left time: 241.2588s
	iters: 200, epoch: 1 | loss: 0.1456833
	speed: 0.0626s/iter; left time: 186.6337s
	iters: 300, epoch: 1 | loss: 0.1434003
	speed: 0.0624s/iter; left time: 179.6591s
Epoch: 1 cost time: 21.475404977798462
Epoch: 1, Steps: 318 | Train Loss: 0.2010566 Vali Loss: 0.1454125 Test Loss: 0.1090747
Validation loss decreased (inf --> 0.145412).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1387675
	speed: 0.0660s/iter; left time: 182.4109s
	iters: 200, epoch: 2 | loss: 0.1156392
	speed: 0.0622s/iter; left time: 165.5954s
	iters: 300, epoch: 2 | loss: 0.1036413
	speed: 0.0629s/iter; left time: 161.3217s
Epoch: 2 cost time: 20.294436931610107
Epoch: 2, Steps: 318 | Train Loss: 0.1232072 Vali Loss: 0.1322609 Test Loss: 0.0982473
Validation loss decreased (0.145412 --> 0.132261).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1314494
	speed: 0.0673s/iter; left time: 164.5145s
	iters: 200, epoch: 3 | loss: 0.1011624
	speed: 0.0617s/iter; left time: 144.7044s
	iters: 300, epoch: 3 | loss: 0.0912315
	speed: 0.0607s/iter; left time: 136.3153s
Epoch: 3 cost time: 20.122071743011475
Epoch: 3, Steps: 318 | Train Loss: 0.1141731 Vali Loss: 0.1299719 Test Loss: 0.0962230
Validation loss decreased (0.132261 --> 0.129972).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0967228
	speed: 0.0652s/iter; left time: 138.6612s
	iters: 200, epoch: 4 | loss: 0.1049603
	speed: 0.0617s/iter; left time: 125.1402s
	iters: 300, epoch: 4 | loss: 0.1078024
	speed: 0.0621s/iter; left time: 119.6146s
Epoch: 4 cost time: 20.07640266418457
Epoch: 4, Steps: 318 | Train Loss: 0.1100286 Vali Loss: 0.1268308 Test Loss: 0.0939216
Validation loss decreased (0.129972 --> 0.126831).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0929202
	speed: 0.0655s/iter; left time: 118.4234s
	iters: 200, epoch: 5 | loss: 0.0997910
	speed: 0.0615s/iter; left time: 105.0287s
	iters: 300, epoch: 5 | loss: 0.1022667
	speed: 0.0610s/iter; left time: 98.1581s
Epoch: 5 cost time: 19.95527410507202
Epoch: 5, Steps: 318 | Train Loss: 0.1078339 Vali Loss: 0.1251868 Test Loss: 0.0924733
Validation loss decreased (0.126831 --> 0.125187).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0793281
	speed: 0.0660s/iter; left time: 98.3435s
	iters: 200, epoch: 6 | loss: 0.1060724
	speed: 0.0612s/iter; left time: 85.1602s
	iters: 300, epoch: 6 | loss: 0.0966685
	speed: 0.0616s/iter; left time: 79.5529s
Epoch: 6 cost time: 20.046569347381592
Epoch: 6, Steps: 318 | Train Loss: 0.1068514 Vali Loss: 0.1238477 Test Loss: 0.0922800
Validation loss decreased (0.125187 --> 0.123848).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1123988
	speed: 0.0651s/iter; left time: 76.4024s
	iters: 200, epoch: 7 | loss: 0.0930136
	speed: 0.0618s/iter; left time: 66.3423s
	iters: 300, epoch: 7 | loss: 0.1485182
	speed: 0.0620s/iter; left time: 60.3121s
Epoch: 7 cost time: 20.09278130531311
Epoch: 7, Steps: 318 | Train Loss: 0.1060721 Vali Loss: 0.1241887 Test Loss: 0.0920615
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0721751
	speed: 0.0660s/iter; left time: 56.3990s
	iters: 200, epoch: 8 | loss: 0.0873372
	speed: 0.0617s/iter; left time: 46.6035s
	iters: 300, epoch: 8 | loss: 0.1119568
	speed: 0.0614s/iter; left time: 40.2119s
Epoch: 8 cost time: 20.09466814994812
Epoch: 8, Steps: 318 | Train Loss: 0.1060609 Vali Loss: 0.1245814 Test Loss: 0.0922019
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1203467
	speed: 0.0665s/iter; left time: 35.7056s
	iters: 200, epoch: 9 | loss: 0.1014942
	speed: 0.0635s/iter; left time: 27.7708s
	iters: 300, epoch: 9 | loss: 0.1044282
	speed: 0.0621s/iter; left time: 20.9188s
Epoch: 9 cost time: 20.37867760658264
Epoch: 9, Steps: 318 | Train Loss: 0.1057245 Vali Loss: 0.1243680 Test Loss: 0.0923106
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1373698
	speed: 0.0761s/iter; left time: 234.5717s
	iters: 200, epoch: 1 | loss: 0.1378870
	speed: 0.0573s/iter; left time: 170.9087s
	iters: 300, epoch: 1 | loss: 0.1229004
	speed: 0.0577s/iter; left time: 166.1586s
Epoch: 1 cost time: 20.182884216308594
Epoch: 1, Steps: 318 | Train Loss: 0.2150411 Vali Loss: 0.1423010 Test Loss: 0.1095121
Validation loss decreased (inf --> 0.142301).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1666767
	speed: 0.0617s/iter; left time: 170.3999s
	iters: 200, epoch: 2 | loss: 0.1099918
	speed: 0.0578s/iter; left time: 153.8299s
	iters: 300, epoch: 2 | loss: 0.1205627
	speed: 0.0586s/iter; left time: 150.1968s
Epoch: 2 cost time: 18.910619735717773
Epoch: 2, Steps: 318 | Train Loss: 0.1278834 Vali Loss: 0.1339458 Test Loss: 0.1038145
Validation loss decreased (0.142301 --> 0.133946).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1194423
	speed: 0.0614s/iter; left time: 150.1297s
	iters: 200, epoch: 3 | loss: 0.0939666
	speed: 0.0582s/iter; left time: 136.4097s
	iters: 300, epoch: 3 | loss: 0.1121064
	speed: 0.0586s/iter; left time: 131.4773s
Epoch: 3 cost time: 18.92907404899597
Epoch: 3, Steps: 318 | Train Loss: 0.1191849 Vali Loss: 0.1313189 Test Loss: 0.1030724
Validation loss decreased (0.133946 --> 0.131319).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1294958
	speed: 0.0629s/iter; left time: 133.7015s
	iters: 200, epoch: 4 | loss: 0.1082890
	speed: 0.0595s/iter; left time: 120.6600s
	iters: 300, epoch: 4 | loss: 0.1462813
	speed: 0.0596s/iter; left time: 114.7639s
Epoch: 4 cost time: 19.312426328659058
Epoch: 4, Steps: 318 | Train Loss: 0.1164097 Vali Loss: 0.1282109 Test Loss: 0.0974458
Validation loss decreased (0.131319 --> 0.128211).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1083597
	speed: 0.0624s/iter; left time: 112.9209s
	iters: 200, epoch: 5 | loss: 0.1022919
	speed: 0.0578s/iter; left time: 98.8156s
	iters: 300, epoch: 5 | loss: 0.1065122
	speed: 0.0579s/iter; left time: 93.1250s
Epoch: 5 cost time: 18.93583369255066
Epoch: 5, Steps: 318 | Train Loss: 0.1145858 Vali Loss: 0.1273104 Test Loss: 0.0982663
Validation loss decreased (0.128211 --> 0.127310).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1303556
	speed: 0.0642s/iter; left time: 95.6981s
	iters: 200, epoch: 6 | loss: 0.1161727
	speed: 0.0586s/iter; left time: 81.5049s
	iters: 300, epoch: 6 | loss: 0.1039512
	speed: 0.0588s/iter; left time: 75.9702s
Epoch: 6 cost time: 19.277658939361572
Epoch: 6, Steps: 318 | Train Loss: 0.1137813 Vali Loss: 0.1267944 Test Loss: 0.0974643
Validation loss decreased (0.127310 --> 0.126794).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1278107
	speed: 0.0627s/iter; left time: 73.5367s
	iters: 200, epoch: 7 | loss: 0.1078106
	speed: 0.0599s/iter; left time: 64.2364s
	iters: 300, epoch: 7 | loss: 0.1043503
	speed: 0.0615s/iter; left time: 59.8261s
Epoch: 7 cost time: 19.54298424720764
Epoch: 7, Steps: 318 | Train Loss: 0.1137188 Vali Loss: 0.1269841 Test Loss: 0.0970194
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1106829
	speed: 0.0638s/iter; left time: 54.5562s
	iters: 200, epoch: 8 | loss: 0.1217140
	speed: 0.0583s/iter; left time: 43.9991s
	iters: 300, epoch: 8 | loss: 0.1081643
	speed: 0.0586s/iter; left time: 38.3577s
Epoch: 8 cost time: 19.188257455825806
Epoch: 8, Steps: 318 | Train Loss: 0.1133629 Vali Loss: 0.1264583 Test Loss: 0.0969167
Validation loss decreased (0.126794 --> 0.126458).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0948526
	speed: 0.0628s/iter; left time: 33.7111s
	iters: 200, epoch: 9 | loss: 0.1548553
	speed: 0.0601s/iter; left time: 26.2808s
	iters: 300, epoch: 9 | loss: 0.1059248
	speed: 0.0596s/iter; left time: 20.0914s
Epoch: 9 cost time: 19.385882139205933
Epoch: 9, Steps: 318 | Train Loss: 0.1132356 Vali Loss: 0.1258421 Test Loss: 0.0969362
Validation loss decreased (0.126458 --> 0.125842).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1730767
	speed: 0.0623s/iter; left time: 13.6521s
	iters: 200, epoch: 10 | loss: 0.1129725
	speed: 0.0596s/iter; left time: 7.0981s
	iters: 300, epoch: 10 | loss: 0.0952604
	speed: 0.0585s/iter; left time: 1.1121s
Epoch: 10 cost time: 19.165250539779663
Epoch: 10, Steps: 318 | Train Loss: 0.1130354 Vali Loss: 0.1260551 Test Loss: 0.0969291
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1388524
	speed: 0.0774s/iter; left time: 237.8224s
	iters: 200, epoch: 1 | loss: 0.1687399
	speed: 0.0634s/iter; left time: 188.3799s
	iters: 300, epoch: 1 | loss: 0.1215557
	speed: 0.0635s/iter; left time: 182.3750s
Epoch: 1 cost time: 21.555065631866455
Epoch: 1, Steps: 317 | Train Loss: 0.2060794 Vali Loss: 0.1527392 Test Loss: 0.1228640
Validation loss decreased (inf --> 0.152739).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1283725
	speed: 0.0670s/iter; left time: 184.4959s
	iters: 200, epoch: 2 | loss: 0.1468246
	speed: 0.0646s/iter; left time: 171.5255s
	iters: 300, epoch: 2 | loss: 0.1639621
	speed: 0.0639s/iter; left time: 163.1690s
Epoch: 2 cost time: 20.721803426742554
Epoch: 2, Steps: 317 | Train Loss: 0.1289223 Vali Loss: 0.1422554 Test Loss: 0.1062450
Validation loss decreased (0.152739 --> 0.142255).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1296123
	speed: 0.0673s/iter; left time: 163.9384s
	iters: 200, epoch: 3 | loss: 0.1350203
	speed: 0.0644s/iter; left time: 150.4563s
	iters: 300, epoch: 3 | loss: 0.1229798
	speed: 0.0652s/iter; left time: 145.8749s
Epoch: 3 cost time: 20.813227653503418
Epoch: 3, Steps: 317 | Train Loss: 0.1216891 Vali Loss: 0.1395739 Test Loss: 0.1014150
Validation loss decreased (0.142255 --> 0.139574).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1334147
	speed: 0.0669s/iter; left time: 141.9229s
	iters: 200, epoch: 4 | loss: 0.1417593
	speed: 0.0635s/iter; left time: 128.3470s
	iters: 300, epoch: 4 | loss: 0.1181004
	speed: 0.0649s/iter; left time: 124.6300s
Epoch: 4 cost time: 20.66146206855774
Epoch: 4, Steps: 317 | Train Loss: 0.1175686 Vali Loss: 0.1389804 Test Loss: 0.1014513
Validation loss decreased (0.139574 --> 0.138980).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1117388
	speed: 0.0664s/iter; left time: 119.6774s
	iters: 200, epoch: 5 | loss: 0.0959958
	speed: 0.0653s/iter; left time: 111.1565s
	iters: 300, epoch: 5 | loss: 0.0981475
	speed: 0.0644s/iter; left time: 103.2016s
Epoch: 5 cost time: 20.76905918121338
Epoch: 5, Steps: 317 | Train Loss: 0.1155387 Vali Loss: 0.1377652 Test Loss: 0.1038442
Validation loss decreased (0.138980 --> 0.137765).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1091940
	speed: 0.0663s/iter; left time: 98.4628s
	iters: 200, epoch: 6 | loss: 0.0909254
	speed: 0.0641s/iter; left time: 88.8429s
	iters: 300, epoch: 6 | loss: 0.0986610
	speed: 0.0645s/iter; left time: 82.9252s
Epoch: 6 cost time: 20.645151376724243
Epoch: 6, Steps: 317 | Train Loss: 0.1148275 Vali Loss: 0.1366790 Test Loss: 0.1006236
Validation loss decreased (0.137765 --> 0.136679).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0821982
	speed: 0.0671s/iter; left time: 78.4530s
	iters: 200, epoch: 7 | loss: 0.0913869
	speed: 0.0634s/iter; left time: 67.8137s
	iters: 300, epoch: 7 | loss: 0.1201756
	speed: 0.0635s/iter; left time: 61.5608s
Epoch: 7 cost time: 20.542591094970703
Epoch: 7, Steps: 317 | Train Loss: 0.1142950 Vali Loss: 0.1364391 Test Loss: 0.1010425
Validation loss decreased (0.136679 --> 0.136439).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1146473
	speed: 0.0671s/iter; left time: 57.2002s
	iters: 200, epoch: 8 | loss: 0.1057582
	speed: 0.0634s/iter; left time: 47.6745s
	iters: 300, epoch: 8 | loss: 0.1538607
	speed: 0.0631s/iter; left time: 41.1255s
Epoch: 8 cost time: 20.49580430984497
Epoch: 8, Steps: 317 | Train Loss: 0.1139216 Vali Loss: 0.1373574 Test Loss: 0.1013344
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1055687
	speed: 0.0682s/iter; left time: 36.5093s
	iters: 200, epoch: 9 | loss: 0.1181638
	speed: 0.0630s/iter; left time: 27.4150s
	iters: 300, epoch: 9 | loss: 0.1217325
	speed: 0.0644s/iter; left time: 21.5873s
Epoch: 9 cost time: 20.70847249031067
Epoch: 9, Steps: 317 | Train Loss: 0.1137072 Vali Loss: 0.1367927 Test Loss: 0.1009020
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0851670
	speed: 0.0670s/iter; left time: 14.5999s
	iters: 200, epoch: 10 | loss: 0.1027733
	speed: 0.0625s/iter; left time: 7.3741s
	iters: 300, epoch: 10 | loss: 0.1370370
	speed: 0.0635s/iter; left time: 1.1432s
Epoch: 10 cost time: 20.454522132873535
Epoch: 10, Steps: 317 | Train Loss: 0.1142032 Vali Loss: 0.1365212 Test Loss: 0.1008945
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1904076
	speed: 0.0946s/iter; left time: 291.4186s
	iters: 200, epoch: 1 | loss: 0.1510279
	speed: 0.0757s/iter; left time: 225.7418s
	iters: 300, epoch: 1 | loss: 0.1267908
	speed: 0.0760s/iter; left time: 218.9251s
Epoch: 1 cost time: 26.044579029083252
Epoch: 1, Steps: 318 | Train Loss: 0.2220183 Vali Loss: 0.1305976 Test Loss: 0.0976535
Validation loss decreased (inf --> 0.130598).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1448593
	speed: 0.0803s/iter; left time: 221.7708s
	iters: 200, epoch: 2 | loss: 0.0899353
	speed: 0.0761s/iter; left time: 202.7337s
	iters: 300, epoch: 2 | loss: 0.0876773
	speed: 0.0756s/iter; left time: 193.7402s
Epoch: 2 cost time: 24.625428915023804
Epoch: 2, Steps: 318 | Train Loss: 0.1156920 Vali Loss: 0.1178029 Test Loss: 0.0861255
Validation loss decreased (0.130598 --> 0.117803).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1235167
	speed: 0.0776s/iter; left time: 189.7878s
	iters: 200, epoch: 3 | loss: 0.1100539
	speed: 0.0747s/iter; left time: 175.0957s
	iters: 300, epoch: 3 | loss: 0.1103132
	speed: 0.0757s/iter; left time: 170.0051s
Epoch: 3 cost time: 24.23202157020569
Epoch: 3, Steps: 318 | Train Loss: 0.1038705 Vali Loss: 0.1091008 Test Loss: 0.0809844
Validation loss decreased (0.117803 --> 0.109101).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1044826
	speed: 0.0789s/iter; left time: 167.9212s
	iters: 200, epoch: 4 | loss: 0.0939443
	speed: 0.0756s/iter; left time: 153.2579s
	iters: 300, epoch: 4 | loss: 0.1363164
	speed: 0.0747s/iter; left time: 143.8667s
Epoch: 4 cost time: 24.348138093948364
Epoch: 4, Steps: 318 | Train Loss: 0.0991956 Vali Loss: 0.1068097 Test Loss: 0.0814970
Validation loss decreased (0.109101 --> 0.106810).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1248628
	speed: 0.0790s/iter; left time: 142.9088s
	iters: 200, epoch: 5 | loss: 0.0789511
	speed: 0.0746s/iter; left time: 127.4062s
	iters: 300, epoch: 5 | loss: 0.1217699
	speed: 0.0759s/iter; left time: 122.1394s
Epoch: 5 cost time: 24.36583137512207
Epoch: 5, Steps: 318 | Train Loss: 0.0968920 Vali Loss: 0.1044977 Test Loss: 0.0789745
Validation loss decreased (0.106810 --> 0.104498).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1368344
	speed: 0.0777s/iter; left time: 115.7950s
	iters: 200, epoch: 6 | loss: 0.1027147
	speed: 0.0740s/iter; left time: 102.9453s
	iters: 300, epoch: 6 | loss: 0.1053709
	speed: 0.0743s/iter; left time: 95.9727s
Epoch: 6 cost time: 24.05052399635315
Epoch: 6, Steps: 318 | Train Loss: 0.0958294 Vali Loss: 0.1044312 Test Loss: 0.0789183
Validation loss decreased (0.104498 --> 0.104431).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0921763
	speed: 0.0787s/iter; left time: 92.2811s
	iters: 200, epoch: 7 | loss: 0.0575498
	speed: 0.0755s/iter; left time: 81.0357s
	iters: 300, epoch: 7 | loss: 0.1312516
	speed: 0.0747s/iter; left time: 72.6675s
Epoch: 7 cost time: 24.307727575302124
Epoch: 7, Steps: 318 | Train Loss: 0.0949600 Vali Loss: 0.1038994 Test Loss: 0.0783023
Validation loss decreased (0.104431 --> 0.103899).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0738082
	speed: 0.0776s/iter; left time: 66.3856s
	iters: 200, epoch: 8 | loss: 0.0794524
	speed: 0.0766s/iter; left time: 57.8273s
	iters: 300, epoch: 8 | loss: 0.0967227
	speed: 0.0753s/iter; left time: 49.3148s
Epoch: 8 cost time: 24.36612558364868
Epoch: 8, Steps: 318 | Train Loss: 0.0949342 Vali Loss: 0.1041328 Test Loss: 0.0786227
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1050506
	speed: 0.0790s/iter; left time: 42.4293s
	iters: 200, epoch: 9 | loss: 0.0970782
	speed: 0.0745s/iter; left time: 32.5364s
	iters: 300, epoch: 9 | loss: 0.1008030
	speed: 0.0749s/iter; left time: 25.2409s
Epoch: 9 cost time: 24.26484990119934
Epoch: 9, Steps: 318 | Train Loss: 0.0947784 Vali Loss: 0.1044237 Test Loss: 0.0786317
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0876016
	speed: 0.0787s/iter; left time: 17.2437s
	iters: 200, epoch: 10 | loss: 0.0921180
	speed: 0.0748s/iter; left time: 8.8976s
	iters: 300, epoch: 10 | loss: 0.0949080
	speed: 0.0758s/iter; left time: 1.4399s
Epoch: 10 cost time: 24.337899208068848
Epoch: 10, Steps: 318 | Train Loss: 0.0952489 Vali Loss: 0.1042459 Test Loss: 0.0786309
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1376942
	speed: 0.0947s/iter; left time: 291.8984s
	iters: 200, epoch: 1 | loss: 0.1134649
	speed: 0.0787s/iter; left time: 234.6183s
	iters: 300, epoch: 1 | loss: 0.0925805
	speed: 0.0797s/iter; left time: 229.6579s
Epoch: 1 cost time: 26.80138874053955
Epoch: 1, Steps: 318 | Train Loss: 0.2212442 Vali Loss: 0.1416964 Test Loss: 0.1067041
Validation loss decreased (inf --> 0.141696).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1423839
	speed: 0.0837s/iter; left time: 231.3629s
	iters: 200, epoch: 2 | loss: 0.1107361
	speed: 0.0794s/iter; left time: 211.3630s
	iters: 300, epoch: 2 | loss: 0.1460820
	speed: 0.0793s/iter; left time: 203.2875s
Epoch: 2 cost time: 25.74659276008606
Epoch: 2, Steps: 318 | Train Loss: 0.1249520 Vali Loss: 0.1238130 Test Loss: 0.0945250
Validation loss decreased (0.141696 --> 0.123813).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1378540
	speed: 0.0834s/iter; left time: 203.8535s
	iters: 200, epoch: 3 | loss: 0.1255720
	speed: 0.0794s/iter; left time: 186.2178s
	iters: 300, epoch: 3 | loss: 0.0952408
	speed: 0.0787s/iter; left time: 176.7125s
Epoch: 3 cost time: 25.64094567298889
Epoch: 3, Steps: 318 | Train Loss: 0.1130128 Vali Loss: 0.1192261 Test Loss: 0.0909120
Validation loss decreased (0.123813 --> 0.119226).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1021338
	speed: 0.0824s/iter; left time: 175.2107s
	iters: 200, epoch: 4 | loss: 0.1233130
	speed: 0.0790s/iter; left time: 160.2331s
	iters: 300, epoch: 4 | loss: 0.0943934
	speed: 0.0795s/iter; left time: 153.2669s
Epoch: 4 cost time: 25.568325996398926
Epoch: 4, Steps: 318 | Train Loss: 0.1076359 Vali Loss: 0.1174991 Test Loss: 0.0882312
Validation loss decreased (0.119226 --> 0.117499).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1016738
	speed: 0.0824s/iter; left time: 149.0931s
	iters: 200, epoch: 5 | loss: 0.0918136
	speed: 0.0803s/iter; left time: 137.2492s
	iters: 300, epoch: 5 | loss: 0.0884320
	speed: 0.0793s/iter; left time: 127.5432s
Epoch: 5 cost time: 25.74044418334961
Epoch: 5, Steps: 318 | Train Loss: 0.1054722 Vali Loss: 0.1145988 Test Loss: 0.0882098
Validation loss decreased (0.117499 --> 0.114599).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0982669
	speed: 0.0820s/iter; left time: 122.2879s
	iters: 200, epoch: 6 | loss: 0.1323198
	speed: 0.0797s/iter; left time: 110.9027s
	iters: 300, epoch: 6 | loss: 0.0863566
	speed: 0.0791s/iter; left time: 102.0701s
Epoch: 6 cost time: 25.64201855659485
Epoch: 6, Steps: 318 | Train Loss: 0.1041139 Vali Loss: 0.1148686 Test Loss: 0.0873868
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0890036
	speed: 0.0841s/iter; left time: 98.6561s
	iters: 200, epoch: 7 | loss: 0.0939595
	speed: 0.0802s/iter; left time: 86.0871s
	iters: 300, epoch: 7 | loss: 0.1013075
	speed: 0.0805s/iter; left time: 78.3072s
Epoch: 7 cost time: 25.95514464378357
Epoch: 7, Steps: 318 | Train Loss: 0.1038960 Vali Loss: 0.1139361 Test Loss: 0.0870997
Validation loss decreased (0.114599 --> 0.113936).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1085818
	speed: 0.0835s/iter; left time: 71.4016s
	iters: 200, epoch: 8 | loss: 0.1148281
	speed: 0.0795s/iter; left time: 60.0590s
	iters: 300, epoch: 8 | loss: 0.0948315
	speed: 0.0796s/iter; left time: 52.1178s
Epoch: 8 cost time: 25.743215560913086
Epoch: 8, Steps: 318 | Train Loss: 0.1037190 Vali Loss: 0.1150742 Test Loss: 0.0871412
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1366238
	speed: 0.0820s/iter; left time: 44.0319s
	iters: 200, epoch: 9 | loss: 0.0823641
	speed: 0.0793s/iter; left time: 34.6533s
	iters: 300, epoch: 9 | loss: 0.1207654
	speed: 0.0787s/iter; left time: 26.5195s
Epoch: 9 cost time: 25.481750965118408
Epoch: 9, Steps: 318 | Train Loss: 0.1036317 Vali Loss: 0.1151957 Test Loss: 0.0869777
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0848547
	speed: 0.0825s/iter; left time: 18.0755s
	iters: 200, epoch: 10 | loss: 0.1019501
	speed: 0.0795s/iter; left time: 9.4564s
	iters: 300, epoch: 10 | loss: 0.1144124
	speed: 0.0791s/iter; left time: 1.5033s
Epoch: 10 cost time: 25.600956439971924
Epoch: 10, Steps: 318 | Train Loss: 0.1038389 Vali Loss: 0.1144275 Test Loss: 0.0869535
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1788538
	speed: 0.0956s/iter; left time: 294.6301s
	iters: 200, epoch: 1 | loss: 0.1455259
	speed: 0.0837s/iter; left time: 249.4571s
	iters: 300, epoch: 1 | loss: 0.1432390
	speed: 0.0849s/iter; left time: 244.5425s
Epoch: 1 cost time: 27.983139753341675
Epoch: 1, Steps: 318 | Train Loss: 0.1973537 Vali Loss: 0.1454540 Test Loss: 0.1092569
Validation loss decreased (inf --> 0.145454).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1385366
	speed: 0.0872s/iter; left time: 241.0608s
	iters: 200, epoch: 2 | loss: 0.1157084
	speed: 0.0839s/iter; left time: 223.3766s
	iters: 300, epoch: 2 | loss: 0.1033401
	speed: 0.0846s/iter; left time: 216.9318s
Epoch: 2 cost time: 27.168641567230225
Epoch: 2, Steps: 318 | Train Loss: 0.1231270 Vali Loss: 0.1316690 Test Loss: 0.0977198
Validation loss decreased (0.145454 --> 0.131669).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1323343
	speed: 0.0872s/iter; left time: 213.1883s
	iters: 200, epoch: 3 | loss: 0.1007389
	speed: 0.0839s/iter; left time: 196.8321s
	iters: 300, epoch: 3 | loss: 0.0896887
	speed: 0.0845s/iter; left time: 189.7528s
Epoch: 3 cost time: 27.15315008163452
Epoch: 3, Steps: 318 | Train Loss: 0.1136690 Vali Loss: 0.1285609 Test Loss: 0.0953924
Validation loss decreased (0.131669 --> 0.128561).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0975392
	speed: 0.0880s/iter; left time: 187.0809s
	iters: 200, epoch: 4 | loss: 0.1057058
	speed: 0.0853s/iter; left time: 172.8394s
	iters: 300, epoch: 4 | loss: 0.1087200
	speed: 0.0848s/iter; left time: 163.3286s
Epoch: 4 cost time: 27.38717484474182
Epoch: 4, Steps: 318 | Train Loss: 0.1094845 Vali Loss: 0.1254765 Test Loss: 0.0931697
Validation loss decreased (0.128561 --> 0.125476).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0927932
	speed: 0.0871s/iter; left time: 157.5571s
	iters: 200, epoch: 5 | loss: 0.0989417
	speed: 0.0840s/iter; left time: 143.5830s
	iters: 300, epoch: 5 | loss: 0.1018179
	speed: 0.0839s/iter; left time: 134.9367s
Epoch: 5 cost time: 27.09719467163086
Epoch: 5, Steps: 318 | Train Loss: 0.1073181 Vali Loss: 0.1237827 Test Loss: 0.0918406
Validation loss decreased (0.125476 --> 0.123783).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0797377
	speed: 0.0868s/iter; left time: 129.4257s
	iters: 200, epoch: 6 | loss: 0.1054140
	speed: 0.0830s/iter; left time: 115.4637s
	iters: 300, epoch: 6 | loss: 0.0976512
	speed: 0.0837s/iter; left time: 108.0997s
Epoch: 6 cost time: 26.936365604400635
Epoch: 6, Steps: 318 | Train Loss: 0.1063287 Vali Loss: 0.1224739 Test Loss: 0.0915811
Validation loss decreased (0.123783 --> 0.122474).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1104899
	speed: 0.0864s/iter; left time: 101.3096s
	iters: 200, epoch: 7 | loss: 0.0925366
	speed: 0.0833s/iter; left time: 89.4152s
	iters: 300, epoch: 7 | loss: 0.1477743
	speed: 0.0840s/iter; left time: 81.7743s
Epoch: 7 cost time: 26.947916746139526
Epoch: 7, Steps: 318 | Train Loss: 0.1055488 Vali Loss: 0.1228067 Test Loss: 0.0914216
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0713514
	speed: 0.0887s/iter; left time: 75.8247s
	iters: 200, epoch: 8 | loss: 0.0890477
	speed: 0.0832s/iter; left time: 62.8151s
	iters: 300, epoch: 8 | loss: 0.1109178
	speed: 0.0839s/iter; left time: 54.9226s
Epoch: 8 cost time: 27.19300603866577
Epoch: 8, Steps: 318 | Train Loss: 0.1055440 Vali Loss: 0.1231181 Test Loss: 0.0915459
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1205007
	speed: 0.0877s/iter; left time: 47.1042s
	iters: 200, epoch: 9 | loss: 0.1013082
	speed: 0.0841s/iter; left time: 36.7519s
	iters: 300, epoch: 9 | loss: 0.1018372
	speed: 0.0845s/iter; left time: 28.4832s
Epoch: 9 cost time: 27.213228702545166
Epoch: 9, Steps: 318 | Train Loss: 0.1052143 Vali Loss: 0.1229772 Test Loss: 0.0916709
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1423491
	speed: 0.0931s/iter; left time: 286.8257s
	iters: 200, epoch: 1 | loss: 0.1436980
	speed: 0.0764s/iter; left time: 227.8653s
	iters: 300, epoch: 1 | loss: 0.1315573
	speed: 0.0764s/iter; left time: 220.2124s
Epoch: 1 cost time: 26.020167112350464
Epoch: 1, Steps: 318 | Train Loss: 0.2075658 Vali Loss: 0.1447220 Test Loss: 0.1096834
Validation loss decreased (inf --> 0.144722).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1566976
	speed: 0.0801s/iter; left time: 221.1951s
	iters: 200, epoch: 2 | loss: 0.1154731
	speed: 0.0765s/iter; left time: 203.5993s
	iters: 300, epoch: 2 | loss: 0.1199340
	speed: 0.0768s/iter; left time: 196.9348s
Epoch: 2 cost time: 24.772862672805786
Epoch: 2, Steps: 318 | Train Loss: 0.1276386 Vali Loss: 0.1416272 Test Loss: 0.1091526
Validation loss decreased (0.144722 --> 0.141627).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1147355
	speed: 0.0803s/iter; left time: 196.2860s
	iters: 200, epoch: 3 | loss: 0.0976798
	speed: 0.0763s/iter; left time: 178.9079s
	iters: 300, epoch: 3 | loss: 0.1136024
	speed: 0.0763s/iter; left time: 171.3249s
Epoch: 3 cost time: 24.72792935371399
Epoch: 3, Steps: 318 | Train Loss: 0.1190760 Vali Loss: 0.1366667 Test Loss: 0.1038948
Validation loss decreased (0.141627 --> 0.136667).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1309216
	speed: 0.0799s/iter; left time: 169.9041s
	iters: 200, epoch: 4 | loss: 0.1085763
	speed: 0.0764s/iter; left time: 154.9263s
	iters: 300, epoch: 4 | loss: 0.1451847
	speed: 0.0761s/iter; left time: 146.5906s
Epoch: 4 cost time: 24.672945022583008
Epoch: 4, Steps: 318 | Train Loss: 0.1153363 Vali Loss: 0.1323938 Test Loss: 0.0985418
Validation loss decreased (0.136667 --> 0.132394).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1089153
	speed: 0.0801s/iter; left time: 144.9793s
	iters: 200, epoch: 5 | loss: 0.1013582
	speed: 0.0770s/iter; left time: 131.6374s
	iters: 300, epoch: 5 | loss: 0.1104255
	speed: 0.0761s/iter; left time: 122.4827s
Epoch: 5 cost time: 24.770447731018066
Epoch: 5, Steps: 318 | Train Loss: 0.1131119 Vali Loss: 0.1311793 Test Loss: 0.0981877
Validation loss decreased (0.132394 --> 0.131179).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1352329
	speed: 0.0801s/iter; left time: 119.3659s
	iters: 200, epoch: 6 | loss: 0.1192257
	speed: 0.0757s/iter; left time: 105.3503s
	iters: 300, epoch: 6 | loss: 0.1063478
	speed: 0.0763s/iter; left time: 98.5081s
Epoch: 6 cost time: 24.652758836746216
Epoch: 6, Steps: 318 | Train Loss: 0.1121041 Vali Loss: 0.1306190 Test Loss: 0.0969572
Validation loss decreased (0.131179 --> 0.130619).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1243561
	speed: 0.0800s/iter; left time: 93.8313s
	iters: 200, epoch: 7 | loss: 0.1043462
	speed: 0.0757s/iter; left time: 81.2612s
	iters: 300, epoch: 7 | loss: 0.1033994
	speed: 0.0765s/iter; left time: 74.3914s
Epoch: 7 cost time: 24.65972065925598
Epoch: 7, Steps: 318 | Train Loss: 0.1117804 Vali Loss: 0.1306019 Test Loss: 0.0966545
Validation loss decreased (0.130619 --> 0.130602).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1032144
	speed: 0.0789s/iter; left time: 67.4473s
	iters: 200, epoch: 8 | loss: 0.1144693
	speed: 0.0758s/iter; left time: 57.2340s
	iters: 300, epoch: 8 | loss: 0.1008373
	speed: 0.0758s/iter; left time: 49.6693s
Epoch: 8 cost time: 24.514736890792847
Epoch: 8, Steps: 318 | Train Loss: 0.1112094 Vali Loss: 0.1301232 Test Loss: 0.0965055
Validation loss decreased (0.130602 --> 0.130123).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0947657
	speed: 0.0807s/iter; left time: 43.3412s
	iters: 200, epoch: 9 | loss: 0.1425798
	speed: 0.0780s/iter; left time: 34.0888s
	iters: 300, epoch: 9 | loss: 0.1030288
	speed: 0.0764s/iter; left time: 25.7505s
Epoch: 9 cost time: 24.95966410636902
Epoch: 9, Steps: 318 | Train Loss: 0.1113082 Vali Loss: 0.1295169 Test Loss: 0.0965372
Validation loss decreased (0.130123 --> 0.129517).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1661161
	speed: 0.0794s/iter; left time: 17.3858s
	iters: 200, epoch: 10 | loss: 0.1077294
	speed: 0.0759s/iter; left time: 9.0314s
	iters: 300, epoch: 10 | loss: 0.0931851
	speed: 0.0760s/iter; left time: 1.4447s
Epoch: 10 cost time: 24.579453706741333
Epoch: 10, Steps: 318 | Train Loss: 0.1110865 Vali Loss: 0.1298105 Test Loss: 0.0965441
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1387454
	speed: 0.0978s/iter; left time: 300.2194s
	iters: 200, epoch: 1 | loss: 0.1686977
	speed: 0.0811s/iter; left time: 240.9288s
	iters: 300, epoch: 1 | loss: 0.1211677
	speed: 0.0814s/iter; left time: 233.6851s
Epoch: 1 cost time: 27.4671847820282
Epoch: 1, Steps: 317 | Train Loss: 0.2064969 Vali Loss: 0.1530253 Test Loss: 0.1232351
Validation loss decreased (inf --> 0.153025).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1281771
	speed: 0.0845s/iter; left time: 232.6058s
	iters: 200, epoch: 2 | loss: 0.1471204
	speed: 0.0805s/iter; left time: 213.7662s
	iters: 300, epoch: 2 | loss: 0.1643594
	speed: 0.0801s/iter; left time: 204.4518s
Epoch: 2 cost time: 25.91274929046631
Epoch: 2, Steps: 317 | Train Loss: 0.1289523 Vali Loss: 0.1424406 Test Loss: 0.1061730
Validation loss decreased (0.153025 --> 0.142441).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1293329
	speed: 0.0831s/iter; left time: 202.4343s
	iters: 200, epoch: 3 | loss: 0.1352250
	speed: 0.0794s/iter; left time: 185.5531s
	iters: 300, epoch: 3 | loss: 0.1229681
	speed: 0.0789s/iter; left time: 176.5016s
Epoch: 3 cost time: 25.540093421936035
Epoch: 3, Steps: 317 | Train Loss: 0.1216479 Vali Loss: 0.1395664 Test Loss: 0.1012076
Validation loss decreased (0.142441 --> 0.139566).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1330931
	speed: 0.0845s/iter; left time: 179.0701s
	iters: 200, epoch: 4 | loss: 0.1440675
	speed: 0.0797s/iter; left time: 161.0490s
	iters: 300, epoch: 4 | loss: 0.1179055
	speed: 0.0803s/iter; left time: 154.1445s
Epoch: 4 cost time: 25.890459775924683
Epoch: 4, Steps: 317 | Train Loss: 0.1174676 Vali Loss: 0.1388560 Test Loss: 0.1010717
Validation loss decreased (0.139566 --> 0.138856).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1118493
	speed: 0.0823s/iter; left time: 148.3479s
	iters: 200, epoch: 5 | loss: 0.0962125
	speed: 0.0795s/iter; left time: 135.4694s
	iters: 300, epoch: 5 | loss: 0.0982995
	speed: 0.0788s/iter; left time: 126.2415s
Epoch: 5 cost time: 25.46579146385193
Epoch: 5, Steps: 317 | Train Loss: 0.1155222 Vali Loss: 0.1376914 Test Loss: 0.1034857
Validation loss decreased (0.138856 --> 0.137691).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1094064
	speed: 0.0830s/iter; left time: 123.3922s
	iters: 200, epoch: 6 | loss: 0.0908690
	speed: 0.0787s/iter; left time: 109.0661s
	iters: 300, epoch: 6 | loss: 0.0990912
	speed: 0.0792s/iter; left time: 101.9135s
Epoch: 6 cost time: 25.495217323303223
Epoch: 6, Steps: 317 | Train Loss: 0.1147263 Vali Loss: 0.1367388 Test Loss: 0.1003950
Validation loss decreased (0.137691 --> 0.136739).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0818773
	speed: 0.0832s/iter; left time: 97.2064s
	iters: 200, epoch: 7 | loss: 0.0916103
	speed: 0.0787s/iter; left time: 84.1281s
	iters: 300, epoch: 7 | loss: 0.1203085
	speed: 0.0790s/iter; left time: 76.5783s
Epoch: 7 cost time: 25.481504201889038
Epoch: 7, Steps: 317 | Train Loss: 0.1142107 Vali Loss: 0.1364644 Test Loss: 0.1007572
Validation loss decreased (0.136739 --> 0.136464).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1148709
	speed: 0.0828s/iter; left time: 70.5332s
	iters: 200, epoch: 8 | loss: 0.1047057
	speed: 0.0794s/iter; left time: 59.6848s
	iters: 300, epoch: 8 | loss: 0.1544904
	speed: 0.0788s/iter; left time: 51.3648s
Epoch: 8 cost time: 25.498100519180298
Epoch: 8, Steps: 317 | Train Loss: 0.1138330 Vali Loss: 0.1373579 Test Loss: 0.1010611
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1080940
	speed: 0.0844s/iter; left time: 45.1707s
	iters: 200, epoch: 9 | loss: 0.1184955
	speed: 0.0796s/iter; left time: 34.6252s
	iters: 300, epoch: 9 | loss: 0.1213507
	speed: 0.0797s/iter; left time: 26.6831s
Epoch: 9 cost time: 25.77716064453125
Epoch: 9, Steps: 317 | Train Loss: 0.1135994 Vali Loss: 0.1368245 Test Loss: 0.1006300
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0888127
	speed: 0.0814s/iter; left time: 17.7492s
	iters: 200, epoch: 10 | loss: 0.1020344
	speed: 0.0781s/iter; left time: 9.2186s
	iters: 300, epoch: 10 | loss: 0.1370541
	speed: 0.0786s/iter; left time: 1.4153s
Epoch: 10 cost time: 25.217750310897827
Epoch: 10, Steps: 317 | Train Loss: 0.1140792 Vali Loss: 0.1365325 Test Loss: 0.1006140
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1831095
	speed: 0.1056s/iter; left time: 325.4210s
	iters: 200, epoch: 1 | loss: 0.1413312
	speed: 0.0902s/iter; left time: 268.9691s
	iters: 300, epoch: 1 | loss: 0.1273223
	speed: 0.0922s/iter; left time: 265.5099s
Epoch: 1 cost time: 30.496389150619507
Epoch: 1, Steps: 318 | Train Loss: 0.2146246 Vali Loss: 0.1319556 Test Loss: 0.0989030
Validation loss decreased (inf --> 0.131956).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1310390
	speed: 0.0957s/iter; left time: 264.2962s
	iters: 200, epoch: 2 | loss: 0.0827685
	speed: 0.0913s/iter; left time: 243.0591s
	iters: 300, epoch: 2 | loss: 0.0822211
	speed: 0.0907s/iter; left time: 232.5141s
Epoch: 2 cost time: 29.491675853729248
Epoch: 2, Steps: 318 | Train Loss: 0.1122388 Vali Loss: 0.1191755 Test Loss: 0.0881129
Validation loss decreased (0.131956 --> 0.119175).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1152639
	speed: 0.0952s/iter; left time: 232.7010s
	iters: 200, epoch: 3 | loss: 0.1121201
	speed: 0.0910s/iter; left time: 213.4395s
	iters: 300, epoch: 3 | loss: 0.1083814
	speed: 0.0895s/iter; left time: 200.8190s
Epoch: 3 cost time: 29.265610456466675
Epoch: 3, Steps: 318 | Train Loss: 0.1001586 Vali Loss: 0.1103247 Test Loss: 0.0819060
Validation loss decreased (0.119175 --> 0.110325).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1124453
	speed: 0.0932s/iter; left time: 198.2747s
	iters: 200, epoch: 4 | loss: 0.0915490
	speed: 0.0912s/iter; left time: 184.8746s
	iters: 300, epoch: 4 | loss: 0.1283531
	speed: 0.0900s/iter; left time: 173.4175s
Epoch: 4 cost time: 29.13249111175537
Epoch: 4, Steps: 318 | Train Loss: 0.0954039 Vali Loss: 0.1083572 Test Loss: 0.0821713
Validation loss decreased (0.110325 --> 0.108357).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1259422
	speed: 0.0922s/iter; left time: 166.7493s
	iters: 200, epoch: 5 | loss: 0.0733812
	speed: 0.0884s/iter; left time: 151.1238s
	iters: 300, epoch: 5 | loss: 0.1183165
	speed: 0.0898s/iter; left time: 144.5214s
Epoch: 5 cost time: 28.719001293182373
Epoch: 5, Steps: 318 | Train Loss: 0.0925994 Vali Loss: 0.1063135 Test Loss: 0.0799878
Validation loss decreased (0.108357 --> 0.106313).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1243248
	speed: 0.0933s/iter; left time: 139.1636s
	iters: 200, epoch: 6 | loss: 0.1040822
	speed: 0.0900s/iter; left time: 125.2297s
	iters: 300, epoch: 6 | loss: 0.1076563
	speed: 0.0888s/iter; left time: 114.5796s
Epoch: 6 cost time: 28.894347667694092
Epoch: 6, Steps: 318 | Train Loss: 0.0914823 Vali Loss: 0.1061943 Test Loss: 0.0796205
Validation loss decreased (0.106313 --> 0.106194).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0849982
	speed: 0.0930s/iter; left time: 109.1318s
	iters: 200, epoch: 7 | loss: 0.0580400
	speed: 0.0900s/iter; left time: 96.6156s
	iters: 300, epoch: 7 | loss: 0.1207064
	speed: 0.0885s/iter; left time: 86.1139s
Epoch: 7 cost time: 28.83908987045288
Epoch: 7, Steps: 318 | Train Loss: 0.0904710 Vali Loss: 0.1061976 Test Loss: 0.0791535
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0696662
	speed: 0.0932s/iter; left time: 79.6699s
	iters: 200, epoch: 8 | loss: 0.0788314
	speed: 0.0889s/iter; left time: 67.1147s
	iters: 300, epoch: 8 | loss: 0.0952236
	speed: 0.0893s/iter; left time: 58.5074s
Epoch: 8 cost time: 28.81681251525879
Epoch: 8, Steps: 318 | Train Loss: 0.0905042 Vali Loss: 0.1064066 Test Loss: 0.0795416
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1018232
	speed: 0.0926s/iter; left time: 49.7328s
	iters: 200, epoch: 9 | loss: 0.0905360
	speed: 0.0893s/iter; left time: 39.0319s
	iters: 300, epoch: 9 | loss: 0.0926405
	speed: 0.0887s/iter; left time: 29.8973s
Epoch: 9 cost time: 28.75918436050415
Epoch: 9, Steps: 318 | Train Loss: 0.0903001 Vali Loss: 0.1065295 Test Loss: 0.0795847
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1377062
	speed: 0.1112s/iter; left time: 342.6398s
	iters: 200, epoch: 1 | loss: 0.1135841
	speed: 0.0950s/iter; left time: 283.2864s
	iters: 300, epoch: 1 | loss: 0.0925451
	speed: 0.0959s/iter; left time: 276.2689s
Epoch: 1 cost time: 31.97846221923828
Epoch: 1, Steps: 318 | Train Loss: 0.2217927 Vali Loss: 0.1420606 Test Loss: 0.1071299
Validation loss decreased (inf --> 0.142061).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1426267
	speed: 0.0976s/iter; left time: 269.6757s
	iters: 200, epoch: 2 | loss: 0.1127446
	speed: 0.0944s/iter; left time: 251.4677s
	iters: 300, epoch: 2 | loss: 0.1477383
	speed: 0.0950s/iter; left time: 243.5230s
Epoch: 2 cost time: 30.500224113464355
Epoch: 2, Steps: 318 | Train Loss: 0.1255650 Vali Loss: 0.1248479 Test Loss: 0.0953003
Validation loss decreased (0.142061 --> 0.124848).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1392989
	speed: 0.0991s/iter; left time: 242.1837s
	iters: 200, epoch: 3 | loss: 0.1257265
	speed: 0.0967s/iter; left time: 226.7474s
	iters: 300, epoch: 3 | loss: 0.0955497
	speed: 0.0954s/iter; left time: 214.2059s
Epoch: 3 cost time: 30.914549827575684
Epoch: 3, Steps: 318 | Train Loss: 0.1137044 Vali Loss: 0.1198488 Test Loss: 0.0912681
Validation loss decreased (0.124848 --> 0.119849).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1028606
	speed: 0.0996s/iter; left time: 211.7744s
	iters: 200, epoch: 4 | loss: 0.1234083
	speed: 0.0972s/iter; left time: 197.1053s
	iters: 300, epoch: 4 | loss: 0.0946618
	speed: 0.0962s/iter; left time: 185.4017s
Epoch: 4 cost time: 31.104745388031006
Epoch: 4, Steps: 318 | Train Loss: 0.1081577 Vali Loss: 0.1181719 Test Loss: 0.0886863
Validation loss decreased (0.119849 --> 0.118172).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1017132
	speed: 0.0994s/iter; left time: 179.8023s
	iters: 200, epoch: 5 | loss: 0.0920940
	speed: 0.0964s/iter; left time: 164.7154s
	iters: 300, epoch: 5 | loss: 0.0884098
	speed: 0.0957s/iter; left time: 153.9967s
Epoch: 5 cost time: 30.945600986480713
Epoch: 5, Steps: 318 | Train Loss: 0.1059283 Vali Loss: 0.1150370 Test Loss: 0.0885432
Validation loss decreased (0.118172 --> 0.115037).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0980211
	speed: 0.0979s/iter; left time: 146.0362s
	iters: 200, epoch: 6 | loss: 0.1326966
	speed: 0.0951s/iter; left time: 132.3500s
	iters: 300, epoch: 6 | loss: 0.0865975
	speed: 0.0952s/iter; left time: 122.9304s
Epoch: 6 cost time: 30.669432640075684
Epoch: 6, Steps: 318 | Train Loss: 0.1045057 Vali Loss: 0.1153150 Test Loss: 0.0877201
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0889812
	speed: 0.1001s/iter; left time: 117.3610s
	iters: 200, epoch: 7 | loss: 0.0950505
	speed: 0.0956s/iter; left time: 102.6145s
	iters: 300, epoch: 7 | loss: 0.1019605
	speed: 0.0950s/iter; left time: 92.4509s
Epoch: 7 cost time: 30.88831377029419
Epoch: 7, Steps: 318 | Train Loss: 0.1042808 Vali Loss: 0.1143585 Test Loss: 0.0874069
Validation loss decreased (0.115037 --> 0.114359).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1097293
	speed: 0.0995s/iter; left time: 85.0826s
	iters: 200, epoch: 8 | loss: 0.1145715
	speed: 0.0951s/iter; left time: 71.8207s
	iters: 300, epoch: 8 | loss: 0.0949593
	speed: 0.0966s/iter; left time: 63.2758s
Epoch: 8 cost time: 30.91600728034973
Epoch: 8, Steps: 318 | Train Loss: 0.1040965 Vali Loss: 0.1154832 Test Loss: 0.0874371
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1363907
	speed: 0.0984s/iter; left time: 52.8167s
	iters: 200, epoch: 9 | loss: 0.0829547
	speed: 0.0961s/iter; left time: 42.0122s
	iters: 300, epoch: 9 | loss: 0.1208815
	speed: 0.0959s/iter; left time: 32.3186s
Epoch: 9 cost time: 30.83495259284973
Epoch: 9, Steps: 318 | Train Loss: 0.1039884 Vali Loss: 0.1156139 Test Loss: 0.0872778
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0851166
	speed: 0.0993s/iter; left time: 21.7493s
	iters: 200, epoch: 10 | loss: 0.1029937
	speed: 0.0963s/iter; left time: 11.4644s
	iters: 300, epoch: 10 | loss: 0.1156700
	speed: 0.0953s/iter; left time: 1.8101s
Epoch: 10 cost time: 30.89151406288147
Epoch: 10, Steps: 318 | Train Loss: 0.1042009 Vali Loss: 0.1148347 Test Loss: 0.0872533
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1791198
	speed: 0.1133s/iter; left time: 349.0063s
	iters: 200, epoch: 1 | loss: 0.1456186
	speed: 0.0997s/iter; left time: 297.2772s
	iters: 300, epoch: 1 | loss: 0.1435447
	speed: 0.1011s/iter; left time: 291.1772s
Epoch: 1 cost time: 33.283068895339966
Epoch: 1, Steps: 318 | Train Loss: 0.1974963 Vali Loss: 0.1455369 Test Loss: 0.1094295
Validation loss decreased (inf --> 0.145537).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1389392
	speed: 0.1045s/iter; left time: 288.7989s
	iters: 200, epoch: 2 | loss: 0.1158211
	speed: 0.1021s/iter; left time: 271.8257s
	iters: 300, epoch: 2 | loss: 0.1035054
	speed: 0.1032s/iter; left time: 264.5240s
Epoch: 2 cost time: 32.87701439857483
Epoch: 2, Steps: 318 | Train Loss: 0.1233282 Vali Loss: 0.1318602 Test Loss: 0.0979771
Validation loss decreased (0.145537 --> 0.131860).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1326101
	speed: 0.1056s/iter; left time: 258.1152s
	iters: 200, epoch: 3 | loss: 0.1011213
	speed: 0.1019s/iter; left time: 238.9292s
	iters: 300, epoch: 3 | loss: 0.0902083
	speed: 0.1024s/iter; left time: 229.9846s
Epoch: 3 cost time: 32.92175483703613
Epoch: 3, Steps: 318 | Train Loss: 0.1140204 Vali Loss: 0.1289565 Test Loss: 0.0957179
Validation loss decreased (0.131860 --> 0.128956).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0974136
	speed: 0.1057s/iter; left time: 224.7961s
	iters: 200, epoch: 4 | loss: 0.1057986
	speed: 0.1022s/iter; left time: 207.2023s
	iters: 300, epoch: 4 | loss: 0.1088169
	speed: 0.1027s/iter; left time: 197.8376s
Epoch: 4 cost time: 33.00045037269592
Epoch: 4, Steps: 318 | Train Loss: 0.1098312 Vali Loss: 0.1257747 Test Loss: 0.0934523
Validation loss decreased (0.128956 --> 0.125775).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0929968
	speed: 0.1055s/iter; left time: 190.8214s
	iters: 200, epoch: 5 | loss: 0.0993632
	speed: 0.1024s/iter; left time: 175.0203s
	iters: 300, epoch: 5 | loss: 0.1020131
	speed: 0.1028s/iter; left time: 165.4025s
Epoch: 5 cost time: 33.02503776550293
Epoch: 5, Steps: 318 | Train Loss: 0.1076553 Vali Loss: 0.1240540 Test Loss: 0.0920589
Validation loss decreased (0.125775 --> 0.124054).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0795359
	speed: 0.1064s/iter; left time: 158.6426s
	iters: 200, epoch: 6 | loss: 0.1059336
	speed: 0.1028s/iter; left time: 142.9891s
	iters: 300, epoch: 6 | loss: 0.0978242
	speed: 0.1037s/iter; left time: 133.8682s
Epoch: 6 cost time: 33.237632751464844
Epoch: 6, Steps: 318 | Train Loss: 0.1066569 Vali Loss: 0.1227318 Test Loss: 0.0918032
Validation loss decreased (0.124054 --> 0.122732).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1113018
	speed: 0.1061s/iter; left time: 124.4995s
	iters: 200, epoch: 7 | loss: 0.0931957
	speed: 0.1029s/iter; left time: 110.3797s
	iters: 300, epoch: 7 | loss: 0.1482120
	speed: 0.1030s/iter; left time: 100.2028s
Epoch: 7 cost time: 33.10848116874695
Epoch: 7, Steps: 318 | Train Loss: 0.1058688 Vali Loss: 0.1230382 Test Loss: 0.0916228
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0716661
	speed: 0.1057s/iter; left time: 90.3585s
	iters: 200, epoch: 8 | loss: 0.0890724
	speed: 0.1037s/iter; left time: 78.2622s
	iters: 300, epoch: 8 | loss: 0.1114902
	speed: 0.1023s/iter; left time: 67.0227s
Epoch: 8 cost time: 33.09700918197632
Epoch: 8, Steps: 318 | Train Loss: 0.1058693 Vali Loss: 0.1233644 Test Loss: 0.0917573
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1211737
	speed: 0.1054s/iter; left time: 56.6182s
	iters: 200, epoch: 9 | loss: 0.1014390
	speed: 0.1035s/iter; left time: 45.2171s
	iters: 300, epoch: 9 | loss: 0.1023607
	speed: 0.1032s/iter; left time: 34.7729s
Epoch: 9 cost time: 33.15082883834839
Epoch: 9, Steps: 318 | Train Loss: 0.1055285 Vali Loss: 0.1232041 Test Loss: 0.0918797
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1422241
	speed: 0.1096s/iter; left time: 337.7242s
	iters: 200, epoch: 1 | loss: 0.1436159
	speed: 0.0941s/iter; left time: 280.3986s
	iters: 300, epoch: 1 | loss: 0.1306525
	speed: 0.0939s/iter; left time: 270.4600s
Epoch: 1 cost time: 31.466830730438232
Epoch: 1, Steps: 318 | Train Loss: 0.2075832 Vali Loss: 0.1442642 Test Loss: 0.1095964
Validation loss decreased (inf --> 0.144264).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1559799
	speed: 0.0972s/iter; left time: 268.6163s
	iters: 200, epoch: 2 | loss: 0.1156828
	speed: 0.0923s/iter; left time: 245.8389s
	iters: 300, epoch: 2 | loss: 0.1197988
	speed: 0.0929s/iter; left time: 238.0972s
Epoch: 2 cost time: 29.973780393600464
Epoch: 2, Steps: 318 | Train Loss: 0.1273320 Vali Loss: 0.1410595 Test Loss: 0.1090427
Validation loss decreased (0.144264 --> 0.141060).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1142816
	speed: 0.0966s/iter; left time: 236.0758s
	iters: 200, epoch: 3 | loss: 0.0978203
	speed: 0.0917s/iter; left time: 214.9664s
	iters: 300, epoch: 3 | loss: 0.1135593
	speed: 0.0922s/iter; left time: 206.9093s
Epoch: 3 cost time: 29.76260495185852
Epoch: 3, Steps: 318 | Train Loss: 0.1187182 Vali Loss: 0.1359668 Test Loss: 0.1037425
Validation loss decreased (0.141060 --> 0.135967).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1306238
	speed: 0.0958s/iter; left time: 203.7651s
	iters: 200, epoch: 4 | loss: 0.1083501
	speed: 0.0951s/iter; left time: 192.8600s
	iters: 300, epoch: 4 | loss: 0.1450046
	speed: 0.0933s/iter; left time: 179.7753s
Epoch: 4 cost time: 30.166757106781006
Epoch: 4, Steps: 318 | Train Loss: 0.1149529 Vali Loss: 0.1316837 Test Loss: 0.0983280
Validation loss decreased (0.135967 --> 0.131684).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1080507
	speed: 0.0957s/iter; left time: 173.0441s
	iters: 200, epoch: 5 | loss: 0.1007005
	speed: 0.0920s/iter; left time: 157.2391s
	iters: 300, epoch: 5 | loss: 0.1104499
	speed: 0.0925s/iter; left time: 148.7914s
Epoch: 5 cost time: 29.742441177368164
Epoch: 5, Steps: 318 | Train Loss: 0.1127662 Vali Loss: 0.1305197 Test Loss: 0.0980148
Validation loss decreased (0.131684 --> 0.130520).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1347300
	speed: 0.0958s/iter; left time: 142.7855s
	iters: 200, epoch: 6 | loss: 0.1183762
	speed: 0.0924s/iter; left time: 128.5396s
	iters: 300, epoch: 6 | loss: 0.1059267
	speed: 0.0932s/iter; left time: 120.3154s
Epoch: 6 cost time: 29.88121461868286
Epoch: 6, Steps: 318 | Train Loss: 0.1117630 Vali Loss: 0.1299223 Test Loss: 0.0967442
Validation loss decreased (0.130520 --> 0.129922).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1235488
	speed: 0.0969s/iter; left time: 113.6535s
	iters: 200, epoch: 7 | loss: 0.1039971
	speed: 0.0940s/iter; left time: 100.9103s
	iters: 300, epoch: 7 | loss: 0.1025698
	speed: 0.0936s/iter; left time: 91.0794s
Epoch: 7 cost time: 30.187639236450195
Epoch: 7, Steps: 318 | Train Loss: 0.1114480 Vali Loss: 0.1299136 Test Loss: 0.0964505
Validation loss decreased (0.129922 --> 0.129914).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1031337
	speed: 0.0976s/iter; left time: 83.4443s
	iters: 200, epoch: 8 | loss: 0.1144244
	speed: 0.0933s/iter; left time: 70.4290s
	iters: 300, epoch: 8 | loss: 0.1002933
	speed: 0.0923s/iter; left time: 60.4886s
Epoch: 8 cost time: 30.063865423202515
Epoch: 8, Steps: 318 | Train Loss: 0.1108754 Vali Loss: 0.1294431 Test Loss: 0.0963004
Validation loss decreased (0.129914 --> 0.129443).  Saving model ...
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.0940979
	speed: 0.0966s/iter; left time: 51.8650s
	iters: 200, epoch: 9 | loss: 0.1417196
	speed: 0.0932s/iter; left time: 40.7365s
	iters: 300, epoch: 9 | loss: 0.1030465
	speed: 0.0934s/iter; left time: 31.4861s
Epoch: 9 cost time: 30.050318956375122
Epoch: 9, Steps: 318 | Train Loss: 0.1109826 Vali Loss: 0.1288617 Test Loss: 0.0963378
Validation loss decreased (0.129443 --> 0.128862).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1660914
	speed: 0.0961s/iter; left time: 21.0433s
	iters: 200, epoch: 10 | loss: 0.1071841
	speed: 0.0932s/iter; left time: 11.0898s
	iters: 300, epoch: 10 | loss: 0.0926690
	speed: 0.0930s/iter; left time: 1.7670s
Epoch: 10 cost time: 29.958020210266113
Epoch: 10, Steps: 318 | Train Loss: 0.1107626 Vali Loss: 0.1291319 Test Loss: 0.0963498
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=8, e_layers=3, d_layers=1, d_ff=32, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1388572
	speed: 0.1180s/iter; left time: 362.4190s
	iters: 200, epoch: 1 | loss: 0.1695190
	speed: 0.1039s/iter; left time: 308.7334s
	iters: 300, epoch: 1 | loss: 0.1228776
	speed: 0.1016s/iter; left time: 291.7103s
Epoch: 1 cost time: 34.07020378112793
Epoch: 1, Steps: 317 | Train Loss: 0.2032854 Vali Loss: 0.1520757 Test Loss: 0.1227424
Validation loss decreased (inf --> 0.152076).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1271195
	speed: 0.1009s/iter; left time: 277.8309s
	iters: 200, epoch: 2 | loss: 0.1463990
	speed: 0.0983s/iter; left time: 260.8046s
	iters: 300, epoch: 2 | loss: 0.1640213
	speed: 0.0978s/iter; left time: 249.8950s
Epoch: 2 cost time: 31.44372010231018
Epoch: 2, Steps: 317 | Train Loss: 0.1287574 Vali Loss: 0.1413917 Test Loss: 0.1059751
Validation loss decreased (0.152076 --> 0.141392).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1285362
	speed: 0.1012s/iter; left time: 246.5221s
	iters: 200, epoch: 3 | loss: 0.1335596
	speed: 0.0988s/iter; left time: 230.8354s
	iters: 300, epoch: 3 | loss: 0.1235360
	speed: 0.0998s/iter; left time: 223.2674s
Epoch: 3 cost time: 31.712594985961914
Epoch: 3, Steps: 317 | Train Loss: 0.1214261 Vali Loss: 0.1385092 Test Loss: 0.1011894
Validation loss decreased (0.141392 --> 0.138509).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1327167
	speed: 0.1006s/iter; left time: 213.1725s
	iters: 200, epoch: 4 | loss: 0.1433792
	speed: 0.0980s/iter; left time: 197.8793s
	iters: 300, epoch: 4 | loss: 0.1157443
	speed: 0.0980s/iter; left time: 188.1814s
Epoch: 4 cost time: 31.421791791915894
Epoch: 4, Steps: 317 | Train Loss: 0.1174064 Vali Loss: 0.1378894 Test Loss: 0.1012890
Validation loss decreased (0.138509 --> 0.137889).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1127408
	speed: 0.1007s/iter; left time: 181.5585s
	iters: 200, epoch: 5 | loss: 0.0958281
	speed: 0.0983s/iter; left time: 167.4419s
	iters: 300, epoch: 5 | loss: 0.0985417
	speed: 0.0984s/iter; left time: 157.6565s
Epoch: 5 cost time: 31.50108551979065
Epoch: 5, Steps: 317 | Train Loss: 0.1154657 Vali Loss: 0.1367263 Test Loss: 0.1036578
Validation loss decreased (0.137889 --> 0.136726).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1090619
	speed: 0.1019s/iter; left time: 151.3873s
	iters: 200, epoch: 6 | loss: 0.0912307
	speed: 0.0984s/iter; left time: 136.4481s
	iters: 300, epoch: 6 | loss: 0.0986354
	speed: 0.0988s/iter; left time: 127.0273s
Epoch: 6 cost time: 31.674880504608154
Epoch: 6, Steps: 317 | Train Loss: 0.1147721 Vali Loss: 0.1355904 Test Loss: 0.1005275
Validation loss decreased (0.136726 --> 0.135590).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0820002
	speed: 0.1022s/iter; left time: 119.4238s
	iters: 200, epoch: 7 | loss: 0.0916897
	speed: 0.0986s/iter; left time: 105.4410s
	iters: 300, epoch: 7 | loss: 0.1206302
	speed: 0.0986s/iter; left time: 95.5836s
Epoch: 7 cost time: 31.69727873802185
Epoch: 7, Steps: 317 | Train Loss: 0.1142619 Vali Loss: 0.1353456 Test Loss: 0.1008973
Validation loss decreased (0.135590 --> 0.135346).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1143486
	speed: 0.1027s/iter; left time: 87.5005s
	iters: 200, epoch: 8 | loss: 0.1057524
	speed: 0.0985s/iter; left time: 74.0758s
	iters: 300, epoch: 8 | loss: 0.1546935
	speed: 0.0988s/iter; left time: 64.3875s
Epoch: 8 cost time: 31.75228238105774
Epoch: 8, Steps: 317 | Train Loss: 0.1139379 Vali Loss: 0.1361993 Test Loss: 0.1011856
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1077509
	speed: 0.1022s/iter; left time: 54.6901s
	iters: 200, epoch: 9 | loss: 0.1188429
	speed: 0.0981s/iter; left time: 42.6836s
	iters: 300, epoch: 9 | loss: 0.1229284
	speed: 0.0989s/iter; left time: 33.1253s
Epoch: 9 cost time: 31.68588638305664
Epoch: 9, Steps: 317 | Train Loss: 0.1136732 Vali Loss: 0.1356539 Test Loss: 0.1007797
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.0886011
	speed: 0.1012s/iter; left time: 22.0704s
	iters: 200, epoch: 10 | loss: 0.1009107
	speed: 0.0986s/iter; left time: 11.6348s
	iters: 300, epoch: 10 | loss: 0.1373132
	speed: 0.0990s/iter; left time: 1.7824s
Epoch: 10 cost time: 31.633291959762573
Epoch: 10, Steps: 317 | Train Loss: 0.1141485 Vali Loss: 0.1353874 Test Loss: 0.1007414
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm32_nh8_el3_dl1_df32_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1663664
	speed: 0.0512s/iter; left time: 157.6728s
	iters: 200, epoch: 1 | loss: 0.1316973
	speed: 0.0379s/iter; left time: 112.8961s
	iters: 300, epoch: 1 | loss: 0.1142298
	speed: 0.0361s/iter; left time: 103.9820s
Epoch: 1 cost time: 13.185146570205688
Epoch: 1, Steps: 318 | Train Loss: 0.2233786 Vali Loss: 0.1291519 Test Loss: 0.1774997
Validation loss decreased (inf --> 0.129152).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1433155
	speed: 0.0373s/iter; left time: 102.9874s
	iters: 200, epoch: 2 | loss: 0.1140141
	speed: 0.0337s/iter; left time: 89.7232s
	iters: 300, epoch: 2 | loss: 0.0904311
	speed: 0.0345s/iter; left time: 88.3243s
Epoch: 2 cost time: 11.198776721954346
Epoch: 2, Steps: 318 | Train Loss: 0.1050058 Vali Loss: 0.1108354 Test Loss: 0.1628937
Validation loss decreased (0.129152 --> 0.110835).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0797087
	speed: 0.0371s/iter; left time: 90.7843s
	iters: 200, epoch: 3 | loss: 0.0944247
	speed: 0.0341s/iter; left time: 79.8643s
	iters: 300, epoch: 3 | loss: 0.1083466
	speed: 0.0354s/iter; left time: 79.5084s
Epoch: 3 cost time: 11.320764064788818
Epoch: 3, Steps: 318 | Train Loss: 0.0919857 Vali Loss: 0.1071779 Test Loss: 0.1619541
Validation loss decreased (0.110835 --> 0.107178).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1224075
	speed: 0.0377s/iter; left time: 80.1860s
	iters: 200, epoch: 4 | loss: 0.0987324
	speed: 0.0342s/iter; left time: 69.3857s
	iters: 300, epoch: 4 | loss: 0.0870370
	speed: 0.0348s/iter; left time: 67.0267s
Epoch: 4 cost time: 11.331279754638672
Epoch: 4, Steps: 318 | Train Loss: 0.0882255 Vali Loss: 0.1050554 Test Loss: 0.1625868
Validation loss decreased (0.107178 --> 0.105055).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0890396
	speed: 0.0371s/iter; left time: 67.0895s
	iters: 200, epoch: 5 | loss: 0.0726567
	speed: 0.0341s/iter; left time: 58.3240s
	iters: 300, epoch: 5 | loss: 0.0971507
	speed: 0.0340s/iter; left time: 54.7408s
Epoch: 5 cost time: 11.17717695236206
Epoch: 5, Steps: 318 | Train Loss: 0.0856639 Vali Loss: 0.1045114 Test Loss: 0.1614989
Validation loss decreased (0.105055 --> 0.104511).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0899790
	speed: 0.0385s/iter; left time: 57.3629s
	iters: 200, epoch: 6 | loss: 0.0812011
	speed: 0.0342s/iter; left time: 47.6328s
	iters: 300, epoch: 6 | loss: 0.0801691
	speed: 0.0339s/iter; left time: 43.7395s
Epoch: 6 cost time: 11.319231510162354
Epoch: 6, Steps: 318 | Train Loss: 0.0845466 Vali Loss: 0.1052747 Test Loss: 0.1618022
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0900239
	speed: 0.0380s/iter; left time: 44.5204s
	iters: 200, epoch: 7 | loss: 0.0804043
	speed: 0.0341s/iter; left time: 36.6247s
	iters: 300, epoch: 7 | loss: 0.0736396
	speed: 0.0336s/iter; left time: 32.7025s
Epoch: 7 cost time: 11.226978778839111
Epoch: 7, Steps: 318 | Train Loss: 0.0850508 Vali Loss: 0.1065641 Test Loss: 0.1622602
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0803977
	speed: 0.0369s/iter; left time: 31.5801s
	iters: 200, epoch: 8 | loss: 0.0899908
	speed: 0.0337s/iter; left time: 25.4659s
	iters: 300, epoch: 8 | loss: 0.0754420
	speed: 0.0339s/iter; left time: 22.2041s
Epoch: 8 cost time: 11.102452039718628
Epoch: 8, Steps: 318 | Train Loss: 0.0847210 Vali Loss: 0.1064449 Test Loss: 0.1623260
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1803567
	speed: 0.0497s/iter; left time: 153.2414s
	iters: 200, epoch: 1 | loss: 0.3000470
	speed: 0.0390s/iter; left time: 116.1170s
	iters: 300, epoch: 1 | loss: 0.1061880
	speed: 0.0442s/iter; left time: 127.3599s
Epoch: 1 cost time: 14.101311445236206
Epoch: 1, Steps: 318 | Train Loss: 0.2259118 Vali Loss: 0.1555197 Test Loss: 0.1509265
Validation loss decreased (inf --> 0.155520).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1358336
	speed: 0.0481s/iter; left time: 132.9941s
	iters: 200, epoch: 2 | loss: 0.1415852
	speed: 0.0442s/iter; left time: 117.6541s
	iters: 300, epoch: 2 | loss: 0.1612865
	speed: 0.0440s/iter; left time: 112.6708s
Epoch: 2 cost time: 14.454873085021973
Epoch: 2, Steps: 318 | Train Loss: 0.1403952 Vali Loss: 0.1523134 Test Loss: 0.1304977
Validation loss decreased (0.155520 --> 0.152313).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1422236
	speed: 0.0478s/iter; left time: 116.8643s
	iters: 200, epoch: 3 | loss: 0.1175882
	speed: 0.0439s/iter; left time: 102.8526s
	iters: 300, epoch: 3 | loss: 0.1639429
	speed: 0.0441s/iter; left time: 98.9725s
Epoch: 3 cost time: 14.408565044403076
Epoch: 3, Steps: 318 | Train Loss: 0.1325786 Vali Loss: 0.1485939 Test Loss: 0.1271080
Validation loss decreased (0.152313 --> 0.148594).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1432306
	speed: 0.0476s/iter; left time: 101.3424s
	iters: 200, epoch: 4 | loss: 0.1134830
	speed: 0.0439s/iter; left time: 88.9807s
	iters: 300, epoch: 4 | loss: 0.1255459
	speed: 0.0446s/iter; left time: 85.8507s
Epoch: 4 cost time: 14.435077905654907
Epoch: 4, Steps: 318 | Train Loss: 0.1297779 Vali Loss: 0.1494473 Test Loss: 0.1265895
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1003625
	speed: 0.0479s/iter; left time: 86.5658s
	iters: 200, epoch: 5 | loss: 0.1368937
	speed: 0.0428s/iter; left time: 73.0849s
	iters: 300, epoch: 5 | loss: 0.1218161
	speed: 0.0430s/iter; left time: 69.1395s
Epoch: 5 cost time: 14.189002990722656
Epoch: 5, Steps: 318 | Train Loss: 0.1284933 Vali Loss: 0.1476819 Test Loss: 0.1258028
Validation loss decreased (0.148594 --> 0.147682).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1418751
	speed: 0.0469s/iter; left time: 69.8996s
	iters: 200, epoch: 6 | loss: 0.1447542
	speed: 0.0436s/iter; left time: 60.6061s
	iters: 300, epoch: 6 | loss: 0.1268523
	speed: 0.0438s/iter; left time: 56.5796s
Epoch: 6 cost time: 14.249200344085693
Epoch: 6, Steps: 318 | Train Loss: 0.1280907 Vali Loss: 0.1463220 Test Loss: 0.1256816
Validation loss decreased (0.147682 --> 0.146322).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1450174
	speed: 0.0473s/iter; left time: 55.4348s
	iters: 200, epoch: 7 | loss: 0.1125608
	speed: 0.0436s/iter; left time: 46.7387s
	iters: 300, epoch: 7 | loss: 0.1208708
	speed: 0.0443s/iter; left time: 43.1021s
Epoch: 7 cost time: 14.328819036483765
Epoch: 7, Steps: 318 | Train Loss: 0.1274822 Vali Loss: 0.1469320 Test Loss: 0.1253524
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1502742
	speed: 0.0472s/iter; left time: 40.3987s
	iters: 200, epoch: 8 | loss: 0.1250505
	speed: 0.0436s/iter; left time: 32.9317s
	iters: 300, epoch: 8 | loss: 0.0949976
	speed: 0.0440s/iter; left time: 28.8100s
Epoch: 8 cost time: 14.329360246658325
Epoch: 8, Steps: 318 | Train Loss: 0.1275789 Vali Loss: 0.1485654 Test Loss: 0.1254531
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1356456
	speed: 0.0486s/iter; left time: 26.0812s
	iters: 200, epoch: 9 | loss: 0.1789270
	speed: 0.0457s/iter; left time: 19.9834s
	iters: 300, epoch: 9 | loss: 0.1153185
	speed: 0.0438s/iter; left time: 14.7695s
Epoch: 9 cost time: 14.631160020828247
Epoch: 9, Steps: 318 | Train Loss: 0.1271372 Vali Loss: 0.1469015 Test Loss: 0.1254444
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000002
Early stopping
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1929936
	speed: 0.0577s/iter; left time: 177.6974s
	iters: 200, epoch: 1 | loss: 0.1637378
	speed: 0.0439s/iter; left time: 130.9408s
	iters: 300, epoch: 1 | loss: 0.1371779
	speed: 0.0441s/iter; left time: 126.9336s
Epoch: 1 cost time: 15.364579200744629
Epoch: 1, Steps: 318 | Train Loss: 0.2388666 Vali Loss: 0.1412115 Test Loss: 0.1455890
Validation loss decreased (inf --> 0.141211).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1037947
	speed: 0.0459s/iter; left time: 126.9544s
	iters: 200, epoch: 2 | loss: 0.1060505
	speed: 0.0434s/iter; left time: 115.6150s
	iters: 300, epoch: 2 | loss: 0.1164181
	speed: 0.0435s/iter; left time: 111.4193s
Epoch: 2 cost time: 14.105280876159668
Epoch: 2, Steps: 318 | Train Loss: 0.1302677 Vali Loss: 0.1387681 Test Loss: 0.1227410
Validation loss decreased (0.141211 --> 0.138768).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1317761
	speed: 0.0470s/iter; left time: 114.8800s
	iters: 200, epoch: 3 | loss: 0.1241842
	speed: 0.0433s/iter; left time: 101.5627s
	iters: 300, epoch: 3 | loss: 0.1109977
	speed: 0.0438s/iter; left time: 98.2731s
Epoch: 3 cost time: 14.234062671661377
Epoch: 3, Steps: 318 | Train Loss: 0.1240625 Vali Loss: 0.1374431 Test Loss: 0.1193220
Validation loss decreased (0.138768 --> 0.137443).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1160518
	speed: 0.0472s/iter; left time: 100.4338s
	iters: 200, epoch: 4 | loss: 0.1207028
	speed: 0.0432s/iter; left time: 87.5769s
	iters: 300, epoch: 4 | loss: 0.1146281
	speed: 0.0448s/iter; left time: 86.3203s
Epoch: 4 cost time: 14.344777584075928
Epoch: 4, Steps: 318 | Train Loss: 0.1218088 Vali Loss: 0.1346554 Test Loss: 0.1198896
Validation loss decreased (0.137443 --> 0.134655).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1730404
	speed: 0.0463s/iter; left time: 83.6743s
	iters: 200, epoch: 5 | loss: 0.1160180
	speed: 0.0438s/iter; left time: 74.8510s
	iters: 300, epoch: 5 | loss: 0.1159909
	speed: 0.0435s/iter; left time: 69.9609s
Epoch: 5 cost time: 14.178053617477417
Epoch: 5, Steps: 318 | Train Loss: 0.1203476 Vali Loss: 0.1352574 Test Loss: 0.1210387
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1072406
	speed: 0.0472s/iter; left time: 70.4213s
	iters: 200, epoch: 6 | loss: 0.1018601
	speed: 0.0430s/iter; left time: 59.8225s
	iters: 300, epoch: 6 | loss: 0.1087754
	speed: 0.0434s/iter; left time: 56.0159s
Epoch: 6 cost time: 14.17348051071167
Epoch: 6, Steps: 318 | Train Loss: 0.1198584 Vali Loss: 0.1346458 Test Loss: 0.1202027
Validation loss decreased (0.134655 --> 0.134646).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1164682
	speed: 0.0471s/iter; left time: 55.2403s
	iters: 200, epoch: 7 | loss: 0.1574429
	speed: 0.0434s/iter; left time: 46.6080s
	iters: 300, epoch: 7 | loss: 0.0978275
	speed: 0.0442s/iter; left time: 43.0190s
Epoch: 7 cost time: 14.306451797485352
Epoch: 7, Steps: 318 | Train Loss: 0.1196450 Vali Loss: 0.1325444 Test Loss: 0.1211047
Validation loss decreased (0.134646 --> 0.132544).  Saving model ...
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1282753
	speed: 0.0463s/iter; left time: 39.5732s
	iters: 200, epoch: 8 | loss: 0.1335109
	speed: 0.0430s/iter; left time: 32.4875s
	iters: 300, epoch: 8 | loss: 0.1229300
	speed: 0.0445s/iter; left time: 29.1182s
Epoch: 8 cost time: 14.197178602218628
Epoch: 8, Steps: 318 | Train Loss: 0.1193115 Vali Loss: 0.1337485 Test Loss: 0.1203908
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1199016
	speed: 0.0473s/iter; left time: 25.3858s
	iters: 200, epoch: 9 | loss: 0.1105150
	speed: 0.0430s/iter; left time: 18.8064s
	iters: 300, epoch: 9 | loss: 0.1165261
	speed: 0.0432s/iter; left time: 14.5695s
Epoch: 9 cost time: 14.181771993637085
Epoch: 9, Steps: 318 | Train Loss: 0.1191229 Vali Loss: 0.1338376 Test Loss: 0.1202871
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1211273
	speed: 0.0484s/iter; left time: 10.5965s
	iters: 200, epoch: 10 | loss: 0.1400542
	speed: 0.0430s/iter; left time: 5.1200s
	iters: 300, epoch: 10 | loss: 0.1068926
	speed: 0.0436s/iter; left time: 0.8280s
Epoch: 10 cost time: 14.317822456359863
Epoch: 10, Steps: 318 | Train Loss: 0.1190868 Vali Loss: 0.1328561 Test Loss: 0.1202886
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000001
Early stopping
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1803041
	speed: 0.0525s/iter; left time: 161.6454s
	iters: 200, epoch: 1 | loss: 0.2097496
	speed: 0.0408s/iter; left time: 121.4931s
	iters: 300, epoch: 1 | loss: 0.1414586
	speed: 0.0380s/iter; left time: 109.5877s
Epoch: 1 cost time: 13.792843341827393
Epoch: 1, Steps: 318 | Train Loss: 0.2254939 Vali Loss: 0.1546475 Test Loss: 0.1344408
Validation loss decreased (inf --> 0.154648).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1278467
	speed: 0.0424s/iter; left time: 117.2496s
	iters: 200, epoch: 2 | loss: 0.1568542
	speed: 0.0395s/iter; left time: 105.0629s
	iters: 300, epoch: 2 | loss: 0.1135247
	speed: 0.0410s/iter; left time: 105.0306s
Epoch: 2 cost time: 13.08350682258606
Epoch: 2, Steps: 318 | Train Loss: 0.1287393 Vali Loss: 0.1459715 Test Loss: 0.1324839
Validation loss decreased (0.154648 --> 0.145971).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1076325
	speed: 0.0449s/iter; left time: 109.8810s
	iters: 200, epoch: 3 | loss: 0.1549560
	speed: 0.0414s/iter; left time: 97.0121s
	iters: 300, epoch: 3 | loss: 0.1039764
	speed: 0.0432s/iter; left time: 97.0373s
Epoch: 3 cost time: 13.751181364059448
Epoch: 3, Steps: 318 | Train Loss: 0.1243406 Vali Loss: 0.1398929 Test Loss: 0.1270339
Validation loss decreased (0.145971 --> 0.139893).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1413414
	speed: 0.0448s/iter; left time: 95.2972s
	iters: 200, epoch: 4 | loss: 0.1553716
	speed: 0.0419s/iter; left time: 84.9873s
	iters: 300, epoch: 4 | loss: 0.1523204
	speed: 0.0422s/iter; left time: 81.4115s
Epoch: 4 cost time: 13.72628927230835
Epoch: 4, Steps: 318 | Train Loss: 0.1314782 Vali Loss: 0.1515147 Test Loss: 0.1206026
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1264377
	speed: 0.0463s/iter; left time: 83.8050s
	iters: 200, epoch: 5 | loss: 0.1161331
	speed: 0.0431s/iter; left time: 73.7342s
	iters: 300, epoch: 5 | loss: 0.1400636
	speed: 0.0428s/iter; left time: 68.9290s
Epoch: 5 cost time: 14.054906606674194
Epoch: 5, Steps: 318 | Train Loss: 0.1333169 Vali Loss: 0.1480698 Test Loss: 0.1212217
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0978441
	speed: 0.0464s/iter; left time: 69.1127s
	iters: 200, epoch: 6 | loss: 0.1584442
	speed: 0.0429s/iter; left time: 59.7216s
	iters: 300, epoch: 6 | loss: 0.1051828
	speed: 0.0428s/iter; left time: 55.1917s
Epoch: 6 cost time: 14.025311470031738
Epoch: 6, Steps: 318 | Train Loss: 0.1322047 Vali Loss: 0.1514162 Test Loss: 0.1216462
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test1', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=1, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2272411
	speed: 0.0580s/iter; left time: 178.2072s
	iters: 200, epoch: 1 | loss: 0.1191573
	speed: 0.0447s/iter; left time: 132.6921s
	iters: 300, epoch: 1 | loss: 0.1362477
	speed: 0.0444s/iter; left time: 127.3741s
Epoch: 1 cost time: 15.513733386993408
Epoch: 1, Steps: 317 | Train Loss: 0.2289465 Vali Loss: 0.1551609 Test Loss: 0.1188916
Validation loss decreased (inf --> 0.155161).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1111137
	speed: 0.0482s/iter; left time: 132.7142s
	iters: 200, epoch: 2 | loss: 0.1228404
	speed: 0.0457s/iter; left time: 121.1595s
	iters: 300, epoch: 2 | loss: 0.1527620
	speed: 0.0453s/iter; left time: 115.6265s
Epoch: 2 cost time: 14.706094980239868
Epoch: 2, Steps: 317 | Train Loss: 0.1347301 Vali Loss: 0.1549396 Test Loss: 0.1209064
Validation loss decreased (0.155161 --> 0.154940).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1242160
	speed: 0.0479s/iter; left time: 116.8460s
	iters: 200, epoch: 3 | loss: 0.1173382
	speed: 0.0439s/iter; left time: 102.5481s
	iters: 300, epoch: 3 | loss: 0.1276614
	speed: 0.0443s/iter; left time: 99.1909s
Epoch: 3 cost time: 14.41869044303894
Epoch: 3, Steps: 317 | Train Loss: 0.1295933 Vali Loss: 0.1464869 Test Loss: 0.1103503
Validation loss decreased (0.154940 --> 0.146487).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1112759
	speed: 0.0476s/iter; left time: 100.8450s
	iters: 200, epoch: 4 | loss: 0.1064550
	speed: 0.0459s/iter; left time: 92.7494s
	iters: 300, epoch: 4 | loss: 0.1225252
	speed: 0.0445s/iter; left time: 85.3998s
Epoch: 4 cost time: 14.590989351272583
Epoch: 4, Steps: 317 | Train Loss: 0.1273689 Vali Loss: 0.1472565 Test Loss: 0.1082750
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1270185
	speed: 0.0481s/iter; left time: 86.6574s
	iters: 200, epoch: 5 | loss: 0.1339089
	speed: 0.0447s/iter; left time: 76.0903s
	iters: 300, epoch: 5 | loss: 0.1314876
	speed: 0.0447s/iter; left time: 71.6978s
Epoch: 5 cost time: 14.547728061676025
Epoch: 5, Steps: 317 | Train Loss: 0.1261660 Vali Loss: 0.1462985 Test Loss: 0.1080060
Validation loss decreased (0.146487 --> 0.146298).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.1262761
	speed: 0.0483s/iter; left time: 71.7805s
	iters: 200, epoch: 6 | loss: 0.1379671
	speed: 0.0452s/iter; left time: 62.7148s
	iters: 300, epoch: 6 | loss: 0.1175767
	speed: 0.0455s/iter; left time: 58.5153s
Epoch: 6 cost time: 14.706331253051758
Epoch: 6, Steps: 317 | Train Loss: 0.1257462 Vali Loss: 0.1452483 Test Loss: 0.1089574
Validation loss decreased (0.146298 --> 0.145248).  Saving model ...
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0930925
	speed: 0.0486s/iter; left time: 56.7937s
	iters: 200, epoch: 7 | loss: 0.1281802
	speed: 0.0448s/iter; left time: 47.9324s
	iters: 300, epoch: 7 | loss: 0.1313969
	speed: 0.0447s/iter; left time: 43.3228s
Epoch: 7 cost time: 14.60911512374878
Epoch: 7, Steps: 317 | Train Loss: 0.1250331 Vali Loss: 0.1457575 Test Loss: 0.1085721
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.1340607
	speed: 0.0485s/iter; left time: 41.2829s
	iters: 200, epoch: 8 | loss: 0.1324744
	speed: 0.0446s/iter; left time: 33.5290s
	iters: 300, epoch: 8 | loss: 0.1287268
	speed: 0.0450s/iter; left time: 29.3583s
Epoch: 8 cost time: 14.600117444992065
Epoch: 8, Steps: 317 | Train Loss: 0.1251441 Vali Loss: 0.1454375 Test Loss: 0.1086247
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000004
	iters: 100, epoch: 9 | loss: 0.1471358
	speed: 0.0495s/iter; left time: 26.4834s
	iters: 200, epoch: 9 | loss: 0.1226573
	speed: 0.0449s/iter; left time: 19.5229s
	iters: 300, epoch: 9 | loss: 0.1023529
	speed: 0.0449s/iter; left time: 15.0579s
Epoch: 9 cost time: 14.729880809783936
Epoch: 9, Steps: 317 | Train Loss: 0.1249148 Vali Loss: 0.1451876 Test Loss: 0.1087139
Validation loss decreased (0.145248 --> 0.145188).  Saving model ...
Adjusting learning rate to: 0.0000002
	iters: 100, epoch: 10 | loss: 0.1223861
	speed: 0.0479s/iter; left time: 10.4473s
	iters: 200, epoch: 10 | loss: 0.1538869
	speed: 0.0447s/iter; left time: 5.2710s
	iters: 300, epoch: 10 | loss: 0.1451253
	speed: 0.0448s/iter; left time: 0.8060s
Epoch: 10 cost time: 14.530244827270508
Epoch: 10, Steps: 317 | Train Loss: 0.1250119 Vali Loss: 0.1451354 Test Loss: 0.1087906
Validation loss decreased (0.145188 --> 0.145135).  Saving model ...
Adjusting learning rate to: 0.0000001
>>>>>>>testing : omdata4_test1_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1461296
	speed: 0.0775s/iter; left time: 238.8352s
	iters: 200, epoch: 1 | loss: 0.1205023
	speed: 0.0628s/iter; left time: 187.0698s
	iters: 300, epoch: 1 | loss: 0.1073762
	speed: 0.0617s/iter; left time: 177.6998s
Epoch: 1 cost time: 21.338518381118774
Epoch: 1, Steps: 318 | Train Loss: 0.1654805 Vali Loss: 0.1212182 Test Loss: 0.0922655
Validation loss decreased (inf --> 0.121218).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1281067
	speed: 0.0651s/iter; left time: 179.9515s
	iters: 200, epoch: 2 | loss: 0.1063925
	speed: 0.0618s/iter; left time: 164.6552s
	iters: 300, epoch: 2 | loss: 0.0867790
	speed: 0.0619s/iter; left time: 158.5866s
Epoch: 2 cost time: 20.05131697654724
Epoch: 2, Steps: 318 | Train Loss: 0.0955186 Vali Loss: 0.1060657 Test Loss: 0.0796965
Validation loss decreased (0.121218 --> 0.106066).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0812622
	speed: 0.0660s/iter; left time: 161.3657s
	iters: 200, epoch: 3 | loss: 0.0910086
	speed: 0.0623s/iter; left time: 146.0420s
	iters: 300, epoch: 3 | loss: 0.1008946
	speed: 0.0624s/iter; left time: 140.1691s
Epoch: 3 cost time: 20.257141828536987
Epoch: 3, Steps: 318 | Train Loss: 0.0853033 Vali Loss: 0.1039218 Test Loss: 0.0776664
Validation loss decreased (0.106066 --> 0.103922).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1047570
	speed: 0.0652s/iter; left time: 138.7170s
	iters: 200, epoch: 4 | loss: 0.0852911
	speed: 0.0623s/iter; left time: 126.2913s
	iters: 300, epoch: 4 | loss: 0.0760555
	speed: 0.0627s/iter; left time: 120.7850s
Epoch: 4 cost time: 20.200756311416626
Epoch: 4, Steps: 318 | Train Loss: 0.0812251 Vali Loss: 0.1017423 Test Loss: 0.0763935
Validation loss decreased (0.103922 --> 0.101742).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0839127
	speed: 0.0646s/iter; left time: 116.9082s
	iters: 200, epoch: 5 | loss: 0.0727768
	speed: 0.0623s/iter; left time: 106.4578s
	iters: 300, epoch: 5 | loss: 0.0831524
	speed: 0.0621s/iter; left time: 99.8637s
Epoch: 5 cost time: 20.092212915420532
Epoch: 5, Steps: 318 | Train Loss: 0.0791333 Vali Loss: 0.1023816 Test Loss: 0.0768482
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0881657
	speed: 0.0659s/iter; left time: 98.2199s
	iters: 200, epoch: 6 | loss: 0.0763908
	speed: 0.0622s/iter; left time: 86.4792s
	iters: 300, epoch: 6 | loss: 0.0763948
	speed: 0.0623s/iter; left time: 80.4530s
Epoch: 6 cost time: 20.228843927383423
Epoch: 6, Steps: 318 | Train Loss: 0.0780373 Vali Loss: 0.1029840 Test Loss: 0.0769982
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0821031
	speed: 0.0662s/iter; left time: 77.6337s
	iters: 200, epoch: 7 | loss: 0.0722356
	speed: 0.0621s/iter; left time: 66.6611s
	iters: 300, epoch: 7 | loss: 0.0716848
	speed: 0.0628s/iter; left time: 61.1318s
Epoch: 7 cost time: 20.30400824546814
Epoch: 7, Steps: 318 | Train Loss: 0.0772309 Vali Loss: 0.1024677 Test Loss: 0.0768920
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1366973
	speed: 0.0803s/iter; left time: 247.4804s
	iters: 200, epoch: 1 | loss: 0.1842481
	speed: 0.0646s/iter; left time: 192.6737s
	iters: 300, epoch: 1 | loss: 0.0866157
	speed: 0.0649s/iter; left time: 186.9336s
Epoch: 1 cost time: 22.205963850021362
Epoch: 1, Steps: 318 | Train Loss: 0.1678290 Vali Loss: 0.1352221 Test Loss: 0.1029809
Validation loss decreased (inf --> 0.135222).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.0932279
	speed: 0.0695s/iter; left time: 191.9008s
	iters: 200, epoch: 2 | loss: 0.1043847
	speed: 0.0661s/iter; left time: 176.1455s
	iters: 300, epoch: 2 | loss: 0.1208587
	speed: 0.0659s/iter; left time: 168.9519s
Epoch: 2 cost time: 21.513442277908325
Epoch: 2, Steps: 318 | Train Loss: 0.1017105 Vali Loss: 0.1206272 Test Loss: 0.0870502
Validation loss decreased (0.135222 --> 0.120627).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1026591
	speed: 0.0694s/iter; left time: 169.5803s
	iters: 200, epoch: 3 | loss: 0.0863620
	speed: 0.0654s/iter; left time: 153.4707s
	iters: 300, epoch: 3 | loss: 0.1160440
	speed: 0.0655s/iter; left time: 147.0302s
Epoch: 3 cost time: 21.27530598640442
Epoch: 3, Steps: 318 | Train Loss: 0.0916549 Vali Loss: 0.1170381 Test Loss: 0.0864504
Validation loss decreased (0.120627 --> 0.117038).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1081689
	speed: 0.0691s/iter; left time: 146.9395s
	iters: 200, epoch: 4 | loss: 0.0674536
	speed: 0.0657s/iter; left time: 133.2239s
	iters: 300, epoch: 4 | loss: 0.0931813
	speed: 0.0655s/iter; left time: 126.2409s
Epoch: 4 cost time: 21.2697012424469
Epoch: 4, Steps: 318 | Train Loss: 0.0872901 Vali Loss: 0.1174388 Test Loss: 0.0854367
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0703879
	speed: 0.0693s/iter; left time: 125.2942s
	iters: 200, epoch: 5 | loss: 0.0724906
	speed: 0.0667s/iter; left time: 113.9806s
	iters: 300, epoch: 5 | loss: 0.0825832
	speed: 0.0668s/iter; left time: 107.4701s
Epoch: 5 cost time: 21.536029815673828
Epoch: 5, Steps: 318 | Train Loss: 0.0848760 Vali Loss: 0.1174116 Test Loss: 0.0860821
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0979171
	speed: 0.0697s/iter; left time: 103.9812s
	iters: 200, epoch: 6 | loss: 0.1053672
	speed: 0.0662s/iter; left time: 92.1120s
	iters: 300, epoch: 6 | loss: 0.0931438
	speed: 0.0661s/iter; left time: 85.2777s
Epoch: 6 cost time: 21.450772047042847
Epoch: 6, Steps: 318 | Train Loss: 0.0837031 Vali Loss: 0.1178020 Test Loss: 0.0862225
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1375339
	speed: 0.0765s/iter; left time: 235.7191s
	iters: 200, epoch: 1 | loss: 0.1342619
	speed: 0.0651s/iter; left time: 194.0233s
	iters: 300, epoch: 1 | loss: 0.1127434
	speed: 0.0675s/iter; left time: 194.5282s
Epoch: 1 cost time: 22.179785013198853
Epoch: 1, Steps: 318 | Train Loss: 0.1805662 Vali Loss: 0.1349213 Test Loss: 0.1047554
Validation loss decreased (inf --> 0.134921).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.0921351
	speed: 0.0718s/iter; left time: 198.3421s
	iters: 200, epoch: 2 | loss: 0.0867425
	speed: 0.0685s/iter; left time: 182.2987s
	iters: 300, epoch: 2 | loss: 0.0974265
	speed: 0.0681s/iter; left time: 174.5663s
Epoch: 2 cost time: 22.098428964614868
Epoch: 2, Steps: 318 | Train Loss: 0.1108622 Vali Loss: 0.1241559 Test Loss: 0.0942211
Validation loss decreased (0.134921 --> 0.124156).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1119720
	speed: 0.0703s/iter; left time: 171.8574s
	iters: 200, epoch: 3 | loss: 0.1018099
	speed: 0.0679s/iter; left time: 159.2634s
	iters: 300, epoch: 3 | loss: 0.0917537
	speed: 0.0674s/iter; left time: 151.3210s
Epoch: 3 cost time: 21.79571533203125
Epoch: 3, Steps: 318 | Train Loss: 0.1018136 Vali Loss: 0.1257843 Test Loss: 0.0937133
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0938955
	speed: 0.0699s/iter; left time: 148.6838s
	iters: 200, epoch: 4 | loss: 0.1034749
	speed: 0.0667s/iter; left time: 135.1024s
	iters: 300, epoch: 4 | loss: 0.0977687
	speed: 0.0664s/iter; left time: 128.0185s
Epoch: 4 cost time: 21.553600788116455
Epoch: 4, Steps: 318 | Train Loss: 0.0970826 Vali Loss: 0.1260841 Test Loss: 0.0941187
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1233040
	speed: 0.0692s/iter; left time: 125.1989s
	iters: 200, epoch: 5 | loss: 0.1063193
	speed: 0.0665s/iter; left time: 113.6809s
	iters: 300, epoch: 5 | loss: 0.0958037
	speed: 0.0660s/iter; left time: 106.2645s
Epoch: 5 cost time: 21.419474363327026
Epoch: 5, Steps: 318 | Train Loss: 0.0948182 Vali Loss: 0.1283400 Test Loss: 0.0944434
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000031
Early stopping
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1554291
	speed: 0.0817s/iter; left time: 251.6999s
	iters: 200, epoch: 1 | loss: 0.1723303
	speed: 0.0689s/iter; left time: 205.4525s
	iters: 300, epoch: 1 | loss: 0.1308612
	speed: 0.0732s/iter; left time: 211.0125s
Epoch: 1 cost time: 23.682305574417114
Epoch: 1, Steps: 318 | Train Loss: 0.1985075 Vali Loss: 0.1474712 Test Loss: 0.1134600
Validation loss decreased (inf --> 0.147471).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1363601
	speed: 0.0736s/iter; left time: 203.2262s
	iters: 200, epoch: 2 | loss: 0.1408961
	speed: 0.0728s/iter; left time: 193.8301s
	iters: 300, epoch: 2 | loss: 0.1090312
	speed: 0.0649s/iter; left time: 166.3304s
Epoch: 2 cost time: 22.302852392196655
Epoch: 2, Steps: 318 | Train Loss: 0.1243340 Vali Loss: 0.1411043 Test Loss: 0.1062142
Validation loss decreased (0.147471 --> 0.141104).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1041393
	speed: 0.0661s/iter; left time: 161.6181s
	iters: 200, epoch: 3 | loss: 0.1416965
	speed: 0.0627s/iter; left time: 147.0686s
	iters: 300, epoch: 3 | loss: 0.0986957
	speed: 0.0643s/iter; left time: 144.4079s
Epoch: 3 cost time: 20.555052757263184
Epoch: 3, Steps: 318 | Train Loss: 0.1149533 Vali Loss: 0.1396734 Test Loss: 0.1045196
Validation loss decreased (0.141104 --> 0.139673).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1038608
	speed: 0.0726s/iter; left time: 154.4053s
	iters: 200, epoch: 4 | loss: 0.1484206
	speed: 0.0719s/iter; left time: 145.6902s
	iters: 300, epoch: 4 | loss: 0.1142546
	speed: 0.0634s/iter; left time: 122.1459s
Epoch: 4 cost time: 21.99800944328308
Epoch: 4, Steps: 318 | Train Loss: 0.1102418 Vali Loss: 0.1365379 Test Loss: 0.1044818
Validation loss decreased (0.139673 --> 0.136538).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1159712
	speed: 0.0661s/iter; left time: 119.6410s
	iters: 200, epoch: 5 | loss: 0.0931808
	speed: 0.0632s/iter; left time: 107.9812s
	iters: 300, epoch: 5 | loss: 0.1160630
	speed: 0.0634s/iter; left time: 102.0066s
Epoch: 5 cost time: 20.49311327934265
Epoch: 5, Steps: 318 | Train Loss: 0.1078851 Vali Loss: 0.1374716 Test Loss: 0.1030401
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0784793
	speed: 0.0680s/iter; left time: 101.4269s
	iters: 200, epoch: 6 | loss: 0.1187652
	speed: 0.0634s/iter; left time: 88.1678s
	iters: 300, epoch: 6 | loss: 0.0958737
	speed: 0.0635s/iter; left time: 82.0096s
Epoch: 6 cost time: 20.715582609176636
Epoch: 6, Steps: 318 | Train Loss: 0.1068905 Vali Loss: 0.1382207 Test Loss: 0.1030360
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.1028363
	speed: 0.0681s/iter; left time: 79.9364s
	iters: 200, epoch: 7 | loss: 0.0820064
	speed: 0.0637s/iter; left time: 68.3080s
	iters: 300, epoch: 7 | loss: 0.1172450
	speed: 0.0660s/iter; left time: 64.2580s
Epoch: 7 cost time: 21.03681969642639
Epoch: 7, Steps: 318 | Train Loss: 0.1062139 Vali Loss: 0.1380219 Test Loss: 0.1037379
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test2', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=2, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1937924
	speed: 0.0901s/iter; left time: 276.7372s
	iters: 200, epoch: 1 | loss: 0.1183698
	speed: 0.0745s/iter; left time: 221.4456s
	iters: 300, epoch: 1 | loss: 0.1459944
	speed: 0.0768s/iter; left time: 220.3675s
Epoch: 1 cost time: 25.523836374282837
Epoch: 1, Steps: 317 | Train Loss: 0.1830199 Vali Loss: 0.1676145 Test Loss: 0.1380690
Validation loss decreased (inf --> 0.167614).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1019144
	speed: 0.0813s/iter; left time: 224.0345s
	iters: 200, epoch: 2 | loss: 0.1186692
	speed: 0.0786s/iter; left time: 208.5649s
	iters: 300, epoch: 2 | loss: 0.1516056
	speed: 0.0776s/iter; left time: 198.2750s
Epoch: 2 cost time: 25.136321544647217
Epoch: 2, Steps: 317 | Train Loss: 0.1209160 Vali Loss: 0.1495016 Test Loss: 0.1109077
Validation loss decreased (0.167614 --> 0.149502).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1055506
	speed: 0.0801s/iter; left time: 195.1146s
	iters: 200, epoch: 3 | loss: 0.0965757
	speed: 0.0757s/iter; left time: 177.0270s
	iters: 300, epoch: 3 | loss: 0.1177552
	speed: 0.0751s/iter; left time: 168.0612s
Epoch: 3 cost time: 24.41610884666443
Epoch: 3, Steps: 317 | Train Loss: 0.1123974 Vali Loss: 0.1500438 Test Loss: 0.1099912
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0975623
	speed: 0.0793s/iter; left time: 168.2122s
	iters: 200, epoch: 4 | loss: 0.0958075
	speed: 0.0747s/iter; left time: 150.9806s
	iters: 300, epoch: 4 | loss: 0.1092964
	speed: 0.0746s/iter; left time: 143.2602s
Epoch: 4 cost time: 24.171577215194702
Epoch: 4, Steps: 317 | Train Loss: 0.1087112 Vali Loss: 0.1395243 Test Loss: 0.1007650
Validation loss decreased (0.149502 --> 0.139524).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1071631
	speed: 0.0775s/iter; left time: 139.6860s
	iters: 200, epoch: 5 | loss: 0.1044988
	speed: 0.0743s/iter; left time: 126.4570s
	iters: 300, epoch: 5 | loss: 0.1038947
	speed: 0.0758s/iter; left time: 121.5135s
Epoch: 5 cost time: 24.077465772628784
Epoch: 5, Steps: 317 | Train Loss: 0.1061584 Vali Loss: 0.1397724 Test Loss: 0.1021545
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0963405
	speed: 0.0780s/iter; left time: 115.9737s
	iters: 200, epoch: 6 | loss: 0.1189260
	speed: 0.0739s/iter; left time: 102.4564s
	iters: 300, epoch: 6 | loss: 0.0913695
	speed: 0.0744s/iter; left time: 95.7228s
Epoch: 6 cost time: 23.934290170669556
Epoch: 6, Steps: 317 | Train Loss: 0.1047808 Vali Loss: 0.1424924 Test Loss: 0.1022910
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0816409
	speed: 0.0763s/iter; left time: 89.2330s
	iters: 200, epoch: 7 | loss: 0.1017790
	speed: 0.0733s/iter; left time: 78.3150s
	iters: 300, epoch: 7 | loss: 0.1022443
	speed: 0.0732s/iter; left time: 70.9668s
Epoch: 7 cost time: 23.581897735595703
Epoch: 7, Steps: 317 | Train Loss: 0.1044219 Vali Loss: 0.1415471 Test Loss: 0.1022318
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata4_test2_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1451301
	speed: 0.1085s/iter; left time: 334.1513s
	iters: 200, epoch: 1 | loss: 0.1203317
	speed: 0.0911s/iter; left time: 271.6512s
	iters: 300, epoch: 1 | loss: 0.1076415
	speed: 0.0919s/iter; left time: 264.8703s
Epoch: 1 cost time: 30.84915828704834
Epoch: 1, Steps: 318 | Train Loss: 0.1639839 Vali Loss: 0.1208866 Test Loss: 0.0923311
Validation loss decreased (inf --> 0.120887).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1263710
	speed: 0.0950s/iter; left time: 262.5072s
	iters: 200, epoch: 2 | loss: 0.1056946
	speed: 0.0913s/iter; left time: 243.2262s
	iters: 300, epoch: 2 | loss: 0.0861514
	speed: 0.0917s/iter; left time: 235.0518s
Epoch: 2 cost time: 29.516542196273804
Epoch: 2, Steps: 318 | Train Loss: 0.0948665 Vali Loss: 0.1058672 Test Loss: 0.0796045
Validation loss decreased (0.120887 --> 0.105867).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0807116
	speed: 0.0939s/iter; left time: 229.6220s
	iters: 200, epoch: 3 | loss: 0.0904066
	speed: 0.0916s/iter; left time: 214.8780s
	iters: 300, epoch: 3 | loss: 0.1000416
	speed: 0.0921s/iter; left time: 206.7715s
Epoch: 3 cost time: 29.50100064277649
Epoch: 3, Steps: 318 | Train Loss: 0.0847295 Vali Loss: 0.1035571 Test Loss: 0.0774923
Validation loss decreased (0.105867 --> 0.103557).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1033488
	speed: 0.0949s/iter; left time: 201.7617s
	iters: 200, epoch: 4 | loss: 0.0837360
	speed: 0.0925s/iter; left time: 187.5962s
	iters: 300, epoch: 4 | loss: 0.0748803
	speed: 0.0921s/iter; left time: 177.4223s
Epoch: 4 cost time: 29.66562294960022
Epoch: 4, Steps: 318 | Train Loss: 0.0806839 Vali Loss: 0.1015472 Test Loss: 0.0762844
Validation loss decreased (0.103557 --> 0.101547).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0835634
	speed: 0.0950s/iter; left time: 171.9073s
	iters: 200, epoch: 5 | loss: 0.0727398
	speed: 0.0921s/iter; left time: 157.3496s
	iters: 300, epoch: 5 | loss: 0.0825596
	speed: 0.0924s/iter; left time: 148.7290s
Epoch: 5 cost time: 29.68021559715271
Epoch: 5, Steps: 318 | Train Loss: 0.0786112 Vali Loss: 0.1021199 Test Loss: 0.0767412
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0878674
	speed: 0.0940s/iter; left time: 140.0939s
	iters: 200, epoch: 6 | loss: 0.0750489
	speed: 0.0919s/iter; left time: 127.8117s
	iters: 300, epoch: 6 | loss: 0.0761959
	speed: 0.0918s/iter; left time: 118.4813s
Epoch: 6 cost time: 29.482847690582275
Epoch: 6, Steps: 318 | Train Loss: 0.0775097 Vali Loss: 0.1028012 Test Loss: 0.0769442
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0810799
	speed: 0.0965s/iter; left time: 113.1395s
	iters: 200, epoch: 7 | loss: 0.0713340
	speed: 0.0923s/iter; left time: 98.9974s
	iters: 300, epoch: 7 | loss: 0.0713417
	speed: 0.0918s/iter; left time: 89.2817s
Epoch: 7 cost time: 29.77661657333374
Epoch: 7, Steps: 318 | Train Loss: 0.0767176 Vali Loss: 0.1022950 Test Loss: 0.0768536
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1358987
	speed: 0.1121s/iter; left time: 345.3650s
	iters: 200, epoch: 1 | loss: 0.1848430
	speed: 0.0979s/iter; left time: 291.8364s
	iters: 300, epoch: 1 | loss: 0.0869241
	speed: 0.0991s/iter; left time: 285.4191s
Epoch: 1 cost time: 32.73055624961853
Epoch: 1, Steps: 318 | Train Loss: 0.1681656 Vali Loss: 0.1352869 Test Loss: 0.1031114
Validation loss decreased (inf --> 0.135287).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.0937654
	speed: 0.1017s/iter; left time: 280.9106s
	iters: 200, epoch: 2 | loss: 0.1043234
	speed: 0.0982s/iter; left time: 261.3976s
	iters: 300, epoch: 2 | loss: 0.1206295
	speed: 0.0981s/iter; left time: 251.3281s
Epoch: 2 cost time: 31.599934339523315
Epoch: 2, Steps: 318 | Train Loss: 0.1017840 Vali Loss: 0.1209913 Test Loss: 0.0870116
Validation loss decreased (0.135287 --> 0.120991).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1032533
	speed: 0.1003s/iter; left time: 245.1245s
	iters: 200, epoch: 3 | loss: 0.0866610
	speed: 0.0979s/iter; left time: 229.4701s
	iters: 300, epoch: 3 | loss: 0.1157634
	speed: 0.0982s/iter; left time: 220.5504s
Epoch: 3 cost time: 31.472019910812378
Epoch: 3, Steps: 318 | Train Loss: 0.0916666 Vali Loss: 0.1171682 Test Loss: 0.0863890
Validation loss decreased (0.120991 --> 0.117168).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1076003
	speed: 0.1011s/iter; left time: 215.0468s
	iters: 200, epoch: 4 | loss: 0.0670676
	speed: 0.0980s/iter; left time: 198.7359s
	iters: 300, epoch: 4 | loss: 0.0927829
	speed: 0.0992s/iter; left time: 191.1502s
Epoch: 4 cost time: 31.672600507736206
Epoch: 4, Steps: 318 | Train Loss: 0.0872678 Vali Loss: 0.1175913 Test Loss: 0.0853631
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0705080
	speed: 0.1021s/iter; left time: 184.6232s
	iters: 200, epoch: 5 | loss: 0.0722969
	speed: 0.0993s/iter; left time: 169.7292s
	iters: 300, epoch: 5 | loss: 0.0825208
	speed: 0.0988s/iter; left time: 159.0198s
Epoch: 5 cost time: 31.886354446411133
Epoch: 5, Steps: 318 | Train Loss: 0.0848335 Vali Loss: 0.1176296 Test Loss: 0.0860531
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0975607
	speed: 0.1024s/iter; left time: 152.6530s
	iters: 200, epoch: 6 | loss: 0.1049502
	speed: 0.0988s/iter; left time: 137.4401s
	iters: 300, epoch: 6 | loss: 0.0932534
	speed: 0.0994s/iter; left time: 128.2708s
Epoch: 6 cost time: 31.9160373210907
Epoch: 6, Steps: 318 | Train Loss: 0.0836628 Vali Loss: 0.1180560 Test Loss: 0.0862249
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1355444
	speed: 0.1098s/iter; left time: 338.2472s
	iters: 200, epoch: 1 | loss: 0.1333042
	speed: 0.0990s/iter; left time: 295.2618s
	iters: 300, epoch: 1 | loss: 0.1427070
	speed: 0.0995s/iter; left time: 286.6017s
Epoch: 1 cost time: 32.64720821380615
Epoch: 1, Steps: 318 | Train Loss: 0.1825629 Vali Loss: 0.1331548 Test Loss: 0.1086231
Validation loss decreased (inf --> 0.133155).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.0855291
	speed: 0.1006s/iter; left time: 277.9650s
	iters: 200, epoch: 2 | loss: 0.0895027
	speed: 0.0977s/iter; left time: 260.3051s
	iters: 300, epoch: 2 | loss: 0.0971816
	speed: 0.0978s/iter; left time: 250.6540s
Epoch: 2 cost time: 31.426333904266357
Epoch: 2, Steps: 318 | Train Loss: 0.1098061 Vali Loss: 0.1254412 Test Loss: 0.0949735
Validation loss decreased (0.133155 --> 0.125441).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1067315
	speed: 0.0997s/iter; left time: 243.6702s
	iters: 200, epoch: 3 | loss: 0.1002331
	speed: 0.0967s/iter; left time: 226.8260s
	iters: 300, epoch: 3 | loss: 0.0920800
	speed: 0.0963s/iter; left time: 216.1288s
Epoch: 3 cost time: 31.075814723968506
Epoch: 3, Steps: 318 | Train Loss: 0.0997776 Vali Loss: 0.1270034 Test Loss: 0.0938267
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0936824
	speed: 0.1002s/iter; left time: 213.1423s
	iters: 200, epoch: 4 | loss: 0.0982416
	speed: 0.0968s/iter; left time: 196.2645s
	iters: 300, epoch: 4 | loss: 0.0994198
	speed: 0.0965s/iter; left time: 186.0098s
Epoch: 4 cost time: 31.155738830566406
Epoch: 4, Steps: 318 | Train Loss: 0.0948221 Vali Loss: 0.1257915 Test Loss: 0.0954949
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1239376
	speed: 0.0992s/iter; left time: 179.4695s
	iters: 200, epoch: 5 | loss: 0.1014545
	speed: 0.0964s/iter; left time: 164.8322s
	iters: 300, epoch: 5 | loss: 0.0943076
	speed: 0.0963s/iter; left time: 154.9931s
Epoch: 5 cost time: 30.998655319213867
Epoch: 5, Steps: 318 | Train Loss: 0.0923595 Vali Loss: 0.1291918 Test Loss: 0.0965871
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000031
Early stopping
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1524321
	speed: 0.1127s/iter; left time: 347.3601s
	iters: 200, epoch: 1 | loss: 0.1650615
	speed: 0.0959s/iter; left time: 285.9218s
	iters: 300, epoch: 1 | loss: 0.1570011
	speed: 0.0978s/iter; left time: 281.7984s
Epoch: 1 cost time: 32.473201751708984
Epoch: 1, Steps: 318 | Train Loss: 0.1843263 Vali Loss: 0.1454615 Test Loss: 0.1104186
Validation loss decreased (inf --> 0.145461).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1250159
	speed: 0.1053s/iter; left time: 290.8326s
	iters: 200, epoch: 2 | loss: 0.1282324
	speed: 0.1029s/iter; left time: 273.8968s
	iters: 300, epoch: 2 | loss: 0.1056089
	speed: 0.1045s/iter; left time: 267.8049s
Epoch: 2 cost time: 33.200661182403564
Epoch: 2, Steps: 318 | Train Loss: 0.1137819 Vali Loss: 0.1395733 Test Loss: 0.1057021
Validation loss decreased (0.145461 --> 0.139573).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0989808
	speed: 0.1070s/iter; left time: 261.6849s
	iters: 200, epoch: 3 | loss: 0.1215287
	speed: 0.1042s/iter; left time: 244.4131s
	iters: 300, epoch: 3 | loss: 0.0895891
	speed: 0.1045s/iter; left time: 234.6120s
Epoch: 3 cost time: 33.536865234375
Epoch: 3, Steps: 318 | Train Loss: 0.1043289 Vali Loss: 0.1399858 Test Loss: 0.0993999
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0978781
	speed: 0.1081s/iter; left time: 229.9117s
	iters: 200, epoch: 4 | loss: 0.1356773
	speed: 0.1042s/iter; left time: 211.3041s
	iters: 300, epoch: 4 | loss: 0.1071281
	speed: 0.1049s/iter; left time: 202.1277s
Epoch: 4 cost time: 33.6783881187439
Epoch: 4, Steps: 318 | Train Loss: 0.0994206 Vali Loss: 0.1380553 Test Loss: 0.1010053
Validation loss decreased (0.139573 --> 0.138055).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1131566
	speed: 0.1079s/iter; left time: 195.2590s
	iters: 200, epoch: 5 | loss: 0.0853970
	speed: 0.1057s/iter; left time: 180.6781s
	iters: 300, epoch: 5 | loss: 0.1027834
	speed: 0.1060s/iter; left time: 170.5520s
Epoch: 5 cost time: 33.94348096847534
Epoch: 5, Steps: 318 | Train Loss: 0.0969529 Vali Loss: 0.1380154 Test Loss: 0.0987955
Validation loss decreased (0.138055 --> 0.138015).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0687795
	speed: 0.1089s/iter; left time: 162.2983s
	iters: 200, epoch: 6 | loss: 0.1077302
	speed: 0.1064s/iter; left time: 148.0529s
	iters: 300, epoch: 6 | loss: 0.0905491
	speed: 0.1061s/iter; left time: 136.9706s
Epoch: 6 cost time: 34.11372900009155
Epoch: 6, Steps: 318 | Train Loss: 0.0956065 Vali Loss: 0.1396399 Test Loss: 0.0995919
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0890433
	speed: 0.1087s/iter; left time: 127.4824s
	iters: 200, epoch: 7 | loss: 0.0815363
	speed: 0.1059s/iter; left time: 113.6675s
	iters: 300, epoch: 7 | loss: 0.1054226
	speed: 0.1055s/iter; left time: 102.6408s
Epoch: 7 cost time: 33.99221444129944
Epoch: 7, Steps: 318 | Train Loss: 0.0947159 Vali Loss: 0.1408757 Test Loss: 0.1008831
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0928725
	speed: 0.1086s/iter; left time: 92.8423s
	iters: 200, epoch: 8 | loss: 0.0729182
	speed: 0.1060s/iter; left time: 80.0664s
	iters: 300, epoch: 8 | loss: 0.1154010
	speed: 0.1070s/iter; left time: 70.0549s
Epoch: 8 cost time: 34.16418933868408
Epoch: 8, Steps: 318 | Train Loss: 0.0943382 Vali Loss: 0.1393888 Test Loss: 0.0999741
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test3', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=3, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1976524
	speed: 0.1225s/iter; left time: 376.3029s
	iters: 200, epoch: 1 | loss: 0.1158060
	speed: 0.1151s/iter; left time: 341.9097s
	iters: 300, epoch: 1 | loss: 0.1460896
	speed: 0.1194s/iter; left time: 342.8051s
Epoch: 1 cost time: 37.82560086250305
Epoch: 1, Steps: 317 | Train Loss: 0.1837118 Vali Loss: 0.1673795 Test Loss: 0.1365430
Validation loss decreased (inf --> 0.167380).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1018460
	speed: 0.1261s/iter; left time: 347.1856s
	iters: 200, epoch: 2 | loss: 0.1193815
	speed: 0.1229s/iter; left time: 326.1080s
	iters: 300, epoch: 2 | loss: 0.1538422
	speed: 0.1236s/iter; left time: 315.7447s
Epoch: 2 cost time: 39.39521670341492
Epoch: 2, Steps: 317 | Train Loss: 0.1211071 Vali Loss: 0.1500724 Test Loss: 0.1108521
Validation loss decreased (0.167380 --> 0.150072).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1058286
	speed: 0.1209s/iter; left time: 294.5296s
	iters: 200, epoch: 3 | loss: 0.0972241
	speed: 0.1174s/iter; left time: 274.4705s
	iters: 300, epoch: 3 | loss: 0.1178071
	speed: 0.1185s/iter; left time: 265.0316s
Epoch: 3 cost time: 37.762779712677
Epoch: 3, Steps: 317 | Train Loss: 0.1125321 Vali Loss: 0.1500644 Test Loss: 0.1090643
Validation loss decreased (0.150072 --> 0.150064).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0973235
	speed: 0.1216s/iter; left time: 257.8834s
	iters: 200, epoch: 4 | loss: 0.0950657
	speed: 0.1207s/iter; left time: 243.8784s
	iters: 300, epoch: 4 | loss: 0.1090437
	speed: 0.1219s/iter; left time: 234.0011s
Epoch: 4 cost time: 38.58696889877319
Epoch: 4, Steps: 317 | Train Loss: 0.1088077 Vali Loss: 0.1393904 Test Loss: 0.1001733
Validation loss decreased (0.150064 --> 0.139390).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1065696
	speed: 0.1242s/iter; left time: 224.0014s
	iters: 200, epoch: 5 | loss: 0.1030719
	speed: 0.1223s/iter; left time: 208.2862s
	iters: 300, epoch: 5 | loss: 0.1032526
	speed: 0.1224s/iter; left time: 196.2482s
Epoch: 5 cost time: 39.05853891372681
Epoch: 5, Steps: 317 | Train Loss: 0.1061417 Vali Loss: 0.1399389 Test Loss: 0.1013863
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0970452
	speed: 0.1253s/iter; left time: 186.2526s
	iters: 200, epoch: 6 | loss: 0.1195564
	speed: 0.1219s/iter; left time: 168.9918s
	iters: 300, epoch: 6 | loss: 0.0932264
	speed: 0.1229s/iter; left time: 158.0083s
Epoch: 6 cost time: 39.16135358810425
Epoch: 6, Steps: 317 | Train Loss: 0.1046976 Vali Loss: 0.1418847 Test Loss: 0.1014073
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0808932
	speed: 0.1258s/iter; left time: 147.0402s
	iters: 200, epoch: 7 | loss: 0.1006961
	speed: 0.1218s/iter; left time: 130.2101s
	iters: 300, epoch: 7 | loss: 0.1024318
	speed: 0.1222s/iter; left time: 118.4454s
Epoch: 7 cost time: 39.132009744644165
Epoch: 7, Steps: 317 | Train Loss: 0.1042911 Vali Loss: 0.1410946 Test Loss: 0.1013092
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata4_test3_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1449852
	speed: 0.1361s/iter; left time: 419.4037s
	iters: 200, epoch: 1 | loss: 0.1206426
	speed: 0.1213s/iter; left time: 361.6572s
	iters: 300, epoch: 1 | loss: 0.1079191
	speed: 0.1211s/iter; left time: 348.9723s
Epoch: 1 cost time: 40.09410762786865
Epoch: 1, Steps: 318 | Train Loss: 0.1647645 Vali Loss: 0.1207508 Test Loss: 0.0922086
Validation loss decreased (inf --> 0.120751).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1267333
	speed: 0.1235s/iter; left time: 341.2201s
	iters: 200, epoch: 2 | loss: 0.1048884
	speed: 0.1205s/iter; left time: 320.8793s
	iters: 300, epoch: 2 | loss: 0.0861383
	speed: 0.1209s/iter; left time: 309.8794s
Epoch: 2 cost time: 38.75151705741882
Epoch: 2, Steps: 318 | Train Loss: 0.0950507 Vali Loss: 0.1056005 Test Loss: 0.0794964
Validation loss decreased (0.120751 --> 0.105600).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0804626
	speed: 0.1256s/iter; left time: 307.0350s
	iters: 200, epoch: 3 | loss: 0.0903028
	speed: 0.1212s/iter; left time: 284.1306s
	iters: 300, epoch: 3 | loss: 0.0998971
	speed: 0.1215s/iter; left time: 272.6955s
Epoch: 3 cost time: 39.09205412864685
Epoch: 3, Steps: 318 | Train Loss: 0.0848299 Vali Loss: 0.1033271 Test Loss: 0.0774548
Validation loss decreased (0.105600 --> 0.103327).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1040245
	speed: 0.1234s/iter; left time: 262.4317s
	iters: 200, epoch: 4 | loss: 0.0842192
	speed: 0.1209s/iter; left time: 245.0808s
	iters: 300, epoch: 4 | loss: 0.0751514
	speed: 0.1212s/iter; left time: 233.5899s
Epoch: 4 cost time: 38.83355927467346
Epoch: 4, Steps: 318 | Train Loss: 0.0807951 Vali Loss: 0.1012804 Test Loss: 0.0761781
Validation loss decreased (0.103327 --> 0.101280).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0833891
	speed: 0.1243s/iter; left time: 224.9257s
	iters: 200, epoch: 5 | loss: 0.0728188
	speed: 0.1208s/iter; left time: 206.5213s
	iters: 300, epoch: 5 | loss: 0.0829149
	speed: 0.1213s/iter; left time: 195.1496s
Epoch: 5 cost time: 38.92011022567749
Epoch: 5, Steps: 318 | Train Loss: 0.0787411 Vali Loss: 0.1018869 Test Loss: 0.0766884
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0877765
	speed: 0.1247s/iter; left time: 185.9869s
	iters: 200, epoch: 6 | loss: 0.0753608
	speed: 0.1215s/iter; left time: 169.0247s
	iters: 300, epoch: 6 | loss: 0.0761064
	speed: 0.1219s/iter; left time: 157.3421s
Epoch: 6 cost time: 39.12106275558472
Epoch: 6, Steps: 318 | Train Loss: 0.0776285 Vali Loss: 0.1025519 Test Loss: 0.0768912
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0814857
	speed: 0.1248s/iter; left time: 146.3581s
	iters: 200, epoch: 7 | loss: 0.0716202
	speed: 0.1214s/iter; left time: 130.2906s
	iters: 300, epoch: 7 | loss: 0.0716298
	speed: 0.1219s/iter; left time: 118.6321s
Epoch: 7 cost time: 39.10116457939148
Epoch: 7, Steps: 318 | Train Loss: 0.0768280 Vali Loss: 0.1020515 Test Loss: 0.0767834
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1356064
	speed: 0.1430s/iter; left time: 440.4981s
	iters: 200, epoch: 1 | loss: 0.1856797
	speed: 0.1290s/iter; left time: 384.6583s
	iters: 300, epoch: 1 | loss: 0.0866266
	speed: 0.1327s/iter; left time: 382.4404s
Epoch: 1 cost time: 42.916022300720215
Epoch: 1, Steps: 318 | Train Loss: 0.1687518 Vali Loss: 0.1354590 Test Loss: 0.1035075
Validation loss decreased (inf --> 0.135459).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.0938595
	speed: 0.1331s/iter; left time: 367.7585s
	iters: 200, epoch: 2 | loss: 0.1040348
	speed: 0.1298s/iter; left time: 345.6029s
	iters: 300, epoch: 2 | loss: 0.1202244
	speed: 0.1300s/iter; left time: 333.1082s
Epoch: 2 cost time: 41.705796003341675
Epoch: 2, Steps: 318 | Train Loss: 0.1017469 Vali Loss: 0.1211341 Test Loss: 0.0870215
Validation loss decreased (0.135459 --> 0.121134).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1032561
	speed: 0.1321s/iter; left time: 323.0250s
	iters: 200, epoch: 3 | loss: 0.0871423
	speed: 0.1296s/iter; left time: 303.9064s
	iters: 300, epoch: 3 | loss: 0.1156775
	speed: 0.1326s/iter; left time: 297.7968s
Epoch: 3 cost time: 41.90973925590515
Epoch: 3, Steps: 318 | Train Loss: 0.0917808 Vali Loss: 0.1173562 Test Loss: 0.0863064
Validation loss decreased (0.121134 --> 0.117356).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1081265
	speed: 0.1343s/iter; left time: 285.5734s
	iters: 200, epoch: 4 | loss: 0.0674079
	speed: 0.1320s/iter; left time: 267.5403s
	iters: 300, epoch: 4 | loss: 0.0929209
	speed: 0.1329s/iter; left time: 256.0598s
Epoch: 4 cost time: 42.369906425476074
Epoch: 4, Steps: 318 | Train Loss: 0.0873248 Vali Loss: 0.1178008 Test Loss: 0.0853627
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0706418
	speed: 0.1339s/iter; left time: 242.3148s
	iters: 200, epoch: 5 | loss: 0.0721244
	speed: 0.1313s/iter; left time: 224.4448s
	iters: 300, epoch: 5 | loss: 0.0823787
	speed: 0.1316s/iter; left time: 211.7266s
Epoch: 5 cost time: 42.14416718482971
Epoch: 5, Steps: 318 | Train Loss: 0.0849183 Vali Loss: 0.1178196 Test Loss: 0.0860425
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0976026
	speed: 0.1338s/iter; left time: 199.4930s
	iters: 200, epoch: 6 | loss: 0.1051373
	speed: 0.1316s/iter; left time: 183.0201s
	iters: 300, epoch: 6 | loss: 0.0933074
	speed: 0.1314s/iter; left time: 169.6282s
Epoch: 6 cost time: 42.14532446861267
Epoch: 6, Steps: 318 | Train Loss: 0.0837595 Vali Loss: 0.1182501 Test Loss: 0.0862127
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1429534
	speed: 0.1440s/iter; left time: 443.6031s
	iters: 200, epoch: 1 | loss: 0.1370479
	speed: 0.1320s/iter; left time: 393.4230s
	iters: 300, epoch: 1 | loss: 0.1255307
	speed: 0.1366s/iter; left time: 393.5350s
Epoch: 1 cost time: 43.81801962852478
Epoch: 1, Steps: 318 | Train Loss: 0.1830913 Vali Loss: 0.1296828 Test Loss: 0.1031948
Validation loss decreased (inf --> 0.129683).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.0872975
	speed: 0.1396s/iter; left time: 385.8453s
	iters: 200, epoch: 2 | loss: 0.0882993
	speed: 0.1395s/iter; left time: 371.5077s
	iters: 300, epoch: 2 | loss: 0.0984635
	speed: 0.1379s/iter; left time: 353.4179s
Epoch: 2 cost time: 44.223867654800415
Epoch: 2, Steps: 318 | Train Loss: 0.1107260 Vali Loss: 0.1225576 Test Loss: 0.0959161
Validation loss decreased (0.129683 --> 0.122558).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1075429
	speed: 0.1389s/iter; left time: 339.6880s
	iters: 200, epoch: 3 | loss: 0.0991074
	speed: 0.1366s/iter; left time: 320.4230s
	iters: 300, epoch: 3 | loss: 0.0947809
	speed: 0.1353s/iter; left time: 303.8034s
Epoch: 3 cost time: 43.66856145858765
Epoch: 3, Steps: 318 | Train Loss: 0.1007610 Vali Loss: 0.1230674 Test Loss: 0.0943860
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0963321
	speed: 0.1383s/iter; left time: 294.1835s
	iters: 200, epoch: 4 | loss: 0.0983202
	speed: 0.1352s/iter; left time: 274.0383s
	iters: 300, epoch: 4 | loss: 0.0955343
	speed: 0.1344s/iter; left time: 259.0248s
Epoch: 4 cost time: 43.317302942276
Epoch: 4, Steps: 318 | Train Loss: 0.0957953 Vali Loss: 0.1230487 Test Loss: 0.0952516
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1264816
	speed: 0.1371s/iter; left time: 248.0367s
	iters: 200, epoch: 5 | loss: 0.1026149
	speed: 0.1351s/iter; left time: 230.9177s
	iters: 300, epoch: 5 | loss: 0.0950247
	speed: 0.1337s/iter; left time: 215.1810s
Epoch: 5 cost time: 43.04583978652954
Epoch: 5, Steps: 318 | Train Loss: 0.0932614 Vali Loss: 0.1255973 Test Loss: 0.0965002
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000031
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1495330
	speed: 0.1505s/iter; left time: 463.5670s
	iters: 200, epoch: 1 | loss: 0.1575547
	speed: 0.1398s/iter; left time: 416.6402s
	iters: 300, epoch: 1 | loss: 0.1515631
	speed: 0.1411s/iter; left time: 406.6128s
Epoch: 1 cost time: 45.77406120300293
Epoch: 1, Steps: 318 | Train Loss: 0.1816506 Vali Loss: 0.1492074 Test Loss: 0.1137334
Validation loss decreased (inf --> 0.149207).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1305659
	speed: 0.1449s/iter; left time: 400.2661s
	iters: 200, epoch: 2 | loss: 0.1408800
	speed: 0.1431s/iter; left time: 381.0124s
	iters: 300, epoch: 2 | loss: 0.1050129
	speed: 0.1430s/iter; left time: 366.4301s
Epoch: 2 cost time: 45.67232394218445
Epoch: 2, Steps: 318 | Train Loss: 0.1141765 Vali Loss: 0.1385305 Test Loss: 0.1042123
Validation loss decreased (0.149207 --> 0.138530).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1017764
	speed: 0.1393s/iter; left time: 340.7012s
	iters: 200, epoch: 3 | loss: 0.1170837
	speed: 0.1363s/iter; left time: 319.5740s
	iters: 300, epoch: 3 | loss: 0.0877373
	speed: 0.1388s/iter; left time: 311.6285s
Epoch: 3 cost time: 44.00663733482361
Epoch: 3, Steps: 318 | Train Loss: 0.1040758 Vali Loss: 0.1411169 Test Loss: 0.0996582
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0879241
	speed: 0.1422s/iter; left time: 302.4578s
	iters: 200, epoch: 4 | loss: 0.1317445
	speed: 0.1387s/iter; left time: 281.1485s
	iters: 300, epoch: 4 | loss: 0.1088201
	speed: 0.1383s/iter; left time: 266.5842s
Epoch: 4 cost time: 44.4993052482605
Epoch: 4, Steps: 318 | Train Loss: 0.0985681 Vali Loss: 0.1401219 Test Loss: 0.1027079
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1102946
	speed: 0.1391s/iter; left time: 251.5707s
	iters: 200, epoch: 5 | loss: 0.0819273
	speed: 0.1374s/iter; left time: 234.7456s
	iters: 300, epoch: 5 | loss: 0.1026534
	speed: 0.1378s/iter; left time: 221.6666s
Epoch: 5 cost time: 44.00958585739136
Epoch: 5, Steps: 318 | Train Loss: 0.0956252 Vali Loss: 0.1391373 Test Loss: 0.0992834
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000031
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test4', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=4, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2088516
	speed: 0.1614s/iter; left time: 495.6819s
	iters: 200, epoch: 1 | loss: 0.1154683
	speed: 0.1509s/iter; left time: 448.2537s
	iters: 300, epoch: 1 | loss: 0.1439930
	speed: 0.1528s/iter; left time: 438.8205s
Epoch: 1 cost time: 49.17007493972778
Epoch: 1, Steps: 317 | Train Loss: 0.1854603 Vali Loss: 0.1725951 Test Loss: 0.1432879
Validation loss decreased (inf --> 0.172595).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1024357
	speed: 0.1546s/iter; left time: 425.8678s
	iters: 200, epoch: 2 | loss: 0.1193523
	speed: 0.1525s/iter; left time: 404.7413s
	iters: 300, epoch: 2 | loss: 0.1524314
	speed: 0.1539s/iter; left time: 393.0643s
Epoch: 2 cost time: 48.773279905319214
Epoch: 2, Steps: 317 | Train Loss: 0.1200920 Vali Loss: 0.1555668 Test Loss: 0.1145560
Validation loss decreased (0.172595 --> 0.155567).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1086438
	speed: 0.1561s/iter; left time: 380.5179s
	iters: 200, epoch: 3 | loss: 0.0943913
	speed: 0.1541s/iter; left time: 360.0527s
	iters: 300, epoch: 3 | loss: 0.1129982
	speed: 0.1546s/iter; left time: 345.8000s
Epoch: 3 cost time: 49.192254066467285
Epoch: 3, Steps: 317 | Train Loss: 0.1099151 Vali Loss: 0.1549592 Test Loss: 0.1147077
Validation loss decreased (0.155567 --> 0.154959).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0965831
	speed: 0.1541s/iter; left time: 326.6234s
	iters: 200, epoch: 4 | loss: 0.0913554
	speed: 0.1549s/iter; left time: 312.9836s
	iters: 300, epoch: 4 | loss: 0.1097902
	speed: 0.1545s/iter; left time: 296.5651s
Epoch: 4 cost time: 49.07805037498474
Epoch: 4, Steps: 317 | Train Loss: 0.1049717 Vali Loss: 0.1458446 Test Loss: 0.1054072
Validation loss decreased (0.154959 --> 0.145845).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1051166
	speed: 0.1554s/iter; left time: 280.1303s
	iters: 200, epoch: 5 | loss: 0.0972461
	speed: 0.1536s/iter; left time: 261.5877s
	iters: 300, epoch: 5 | loss: 0.1042081
	speed: 0.1544s/iter; left time: 247.5144s
Epoch: 5 cost time: 49.058943033218384
Epoch: 5, Steps: 317 | Train Loss: 0.1014134 Vali Loss: 0.1464794 Test Loss: 0.1069504
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0899743
	speed: 0.1569s/iter; left time: 233.1488s
	iters: 200, epoch: 6 | loss: 0.1165733
	speed: 0.1541s/iter; left time: 213.5362s
	iters: 300, epoch: 6 | loss: 0.0913428
	speed: 0.1558s/iter; left time: 200.3480s
Epoch: 6 cost time: 49.40742802619934
Epoch: 6, Steps: 317 | Train Loss: 0.0996237 Vali Loss: 0.1480860 Test Loss: 0.1069293
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0773928
	speed: 0.1579s/iter; left time: 184.6313s
	iters: 200, epoch: 7 | loss: 0.1001269
	speed: 0.1531s/iter; left time: 163.6559s
	iters: 300, epoch: 7 | loss: 0.0957554
	speed: 0.1542s/iter; left time: 149.4359s
Epoch: 7 cost time: 49.248478412628174
Epoch: 7, Steps: 317 | Train Loss: 0.0989761 Vali Loss: 0.1490453 Test Loss: 0.1080436
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata4_test4_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=10, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10193
val 1502
test 3010
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1448392
	speed: 0.1656s/iter; left time: 510.3367s
	iters: 200, epoch: 1 | loss: 0.1206041
	speed: 0.1502s/iter; left time: 447.6862s
	iters: 300, epoch: 1 | loss: 0.1076033
	speed: 0.1498s/iter; left time: 431.6372s
Epoch: 1 cost time: 49.357922315597534
Epoch: 1, Steps: 318 | Train Loss: 0.1648444 Vali Loss: 0.1207490 Test Loss: 0.0921404
Validation loss decreased (inf --> 0.120749).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1266818
	speed: 0.1539s/iter; left time: 425.2022s
	iters: 200, epoch: 2 | loss: 0.1050349
	speed: 0.1500s/iter; left time: 399.5706s
	iters: 300, epoch: 2 | loss: 0.0862614
	speed: 0.1502s/iter; left time: 385.0516s
Epoch: 2 cost time: 48.239872217178345
Epoch: 2, Steps: 318 | Train Loss: 0.0950254 Vali Loss: 0.1056294 Test Loss: 0.0794828
Validation loss decreased (0.120749 --> 0.105629).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.0804039
	speed: 0.1529s/iter; left time: 373.8195s
	iters: 200, epoch: 3 | loss: 0.0903879
	speed: 0.1509s/iter; left time: 353.8773s
	iters: 300, epoch: 3 | loss: 0.0999868
	speed: 0.1506s/iter; left time: 338.0337s
Epoch: 3 cost time: 48.270875453948975
Epoch: 3, Steps: 318 | Train Loss: 0.0848132 Vali Loss: 0.1033642 Test Loss: 0.0774495
Validation loss decreased (0.105629 --> 0.103364).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1040265
	speed: 0.1529s/iter; left time: 325.2720s
	iters: 200, epoch: 4 | loss: 0.0842373
	speed: 0.1513s/iter; left time: 306.6454s
	iters: 300, epoch: 4 | loss: 0.0750176
	speed: 0.1515s/iter; left time: 291.9470s
Epoch: 4 cost time: 48.409337759017944
Epoch: 4, Steps: 318 | Train Loss: 0.0807766 Vali Loss: 0.1013196 Test Loss: 0.0761826
Validation loss decreased (0.103364 --> 0.101320).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0832787
	speed: 0.1538s/iter; left time: 278.2910s
	iters: 200, epoch: 5 | loss: 0.0727086
	speed: 0.1512s/iter; left time: 258.4620s
	iters: 300, epoch: 5 | loss: 0.0828290
	speed: 0.1530s/iter; left time: 246.2059s
Epoch: 5 cost time: 48.64867091178894
Epoch: 5, Steps: 318 | Train Loss: 0.0787233 Vali Loss: 0.1019195 Test Loss: 0.0766946
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0876953
	speed: 0.1540s/iter; left time: 229.6180s
	iters: 200, epoch: 6 | loss: 0.0754049
	speed: 0.1511s/iter; left time: 210.1555s
	iters: 300, epoch: 6 | loss: 0.0759960
	speed: 0.1507s/iter; left time: 194.5532s
Epoch: 6 cost time: 48.4067656993866
Epoch: 6, Steps: 318 | Train Loss: 0.0776133 Vali Loss: 0.1025847 Test Loss: 0.0768982
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0812680
	speed: 0.1544s/iter; left time: 181.0850s
	iters: 200, epoch: 7 | loss: 0.0715530
	speed: 0.1519s/iter; left time: 162.9509s
	iters: 300, epoch: 7 | loss: 0.0715569
	speed: 0.1511s/iter; left time: 147.0654s
Epoch: 7 cost time: 48.572548151016235
Epoch: 7, Steps: 318 | Train Loss: 0.0768089 Vali Loss: 0.1020739 Test Loss: 0.0767846
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl10_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3010
loading supervised model weight
test shape: (3010, 1, 10, 1) (3010, 1, 10, 1)
test shape: (3010, 10, 1) (3010, 10, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=15, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10188
val 1497
test 3005
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1354143
	speed: 0.1707s/iter; left time: 525.9910s
	iters: 200, epoch: 1 | loss: 0.1870418
	speed: 0.1593s/iter; left time: 475.0016s
	iters: 300, epoch: 1 | loss: 0.0865818
	speed: 0.1625s/iter; left time: 468.1026s
Epoch: 1 cost time: 52.25145697593689
Epoch: 1, Steps: 318 | Train Loss: 0.1693881 Vali Loss: 0.1359959 Test Loss: 0.1040565
Validation loss decreased (inf --> 0.135996).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.0939027
	speed: 0.1641s/iter; left time: 453.4523s
	iters: 200, epoch: 2 | loss: 0.1038326
	speed: 0.1621s/iter; left time: 431.7694s
	iters: 300, epoch: 2 | loss: 0.1200984
	speed: 0.1628s/iter; left time: 417.2733s
Epoch: 2 cost time: 51.92700743675232
Epoch: 2, Steps: 318 | Train Loss: 0.1016909 Vali Loss: 0.1213073 Test Loss: 0.0870222
Validation loss decreased (0.135996 --> 0.121307).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1029165
	speed: 0.1647s/iter; left time: 402.6352s
	iters: 200, epoch: 3 | loss: 0.0867667
	speed: 0.1623s/iter; left time: 380.6952s
	iters: 300, epoch: 3 | loss: 0.1156650
	speed: 0.1635s/iter; left time: 366.9751s
Epoch: 3 cost time: 52.116291999816895
Epoch: 3, Steps: 318 | Train Loss: 0.0915573 Vali Loss: 0.1173734 Test Loss: 0.0863779
Validation loss decreased (0.121307 --> 0.117373).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.1076487
	speed: 0.1665s/iter; left time: 354.1092s
	iters: 200, epoch: 4 | loss: 0.0672731
	speed: 0.1644s/iter; left time: 333.2386s
	iters: 300, epoch: 4 | loss: 0.0924685
	speed: 0.1636s/iter; left time: 315.3389s
Epoch: 4 cost time: 52.51154065132141
Epoch: 4, Steps: 318 | Train Loss: 0.0871668 Vali Loss: 0.1179087 Test Loss: 0.0855134
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.0709942
	speed: 0.1672s/iter; left time: 302.4336s
	iters: 200, epoch: 5 | loss: 0.0717423
	speed: 0.1646s/iter; left time: 281.2695s
	iters: 300, epoch: 5 | loss: 0.0822313
	speed: 0.1645s/iter; left time: 264.6113s
Epoch: 5 cost time: 52.69466710090637
Epoch: 5, Steps: 318 | Train Loss: 0.0847355 Vali Loss: 0.1180796 Test Loss: 0.0862250
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0969831
	speed: 0.1678s/iter; left time: 250.1281s
	iters: 200, epoch: 6 | loss: 0.1047687
	speed: 0.1643s/iter; left time: 228.5912s
	iters: 300, epoch: 6 | loss: 0.0932462
	speed: 0.1651s/iter; left time: 213.1906s
Epoch: 6 cost time: 52.787391662597656
Epoch: 6, Steps: 318 | Train Loss: 0.0835626 Vali Loss: 0.1184893 Test Loss: 0.0863851
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl15_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3005
loading supervised model weight
test shape: (3005, 1, 15, 1) (3005, 1, 15, 1)
test shape: (3005, 15, 1) (3005, 15, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=20, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10183
val 1492
test 3000
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1365408
	speed: 0.1730s/iter; left time: 533.0891s
	iters: 200, epoch: 1 | loss: 0.1363963
	speed: 0.1690s/iter; left time: 503.6911s
	iters: 300, epoch: 1 | loss: 0.1227038
	speed: 0.1770s/iter; left time: 509.9785s
Epoch: 1 cost time: 55.22713255882263
Epoch: 1, Steps: 318 | Train Loss: 0.1852298 Vali Loss: 0.1311603 Test Loss: 0.1035581
Validation loss decreased (inf --> 0.131160).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.0876855
	speed: 0.1814s/iter; left time: 501.0796s
	iters: 200, epoch: 2 | loss: 0.0886963
	speed: 0.1775s/iter; left time: 472.6449s
	iters: 300, epoch: 2 | loss: 0.0977734
	speed: 0.1783s/iter; left time: 456.9627s
Epoch: 2 cost time: 57.02836608886719
Epoch: 2, Steps: 318 | Train Loss: 0.1109671 Vali Loss: 0.1236880 Test Loss: 0.0953275
Validation loss decreased (0.131160 --> 0.123688).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1095454
	speed: 0.1794s/iter; left time: 438.5623s
	iters: 200, epoch: 3 | loss: 0.1006062
	speed: 0.1773s/iter; left time: 415.8083s
	iters: 300, epoch: 3 | loss: 0.0954390
	speed: 0.1781s/iter; left time: 399.7384s
Epoch: 3 cost time: 56.77468776702881
Epoch: 3, Steps: 318 | Train Loss: 0.1009965 Vali Loss: 0.1234575 Test Loss: 0.0941648
Validation loss decreased (0.123688 --> 0.123457).  Saving model ...
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0968713
	speed: 0.1788s/iter; left time: 380.2347s
	iters: 200, epoch: 4 | loss: 0.0999367
	speed: 0.1774s/iter; left time: 359.4898s
	iters: 300, epoch: 4 | loss: 0.0933057
	speed: 0.1775s/iter; left time: 341.9916s
Epoch: 4 cost time: 56.62921357154846
Epoch: 4, Steps: 318 | Train Loss: 0.0959938 Vali Loss: 0.1237301 Test Loss: 0.0946039
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1277422
	speed: 0.1775s/iter; left time: 321.1489s
	iters: 200, epoch: 5 | loss: 0.1033896
	speed: 0.1767s/iter; left time: 302.0631s
	iters: 300, epoch: 5 | loss: 0.0958303
	speed: 0.1758s/iter; left time: 282.8844s
Epoch: 5 cost time: 56.27622103691101
Epoch: 5, Steps: 318 | Train Loss: 0.0935763 Vali Loss: 0.1262434 Test Loss: 0.0960210
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0856399
	speed: 0.1810s/iter; left time: 269.8407s
	iters: 200, epoch: 6 | loss: 0.0799024
	speed: 0.1751s/iter; left time: 243.5394s
	iters: 300, epoch: 6 | loss: 0.0794639
	speed: 0.1755s/iter; left time: 226.5792s
Epoch: 6 cost time: 56.41990041732788
Epoch: 6, Steps: 318 | Train Loss: 0.0924716 Vali Loss: 0.1249579 Test Loss: 0.0950588
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000016
Early stopping
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl20_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3000
loading supervised model weight
test shape: (3000, 1, 20, 1) (3000, 1, 20, 1)
test shape: (3000, 20, 1) (3000, 20, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=25, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10178
val 1487
test 2995
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.1499779
	speed: 0.1845s/iter; left time: 568.4615s
	iters: 200, epoch: 1 | loss: 0.1577819
	speed: 0.1730s/iter; left time: 515.5884s
	iters: 300, epoch: 1 | loss: 0.1513139
	speed: 0.1741s/iter; left time: 501.4413s
Epoch: 1 cost time: 56.380411863327026
Epoch: 1, Steps: 318 | Train Loss: 0.1811505 Vali Loss: 0.1458536 Test Loss: 0.1103733
Validation loss decreased (inf --> 0.145854).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1305756
	speed: 0.1779s/iter; left time: 491.5129s
	iters: 200, epoch: 2 | loss: 0.1401712
	speed: 0.1749s/iter; left time: 465.6546s
	iters: 300, epoch: 2 | loss: 0.1037933
	speed: 0.1773s/iter; left time: 454.3148s
Epoch: 2 cost time: 56.28403854370117
Epoch: 2, Steps: 318 | Train Loss: 0.1148021 Vali Loss: 0.1377911 Test Loss: 0.1033035
Validation loss decreased (0.145854 --> 0.137791).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1004083
	speed: 0.1777s/iter; left time: 434.4398s
	iters: 200, epoch: 3 | loss: 0.1166578
	speed: 0.1754s/iter; left time: 411.3062s
	iters: 300, epoch: 3 | loss: 0.0884423
	speed: 0.1752s/iter; left time: 393.3227s
Epoch: 3 cost time: 56.10313391685486
Epoch: 3, Steps: 318 | Train Loss: 0.1050594 Vali Loss: 0.1394805 Test Loss: 0.0992568
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0927316
	speed: 0.1784s/iter; left time: 379.3529s
	iters: 200, epoch: 4 | loss: 0.1388654
	speed: 0.1765s/iter; left time: 357.7029s
	iters: 300, epoch: 4 | loss: 0.1104350
	speed: 0.1764s/iter; left time: 339.9061s
Epoch: 4 cost time: 56.432403802871704
Epoch: 4, Steps: 318 | Train Loss: 0.0996257 Vali Loss: 0.1374830 Test Loss: 0.1017563
Validation loss decreased (0.137791 --> 0.137483).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1100006
	speed: 0.1782s/iter; left time: 322.3816s
	iters: 200, epoch: 5 | loss: 0.0827669
	speed: 0.1763s/iter; left time: 301.2488s
	iters: 300, epoch: 5 | loss: 0.1023003
	speed: 0.1754s/iter; left time: 282.2457s
Epoch: 5 cost time: 56.26916027069092
Epoch: 5, Steps: 318 | Train Loss: 0.0965603 Vali Loss: 0.1372984 Test Loss: 0.0993279
Validation loss decreased (0.137483 --> 0.137298).  Saving model ...
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0690827
	speed: 0.1776s/iter; left time: 264.7459s
	iters: 200, epoch: 6 | loss: 0.1060954
	speed: 0.1751s/iter; left time: 243.5644s
	iters: 300, epoch: 6 | loss: 0.0908601
	speed: 0.1766s/iter; left time: 228.0454s
Epoch: 6 cost time: 56.213463306427
Epoch: 6, Steps: 318 | Train Loss: 0.0950602 Vali Loss: 0.1394548 Test Loss: 0.0999632
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0930514
	speed: 0.1780s/iter; left time: 208.7987s
	iters: 200, epoch: 7 | loss: 0.0774633
	speed: 0.1755s/iter; left time: 188.2615s
	iters: 300, epoch: 7 | loss: 0.1041559
	speed: 0.1761s/iter; left time: 171.3156s
Epoch: 7 cost time: 56.2286319732666
Epoch: 7, Steps: 318 | Train Loss: 0.0939482 Vali Loss: 0.1404029 Test Loss: 0.1014489
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000008
	iters: 100, epoch: 8 | loss: 0.0939229
	speed: 0.1787s/iter; left time: 152.7683s
	iters: 200, epoch: 8 | loss: 0.0750123
	speed: 0.1756s/iter; left time: 132.5568s
	iters: 300, epoch: 8 | loss: 0.1188947
	speed: 0.1755s/iter; left time: 114.9511s
Epoch: 8 cost time: 56.2397997379303
Epoch: 8, Steps: 318 | Train Loss: 0.0937025 Vali Loss: 0.1389644 Test Loss: 0.1004506
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000004
Early stopping
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl25_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2995
loading supervised model weight
test shape: (2995, 1, 25, 1) (2995, 1, 25, 1)
test shape: (2995, 25, 1) (2995, 25, 1)
gates_list shape: (0,)
Args in experiment:
Namespace(is_training=1, model_id='omdata4_test5', model='TimesNet', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_20.0_lon_118.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=False, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=48, pred_len=30, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='20240701', loss='MSE', lradj='HalfLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10173
val 1482
test 2990
Patience count starts from 1 epoch
Using HalfLR learning rate adjustment
Starting training
	iters: 100, epoch: 1 | loss: 0.2005256
	speed: 0.1955s/iter; left time: 600.4515s
	iters: 200, epoch: 1 | loss: 0.1136787
	speed: 0.1889s/iter; left time: 561.0765s
	iters: 300, epoch: 1 | loss: 0.1457026
	speed: 0.1931s/iter; left time: 554.5135s
Epoch: 1 cost time: 61.1718864440918
Epoch: 1, Steps: 317 | Train Loss: 0.1851520 Vali Loss: 0.1674091 Test Loss: 0.1363806
Validation loss decreased (inf --> 0.167409).  Saving model ...
Adjusting learning rate to: 0.0000500
	iters: 100, epoch: 2 | loss: 0.1014982
	speed: 0.1952s/iter; left time: 537.5921s
	iters: 200, epoch: 2 | loss: 0.1186415
	speed: 0.1942s/iter; left time: 515.3114s
	iters: 300, epoch: 2 | loss: 0.1512434
	speed: 0.1946s/iter; left time: 497.1007s
Epoch: 2 cost time: 61.8369517326355
Epoch: 2, Steps: 317 | Train Loss: 0.1212589 Vali Loss: 0.1500047 Test Loss: 0.1104834
Validation loss decreased (0.167409 --> 0.150005).  Saving model ...
Adjusting learning rate to: 0.0000250
	iters: 100, epoch: 3 | loss: 0.1063661
	speed: 0.1972s/iter; left time: 480.5062s
	iters: 200, epoch: 3 | loss: 0.0990893
	speed: 0.1947s/iter; left time: 454.9766s
	iters: 300, epoch: 3 | loss: 0.1166968
	speed: 0.1947s/iter; left time: 435.4903s
Epoch: 3 cost time: 62.09065318107605
Epoch: 3, Steps: 317 | Train Loss: 0.1126692 Vali Loss: 0.1501207 Test Loss: 0.1091813
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000125
	iters: 100, epoch: 4 | loss: 0.0978558
	speed: 0.1965s/iter; left time: 416.6048s
	iters: 200, epoch: 4 | loss: 0.0951928
	speed: 0.1944s/iter; left time: 392.7138s
	iters: 300, epoch: 4 | loss: 0.1088903
	speed: 0.1947s/iter; left time: 373.7695s
Epoch: 4 cost time: 61.99043083190918
Epoch: 4, Steps: 317 | Train Loss: 0.1089338 Vali Loss: 0.1394900 Test Loss: 0.1001321
Validation loss decreased (0.150005 --> 0.139490).  Saving model ...
Adjusting learning rate to: 0.0000063
	iters: 100, epoch: 5 | loss: 0.1060915
	speed: 0.1954s/iter; left time: 352.2901s
	iters: 200, epoch: 5 | loss: 0.1033857
	speed: 0.1933s/iter; left time: 329.2356s
	iters: 300, epoch: 5 | loss: 0.1035106
	speed: 0.1943s/iter; left time: 311.5294s
Epoch: 5 cost time: 61.741196393966675
Epoch: 5, Steps: 317 | Train Loss: 0.1062286 Vali Loss: 0.1396506 Test Loss: 0.1012744
EarlyStopping counter: 1 out of 3
Adjusting learning rate to: 0.0000031
	iters: 100, epoch: 6 | loss: 0.0969621
	speed: 0.1963s/iter; left time: 291.6463s
	iters: 200, epoch: 6 | loss: 0.1193827
	speed: 0.1940s/iter; left time: 268.8773s
	iters: 300, epoch: 6 | loss: 0.0938245
	speed: 0.1943s/iter; left time: 249.8329s
Epoch: 6 cost time: 61.88232493400574
Epoch: 6, Steps: 317 | Train Loss: 0.1047457 Vali Loss: 0.1421708 Test Loss: 0.1015047
EarlyStopping counter: 2 out of 3
Adjusting learning rate to: 0.0000016
	iters: 100, epoch: 7 | loss: 0.0807369
	speed: 0.1939s/iter; left time: 226.6392s
	iters: 200, epoch: 7 | loss: 0.1003587
	speed: 0.1922s/iter; left time: 205.4904s
	iters: 300, epoch: 7 | loss: 0.1023069
	speed: 0.1928s/iter; left time: 186.7914s
Epoch: 7 cost time: 61.29571771621704
Epoch: 7, Steps: 317 | Train Loss: 0.1043129 Vali Loss: 0.1413088 Test Loss: 0.1014663
EarlyStopping counter: 3 out of 3
Adjusting learning rate to: 0.0000008
Early stopping
>>>>>>>testing : omdata4_test5_TimesNet_custom_ftMS_sl365_ll48_pl30_dm64_nh8_el3_dl1_df64_fc1_ebtimeF_dtTrue_20240701_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2990
loading supervised model weight
test shape: (2990, 1, 30, 1) (2990, 1, 30, 1)
test shape: (2990, 30, 1) (2990, 30, 1)
gates_list shape: (0,)
