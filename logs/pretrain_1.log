Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='MSPT_SSL', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=True, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=0, pred_len=0, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=128, n_heads=8, e_layers=2, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.001, des='20240615', loss='MSE', lradj='OneCycleLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 26.484174489974976
Epoch: 1, Steps: 79 | Train Loss: 1.0090559 Vali Loss: 0.8941613 Test Loss: 0.8212218
Validation loss decreased (inf --> 0.894161).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 26.032297611236572
Epoch: 2, Steps: 79 | Train Loss: 0.8975548 Vali Loss: 0.8238866 Test Loss: 0.7544353
Validation loss decreased (0.894161 --> 0.823887).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 26.12357234954834
Epoch: 3, Steps: 79 | Train Loss: 0.7622378 Vali Loss: 0.5932934 Test Loss: 0.5454034
Validation loss decreased (0.823887 --> 0.593293).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 26.397137880325317
Epoch: 4, Steps: 79 | Train Loss: 0.5744344 Vali Loss: 0.4461176 Test Loss: 0.4278479
Validation loss decreased (0.593293 --> 0.446118).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 26.249340534210205
Epoch: 5, Steps: 79 | Train Loss: 0.3999630 Vali Loss: 0.2890364 Test Loss: 0.2849894
Validation loss decreased (0.446118 --> 0.289036).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 26.23927640914917
Epoch: 6, Steps: 79 | Train Loss: 0.2912515 Vali Loss: 0.2004227 Test Loss: 0.2142076
Validation loss decreased (0.289036 --> 0.200423).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 26.31865644454956
Epoch: 7, Steps: 79 | Train Loss: 0.2568956 Vali Loss: 0.1764147 Test Loss: 0.1929136
Validation loss decreased (0.200423 --> 0.176415).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 26.24554944038391
Epoch: 8, Steps: 79 | Train Loss: 0.2381875 Vali Loss: 0.1532259 Test Loss: 0.1781497
Validation loss decreased (0.176415 --> 0.153226).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 26.33694052696228
Epoch: 9, Steps: 79 | Train Loss: 0.2263720 Vali Loss: 0.1480894 Test Loss: 0.1708630
Validation loss decreased (0.153226 --> 0.148089).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 26.145584106445312
Epoch: 10, Steps: 79 | Train Loss: 0.2166321 Vali Loss: 0.1408363 Test Loss: 0.1623237
Validation loss decreased (0.148089 --> 0.140836).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 26.146299839019775
Epoch: 11, Steps: 79 | Train Loss: 0.2082356 Vali Loss: 0.1325362 Test Loss: 0.1545850
Validation loss decreased (0.140836 --> 0.132536).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 26.251840353012085
Epoch: 12, Steps: 79 | Train Loss: 0.2006335 Vali Loss: 0.1253298 Test Loss: 0.1453173
Validation loss decreased (0.132536 --> 0.125330).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 26.317391633987427
Epoch: 13, Steps: 79 | Train Loss: 0.1929710 Vali Loss: 0.1209484 Test Loss: 0.1404907
Validation loss decreased (0.125330 --> 0.120948).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 26.204354763031006
Epoch: 14, Steps: 79 | Train Loss: 0.1837268 Vali Loss: 0.1157696 Test Loss: 0.1313102
Validation loss decreased (0.120948 --> 0.115770).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 26.365137100219727
Epoch: 15, Steps: 79 | Train Loss: 0.1770055 Vali Loss: 0.1096880 Test Loss: 0.1217046
Validation loss decreased (0.115770 --> 0.109688).  Saving model ...
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 26.31396222114563
Epoch: 16, Steps: 79 | Train Loss: 0.1701399 Vali Loss: 0.1042505 Test Loss: 0.1136832
Validation loss decreased (0.109688 --> 0.104250).  Saving model ...
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 26.26759696006775
Epoch: 17, Steps: 79 | Train Loss: 0.1642281 Vali Loss: 0.1036102 Test Loss: 0.1063533
Validation loss decreased (0.104250 --> 0.103610).  Saving model ...
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 26.340041160583496
Epoch: 18, Steps: 79 | Train Loss: 0.1575803 Vali Loss: 0.0978595 Test Loss: 0.1013958
Validation loss decreased (0.103610 --> 0.097860).  Saving model ...
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 26.206429958343506
Epoch: 19, Steps: 79 | Train Loss: 0.1483196 Vali Loss: 0.0894897 Test Loss: 0.0896420
Validation loss decreased (0.097860 --> 0.089490).  Saving model ...
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 26.18365478515625
Epoch: 20, Steps: 79 | Train Loss: 0.1390434 Vali Loss: 0.0781683 Test Loss: 0.0766308
Validation loss decreased (0.089490 --> 0.078168).  Saving model ...
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 26.28335690498352
Epoch: 21, Steps: 79 | Train Loss: 0.1303463 Vali Loss: 0.0739027 Test Loss: 0.0704170
Validation loss decreased (0.078168 --> 0.073903).  Saving model ...
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 26.27438259124756
Epoch: 22, Steps: 79 | Train Loss: 0.1212817 Vali Loss: 0.0644579 Test Loss: 0.0591494
Validation loss decreased (0.073903 --> 0.064458).  Saving model ...
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 26.236305475234985
Epoch: 23, Steps: 79 | Train Loss: 0.1118941 Vali Loss: 0.0578444 Test Loss: 0.0524177
Validation loss decreased (0.064458 --> 0.057844).  Saving model ...
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 26.409924745559692
Epoch: 24, Steps: 79 | Train Loss: 0.1066948 Vali Loss: 0.0547543 Test Loss: 0.0500806
Validation loss decreased (0.057844 --> 0.054754).  Saving model ...
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 26.377124071121216
Epoch: 25, Steps: 79 | Train Loss: 0.1038162 Vali Loss: 0.0521369 Test Loss: 0.0468595
Validation loss decreased (0.054754 --> 0.052137).  Saving model ...
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 26.313921451568604
Epoch: 26, Steps: 79 | Train Loss: 0.1014493 Vali Loss: 0.0513958 Test Loss: 0.0459419
Validation loss decreased (0.052137 --> 0.051396).  Saving model ...
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 26.383472442626953
Epoch: 27, Steps: 79 | Train Loss: 0.0990956 Vali Loss: 0.0520001 Test Loss: 0.0460078
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 26.41464591026306
Epoch: 28, Steps: 79 | Train Loss: 0.0969305 Vali Loss: 0.0500251 Test Loss: 0.0444280
Validation loss decreased (0.051396 --> 0.050025).  Saving model ...
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 26.378034591674805
Epoch: 29, Steps: 79 | Train Loss: 0.0952067 Vali Loss: 0.0500964 Test Loss: 0.0441019
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 26.459238529205322
Epoch: 30, Steps: 79 | Train Loss: 0.0938636 Vali Loss: 0.0485635 Test Loss: 0.0430052
Validation loss decreased (0.050025 --> 0.048563).  Saving model ...
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 25.23820400238037
Epoch: 31, Steps: 79 | Train Loss: 0.0921252 Vali Loss: 0.0474063 Test Loss: 0.0412809
Validation loss decreased (0.048563 --> 0.047406).  Saving model ...
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 25.258758068084717
Epoch: 32, Steps: 79 | Train Loss: 0.0902940 Vali Loss: 0.0430615 Test Loss: 0.0373190
Validation loss decreased (0.047406 --> 0.043062).  Saving model ...
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 25.172066926956177
Epoch: 33, Steps: 79 | Train Loss: 0.0888549 Vali Loss: 0.0440914 Test Loss: 0.0382204
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 25.175797939300537
Epoch: 34, Steps: 79 | Train Loss: 0.0876362 Vali Loss: 0.0425908 Test Loss: 0.0369461
Validation loss decreased (0.043062 --> 0.042591).  Saving model ...
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 26.184911727905273
Epoch: 35, Steps: 79 | Train Loss: 0.0857364 Vali Loss: 0.0403979 Test Loss: 0.0345906
Validation loss decreased (0.042591 --> 0.040398).  Saving model ...
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 26.17710280418396
Epoch: 36, Steps: 79 | Train Loss: 0.0845328 Vali Loss: 0.0397108 Test Loss: 0.0333437
Validation loss decreased (0.040398 --> 0.039711).  Saving model ...
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 26.27665638923645
Epoch: 37, Steps: 79 | Train Loss: 0.0831802 Vali Loss: 0.0424935 Test Loss: 0.0361456
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 26.2345130443573
Epoch: 38, Steps: 79 | Train Loss: 0.0816936 Vali Loss: 0.0409715 Test Loss: 0.0348800
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 26.344693183898926
Epoch: 39, Steps: 79 | Train Loss: 0.0810898 Vali Loss: 0.0383408 Test Loss: 0.0320939
Validation loss decreased (0.039711 --> 0.038341).  Saving model ...
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 26.23894739151001
Epoch: 40, Steps: 79 | Train Loss: 0.0801522 Vali Loss: 0.0395189 Test Loss: 0.0331975
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 26.23800230026245
Epoch: 41, Steps: 79 | Train Loss: 0.0794562 Vali Loss: 0.0375115 Test Loss: 0.0319252
Validation loss decreased (0.038341 --> 0.037512).  Saving model ...
Adjusting learning rate to: 0.0009402
Epoch: 42 cost time: 26.38929796218872
Epoch: 42, Steps: 79 | Train Loss: 0.0786977 Vali Loss: 0.0375236 Test Loss: 0.0321311
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009291
Epoch: 43 cost time: 26.40203833580017
Epoch: 43, Steps: 79 | Train Loss: 0.0778582 Vali Loss: 0.0409914 Test Loss: 0.0349490
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009171
Epoch: 44 cost time: 26.312877416610718
Epoch: 44, Steps: 79 | Train Loss: 0.0772520 Vali Loss: 0.0390737 Test Loss: 0.0330463
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009043
Epoch: 45 cost time: 26.242937088012695
Epoch: 45, Steps: 79 | Train Loss: 0.0766245 Vali Loss: 0.0375187 Test Loss: 0.0319238
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0008907
Epoch: 46 cost time: 26.375098943710327
Epoch: 46, Steps: 79 | Train Loss: 0.0760199 Vali Loss: 0.0394301 Test Loss: 0.0333822
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0008763
Epoch: 47 cost time: 26.358314752578735
Epoch: 47, Steps: 79 | Train Loss: 0.0754773 Vali Loss: 0.0371010 Test Loss: 0.0319621
Validation loss decreased (0.037512 --> 0.037101).  Saving model ...
Adjusting learning rate to: 0.0008612
Epoch: 48 cost time: 26.348183155059814
Epoch: 48, Steps: 79 | Train Loss: 0.0745880 Vali Loss: 0.0386858 Test Loss: 0.0327575
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008453
Epoch: 49 cost time: 26.28926730155945
Epoch: 49, Steps: 79 | Train Loss: 0.0741171 Vali Loss: 0.0374315 Test Loss: 0.0320280
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0008288
Epoch: 50 cost time: 26.192835092544556
Epoch: 50, Steps: 79 | Train Loss: 0.0737615 Vali Loss: 0.0369328 Test Loss: 0.0314741
Validation loss decreased (0.037101 --> 0.036933).  Saving model ...
Adjusting learning rate to: 0.0008115
Epoch: 51 cost time: 26.237553596496582
Epoch: 51, Steps: 79 | Train Loss: 0.0732271 Vali Loss: 0.0398060 Test Loss: 0.0344256
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0007937
Epoch: 52 cost time: 26.251399040222168
Epoch: 52, Steps: 79 | Train Loss: 0.0731562 Vali Loss: 0.0360524 Test Loss: 0.0307934
Validation loss decreased (0.036933 --> 0.036052).  Saving model ...
Adjusting learning rate to: 0.0007752
Epoch: 53 cost time: 26.360002517700195
Epoch: 53, Steps: 79 | Train Loss: 0.0727023 Vali Loss: 0.0369308 Test Loss: 0.0316812
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0007562
Epoch: 54 cost time: 26.251274824142456
Epoch: 54, Steps: 79 | Train Loss: 0.0722764 Vali Loss: 0.0350018 Test Loss: 0.0304520
Validation loss decreased (0.036052 --> 0.035002).  Saving model ...
Adjusting learning rate to: 0.0007367
Epoch: 55 cost time: 26.264347553253174
Epoch: 55, Steps: 79 | Train Loss: 0.0715110 Vali Loss: 0.0372482 Test Loss: 0.0325489
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0007167
Epoch: 56 cost time: 26.235310792922974
Epoch: 56, Steps: 79 | Train Loss: 0.0715850 Vali Loss: 0.0365418 Test Loss: 0.0315716
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0006963
Epoch: 57 cost time: 26.32301354408264
Epoch: 57, Steps: 79 | Train Loss: 0.0710852 Vali Loss: 0.0360344 Test Loss: 0.0309294
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0006754
Epoch: 58 cost time: 26.291298151016235
Epoch: 58, Steps: 79 | Train Loss: 0.0706497 Vali Loss: 0.0367059 Test Loss: 0.0317959
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0006542
Epoch: 59 cost time: 26.293338298797607
Epoch: 59, Steps: 79 | Train Loss: 0.0706047 Vali Loss: 0.0387934 Test Loss: 0.0335105
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0006327
Epoch: 60 cost time: 26.29688286781311
Epoch: 60, Steps: 79 | Train Loss: 0.0700935 Vali Loss: 0.0368350 Test Loss: 0.0321097
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0006110
Epoch: 61 cost time: 26.28285551071167
Epoch: 61, Steps: 79 | Train Loss: 0.0694884 Vali Loss: 0.0370360 Test Loss: 0.0323063
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0005890
Epoch: 62 cost time: 26.304605722427368
Epoch: 62, Steps: 79 | Train Loss: 0.0693454 Vali Loss: 0.0354732 Test Loss: 0.0310969
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0005668
Epoch: 63 cost time: 26.326689958572388
Epoch: 63, Steps: 79 | Train Loss: 0.0688342 Vali Loss: 0.0389164 Test Loss: 0.0339262
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0005445
Epoch: 64 cost time: 26.328307151794434
Epoch: 64, Steps: 79 | Train Loss: 0.0685433 Vali Loss: 0.0390383 Test Loss: 0.0333393
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0005222
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.055951427668333054, mae:0.17806151509284973, rmse:0.2365405410528183, mape:0.006334864534437656, mspe:7.060397911118343e-05, rse:0.1630379855632782, r2_score:0.974387605164565, acc:0.9936651354655623
corr: [37.59396  37.445683 37.64816  37.454346 37.571163 37.41945  37.539932
 37.386177 37.51701  37.367607 37.533405 37.38612  37.497955 37.342316
 37.508743 37.340195 37.464973 37.33689  37.48816  37.40275  37.509014
 37.42256  37.57034  37.502457 37.63784  37.534744 37.65751  37.53775
 37.58817  37.494244 37.55234  37.42247  37.58156  37.452686 37.53421
 37.413036 37.524357 37.3957   37.50972  37.37662  37.53948  37.407646
 37.644123 37.5038   37.587345 37.49769  37.633522 37.546524 37.613224
 37.517906 37.57589  37.46802  37.607365 37.464985 37.42277  37.311787
 37.350403 37.20751  37.33188  37.208107 37.357574 37.234104 37.325645
 37.228836 37.32423  37.215187 37.27631  37.1437   37.29886  37.182175
 37.242744 37.171024 37.21925  37.13293  37.203876 37.073273 37.18828
 37.032482 37.129856 37.008854 37.100327 36.933075 37.07576  36.9159
 37.064816 36.881485 36.971706 36.805874 36.951435 36.783825 36.96774
 36.824104 36.923077 36.83343  36.973934 36.846916 36.92212  36.783237
 36.88195  36.70473  36.878544 36.69593  36.816334 36.639404 36.74831
 36.584904 36.719143 36.602406 36.709854 36.60598  36.724354 36.610302
 36.709908 36.607964 36.719887 36.57737  36.628876 36.48126  36.580795
 36.410595 36.518314 36.343704 36.46176  36.289185 36.40573  36.18393
 36.45026  36.22584  36.40197  36.20821  36.412876 36.20931  36.36678
 36.219112 36.34081  36.189777 36.278416 36.161083 36.295128 36.213207
 36.30581  36.170055 36.24831  36.12257  36.199554 36.06244  36.155636
 35.98317  36.151775 35.923252 36.19336  35.976192 36.12861  35.940575
 36.062683 35.895515 36.00642  35.838024 35.939503 35.816624 36.072422
 35.938484 36.054527 35.91811  36.04192  35.844204 36.09458  35.908672
 36.0252   35.864216 36.033295 35.848274 35.96847  35.771843 35.92369
 35.75047  35.917377 35.745575 35.95955  35.777683 36.030773 35.817863
 36.04437  35.815426 36.030926 35.80678  35.990448 35.78884  36.02959
 35.86546  36.00494  35.88034  36.067314 35.93619  36.038498 35.906113
 36.00059  35.846375 35.951073 35.820023 35.898838 35.77419  35.85963
 35.724777 35.840366 35.685673 35.801716 35.649574 35.77352  35.60674
 35.87228  35.6752   35.881428 35.661633 35.913345 35.66571  35.909496
 35.670902 35.901077 35.749054 35.89464  35.765274 35.86656  35.786026
 35.93296  35.82687  36.000313 35.901413 36.00775  35.90465  36.13832
 35.97125  36.12147  35.922195 36.12219  35.893284 36.12789  35.870335
 36.083992 35.85992  35.998978 35.854652 36.072014 35.887653 36.144436
 35.955296 36.181484 36.01227  36.23322  36.047626 36.1942   35.978386
 36.145546 35.937843 36.14517  35.96881  36.147213 35.975914 36.103764
 35.93931  36.066277 35.945107 36.08882  35.926273 36.041492 35.91155
 36.10018  35.88657  36.10357  35.91452  36.11917  35.90271  36.098488
 35.896427 36.117085 35.932644 36.168423 35.96064  36.212273 35.966125
 36.18419  35.973232 36.216072 36.036762 36.233616 36.068756 36.267536
 36.12605  36.248333 36.118614 36.283577 36.177525 36.24077  36.111187
 36.215935 36.106228 36.27135  36.108494 36.298702 36.13235  36.31251
 36.15113  36.263844 36.139526 36.239864 36.09656  36.20131  36.105907
 36.19586  36.125496 36.249577 36.160133 36.287636 36.203808 36.335148
 36.219734 36.366684 36.214054 36.302654 36.15923  36.28433  36.13413
 36.194126 36.015327 36.171047 35.97534  36.134266 36.000896 36.11374
 35.977394 36.11743  35.96376  36.15325  36.02535  36.13466  36.00113
 36.18236  36.058285 36.21011  36.114246 36.236416 36.142532 36.267216
 36.17766  36.299995 36.179382 36.28908  36.22721  36.403202 36.252274
 36.43249  36.271618 36.42472  36.264458 36.43598  36.24977  36.344223
 36.16449  36.46037  36.254505 36.51527  36.353687 36.54362  36.40294
 36.714592]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 15.632894515991211
Epoch: 1, Steps: 79 | Train Loss: 1.0254217 Vali Loss: 0.9048023 Test Loss: 0.8244305
Validation loss decreased (inf --> 0.904802).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 15.540512323379517
Epoch: 2, Steps: 79 | Train Loss: 0.9114290 Vali Loss: 0.8432242 Test Loss: 0.7747098
Validation loss decreased (0.904802 --> 0.843224).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 15.517536640167236
Epoch: 3, Steps: 79 | Train Loss: 0.8139146 Vali Loss: 0.6687871 Test Loss: 0.6096740
Validation loss decreased (0.843224 --> 0.668787).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 15.51568341255188
Epoch: 4, Steps: 79 | Train Loss: 0.6323524 Vali Loss: 0.5556621 Test Loss: 0.5165127
Validation loss decreased (0.668787 --> 0.555662).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 15.510562419891357
Epoch: 5, Steps: 79 | Train Loss: 0.5279291 Vali Loss: 0.4264993 Test Loss: 0.4110008
Validation loss decreased (0.555662 --> 0.426499).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 15.493117332458496
Epoch: 6, Steps: 79 | Train Loss: 0.4061260 Vali Loss: 0.2768615 Test Loss: 0.2721411
Validation loss decreased (0.426499 --> 0.276861).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 15.517113447189331
Epoch: 7, Steps: 79 | Train Loss: 0.2791452 Vali Loss: 0.1855709 Test Loss: 0.1960576
Validation loss decreased (0.276861 --> 0.185571).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 15.440248250961304
Epoch: 8, Steps: 79 | Train Loss: 0.2408916 Vali Loss: 0.1490932 Test Loss: 0.1664214
Validation loss decreased (0.185571 --> 0.149093).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 15.44254469871521
Epoch: 9, Steps: 79 | Train Loss: 0.2180791 Vali Loss: 0.1309965 Test Loss: 0.1473465
Validation loss decreased (0.149093 --> 0.130996).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 15.53600525856018
Epoch: 10, Steps: 79 | Train Loss: 0.2048794 Vali Loss: 0.1179721 Test Loss: 0.1319134
Validation loss decreased (0.130996 --> 0.117972).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 15.513243436813354
Epoch: 11, Steps: 79 | Train Loss: 0.1946947 Vali Loss: 0.1099617 Test Loss: 0.1230847
Validation loss decreased (0.117972 --> 0.109962).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 15.497110366821289
Epoch: 12, Steps: 79 | Train Loss: 0.1861596 Vali Loss: 0.1062171 Test Loss: 0.1160435
Validation loss decreased (0.109962 --> 0.106217).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 15.520722150802612
Epoch: 13, Steps: 79 | Train Loss: 0.1783921 Vali Loss: 0.0993433 Test Loss: 0.1064242
Validation loss decreased (0.106217 --> 0.099343).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 15.475381851196289
Epoch: 14, Steps: 79 | Train Loss: 0.1695307 Vali Loss: 0.0914430 Test Loss: 0.0964643
Validation loss decreased (0.099343 --> 0.091443).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 15.49104642868042
Epoch: 15, Steps: 79 | Train Loss: 0.1613221 Vali Loss: 0.0875453 Test Loss: 0.0893906
Validation loss decreased (0.091443 --> 0.087545).  Saving model ...
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 15.5656578540802
Epoch: 16, Steps: 79 | Train Loss: 0.1541855 Vali Loss: 0.0838351 Test Loss: 0.0832361
Validation loss decreased (0.087545 --> 0.083835).  Saving model ...
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 15.481157541275024
Epoch: 17, Steps: 79 | Train Loss: 0.1465966 Vali Loss: 0.0793627 Test Loss: 0.0770280
Validation loss decreased (0.083835 --> 0.079363).  Saving model ...
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 15.52016568183899
Epoch: 18, Steps: 79 | Train Loss: 0.1391240 Vali Loss: 0.0747953 Test Loss: 0.0699808
Validation loss decreased (0.079363 --> 0.074795).  Saving model ...
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 15.470645904541016
Epoch: 19, Steps: 79 | Train Loss: 0.1331320 Vali Loss: 0.0700508 Test Loss: 0.0653842
Validation loss decreased (0.074795 --> 0.070051).  Saving model ...
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 15.613194465637207
Epoch: 20, Steps: 79 | Train Loss: 0.1280801 Vali Loss: 0.0670923 Test Loss: 0.0620762
Validation loss decreased (0.070051 --> 0.067092).  Saving model ...
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 15.53274941444397
Epoch: 21, Steps: 79 | Train Loss: 0.1238646 Vali Loss: 0.0669004 Test Loss: 0.0607804
Validation loss decreased (0.067092 --> 0.066900).  Saving model ...
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 15.510765075683594
Epoch: 22, Steps: 79 | Train Loss: 0.1203793 Vali Loss: 0.0684760 Test Loss: 0.0627449
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 15.562236547470093
Epoch: 23, Steps: 79 | Train Loss: 0.1181683 Vali Loss: 0.0620097 Test Loss: 0.0558544
Validation loss decreased (0.066900 --> 0.062010).  Saving model ...
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 15.430956602096558
Epoch: 24, Steps: 79 | Train Loss: 0.1152962 Vali Loss: 0.0618870 Test Loss: 0.0569268
Validation loss decreased (0.062010 --> 0.061887).  Saving model ...
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 15.485405921936035
Epoch: 25, Steps: 79 | Train Loss: 0.1132841 Vali Loss: 0.0600832 Test Loss: 0.0543928
Validation loss decreased (0.061887 --> 0.060083).  Saving model ...
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 15.528429746627808
Epoch: 26, Steps: 79 | Train Loss: 0.1111390 Vali Loss: 0.0592139 Test Loss: 0.0530173
Validation loss decreased (0.060083 --> 0.059214).  Saving model ...
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 15.418347835540771
Epoch: 27, Steps: 79 | Train Loss: 0.1085793 Vali Loss: 0.0565149 Test Loss: 0.0508352
Validation loss decreased (0.059214 --> 0.056515).  Saving model ...
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 15.544787168502808
Epoch: 28, Steps: 79 | Train Loss: 0.1068627 Vali Loss: 0.0548051 Test Loss: 0.0490415
Validation loss decreased (0.056515 --> 0.054805).  Saving model ...
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 15.526830673217773
Epoch: 29, Steps: 79 | Train Loss: 0.1047298 Vali Loss: 0.0582752 Test Loss: 0.0511321
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 15.571717262268066
Epoch: 30, Steps: 79 | Train Loss: 0.1028758 Vali Loss: 0.0563474 Test Loss: 0.0501647
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 15.540213108062744
Epoch: 31, Steps: 79 | Train Loss: 0.1013238 Vali Loss: 0.0541112 Test Loss: 0.0474164
Validation loss decreased (0.054805 --> 0.054111).  Saving model ...
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 15.486322164535522
Epoch: 32, Steps: 79 | Train Loss: 0.0996106 Vali Loss: 0.0534129 Test Loss: 0.0463011
Validation loss decreased (0.054111 --> 0.053413).  Saving model ...
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 15.578556299209595
Epoch: 33, Steps: 79 | Train Loss: 0.0980255 Vali Loss: 0.0540216 Test Loss: 0.0471036
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 15.51862645149231
Epoch: 34, Steps: 79 | Train Loss: 0.0966370 Vali Loss: 0.0515106 Test Loss: 0.0442149
Validation loss decreased (0.053413 --> 0.051511).  Saving model ...
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 15.549068689346313
Epoch: 35, Steps: 79 | Train Loss: 0.0948232 Vali Loss: 0.0498680 Test Loss: 0.0432486
Validation loss decreased (0.051511 --> 0.049868).  Saving model ...
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 15.495358228683472
Epoch: 36, Steps: 79 | Train Loss: 0.0936269 Vali Loss: 0.0489150 Test Loss: 0.0413682
Validation loss decreased (0.049868 --> 0.048915).  Saving model ...
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 15.59286618232727
Epoch: 37, Steps: 79 | Train Loss: 0.0931173 Vali Loss: 0.0489630 Test Loss: 0.0419066
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 15.58426547050476
Epoch: 38, Steps: 79 | Train Loss: 0.0917032 Vali Loss: 0.0485378 Test Loss: 0.0414954
Validation loss decreased (0.048915 --> 0.048538).  Saving model ...
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 15.53082799911499
Epoch: 39, Steps: 79 | Train Loss: 0.0906884 Vali Loss: 0.0513138 Test Loss: 0.0435682
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 15.607869863510132
Epoch: 40, Steps: 79 | Train Loss: 0.0898906 Vali Loss: 0.0489026 Test Loss: 0.0418832
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 15.513930320739746
Epoch: 41, Steps: 79 | Train Loss: 0.0884937 Vali Loss: 0.0496871 Test Loss: 0.0424127
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009402
Epoch: 42 cost time: 15.452877521514893
Epoch: 42, Steps: 79 | Train Loss: 0.0884794 Vali Loss: 0.0480813 Test Loss: 0.0410269
Validation loss decreased (0.048538 --> 0.048081).  Saving model ...
Adjusting learning rate to: 0.0009291
Epoch: 43 cost time: 15.470118045806885
Epoch: 43, Steps: 79 | Train Loss: 0.0877946 Vali Loss: 0.0453838 Test Loss: 0.0395012
Validation loss decreased (0.048081 --> 0.045384).  Saving model ...
Adjusting learning rate to: 0.0009171
Epoch: 44 cost time: 15.480786561965942
Epoch: 44, Steps: 79 | Train Loss: 0.0868688 Vali Loss: 0.0468568 Test Loss: 0.0404698
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009043
Epoch: 45 cost time: 15.521277666091919
Epoch: 45, Steps: 79 | Train Loss: 0.0863599 Vali Loss: 0.0467853 Test Loss: 0.0392697
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0008907
Epoch: 46 cost time: 15.560466766357422
Epoch: 46, Steps: 79 | Train Loss: 0.0860492 Vali Loss: 0.0468700 Test Loss: 0.0404543
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0008763
Epoch: 47 cost time: 15.552533388137817
Epoch: 47, Steps: 79 | Train Loss: 0.0850652 Vali Loss: 0.0451324 Test Loss: 0.0388450
Validation loss decreased (0.045384 --> 0.045132).  Saving model ...
Adjusting learning rate to: 0.0008612
Epoch: 48 cost time: 15.546524286270142
Epoch: 48, Steps: 79 | Train Loss: 0.0849726 Vali Loss: 0.0464802 Test Loss: 0.0398986
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008453
Epoch: 49 cost time: 15.559027433395386
Epoch: 49, Steps: 79 | Train Loss: 0.0842421 Vali Loss: 0.0446674 Test Loss: 0.0390923
Validation loss decreased (0.045132 --> 0.044667).  Saving model ...
Adjusting learning rate to: 0.0008288
Epoch: 50 cost time: 15.516733169555664
Epoch: 50, Steps: 79 | Train Loss: 0.0843104 Vali Loss: 0.0486992 Test Loss: 0.0416214
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008115
Epoch: 51 cost time: 15.507498741149902
Epoch: 51, Steps: 79 | Train Loss: 0.0833015 Vali Loss: 0.0442897 Test Loss: 0.0383365
Validation loss decreased (0.044667 --> 0.044290).  Saving model ...
Adjusting learning rate to: 0.0007937
Epoch: 52 cost time: 15.568055868148804
Epoch: 52, Steps: 79 | Train Loss: 0.0829548 Vali Loss: 0.0452886 Test Loss: 0.0387166
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0007752
Epoch: 53 cost time: 15.54213809967041
Epoch: 53, Steps: 79 | Train Loss: 0.0824755 Vali Loss: 0.0446007 Test Loss: 0.0383203
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0007562
Epoch: 54 cost time: 15.483459711074829
Epoch: 54, Steps: 79 | Train Loss: 0.0818398 Vali Loss: 0.0459764 Test Loss: 0.0389311
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0007367
Epoch: 55 cost time: 15.52110242843628
Epoch: 55, Steps: 79 | Train Loss: 0.0813221 Vali Loss: 0.0454689 Test Loss: 0.0393614
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0007167
Epoch: 56 cost time: 15.652177810668945
Epoch: 56, Steps: 79 | Train Loss: 0.0811416 Vali Loss: 0.0445577 Test Loss: 0.0384148
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0006963
Epoch: 57 cost time: 15.685327529907227
Epoch: 57, Steps: 79 | Train Loss: 0.0808608 Vali Loss: 0.0476131 Test Loss: 0.0405805
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0006754
Epoch: 58 cost time: 15.512576818466187
Epoch: 58, Steps: 79 | Train Loss: 0.0800848 Vali Loss: 0.0442020 Test Loss: 0.0377931
Validation loss decreased (0.044290 --> 0.044202).  Saving model ...
Adjusting learning rate to: 0.0006542
Epoch: 59 cost time: 15.5634286403656
Epoch: 59, Steps: 79 | Train Loss: 0.0798316 Vali Loss: 0.0439717 Test Loss: 0.0380333
Validation loss decreased (0.044202 --> 0.043972).  Saving model ...
Adjusting learning rate to: 0.0006327
Epoch: 60 cost time: 15.60439682006836
Epoch: 60, Steps: 79 | Train Loss: 0.0793989 Vali Loss: 0.0456442 Test Loss: 0.0392505
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0006110
Epoch: 61 cost time: 15.52813172340393
Epoch: 61, Steps: 79 | Train Loss: 0.0790591 Vali Loss: 0.0431174 Test Loss: 0.0374119
Validation loss decreased (0.043972 --> 0.043117).  Saving model ...
Adjusting learning rate to: 0.0005890
Epoch: 62 cost time: 15.475990533828735
Epoch: 62, Steps: 79 | Train Loss: 0.0787724 Vali Loss: 0.0452600 Test Loss: 0.0390532
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0005668
Epoch: 63 cost time: 15.546419620513916
Epoch: 63, Steps: 79 | Train Loss: 0.0784707 Vali Loss: 0.0455400 Test Loss: 0.0392315
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0005445
Epoch: 64 cost time: 15.4865403175354
Epoch: 64, Steps: 79 | Train Loss: 0.0780590 Vali Loss: 0.0452776 Test Loss: 0.0386348
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0005222
Epoch: 65 cost time: 15.391992568969727
Epoch: 65, Steps: 79 | Train Loss: 0.0779085 Vali Loss: 0.0460905 Test Loss: 0.0394444
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0004997
Epoch: 66 cost time: 15.56000280380249
Epoch: 66, Steps: 79 | Train Loss: 0.0774892 Vali Loss: 0.0458051 Test Loss: 0.0391641
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0004773
Epoch: 67 cost time: 15.493390798568726
Epoch: 67, Steps: 79 | Train Loss: 0.0772232 Vali Loss: 0.0435765 Test Loss: 0.0382826
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0004549
Epoch: 68 cost time: 15.457183361053467
Epoch: 68, Steps: 79 | Train Loss: 0.0772817 Vali Loss: 0.0461567 Test Loss: 0.0405595
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0004326
Epoch: 69 cost time: 15.576236248016357
Epoch: 69, Steps: 79 | Train Loss: 0.0771167 Vali Loss: 0.0438951 Test Loss: 0.0382536
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0004104
Epoch: 70 cost time: 15.538322448730469
Epoch: 70, Steps: 79 | Train Loss: 0.0765845 Vali Loss: 0.0456364 Test Loss: 0.0388201
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0003885
Epoch: 71 cost time: 15.47339653968811
Epoch: 71, Steps: 79 | Train Loss: 0.0762020 Vali Loss: 0.0442122 Test Loss: 0.0383885
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0003667
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.07880467176437378, mae:0.21227142214775085, rmse:0.2807216942310333, mape:0.007561610545963049, mspe:9.993290586862713e-05, rse:0.19349028170108795, r2_score:0.9626499088604269, acc:0.992438389454037
corr: [37.784252 37.471947 37.12412  37.42831  37.274395 37.06829  37.360973
 37.242123 37.083794 37.38398  37.287777 37.162777 37.27608  37.205097
 37.10896  37.371693 37.277084 37.15379  37.32936  37.242603 37.140614
 37.357506 37.254536 37.14422  37.377453 37.26142  37.14033  37.39675
 37.296528 37.181686 37.38472  37.254257 37.116913 37.41117  37.27045
 37.128746 37.32333  37.257133 37.156826 37.373043 37.241447 37.11134
 37.332985 37.228176 37.09361  37.34574  37.211018 37.057323 37.269432
 37.168945 37.053867 37.308144 37.203827 37.061348 37.27998  37.17595
 37.03686  37.25885  37.141068 36.9877   37.30052  37.18324  37.026592
 37.437572 37.257244 37.04816  37.45361  37.269985 37.060787 37.389015
 37.229836 37.036964 37.316467 37.1864   37.008274 37.184517 37.058212
 36.892086 37.190704 37.039402 36.860947 37.191154 37.063225 36.884434
 37.188717 37.010147 36.814438 37.066353 36.92278  36.742016 37.05107
 36.927883 36.75306  37.00769  36.886345 36.76947  36.826275 36.744537
 36.610973 36.929504 36.758736 36.558605 36.88825  36.749992 36.570896
 36.82216  36.735046 36.592304 36.741898 36.652775 36.555546 36.755318
 36.628967 36.50127  36.647007 36.552017 36.42593  36.610718 36.49413
 36.361675 36.613102 36.489338 36.344116 36.482    36.39436  36.28782
 36.53654  36.43013  36.270996 36.469555 36.330162 36.15557  36.460514
 36.306404 36.12072  36.25495  36.159344 35.9964   36.186752 36.062943
 35.898502 36.21633  36.070427 35.89558  36.168903 36.038425 35.89608
 36.15504  36.045044 35.910477 36.10832  36.026142 35.872955 36.116932
 35.9999   35.870388 36.071896 35.959084 35.830326 36.194107 36.04667
 35.873047 36.19063  36.061    35.90428  36.204994 36.054855 35.856194
 36.183083 36.028553 35.840687 36.13124  35.99933  35.835297 36.177326
 36.047596 35.876022 36.17671  36.074493 35.949173 36.199245 36.08695
 35.936714 36.062305 35.973343 35.88357  36.075832 35.948368 35.79448
 35.962574 35.83622  35.706192 35.901516 35.78775  35.693798 35.896416
 35.786613 35.681767 35.943645 35.841705 35.71855  35.964336 35.844143
 35.69176  35.922283 35.795307 35.656998 35.958427 35.841763 35.697983
 35.93188  35.81511  35.682037 35.8736   35.741436 35.592064 35.954975
 35.803226 35.631218 35.982983 35.831833 35.671986 35.96245  35.827267
 35.700962 35.97268  35.869583 35.756756 36.03223  35.887028 35.74601
 36.038536 35.87763  35.712513 35.98348  35.859406 35.694687 36.116604
 35.94464  35.76457  36.197407 36.023357 35.853554 36.195637 36.04955
 35.927128 36.11441  35.983265 35.850338 36.172356 36.012444 35.860416
 36.073463 35.982384 35.87487  36.10382  35.986023 35.88514  36.149487
 35.98772  35.83036  36.069637 35.91056  35.764084 36.101524 35.95406
 35.80365  35.999874 35.88478  35.76728  35.976425 35.865788 35.751877
 36.061028 35.895924 35.73347  36.027    35.929478 35.803444 36.016174
 35.924694 35.834053 35.943615 35.848133 35.76868  36.03692  35.891964
 35.76745  36.1273   35.996895 35.861732 36.046173 35.980865 35.887257
 36.109722 35.987072 35.850002 36.108624 35.98382  35.882343 36.169666
 36.01767  35.836624 36.149208 35.990482 35.82735  36.138393 35.972168
 35.80978  36.16816  35.99092  35.81604  36.144672 35.96801  35.792667
 36.127354 35.993423 35.819965 36.139736 35.9818   35.79953  36.129208
 35.981976 35.814194 36.03881  35.901688 35.752792 36.158344 36.004906
 35.837955 36.146885 36.00419  35.876305 36.195404 36.047264 35.901405
 36.22702  36.083138 35.93471  36.15725  36.05068  35.90916  36.15122
 36.03416  35.90066  36.063095 35.979416 35.890274 36.184834 36.091595
 35.971413 36.19506  36.10935  35.987053 36.187954 36.0678   35.910385
 36.17687  36.085236 35.94475  36.313927 36.18673  36.004627 36.324562
 36.186165]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 11.635557174682617
Epoch: 1, Steps: 79 | Train Loss: 1.0282291 Vali Loss: 0.8912699 Test Loss: 0.8208514
Validation loss decreased (inf --> 0.891270).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 11.687686443328857
Epoch: 2, Steps: 79 | Train Loss: 0.9107448 Vali Loss: 0.8507100 Test Loss: 0.7791655
Validation loss decreased (0.891270 --> 0.850710).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 11.723935604095459
Epoch: 3, Steps: 79 | Train Loss: 0.7946743 Vali Loss: 0.5511656 Test Loss: 0.5094666
Validation loss decreased (0.850710 --> 0.551166).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 11.791515111923218
Epoch: 4, Steps: 79 | Train Loss: 0.5675895 Vali Loss: 0.4384251 Test Loss: 0.4176645
Validation loss decreased (0.551166 --> 0.438425).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 11.814152956008911
Epoch: 5, Steps: 79 | Train Loss: 0.4423067 Vali Loss: 0.3229915 Test Loss: 0.3152064
Validation loss decreased (0.438425 --> 0.322991).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 11.77768850326538
Epoch: 6, Steps: 79 | Train Loss: 0.3115060 Vali Loss: 0.2048306 Test Loss: 0.2075811
Validation loss decreased (0.322991 --> 0.204831).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 11.746965885162354
Epoch: 7, Steps: 79 | Train Loss: 0.2583216 Vali Loss: 0.1718071 Test Loss: 0.1833979
Validation loss decreased (0.204831 --> 0.171807).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 11.785767078399658
Epoch: 8, Steps: 79 | Train Loss: 0.2352318 Vali Loss: 0.1462207 Test Loss: 0.1604918
Validation loss decreased (0.171807 --> 0.146221).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 11.73001503944397
Epoch: 9, Steps: 79 | Train Loss: 0.2188489 Vali Loss: 0.1362892 Test Loss: 0.1490212
Validation loss decreased (0.146221 --> 0.136289).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 11.722559690475464
Epoch: 10, Steps: 79 | Train Loss: 0.2067616 Vali Loss: 0.1282242 Test Loss: 0.1367040
Validation loss decreased (0.136289 --> 0.128224).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 11.842494010925293
Epoch: 11, Steps: 79 | Train Loss: 0.1976365 Vali Loss: 0.1176338 Test Loss: 0.1273568
Validation loss decreased (0.128224 --> 0.117634).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 11.770084142684937
Epoch: 12, Steps: 79 | Train Loss: 0.1895488 Vali Loss: 0.1127770 Test Loss: 0.1216263
Validation loss decreased (0.117634 --> 0.112777).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 11.70922565460205
Epoch: 13, Steps: 79 | Train Loss: 0.1823744 Vali Loss: 0.1070481 Test Loss: 0.1174158
Validation loss decreased (0.112777 --> 0.107048).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 11.674082040786743
Epoch: 14, Steps: 79 | Train Loss: 0.1752861 Vali Loss: 0.1011412 Test Loss: 0.1071582
Validation loss decreased (0.107048 --> 0.101141).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 11.764193296432495
Epoch: 15, Steps: 79 | Train Loss: 0.1685500 Vali Loss: 0.0961083 Test Loss: 0.1000958
Validation loss decreased (0.101141 --> 0.096108).  Saving model ...
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 11.674011468887329
Epoch: 16, Steps: 79 | Train Loss: 0.1618387 Vali Loss: 0.0907367 Test Loss: 0.0927033
Validation loss decreased (0.096108 --> 0.090737).  Saving model ...
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 11.803623914718628
Epoch: 17, Steps: 79 | Train Loss: 0.1538179 Vali Loss: 0.0886631 Test Loss: 0.0894080
Validation loss decreased (0.090737 --> 0.088663).  Saving model ...
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 11.909698963165283
Epoch: 18, Steps: 79 | Train Loss: 0.1476003 Vali Loss: 0.0807652 Test Loss: 0.0815308
Validation loss decreased (0.088663 --> 0.080765).  Saving model ...
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 11.643708944320679
Epoch: 19, Steps: 79 | Train Loss: 0.1425288 Vali Loss: 0.0792305 Test Loss: 0.0786778
Validation loss decreased (0.080765 --> 0.079231).  Saving model ...
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 11.859153509140015
Epoch: 20, Steps: 79 | Train Loss: 0.1372750 Vali Loss: 0.0757143 Test Loss: 0.0750647
Validation loss decreased (0.079231 --> 0.075714).  Saving model ...
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 11.765233516693115
Epoch: 21, Steps: 79 | Train Loss: 0.1331004 Vali Loss: 0.0736975 Test Loss: 0.0709094
Validation loss decreased (0.075714 --> 0.073698).  Saving model ...
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 11.791878461837769
Epoch: 22, Steps: 79 | Train Loss: 0.1288763 Vali Loss: 0.0700231 Test Loss: 0.0673052
Validation loss decreased (0.073698 --> 0.070023).  Saving model ...
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 11.81071949005127
Epoch: 23, Steps: 79 | Train Loss: 0.1250939 Vali Loss: 0.0719635 Test Loss: 0.0680094
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 11.725798606872559
Epoch: 24, Steps: 79 | Train Loss: 0.1228203 Vali Loss: 0.0714699 Test Loss: 0.0668783
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 11.77055287361145
Epoch: 25, Steps: 79 | Train Loss: 0.1214705 Vali Loss: 0.0680221 Test Loss: 0.0656527
Validation loss decreased (0.070023 --> 0.068022).  Saving model ...
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 11.811298608779907
Epoch: 26, Steps: 79 | Train Loss: 0.1187045 Vali Loss: 0.0663630 Test Loss: 0.0625977
Validation loss decreased (0.068022 --> 0.066363).  Saving model ...
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 11.874703884124756
Epoch: 27, Steps: 79 | Train Loss: 0.1154449 Vali Loss: 0.0671095 Test Loss: 0.0618559
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 11.688462972640991
Epoch: 28, Steps: 79 | Train Loss: 0.1128064 Vali Loss: 0.0628028 Test Loss: 0.0587180
Validation loss decreased (0.066363 --> 0.062803).  Saving model ...
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 11.680553913116455
Epoch: 29, Steps: 79 | Train Loss: 0.1105722 Vali Loss: 0.0606177 Test Loss: 0.0557948
Validation loss decreased (0.062803 --> 0.060618).  Saving model ...
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 11.811644554138184
Epoch: 30, Steps: 79 | Train Loss: 0.1088587 Vali Loss: 0.0622760 Test Loss: 0.0568776
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 11.778026819229126
Epoch: 31, Steps: 79 | Train Loss: 0.1068277 Vali Loss: 0.0584716 Test Loss: 0.0549415
Validation loss decreased (0.060618 --> 0.058472).  Saving model ...
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 11.669329643249512
Epoch: 32, Steps: 79 | Train Loss: 0.1052990 Vali Loss: 0.0613588 Test Loss: 0.0550746
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 11.90909481048584
Epoch: 33, Steps: 79 | Train Loss: 0.1039953 Vali Loss: 0.0620144 Test Loss: 0.0544357
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 11.618766784667969
Epoch: 34, Steps: 79 | Train Loss: 0.1022283 Vali Loss: 0.0591018 Test Loss: 0.0535391
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 11.57737922668457
Epoch: 35, Steps: 79 | Train Loss: 0.1023151 Vali Loss: 0.0587693 Test Loss: 0.0524201
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 11.860661745071411
Epoch: 36, Steps: 79 | Train Loss: 0.1005611 Vali Loss: 0.0583778 Test Loss: 0.0522559
Validation loss decreased (0.058472 --> 0.058378).  Saving model ...
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 11.750784635543823
Epoch: 37, Steps: 79 | Train Loss: 0.0990153 Vali Loss: 0.0621027 Test Loss: 0.0549490
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 11.694151163101196
Epoch: 38, Steps: 79 | Train Loss: 0.0986150 Vali Loss: 0.0576944 Test Loss: 0.0511343
Validation loss decreased (0.058378 --> 0.057694).  Saving model ...
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 11.707338571548462
Epoch: 39, Steps: 79 | Train Loss: 0.0980405 Vali Loss: 0.0609587 Test Loss: 0.0549002
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 11.780703783035278
Epoch: 40, Steps: 79 | Train Loss: 0.0973198 Vali Loss: 0.0560828 Test Loss: 0.0508150
Validation loss decreased (0.057694 --> 0.056083).  Saving model ...
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 11.715060234069824
Epoch: 41, Steps: 79 | Train Loss: 0.0962561 Vali Loss: 0.0602301 Test Loss: 0.0521640
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009402
Epoch: 42 cost time: 11.838676929473877
Epoch: 42, Steps: 79 | Train Loss: 0.0956559 Vali Loss: 0.0590768 Test Loss: 0.0522901
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009291
Epoch: 43 cost time: 11.756647109985352
Epoch: 43, Steps: 79 | Train Loss: 0.0950321 Vali Loss: 0.0591042 Test Loss: 0.0514983
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009171
Epoch: 44 cost time: 11.646092653274536
Epoch: 44, Steps: 79 | Train Loss: 0.0946367 Vali Loss: 0.0570895 Test Loss: 0.0510939
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009043
Epoch: 45 cost time: 11.777166843414307
Epoch: 45, Steps: 79 | Train Loss: 0.0937319 Vali Loss: 0.0596714 Test Loss: 0.0532246
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0008907
Epoch: 46 cost time: 11.834067344665527
Epoch: 46, Steps: 79 | Train Loss: 0.0933387 Vali Loss: 0.0611332 Test Loss: 0.0544891
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0008763
Epoch: 47 cost time: 11.73786449432373
Epoch: 47, Steps: 79 | Train Loss: 0.0924079 Vali Loss: 0.0577045 Test Loss: 0.0510404
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0008612
Epoch: 48 cost time: 11.900174617767334
Epoch: 48, Steps: 79 | Train Loss: 0.0920758 Vali Loss: 0.0585584 Test Loss: 0.0523103
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0008453
Epoch: 49 cost time: 11.767801284790039
Epoch: 49, Steps: 79 | Train Loss: 0.0913168 Vali Loss: 0.0569552 Test Loss: 0.0506172
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0008288
Epoch: 50 cost time: 11.601258993148804
Epoch: 50, Steps: 79 | Train Loss: 0.0905551 Vali Loss: 0.0608393 Test Loss: 0.0541398
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0008115
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.11442097276449203, mae:0.2585163414478302, rmse:0.3382616937160492, mape:0.009216440841555595, mspe:0.00014553031360264868, rse:0.23315030336380005, r2_score:0.9430653977974095, acc:0.9907835591584444
corr: [37.953747 37.919285 37.821316 37.709587 37.728016 37.671455 37.620804
 37.55964  37.67186  37.62893  37.5702   37.53444  37.61163  37.561333
 37.523624 37.496784 37.668667 37.61567  37.56604  37.494953 37.601414
 37.522964 37.482384 37.424763 37.59435  37.519234 37.432323 37.33354
 37.583073 37.519375 37.45459  37.35769  37.61537  37.51215  37.413082
 37.314896 37.66047  37.592293 37.512577 37.40284  37.597446 37.54789
 37.492523 37.44034  37.57991  37.517292 37.439526 37.36266  37.588837
 37.53153  37.468338 37.36916  37.519035 37.43543  37.35407  37.27536
 37.467945 37.406746 37.339268 37.260437 37.494003 37.410362 37.3242
 37.2372   37.373257 37.34671  37.29786  37.22116  37.361225 37.29894
 37.212727 37.11808  37.306408 37.2495   37.17032  37.09233  37.238712
 37.19947  37.159443 37.091213 37.06988  37.012196 36.944717 36.84224
 37.061104 37.024162 36.987873 36.90159  36.976063 36.90945  36.81427
 36.682743 36.910374 36.876167 36.79794  36.70644  36.95852  36.91062
 36.837852 36.767776 36.975204 36.94351  36.90404  36.82053  36.888786
 36.86593  36.819515 36.717125 36.934597 36.85437  36.775295 36.66956
 36.877968 36.832214 36.74744  36.629906 36.81512  36.762184 36.691654
 36.60603  36.708298 36.649258 36.589893 36.515587 36.678226 36.674538
 36.648727 36.58676  36.647743 36.6695   36.65415  36.59509  36.497963
 36.5082   36.480106 36.428654 36.422176 36.4106   36.399002 36.36878
 36.244667 36.275124 36.267166 36.23613  36.20088  36.234074 36.22781
 36.194332 36.12014  36.113808 36.089325 36.04252  36.15667  36.139328
 36.0917   36.02216  36.132107 36.090603 36.01001  35.89174  36.122746
 36.064102 35.97002  35.86167  36.154835 36.10279  36.033703 35.96299
 36.163197 36.13212  36.08205  36.0281   36.277954 36.235264 36.173717
 36.11098  36.267708 36.23038  36.1725   36.11504  36.199852 36.150024
 36.101715 36.052174 36.106506 36.087082 36.045895 35.97938  36.041676
 36.016308 35.95767  35.90477  36.084526 36.040962 35.999454 35.9539
 36.219955 36.175396 36.106575 36.03912  36.179913 36.14787  36.097195
 36.03739  36.148724 36.117645 36.060204 35.985268 36.25211  36.225338
 36.188297 36.11618  36.173145 36.12399  36.047306 35.967777 36.172512
 36.124054 36.04787  35.950386 36.130745 36.077217 36.027206 35.966637
 36.137146 36.10438  36.053307 35.984417 36.22685  36.155674 36.095856
 36.031387 36.23765  36.175632 36.10076  36.030193 36.19649  36.13771
 36.074913 35.986217 36.20672  36.13192  36.05234  36.007042 36.26134
 36.187336 36.08395  35.996624 36.30693  36.24894  36.17274  36.09776
 36.3938   36.342213 36.26692  36.191555 36.318764 36.24896  36.174404
 36.075382 36.24698  36.16935  36.08351  36.00634  36.255527 36.176254
 36.08564  36.006355 36.22834  36.158745 36.074234 35.997887 36.232357
 36.16684  36.090965 36.001076 36.23512  36.15817  36.08773  36.0198
 36.201088 36.13715  36.04464  35.942757 36.246857 36.14937  36.042587
 35.93156  36.37661  36.247593 36.126987 36.023155 36.350643 36.258392
 36.177856 36.089535 36.319973 36.25877  36.179    36.12008  36.40678
 36.311756 36.224865 36.146057 36.293056 36.227055 36.135986 36.048557
 36.278484 36.214664 36.143402 36.05523  36.240116 36.190105 36.13746
 36.074654 36.207336 36.184498 36.13649  36.077915 36.328747 36.276333
 36.216175 36.133724 36.295746 36.234554 36.158375 36.07358  36.366405
 36.300293 36.230274 36.14825  36.40683  36.310425 36.195507 36.06465
 36.39293  36.28418  36.17341  36.036407 36.38555  36.27075  36.149223
 36.015987 36.39354  36.286385 36.15577  36.033405 36.507214 36.442993
 36.340595 36.205803 36.45241  36.404507 36.332325 36.224903 36.556282
 36.5062   36.43126  36.31548  36.573933 36.55105  36.48015  36.389034
 36.8102  ]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 9.563902854919434
Epoch: 1, Steps: 79 | Train Loss: 1.0370390 Vali Loss: 0.9030918 Test Loss: 0.8274392
Validation loss decreased (inf --> 0.903092).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 9.488229990005493
Epoch: 2, Steps: 79 | Train Loss: 0.9200550 Vali Loss: 0.8299932 Test Loss: 0.7576327
Validation loss decreased (0.903092 --> 0.829993).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 9.665580749511719
Epoch: 3, Steps: 79 | Train Loss: 0.7828816 Vali Loss: 0.5918995 Test Loss: 0.5455041
Validation loss decreased (0.829993 --> 0.591899).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 9.593948602676392
Epoch: 4, Steps: 79 | Train Loss: 0.5844293 Vali Loss: 0.4848299 Test Loss: 0.4589384
Validation loss decreased (0.591899 --> 0.484830).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 9.550685405731201
Epoch: 5, Steps: 79 | Train Loss: 0.4970032 Vali Loss: 0.3848086 Test Loss: 0.3746315
Validation loss decreased (0.484830 --> 0.384809).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 9.454215288162231
Epoch: 6, Steps: 79 | Train Loss: 0.3712558 Vali Loss: 0.2396888 Test Loss: 0.2356493
Validation loss decreased (0.384809 --> 0.239689).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 9.719017267227173
Epoch: 7, Steps: 79 | Train Loss: 0.2792314 Vali Loss: 0.1832396 Test Loss: 0.1939039
Validation loss decreased (0.239689 --> 0.183240).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 9.562278985977173
Epoch: 8, Steps: 79 | Train Loss: 0.2498150 Vali Loss: 0.1518859 Test Loss: 0.1699633
Validation loss decreased (0.183240 --> 0.151886).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 9.775128602981567
Epoch: 9, Steps: 79 | Train Loss: 0.2329136 Vali Loss: 0.1459398 Test Loss: 0.1613969
Validation loss decreased (0.151886 --> 0.145940).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 9.497276782989502
Epoch: 10, Steps: 79 | Train Loss: 0.2218727 Vali Loss: 0.1368349 Test Loss: 0.1503052
Validation loss decreased (0.145940 --> 0.136835).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 9.60619831085205
Epoch: 11, Steps: 79 | Train Loss: 0.2134427 Vali Loss: 0.1301360 Test Loss: 0.1469077
Validation loss decreased (0.136835 --> 0.130136).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 9.749773979187012
Epoch: 12, Steps: 79 | Train Loss: 0.2054388 Vali Loss: 0.1256147 Test Loss: 0.1404596
Validation loss decreased (0.130136 --> 0.125615).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 9.58740520477295
Epoch: 13, Steps: 79 | Train Loss: 0.1999735 Vali Loss: 0.1231304 Test Loss: 0.1398463
Validation loss decreased (0.125615 --> 0.123130).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 9.605701208114624
Epoch: 14, Steps: 79 | Train Loss: 0.1929849 Vali Loss: 0.1191573 Test Loss: 0.1328543
Validation loss decreased (0.123130 --> 0.119157).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 9.687920331954956
Epoch: 15, Steps: 79 | Train Loss: 0.1882418 Vali Loss: 0.1188970 Test Loss: 0.1328558
Validation loss decreased (0.119157 --> 0.118897).  Saving model ...
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 9.676275491714478
Epoch: 16, Steps: 79 | Train Loss: 0.1833082 Vali Loss: 0.1181036 Test Loss: 0.1296062
Validation loss decreased (0.118897 --> 0.118104).  Saving model ...
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 9.561351776123047
Epoch: 17, Steps: 79 | Train Loss: 0.1780908 Vali Loss: 0.1157704 Test Loss: 0.1252820
Validation loss decreased (0.118104 --> 0.115770).  Saving model ...
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 9.609410762786865
Epoch: 18, Steps: 79 | Train Loss: 0.1756118 Vali Loss: 0.1149978 Test Loss: 0.1206560
Validation loss decreased (0.115770 --> 0.114998).  Saving model ...
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 9.769948244094849
Epoch: 19, Steps: 79 | Train Loss: 0.1709589 Vali Loss: 0.1151664 Test Loss: 0.1233133
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 9.48267936706543
Epoch: 20, Steps: 79 | Train Loss: 0.1674787 Vali Loss: 0.1124115 Test Loss: 0.1168225
Validation loss decreased (0.114998 --> 0.112412).  Saving model ...
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 9.646289348602295
Epoch: 21, Steps: 79 | Train Loss: 0.1634615 Vali Loss: 0.1082110 Test Loss: 0.1117293
Validation loss decreased (0.112412 --> 0.108211).  Saving model ...
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 9.412230730056763
Epoch: 22, Steps: 79 | Train Loss: 0.1585387 Vali Loss: 0.1019768 Test Loss: 0.1053208
Validation loss decreased (0.108211 --> 0.101977).  Saving model ...
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 9.72532320022583
Epoch: 23, Steps: 79 | Train Loss: 0.1525215 Vali Loss: 0.1024273 Test Loss: 0.1023454
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 9.499404907226562
Epoch: 24, Steps: 79 | Train Loss: 0.1475261 Vali Loss: 0.0941593 Test Loss: 0.0959987
Validation loss decreased (0.101977 --> 0.094159).  Saving model ...
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 9.663752555847168
Epoch: 25, Steps: 79 | Train Loss: 0.1436634 Vali Loss: 0.0912878 Test Loss: 0.0911069
Validation loss decreased (0.094159 --> 0.091288).  Saving model ...
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 9.43048357963562
Epoch: 26, Steps: 79 | Train Loss: 0.1404628 Vali Loss: 0.0881216 Test Loss: 0.0870978
Validation loss decreased (0.091288 --> 0.088122).  Saving model ...
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 9.595978021621704
Epoch: 27, Steps: 79 | Train Loss: 0.1366484 Vali Loss: 0.0848527 Test Loss: 0.0834604
Validation loss decreased (0.088122 --> 0.084853).  Saving model ...
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 9.566308736801147
Epoch: 28, Steps: 79 | Train Loss: 0.1340384 Vali Loss: 0.0856945 Test Loss: 0.0833811
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 9.644552946090698
Epoch: 29, Steps: 79 | Train Loss: 0.1319404 Vali Loss: 0.0852068 Test Loss: 0.0810078
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 9.648855924606323
Epoch: 30, Steps: 79 | Train Loss: 0.1294903 Vali Loss: 0.0874778 Test Loss: 0.0858494
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 9.792091131210327
Epoch: 31, Steps: 79 | Train Loss: 0.1281059 Vali Loss: 0.0854139 Test Loss: 0.0817764
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 9.551185369491577
Epoch: 32, Steps: 79 | Train Loss: 0.1256286 Vali Loss: 0.0811501 Test Loss: 0.0779467
Validation loss decreased (0.084853 --> 0.081150).  Saving model ...
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 9.698421478271484
Epoch: 33, Steps: 79 | Train Loss: 0.1237990 Vali Loss: 0.0786307 Test Loss: 0.0755895
Validation loss decreased (0.081150 --> 0.078631).  Saving model ...
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 9.48136281967163
Epoch: 34, Steps: 79 | Train Loss: 0.1221445 Vali Loss: 0.0797935 Test Loss: 0.0755599
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 9.718453407287598
Epoch: 35, Steps: 79 | Train Loss: 0.1201545 Vali Loss: 0.0793341 Test Loss: 0.0762627
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 9.566702365875244
Epoch: 36, Steps: 79 | Train Loss: 0.1177952 Vali Loss: 0.0767831 Test Loss: 0.0729416
Validation loss decreased (0.078631 --> 0.076783).  Saving model ...
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 9.754379987716675
Epoch: 37, Steps: 79 | Train Loss: 0.1154719 Vali Loss: 0.0753565 Test Loss: 0.0715703
Validation loss decreased (0.076783 --> 0.075357).  Saving model ...
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 9.370614290237427
Epoch: 38, Steps: 79 | Train Loss: 0.1122455 Vali Loss: 0.0768922 Test Loss: 0.0694484
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 9.64357614517212
Epoch: 39, Steps: 79 | Train Loss: 0.1101008 Vali Loss: 0.0752566 Test Loss: 0.0680947
Validation loss decreased (0.075357 --> 0.075257).  Saving model ...
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 9.524410963058472
Epoch: 40, Steps: 79 | Train Loss: 0.1081910 Vali Loss: 0.0755126 Test Loss: 0.0679958
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 9.635492086410522
Epoch: 41, Steps: 79 | Train Loss: 0.1061799 Vali Loss: 0.0722471 Test Loss: 0.0651970
Validation loss decreased (0.075257 --> 0.072247).  Saving model ...
Adjusting learning rate to: 0.0009402
Epoch: 42 cost time: 9.444519519805908
Epoch: 42, Steps: 79 | Train Loss: 0.1050456 Vali Loss: 0.0691901 Test Loss: 0.0637474
Validation loss decreased (0.072247 --> 0.069190).  Saving model ...
Adjusting learning rate to: 0.0009291
Epoch: 43 cost time: 9.587919473648071
Epoch: 43, Steps: 79 | Train Loss: 0.1038986 Vali Loss: 0.0683279 Test Loss: 0.0635834
Validation loss decreased (0.069190 --> 0.068328).  Saving model ...
Adjusting learning rate to: 0.0009171
Epoch: 44 cost time: 9.52631425857544
Epoch: 44, Steps: 79 | Train Loss: 0.1028396 Vali Loss: 0.0698969 Test Loss: 0.0639768
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009043
Epoch: 45 cost time: 9.397677898406982
Epoch: 45, Steps: 79 | Train Loss: 0.1016223 Vali Loss: 0.0718525 Test Loss: 0.0650787
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0008907
Epoch: 46 cost time: 9.669161081314087
Epoch: 46, Steps: 79 | Train Loss: 0.1008795 Vali Loss: 0.0691400 Test Loss: 0.0639438
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0008763
Epoch: 47 cost time: 9.561699151992798
Epoch: 47, Steps: 79 | Train Loss: 0.1001085 Vali Loss: 0.0711354 Test Loss: 0.0647758
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0008612
Epoch: 48 cost time: 9.76012659072876
Epoch: 48, Steps: 79 | Train Loss: 0.0993655 Vali Loss: 0.0701611 Test Loss: 0.0638415
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0008453
Epoch: 49 cost time: 9.476222276687622
Epoch: 49, Steps: 79 | Train Loss: 0.0981427 Vali Loss: 0.0717966 Test Loss: 0.0646270
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0008288
Epoch: 50 cost time: 9.800036191940308
Epoch: 50, Steps: 79 | Train Loss: 0.0978436 Vali Loss: 0.0697262 Test Loss: 0.0642903
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0008115
Epoch: 51 cost time: 9.514333486557007
Epoch: 51, Steps: 79 | Train Loss: 0.0971665 Vali Loss: 0.0700530 Test Loss: 0.0645468
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0007937
Epoch: 52 cost time: 9.59108281135559
Epoch: 52, Steps: 79 | Train Loss: 0.0961036 Vali Loss: 0.0714212 Test Loss: 0.0641909
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0007752
Epoch: 53 cost time: 9.552348136901855
Epoch: 53, Steps: 79 | Train Loss: 0.0952972 Vali Loss: 0.0711835 Test Loss: 0.0656209
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0007562
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.13837599754333496, mae:0.286484032869339, rmse:0.37198925018310547, mape:0.010229858569800854, mspe:0.00017741494229994714, rse:0.2563973665237427, r2_score:0.9298767773187274, acc:0.9897701414301991
corr: [38.381752 38.380894 38.267757 38.192566 38.10034  38.254654 38.20775
 38.06087  37.99572  37.94731  38.078514 38.04747  37.921932 37.86336
 37.825066 38.000996 37.967953 37.85523  37.80434  37.745705 37.971413
 37.93588  37.827602 37.78828  37.750458 37.952858 37.952457 37.85683
 37.824947 37.795673 37.92382  37.901352 37.815643 37.81157  37.77978
 37.87052  37.855286 37.768402 37.76321  37.738037 37.85466  37.841835
 37.7633   37.74614  37.75002  37.782784 37.76082  37.669586 37.645535
 37.64939  37.803856 37.738796 37.631332 37.606037 37.608124 37.751915
 37.703228 37.6151   37.58248  37.577023 37.78675  37.726284 37.607502
 37.552547 37.507637 37.59904  37.508    37.354973 37.2968   37.240738
 37.423542 37.36923  37.240368 37.19574  37.1682   37.36353  37.265656
 37.129288 37.089172 37.054535 37.259182 37.19712  37.084034 37.073513
 37.07278  37.19889  37.155975 37.074196 37.05442  37.007027 37.04602
 37.006634 36.90549  36.896637 36.887974 36.880657 36.875736 36.797756
 36.81081  36.80562  36.84847  36.85159  36.76338  36.758835 36.737564
 36.85386  36.83816  36.767635 36.78737  36.767784 36.783806 36.76309
 36.675503 36.663857 36.631443 36.762524 36.76551  36.68484  36.6821
 36.665703 36.580677 36.579376 36.474842 36.462574 36.42037  36.532993
 36.51147  36.441406 36.42929  36.4123   36.405933 36.37886  36.29446
 36.292126 36.285778 36.309784 36.31939  36.285194 36.300877 36.304756
 36.3555   36.348934 36.273224 36.270584 36.264    36.2565   36.27127
 36.22925  36.244446 36.252357 36.255444 36.27596  36.23999  36.25972
 36.254017 36.19624  36.19331  36.132202 36.155743 36.16704  36.069813
 36.09275  36.07223  36.126175 36.11105  36.05949  36.076496 36.012726
 36.017708 36.016525 36.069626 36.092567 36.057816 36.084873 36.08229
 36.153145 36.18102  36.134037 36.15381  36.14141  36.26494  36.256256
 36.19493  36.18912  36.156227 36.34358  36.326134 36.2758   36.270893
 36.235657 36.365543 36.349113 36.258514 36.253704 36.22186  36.358337
 36.358917 36.275806 36.285725 36.26339  36.25543  36.25246  36.187313
 36.201546 36.205715 36.34908  36.33425  36.28377  36.28741  36.289616
 36.31775  36.304123 36.228825 36.20697  36.184464 36.28295  36.28203
 36.205933 36.173088 36.141987 36.232018 36.21463  36.145386 36.139748
 36.119335 36.32636  36.313408 36.224876 36.19684  36.16977  36.377155
 36.38183  36.316696 36.27658  36.215218 36.37953  36.369617 36.302963
 36.27689  36.254322 36.412754 36.38847  36.320496 36.310135 36.287956
 36.360405 36.362015 36.298203 36.287342 36.25914  36.364555 36.3664
 36.32681  36.345417 36.344757 36.434254 36.426125 36.364586 36.366196
 36.361153 36.486084 36.474632 36.404842 36.40735  36.40286  36.54337
 36.540535 36.478798 36.46137  36.44419  36.575954 36.551083 36.461006
 36.431736 36.422714 36.5148   36.48583  36.3977   36.37681  36.354496
 36.42799  36.397034 36.313927 36.30346  36.303894 36.47368  36.450928
 36.35881  36.335102 36.282166 36.527367 36.476585 36.371117 36.359604
 36.341396 36.54223  36.505318 36.399765 36.36297  36.32383  36.588913
 36.53832  36.40883  36.358597 36.317127 36.51345  36.476994 36.371723
 36.349895 36.307816 36.475044 36.45597  36.37213  36.36592  36.352707
 36.581726 36.574905 36.484295 36.459522 36.419643 36.58384  36.561024
 36.469986 36.4414   36.429047 36.62067  36.586876 36.5007   36.46879
 36.425926 36.599537 36.549072 36.44893  36.42842  36.406487 36.659954
 36.621334 36.53845  36.527264 36.495544 36.58564  36.57592  36.47333
 36.46692  36.441204 36.71839  36.709274 36.618256 36.616516 36.619408
 36.909306 36.87691  36.780247 36.76832  36.77859  36.903355 36.902267
 36.84602  36.91462  36.92167  37.188488 37.23969  37.19365  37.207817
 37.20717 ]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 8.24132490158081
Epoch: 1, Steps: 79 | Train Loss: 1.0429990 Vali Loss: 0.9044353 Test Loss: 0.8244062
Validation loss decreased (inf --> 0.904435).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 8.4035804271698
Epoch: 2, Steps: 79 | Train Loss: 0.9282815 Vali Loss: 0.8406423 Test Loss: 0.7743596
Validation loss decreased (0.904435 --> 0.840642).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 8.180613279342651
Epoch: 3, Steps: 79 | Train Loss: 0.8134525 Vali Loss: 0.6421933 Test Loss: 0.5926285
Validation loss decreased (0.840642 --> 0.642193).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 8.357522964477539
Epoch: 4, Steps: 79 | Train Loss: 0.6187107 Vali Loss: 0.5277241 Test Loss: 0.4931860
Validation loss decreased (0.642193 --> 0.527724).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 8.118996620178223
Epoch: 5, Steps: 79 | Train Loss: 0.5145642 Vali Loss: 0.4303077 Test Loss: 0.4216507
Validation loss decreased (0.527724 --> 0.430308).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 8.316564798355103
Epoch: 6, Steps: 79 | Train Loss: 0.4484802 Vali Loss: 0.3527332 Test Loss: 0.3409507
Validation loss decreased (0.430308 --> 0.352733).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 8.355307579040527
Epoch: 7, Steps: 79 | Train Loss: 0.3326615 Vali Loss: 0.2261831 Test Loss: 0.2189944
Validation loss decreased (0.352733 --> 0.226183).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 8.475834846496582
Epoch: 8, Steps: 79 | Train Loss: 0.2654037 Vali Loss: 0.1769600 Test Loss: 0.1806268
Validation loss decreased (0.226183 --> 0.176960).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 8.167858362197876
Epoch: 9, Steps: 79 | Train Loss: 0.2391109 Vali Loss: 0.1508294 Test Loss: 0.1612740
Validation loss decreased (0.176960 --> 0.150829).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 8.225517272949219
Epoch: 10, Steps: 79 | Train Loss: 0.2236748 Vali Loss: 0.1384762 Test Loss: 0.1502391
Validation loss decreased (0.150829 --> 0.138476).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 8.305144309997559
Epoch: 11, Steps: 79 | Train Loss: 0.2106665 Vali Loss: 0.1210972 Test Loss: 0.1380328
Validation loss decreased (0.138476 --> 0.121097).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 8.29206109046936
Epoch: 12, Steps: 79 | Train Loss: 0.1980739 Vali Loss: 0.1161034 Test Loss: 0.1240009
Validation loss decreased (0.121097 --> 0.116103).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 8.328702449798584
Epoch: 13, Steps: 79 | Train Loss: 0.1870181 Vali Loss: 0.1065946 Test Loss: 0.1154635
Validation loss decreased (0.116103 --> 0.106595).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 8.222837686538696
Epoch: 14, Steps: 79 | Train Loss: 0.1784555 Vali Loss: 0.0985066 Test Loss: 0.1070512
Validation loss decreased (0.106595 --> 0.098507).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 8.097018718719482
Epoch: 15, Steps: 79 | Train Loss: 0.1739203 Vali Loss: 0.0962701 Test Loss: 0.1070476
Validation loss decreased (0.098507 --> 0.096270).  Saving model ...
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 8.005651712417603
Epoch: 16, Steps: 79 | Train Loss: 0.1692922 Vali Loss: 0.0981028 Test Loss: 0.1030877
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 8.288668394088745
Epoch: 17, Steps: 79 | Train Loss: 0.1655292 Vali Loss: 0.0976209 Test Loss: 0.0981038
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 8.202081680297852
Epoch: 18, Steps: 79 | Train Loss: 0.1612196 Vali Loss: 0.0940877 Test Loss: 0.0978640
Validation loss decreased (0.096270 --> 0.094088).  Saving model ...
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 8.230807304382324
Epoch: 19, Steps: 79 | Train Loss: 0.1582512 Vali Loss: 0.0929278 Test Loss: 0.0951683
Validation loss decreased (0.094088 --> 0.092928).  Saving model ...
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 8.048569440841675
Epoch: 20, Steps: 79 | Train Loss: 0.1566250 Vali Loss: 0.0959907 Test Loss: 0.0953206
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 8.555554628372192
Epoch: 21, Steps: 79 | Train Loss: 0.1529547 Vali Loss: 0.0895324 Test Loss: 0.0934295
Validation loss decreased (0.092928 --> 0.089532).  Saving model ...
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 8.184855699539185
Epoch: 22, Steps: 79 | Train Loss: 0.1505967 Vali Loss: 0.0884184 Test Loss: 0.0907164
Validation loss decreased (0.089532 --> 0.088418).  Saving model ...
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 8.232145547866821
Epoch: 23, Steps: 79 | Train Loss: 0.1482616 Vali Loss: 0.0892381 Test Loss: 0.0882734
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 8.304115533828735
Epoch: 24, Steps: 79 | Train Loss: 0.1439360 Vali Loss: 0.0899002 Test Loss: 0.0889496
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 8.202810764312744
Epoch: 25, Steps: 79 | Train Loss: 0.1421676 Vali Loss: 0.0851762 Test Loss: 0.0860608
Validation loss decreased (0.088418 --> 0.085176).  Saving model ...
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 8.357219219207764
Epoch: 26, Steps: 79 | Train Loss: 0.1395030 Vali Loss: 0.0917576 Test Loss: 0.0928178
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 8.235982418060303
Epoch: 27, Steps: 79 | Train Loss: 0.1378872 Vali Loss: 0.0849636 Test Loss: 0.0835371
Validation loss decreased (0.085176 --> 0.084964).  Saving model ...
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 8.39705228805542
Epoch: 28, Steps: 79 | Train Loss: 0.1350945 Vali Loss: 0.0865110 Test Loss: 0.0846036
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 8.333082675933838
Epoch: 29, Steps: 79 | Train Loss: 0.1324420 Vali Loss: 0.0833191 Test Loss: 0.0803166
Validation loss decreased (0.084964 --> 0.083319).  Saving model ...
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 8.086868286132812
Epoch: 30, Steps: 79 | Train Loss: 0.1301347 Vali Loss: 0.0836535 Test Loss: 0.0805154
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 8.249907970428467
Epoch: 31, Steps: 79 | Train Loss: 0.1274272 Vali Loss: 0.0802522 Test Loss: 0.0754169
Validation loss decreased (0.083319 --> 0.080252).  Saving model ...
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 8.392794609069824
Epoch: 32, Steps: 79 | Train Loss: 0.1252440 Vali Loss: 0.0784467 Test Loss: 0.0747987
Validation loss decreased (0.080252 --> 0.078447).  Saving model ...
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 8.268346071243286
Epoch: 33, Steps: 79 | Train Loss: 0.1227135 Vali Loss: 0.0779971 Test Loss: 0.0740509
Validation loss decreased (0.078447 --> 0.077997).  Saving model ...
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 8.239153623580933
Epoch: 34, Steps: 79 | Train Loss: 0.1201746 Vali Loss: 0.0789366 Test Loss: 0.0731442
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 8.217694282531738
Epoch: 35, Steps: 79 | Train Loss: 0.1187028 Vali Loss: 0.0757043 Test Loss: 0.0718327
Validation loss decreased (0.077997 --> 0.075704).  Saving model ...
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 8.313164234161377
Epoch: 36, Steps: 79 | Train Loss: 0.1169501 Vali Loss: 0.0751179 Test Loss: 0.0700056
Validation loss decreased (0.075704 --> 0.075118).  Saving model ...
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 8.22652792930603
Epoch: 37, Steps: 79 | Train Loss: 0.1150947 Vali Loss: 0.0748146 Test Loss: 0.0690388
Validation loss decreased (0.075118 --> 0.074815).  Saving model ...
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 8.203792810440063
Epoch: 38, Steps: 79 | Train Loss: 0.1134137 Vali Loss: 0.0750870 Test Loss: 0.0684827
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 8.200583934783936
Epoch: 39, Steps: 79 | Train Loss: 0.1116204 Vali Loss: 0.0717528 Test Loss: 0.0660448
Validation loss decreased (0.074815 --> 0.071753).  Saving model ...
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 8.272672653198242
Epoch: 40, Steps: 79 | Train Loss: 0.1104874 Vali Loss: 0.0743509 Test Loss: 0.0667006
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 7.97660756111145
Epoch: 41, Steps: 79 | Train Loss: 0.1088208 Vali Loss: 0.0723426 Test Loss: 0.0652831
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009402
Epoch: 42 cost time: 8.34081768989563
Epoch: 42, Steps: 79 | Train Loss: 0.1074247 Vali Loss: 0.0697597 Test Loss: 0.0630328
Validation loss decreased (0.071753 --> 0.069760).  Saving model ...
Adjusting learning rate to: 0.0009291
Epoch: 43 cost time: 8.17510437965393
Epoch: 43, Steps: 79 | Train Loss: 0.1061742 Vali Loss: 0.0667709 Test Loss: 0.0613805
Validation loss decreased (0.069760 --> 0.066771).  Saving model ...
Adjusting learning rate to: 0.0009171
Epoch: 44 cost time: 8.4085214138031
Epoch: 44, Steps: 79 | Train Loss: 0.1047524 Vali Loss: 0.0686210 Test Loss: 0.0618639
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009043
Epoch: 45 cost time: 8.273719072341919
Epoch: 45, Steps: 79 | Train Loss: 0.1037717 Vali Loss: 0.0688609 Test Loss: 0.0607148
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0008907
Epoch: 46 cost time: 8.40043020248413
Epoch: 46, Steps: 79 | Train Loss: 0.1026763 Vali Loss: 0.0689477 Test Loss: 0.0622502
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0008763
Epoch: 47 cost time: 8.099668264389038
Epoch: 47, Steps: 79 | Train Loss: 0.1022647 Vali Loss: 0.0699415 Test Loss: 0.0621253
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0008612
Epoch: 48 cost time: 8.438313484191895
Epoch: 48, Steps: 79 | Train Loss: 0.1014289 Vali Loss: 0.0684662 Test Loss: 0.0612302
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0008453
Epoch: 49 cost time: 8.136074304580688
Epoch: 49, Steps: 79 | Train Loss: 0.1006543 Vali Loss: 0.0675914 Test Loss: 0.0603593
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0008288
Epoch: 50 cost time: 8.548238754272461
Epoch: 50, Steps: 79 | Train Loss: 0.0997224 Vali Loss: 0.0694744 Test Loss: 0.0615514
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0008115
Epoch: 51 cost time: 8.148510217666626
Epoch: 51, Steps: 79 | Train Loss: 0.0996935 Vali Loss: 0.0687150 Test Loss: 0.0606639
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0007937
Epoch: 52 cost time: 8.31261658668518
Epoch: 52, Steps: 79 | Train Loss: 0.0987297 Vali Loss: 0.0692044 Test Loss: 0.0622089
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0007752
Epoch: 53 cost time: 8.108259439468384
Epoch: 53, Steps: 79 | Train Loss: 0.0981021 Vali Loss: 0.0678357 Test Loss: 0.0607032
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0007562
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.13520513474941254, mae:0.2825171947479248, rmse:0.36770251393318176, mape:0.010066162794828415, mspe:0.00017179037968162447, rse:0.2534426748752594, r2_score:0.9299560088503397, acc:0.9899338372051716
corr: [38.083633 38.068947 37.9068   37.70848  37.49294  37.290855 37.711643
 37.69079  37.577904 37.431572 37.27455  37.150543 37.59944  37.58555
 37.483013 37.37187  37.24653  37.17537  37.591194 37.58485  37.514786
 37.420166 37.298775 37.211975 37.634388 37.63973  37.53556  37.420727
 37.299164 37.21724  37.708652 37.698536 37.60996  37.507214 37.37352
 37.284927 37.644207 37.699295 37.66325  37.57874  37.48008  37.405094
 37.752163 37.78497  37.72771  37.63498  37.512264 37.40112  37.720814
 37.725155 37.668316 37.59224  37.483215 37.407345 37.74205  37.739594
 37.683266 37.605484 37.49375  37.410316 37.768738 37.775494 37.705498
 37.623783 37.501785 37.39735  37.728256 37.689888 37.58833  37.484272
 37.359795 37.269543 37.53819  37.528122 37.44123  37.324512 37.164135
 37.05619  37.313118 37.30139  37.22599  37.12777  37.00415  36.899715
 37.145397 37.145157 37.082245 36.99879  36.902878 36.821762 37.024437
 37.05163  37.01292  36.95069  36.859966 36.801674 36.96292  37.018993
 36.99911  36.94951  36.859585 36.77538  37.007587 37.052563 37.00998
 36.942528 36.827717 36.748222 37.032047 37.051086 36.98684  36.905975
 36.7781   36.656815 36.97763  36.97681  36.926167 36.842255 36.69849
 36.589127 36.899757 36.88775  36.840588 36.747463 36.629326 36.55407
 36.68997  36.730003 36.669678 36.607365 36.514    36.430443 36.600117
 36.637955 36.62215  36.568295 36.46846  36.36764  36.403156 36.450214
 36.425068 36.36536  36.253025 36.150536 36.24663  36.284206 36.259964
 36.21923  36.11821  36.0243   36.185436 36.22048  36.2052   36.15765
 36.077682 35.99023  36.14289  36.17372  36.149017 36.107586 36.027443
 35.960464 36.03382  36.07861  36.07963  36.056072 36.002705 35.954258
 36.136166 36.19716  36.19467  36.16346  36.10637  36.063366 36.189774
 36.246952 36.2467   36.240387 36.164833 36.091213 36.196133 36.257835
 36.2497   36.223446 36.155693 36.10169  36.233887 36.285355 36.287567
 36.250896 36.157066 36.088356 36.29825  36.347885 36.32879  36.288445
 36.21317  36.134995 36.276905 36.278404 36.251003 36.214787 36.14836
 36.10985  36.137447 36.13294  36.093586 36.033875 35.978237 35.936695
 36.18306  36.210793 36.18478  36.129597 36.06412  36.042435 36.236687
 36.25995  36.217514 36.145264 36.047165 35.980167 36.333942 36.328342
 36.27969  36.22478  36.154205 36.100822 36.399727 36.386627 36.333412
 36.271175 36.16875  36.095722 36.267246 36.280487 36.238876 36.17249
 36.086796 36.02058  36.148838 36.169144 36.15083  36.1335   36.073456
 36.03663  36.179886 36.183586 36.133785 36.08432  36.006042 35.95213
 36.206245 36.20527  36.153687 36.112556 36.01198  35.937164 36.228016
 36.223442 36.197582 36.14891  36.066734 35.99612  36.24694  36.242676
 36.19807  36.14989  36.09521  36.05662  36.23997  36.250683 36.198143
 36.13268  36.0671   36.026814 36.255753 36.253784 36.195805 36.160267
 36.08507  36.05735  36.256657 36.26208  36.247585 36.212685 36.14576
 36.111526 36.281155 36.30463  36.282497 36.22853  36.161034 36.12613
 36.355846 36.335815 36.290627 36.227966 36.130447 36.07426  36.386776
 36.400078 36.380077 36.34869  36.28569  36.257107 36.472572 36.451294
 36.39471  36.342308 36.269627 36.18918  36.460426 36.461727 36.41279
 36.33966  36.21275  36.150475 36.565098 36.536243 36.488953 36.405544
 36.308876 36.248062 36.616844 36.575825 36.49562  36.418106 36.298035
 36.199463 36.490612 36.46702  36.387455 36.29573  36.18977  36.111126
 36.396072 36.379433 36.321568 36.25847  36.18904  36.178726 36.308144
 36.31658  36.295    36.23452  36.16967  36.12195  36.33966  36.38946
 36.380535 36.32687  36.2562   36.233906 36.4081   36.476505 36.48492
 36.464504 36.44058  36.461742 36.601322 36.72736  36.77543  36.78108
 36.770588]

Args in experiment:
Namespace(is_training=1, model_id='omdata1_test1', model='MSPT_SSL', data='custom', root_path='./dataset/multivariate/', data_path='oisst_lat_14.0_lon_112.0.csv', features='MS', target='sst', freq='d', pretrain_save_path='/root/autodl-tmp/pretrains/', model_save_path='/root/autodl-tmp/checkpoints/', results_save_path='/root/autodl-tmp/results/', test_results_save_path='/root/autodl-tmp/test_results/', model_save_filename='checkpoint.pth', pretrain=True, pretrain_epochs=10, mask_ratio=0.4, finetune=False, freeze_epochs=3, seq_len=365, label_len=0, pred_len=0, inverse=True, individual=False, max_tokens=30, top_k=5, num_kernels=6, enc_in=11, dec_in=11, c_out=11, d_model=128, n_heads=8, e_layers=2, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.001, des='20240615', loss='MSE', lradj='OneCycleLR', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2)
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 7.661109209060669
Epoch: 1, Steps: 79 | Train Loss: 1.0139978 Vali Loss: 0.8824830 Test Loss: 0.8016086
Validation loss decreased (inf --> 0.882483).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 7.0348265171051025
Epoch: 2, Steps: 79 | Train Loss: 0.8935155 Vali Loss: 0.8114342 Test Loss: 0.7247094
Validation loss decreased (0.882483 --> 0.811434).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 6.815826654434204
Epoch: 3, Steps: 79 | Train Loss: 0.7526355 Vali Loss: 0.5588170 Test Loss: 0.5272718
Validation loss decreased (0.811434 --> 0.558817).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 7.016794681549072
Epoch: 4, Steps: 79 | Train Loss: 0.5683387 Vali Loss: 0.4691486 Test Loss: 0.4441107
Validation loss decreased (0.558817 --> 0.469149).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 7.0511674880981445
Epoch: 5, Steps: 79 | Train Loss: 0.4916882 Vali Loss: 0.4049470 Test Loss: 0.3957761
Validation loss decreased (0.469149 --> 0.404947).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 7.10312294960022
Epoch: 6, Steps: 79 | Train Loss: 0.4222190 Vali Loss: 0.3323348 Test Loss: 0.3238175
Validation loss decreased (0.404947 --> 0.332335).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 7.058675289154053
Epoch: 7, Steps: 79 | Train Loss: 0.3422884 Vali Loss: 0.2173589 Test Loss: 0.2145616
Validation loss decreased (0.332335 --> 0.217359).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 7.036877870559692
Epoch: 8, Steps: 79 | Train Loss: 0.2611033 Vali Loss: 0.1705160 Test Loss: 0.1763712
Validation loss decreased (0.217359 --> 0.170516).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 7.0375237464904785
Epoch: 9, Steps: 79 | Train Loss: 0.2359349 Vali Loss: 0.1453480 Test Loss: 0.1598140
Validation loss decreased (0.170516 --> 0.145348).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 6.7238829135894775
Epoch: 10, Steps: 79 | Train Loss: 0.2209122 Vali Loss: 0.1390667 Test Loss: 0.1537123
Validation loss decreased (0.145348 --> 0.139067).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 7.013285875320435
Epoch: 11, Steps: 79 | Train Loss: 0.2096397 Vali Loss: 0.1271561 Test Loss: 0.1420772
Validation loss decreased (0.139067 --> 0.127156).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 7.187402725219727
Epoch: 12, Steps: 79 | Train Loss: 0.2011924 Vali Loss: 0.1221259 Test Loss: 0.1346093
Validation loss decreased (0.127156 --> 0.122126).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 6.947478771209717
Epoch: 13, Steps: 79 | Train Loss: 0.1933448 Vali Loss: 0.1152949 Test Loss: 0.1266097
Validation loss decreased (0.122126 --> 0.115295).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 7.173406362533569
Epoch: 14, Steps: 79 | Train Loss: 0.1857761 Vali Loss: 0.1104419 Test Loss: 0.1232614
Validation loss decreased (0.115295 --> 0.110442).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 6.954336881637573
Epoch: 15, Steps: 79 | Train Loss: 0.1815173 Vali Loss: 0.1062588 Test Loss: 0.1199778
Validation loss decreased (0.110442 --> 0.106259).  Saving model ...
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 7.0522847175598145
Epoch: 16, Steps: 79 | Train Loss: 0.1756330 Vali Loss: 0.1053249 Test Loss: 0.1141994
Validation loss decreased (0.106259 --> 0.105325).  Saving model ...
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 7.114582777023315
Epoch: 17, Steps: 79 | Train Loss: 0.1703826 Vali Loss: 0.1003405 Test Loss: 0.1096572
Validation loss decreased (0.105325 --> 0.100341).  Saving model ...
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 6.86948299407959
Epoch: 18, Steps: 79 | Train Loss: 0.1644076 Vali Loss: 0.0978525 Test Loss: 0.1030006
Validation loss decreased (0.100341 --> 0.097853).  Saving model ...
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 6.955329656600952
Epoch: 19, Steps: 79 | Train Loss: 0.1601952 Vali Loss: 0.0973510 Test Loss: 0.1008923
Validation loss decreased (0.097853 --> 0.097351).  Saving model ...
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 7.093748092651367
Epoch: 20, Steps: 79 | Train Loss: 0.1552765 Vali Loss: 0.0962185 Test Loss: 0.0976549
Validation loss decreased (0.097351 --> 0.096219).  Saving model ...
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 6.810927629470825
Epoch: 21, Steps: 79 | Train Loss: 0.1515979 Vali Loss: 0.0925565 Test Loss: 0.0953495
Validation loss decreased (0.096219 --> 0.092557).  Saving model ...
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 7.090105056762695
Epoch: 22, Steps: 79 | Train Loss: 0.1489094 Vali Loss: 0.0956658 Test Loss: 0.0935036
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 7.126531362533569
Epoch: 23, Steps: 79 | Train Loss: 0.1456873 Vali Loss: 0.0914489 Test Loss: 0.0893167
Validation loss decreased (0.092557 --> 0.091449).  Saving model ...
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 6.7944581508636475
Epoch: 24, Steps: 79 | Train Loss: 0.1431923 Vali Loss: 0.0924549 Test Loss: 0.0905979
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 6.925946950912476
Epoch: 25, Steps: 79 | Train Loss: 0.1423773 Vali Loss: 0.0903278 Test Loss: 0.0878035
Validation loss decreased (0.091449 --> 0.090328).  Saving model ...
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 7.1554319858551025
Epoch: 26, Steps: 79 | Train Loss: 0.1394069 Vali Loss: 0.0926250 Test Loss: 0.0889671
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 6.9699318408966064
Epoch: 27, Steps: 79 | Train Loss: 0.1375634 Vali Loss: 0.0879693 Test Loss: 0.0874354
Validation loss decreased (0.090328 --> 0.087969).  Saving model ...
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 6.928150415420532
Epoch: 28, Steps: 79 | Train Loss: 0.1353721 Vali Loss: 0.0895107 Test Loss: 0.0859213
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 7.157855749130249
Epoch: 29, Steps: 79 | Train Loss: 0.1338546 Vali Loss: 0.0892949 Test Loss: 0.0857767
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 6.9443581104278564
Epoch: 30, Steps: 79 | Train Loss: 0.1312393 Vali Loss: 0.0872505 Test Loss: 0.0847489
Validation loss decreased (0.087969 --> 0.087250).  Saving model ...
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 6.977777481079102
Epoch: 31, Steps: 79 | Train Loss: 0.1289577 Vali Loss: 0.0909789 Test Loss: 0.0855104
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 7.138251543045044
Epoch: 32, Steps: 79 | Train Loss: 0.1271121 Vali Loss: 0.0879424 Test Loss: 0.0835151
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 7.018093109130859
Epoch: 33, Steps: 79 | Train Loss: 0.1265725 Vali Loss: 0.0888808 Test Loss: 0.0835766
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 7.012387275695801
Epoch: 34, Steps: 79 | Train Loss: 0.1245638 Vali Loss: 0.0851881 Test Loss: 0.0813531
Validation loss decreased (0.087250 --> 0.085188).  Saving model ...
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 7.039596080780029
Epoch: 35, Steps: 79 | Train Loss: 0.1231229 Vali Loss: 0.0867937 Test Loss: 0.0846203
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 6.874469041824341
Epoch: 36, Steps: 79 | Train Loss: 0.1214293 Vali Loss: 0.0906509 Test Loss: 0.0843468
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 6.940399169921875
Epoch: 37, Steps: 79 | Train Loss: 0.1193492 Vali Loss: 0.0881950 Test Loss: 0.0819379
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 7.112468481063843
Epoch: 38, Steps: 79 | Train Loss: 0.1182806 Vali Loss: 0.0855186 Test Loss: 0.0812885
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 6.848802804946899
Epoch: 39, Steps: 79 | Train Loss: 0.1161606 Vali Loss: 0.0846493 Test Loss: 0.0798911
Validation loss decreased (0.085188 --> 0.084649).  Saving model ...
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 7.128937005996704
Epoch: 40, Steps: 79 | Train Loss: 0.1133305 Vali Loss: 0.0860395 Test Loss: 0.0783396
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 7.17283296585083
Epoch: 41, Steps: 79 | Train Loss: 0.1111436 Vali Loss: 0.0813104 Test Loss: 0.0745361
Validation loss decreased (0.084649 --> 0.081310).  Saving model ...
Adjusting learning rate to: 0.0009402
Epoch: 42 cost time: 7.06049656867981
Epoch: 42, Steps: 79 | Train Loss: 0.1087036 Vali Loss: 0.0831712 Test Loss: 0.0749670
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009291
Epoch: 43 cost time: 7.236254692077637
Epoch: 43, Steps: 79 | Train Loss: 0.1068817 Vali Loss: 0.0791048 Test Loss: 0.0740073
Validation loss decreased (0.081310 --> 0.079105).  Saving model ...
Adjusting learning rate to: 0.0009171
Epoch: 44 cost time: 7.052854299545288
Epoch: 44, Steps: 79 | Train Loss: 0.1054947 Vali Loss: 0.0778081 Test Loss: 0.0723442
Validation loss decreased (0.079105 --> 0.077808).  Saving model ...
Adjusting learning rate to: 0.0009043
Epoch: 45 cost time: 6.768862724304199
Epoch: 45, Steps: 79 | Train Loss: 0.1043781 Vali Loss: 0.0782491 Test Loss: 0.0708239
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008907
Epoch: 46 cost time: 7.046525716781616
Epoch: 46, Steps: 79 | Train Loss: 0.1032970 Vali Loss: 0.0794997 Test Loss: 0.0710962
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0008763
Epoch: 47 cost time: 7.0706236362457275
Epoch: 47, Steps: 79 | Train Loss: 0.1022886 Vali Loss: 0.0783889 Test Loss: 0.0699372
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0008612
Epoch: 48 cost time: 6.960015773773193
Epoch: 48, Steps: 79 | Train Loss: 0.1015310 Vali Loss: 0.0801265 Test Loss: 0.0747241
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0008453
Epoch: 49 cost time: 7.086651563644409
Epoch: 49, Steps: 79 | Train Loss: 0.1001881 Vali Loss: 0.0768504 Test Loss: 0.0692110
Validation loss decreased (0.077808 --> 0.076850).  Saving model ...
Adjusting learning rate to: 0.0008288
Epoch: 50 cost time: 7.288116455078125
Epoch: 50, Steps: 79 | Train Loss: 0.0997932 Vali Loss: 0.0773969 Test Loss: 0.0719988
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008115
Epoch: 51 cost time: 6.99529504776001
Epoch: 51, Steps: 79 | Train Loss: 0.0985226 Vali Loss: 0.0786220 Test Loss: 0.0731048
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0007937
Epoch: 52 cost time: 7.13116979598999
Epoch: 52, Steps: 79 | Train Loss: 0.0977040 Vali Loss: 0.0776322 Test Loss: 0.0710656
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0007752
Epoch: 53 cost time: 7.100596189498901
Epoch: 53, Steps: 79 | Train Loss: 0.0972757 Vali Loss: 0.0793699 Test Loss: 0.0718306
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0007562
Epoch: 54 cost time: 6.8087475299835205
Epoch: 54, Steps: 79 | Train Loss: 0.0964875 Vali Loss: 0.0831357 Test Loss: 0.0724138
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0007367
Epoch: 55 cost time: 7.266081809997559
Epoch: 55, Steps: 79 | Train Loss: 0.0963504 Vali Loss: 0.0800813 Test Loss: 0.0719251
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0007167
Epoch: 56 cost time: 6.795375108718872
Epoch: 56, Steps: 79 | Train Loss: 0.0947443 Vali Loss: 0.0811444 Test Loss: 0.0738449
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0006963
Epoch: 57 cost time: 6.8559346199035645
Epoch: 57, Steps: 79 | Train Loss: 0.0945196 Vali Loss: 0.0806197 Test Loss: 0.0726572
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0006754
Epoch: 58 cost time: 7.022789239883423
Epoch: 58, Steps: 79 | Train Loss: 0.0937676 Vali Loss: 0.0800791 Test Loss: 0.0748385
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0006542
Epoch: 59 cost time: 7.156357765197754
Epoch: 59, Steps: 79 | Train Loss: 0.0931677 Vali Loss: 0.0791683 Test Loss: 0.0733872
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0006327
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.1488925814628601, mae:0.297992467880249, rmse:0.3858660161495209, mape:0.010635138489305973, mspe:0.00019050249829888344, rse:0.2659620940685272, r2_score:0.9279306138907848, acc:0.989364861510694
corr: [37.667065 37.631176 37.614563 37.611183 37.54058  37.53932  37.558613
 37.410492 37.381016 37.401875 37.41673  37.35879  37.354015 37.399334
 37.361206 37.286304 37.28756  37.299995 37.30298  37.348072 37.43173
 37.310513 37.29082  37.305355 37.344337 37.32294  37.37686  37.4787
 37.322834 37.292595 37.328003 37.3938   37.427135 37.511105 37.632893
 37.465023 37.43867  37.44679  37.487453 37.48291  37.530937 37.621647
 37.40861  37.327713 37.314804 37.333275 37.332005 37.39023  37.49002
 37.34027  37.26633  37.24904  37.270344 37.25257  37.283504 37.34899
 37.180737 37.13863  37.14594  37.162415 37.140965 37.17853  37.2637
 37.17693  37.098427 37.08236  37.08273  37.077896 37.09185  37.13314
 37.05601  37.02952  37.03024  37.06295  37.049583 37.08361  37.119026
 36.96082  36.942078 36.959717 36.991753 36.965538 36.989765 37.042572
 36.92519  36.896038 36.919655 36.974922 36.977055 37.017902 37.075382
 36.853977 36.83232  36.848904 36.877857 36.861824 36.908005 36.963657
 36.908375 36.863453 36.868454 36.89155  36.884583 36.912228 36.944042
 36.898685 36.817688 36.807556 36.821167 36.79296  36.797424 36.828354
 36.737236 36.682022 36.705734 36.723747 36.685608 36.68426  36.69156
 36.53604  36.511444 36.531643 36.544804 36.508476 36.51847  36.523373
 36.39317  36.394665 36.44411  36.4835   36.470737 36.496525 36.53623
 36.351246 36.352505 36.37483  36.401592 36.385666 36.39626  36.420513
 36.30175  36.2946   36.29715  36.304955 36.260963 36.291515 36.3222
 36.224476 36.20092  36.198925 36.213066 36.198868 36.220276 36.277287
 36.181824 36.215256 36.276356 36.29969  36.261627 36.25147  36.25948
 36.149693 36.138966 36.160713 36.200966 36.20505  36.23268  36.29399
 36.16734  36.165497 36.192715 36.237003 36.235138 36.28816  36.380753
 36.16528  36.2038   36.27354  36.342262 36.36782  36.4168   36.486183
 36.272392 36.30988  36.37546  36.427765 36.418957 36.448025 36.502247
 36.346138 36.34512  36.366776 36.389183 36.368824 36.411743 36.475418
 36.413322 36.398697 36.404606 36.43268  36.417076 36.456474 36.52166
 36.446877 36.42648  36.439022 36.454575 36.422905 36.44759  36.492485
 36.351578 36.35137  36.359592 36.36671  36.350986 36.37483  36.412807
 36.43493  36.392113 36.385254 36.419094 36.399    36.437084 36.480312
 36.420277 36.394733 36.412994 36.43392  36.395977 36.398315 36.443245
 36.50153  36.46982  36.46041  36.463783 36.429546 36.442142 36.485268
 36.468727 36.431923 36.442593 36.463387 36.44799  36.48323  36.534943
 36.47874  36.462933 36.485176 36.52496  36.52127  36.569088 36.636684
 36.516308 36.49307  36.505497 36.558968 36.56634  36.623596 36.676384
 36.575817 36.537224 36.536667 36.557323 36.57141  36.634373 36.709602
 36.58585  36.515854 36.50685  36.553246 36.564903 36.620243 36.67796
 36.606636 36.54098  36.536522 36.561172 36.551544 36.59413  36.63752
 36.580883 36.514816 36.510178 36.525288 36.521343 36.557358 36.61401
 36.565956 36.50591  36.488148 36.490665 36.453896 36.475525 36.498577
 36.489193 36.45808  36.487804 36.517815 36.52267  36.55306  36.611042
 36.563663 36.519978 36.541637 36.565975 36.52559  36.529293 36.56159
 36.52303  36.46919  36.47139  36.50608  36.49066  36.4925   36.51769
 36.52182  36.50293  36.52954  36.56185  36.560295 36.58594  36.624897
 36.685158 36.65871  36.671432 36.695736 36.67355  36.686172 36.73177
 36.659836 36.66866  36.712757 36.734573 36.68795  36.7026   36.74377
 36.65373  36.614365 36.643463 36.679337 36.668674 36.684406 36.71355
 36.581257 36.537735 36.546413 36.59287  36.598522 36.654533 36.734505
 36.662064 36.64014  36.69343  36.733192 36.717285 36.762974 36.82654
 36.742355 36.75158  36.834057 36.912323 36.922283 36.968353 37.04144
 36.975872]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 6.342378616333008
Epoch: 1, Steps: 79 | Train Loss: 1.0253890 Vali Loss: 0.8746402 Test Loss: 0.8002730
Validation loss decreased (inf --> 0.874640).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 6.170243263244629
Epoch: 2, Steps: 79 | Train Loss: 0.9040915 Vali Loss: 0.8043414 Test Loss: 0.7373106
Validation loss decreased (0.874640 --> 0.804341).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 6.482212543487549
Epoch: 3, Steps: 79 | Train Loss: 0.7786208 Vali Loss: 0.5824142 Test Loss: 0.5529950
Validation loss decreased (0.804341 --> 0.582414).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 6.633727788925171
Epoch: 4, Steps: 79 | Train Loss: 0.5965828 Vali Loss: 0.4850780 Test Loss: 0.4736436
Validation loss decreased (0.582414 --> 0.485078).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 6.318680286407471
Epoch: 5, Steps: 79 | Train Loss: 0.4839314 Vali Loss: 0.3657144 Test Loss: 0.3554661
Validation loss decreased (0.485078 --> 0.365714).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 6.5900983810424805
Epoch: 6, Steps: 79 | Train Loss: 0.3647055 Vali Loss: 0.2402398 Test Loss: 0.2380846
Validation loss decreased (0.365714 --> 0.240240).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 6.482809543609619
Epoch: 7, Steps: 79 | Train Loss: 0.2841048 Vali Loss: 0.1945665 Test Loss: 0.2020773
Validation loss decreased (0.240240 --> 0.194567).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 6.363221645355225
Epoch: 8, Steps: 79 | Train Loss: 0.2538843 Vali Loss: 0.1520070 Test Loss: 0.1697660
Validation loss decreased (0.194567 --> 0.152007).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 6.369954586029053
Epoch: 9, Steps: 79 | Train Loss: 0.2353675 Vali Loss: 0.1440981 Test Loss: 0.1622250
Validation loss decreased (0.152007 --> 0.144098).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 6.593780517578125
Epoch: 10, Steps: 79 | Train Loss: 0.2230608 Vali Loss: 0.1386658 Test Loss: 0.1553265
Validation loss decreased (0.144098 --> 0.138666).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 6.210926055908203
Epoch: 11, Steps: 79 | Train Loss: 0.2136458 Vali Loss: 0.1310476 Test Loss: 0.1455314
Validation loss decreased (0.138666 --> 0.131048).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 6.484834671020508
Epoch: 12, Steps: 79 | Train Loss: 0.2068289 Vali Loss: 0.1287197 Test Loss: 0.1435642
Validation loss decreased (0.131048 --> 0.128720).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 6.405413866043091
Epoch: 13, Steps: 79 | Train Loss: 0.2006834 Vali Loss: 0.1246859 Test Loss: 0.1412743
Validation loss decreased (0.128720 --> 0.124686).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 6.374269962310791
Epoch: 14, Steps: 79 | Train Loss: 0.1948448 Vali Loss: 0.1257535 Test Loss: 0.1395179
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 6.393340349197388
Epoch: 15, Steps: 79 | Train Loss: 0.1891521 Vali Loss: 0.1229290 Test Loss: 0.1387363
Validation loss decreased (0.124686 --> 0.122929).  Saving model ...
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 6.576945781707764
Epoch: 16, Steps: 79 | Train Loss: 0.1845413 Vali Loss: 0.1262295 Test Loss: 0.1340797
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 6.42879843711853
Epoch: 17, Steps: 79 | Train Loss: 0.1801820 Vali Loss: 0.1174590 Test Loss: 0.1251961
Validation loss decreased (0.122929 --> 0.117459).  Saving model ...
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 6.356395483016968
Epoch: 18, Steps: 79 | Train Loss: 0.1763951 Vali Loss: 0.1266018 Test Loss: 0.1252067
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 6.424105882644653
Epoch: 19, Steps: 79 | Train Loss: 0.1724219 Vali Loss: 0.1194385 Test Loss: 0.1205947
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 6.618372201919556
Epoch: 20, Steps: 79 | Train Loss: 0.1677223 Vali Loss: 0.1183115 Test Loss: 0.1216334
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 6.041261911392212
Epoch: 21, Steps: 79 | Train Loss: 0.1637775 Vali Loss: 0.1206718 Test Loss: 0.1162054
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 6.513254642486572
Epoch: 22, Steps: 79 | Train Loss: 0.1612817 Vali Loss: 0.1154759 Test Loss: 0.1152466
Validation loss decreased (0.117459 --> 0.115476).  Saving model ...
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 6.566149711608887
Epoch: 23, Steps: 79 | Train Loss: 0.1580088 Vali Loss: 0.1079268 Test Loss: 0.1069871
Validation loss decreased (0.115476 --> 0.107927).  Saving model ...
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 6.071202993392944
Epoch: 24, Steps: 79 | Train Loss: 0.1554473 Vali Loss: 0.1066628 Test Loss: 0.1065343
Validation loss decreased (0.107927 --> 0.106663).  Saving model ...
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 6.413465738296509
Epoch: 25, Steps: 79 | Train Loss: 0.1523197 Vali Loss: 0.1094569 Test Loss: 0.1077075
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 6.589627981185913
Epoch: 26, Steps: 79 | Train Loss: 0.1495228 Vali Loss: 0.1061466 Test Loss: 0.1031595
Validation loss decreased (0.106663 --> 0.106147).  Saving model ...
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 6.223046779632568
Epoch: 27, Steps: 79 | Train Loss: 0.1472111 Vali Loss: 0.1007569 Test Loss: 0.0988583
Validation loss decreased (0.106147 --> 0.100757).  Saving model ...
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 6.48410439491272
Epoch: 28, Steps: 79 | Train Loss: 0.1439230 Vali Loss: 0.1050061 Test Loss: 0.1007993
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 6.609867811203003
Epoch: 29, Steps: 79 | Train Loss: 0.1423239 Vali Loss: 0.0958206 Test Loss: 0.0970477
Validation loss decreased (0.100757 --> 0.095821).  Saving model ...
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 6.15886926651001
Epoch: 30, Steps: 79 | Train Loss: 0.1392648 Vali Loss: 0.0991716 Test Loss: 0.1012825
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 6.291020393371582
Epoch: 31, Steps: 79 | Train Loss: 0.1383096 Vali Loss: 0.0978667 Test Loss: 0.0968117
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 6.558610439300537
Epoch: 32, Steps: 79 | Train Loss: 0.1362409 Vali Loss: 0.0996876 Test Loss: 0.0980435
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 6.36806845664978
Epoch: 33, Steps: 79 | Train Loss: 0.1342953 Vali Loss: 0.0962089 Test Loss: 0.0939640
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 6.220378875732422
Epoch: 34, Steps: 79 | Train Loss: 0.1319367 Vali Loss: 0.1027338 Test Loss: 0.0992296
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 6.3934242725372314
Epoch: 35, Steps: 79 | Train Loss: 0.1302562 Vali Loss: 0.0998358 Test Loss: 0.0954291
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 6.270553827285767
Epoch: 36, Steps: 79 | Train Loss: 0.1289211 Vali Loss: 0.1044609 Test Loss: 0.0976747
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 6.444155693054199
Epoch: 37, Steps: 79 | Train Loss: 0.1272057 Vali Loss: 0.1003776 Test Loss: 0.0936060
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 6.586759328842163
Epoch: 38, Steps: 79 | Train Loss: 0.1250609 Vali Loss: 0.1000756 Test Loss: 0.0942672
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 6.326756715774536
Epoch: 39, Steps: 79 | Train Loss: 0.1230012 Vali Loss: 0.0986126 Test Loss: 0.0926393
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0009597
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.20346049964427948, mae:0.35180166363716125, rmse:0.45106595754623413, mape:0.012593373656272888, mspe:0.0002634124830365181, rse:0.31090179085731506, r2_score:0.8958019151263371, acc:0.9874066263437271
corr: [37.849552 37.929947 37.99594  38.023884 38.046684 38.0017   38.007393
 37.993004 37.95356  38.004524 38.02506  37.99698  38.02167  37.980515
 37.99604  38.024906 37.763325 37.806755 37.8575   37.889915 37.960854
 37.934242 37.98008  38.04941  37.97735  38.050117 38.094967 38.106426
 38.148365 38.119133 38.127953 38.165646 37.918724 37.97875  37.99958
 38.0051   38.05391  38.042408 38.07248  38.106544 37.81972  37.885372
 37.92514  37.936764 38.013634 37.993526 38.013866 38.04167  37.79861
 37.865223 37.88209  37.85772  37.884308 37.86191  37.90074  37.92037
 37.68208  37.72189  37.72417  37.68802  37.726383 37.7015   37.733913
 37.76385  37.572273 37.600765 37.600918 37.60574  37.643635 37.602886
 37.591743 37.60902  37.42412  37.455406 37.437206 37.429436 37.446102
 37.41386  37.45144  37.477802 37.22556  37.255325 37.247726 37.226723
 37.248104 37.232246 37.24923  37.264442 37.02805  37.044144 37.03849
 37.033005 37.083256 37.105297 37.14755  37.174076 37.032593 37.056778
 37.084465 37.056232 37.07884  37.084663 37.111397 37.139835 36.892902
 36.939396 36.978878 36.98218  37.02415  37.020344 37.087475 37.125877
 36.865    36.896732 36.922825 36.926517 36.965137 36.964657 36.987385
 37.022087 36.789925 36.82192  36.827965 36.828587 36.872433 36.878628
 36.92951  36.950634 36.698864 36.737473 36.733204 36.723618 36.755806
 36.73506  36.75091  36.741722 36.48681  36.55216  36.555626 36.555016
 36.595654 36.556892 36.59164  36.60567  36.439728 36.49624  36.531994
 36.5414   36.589256 36.56806  36.60841  36.643494 36.35498  36.42814
 36.47104  36.491283 36.538837 36.53834  36.58316  36.583908 36.279324
 36.32882  36.372456 36.42311  36.495262 36.503693 36.56074  36.61126
 36.311836 36.380028 36.449036 36.492237 36.590397 36.63727  36.710186
 36.78035  36.447037 36.52929  36.57618  36.631935 36.737766 36.756264
 36.79608  36.837055 36.545376 36.595497 36.614044 36.619392 36.707874
 36.717453 36.775597 36.801846 36.61241  36.652714 36.690723 36.696083
 36.76156  36.759613 36.820156 36.854046 36.595627 36.628384 36.654392
 36.673656 36.73168  36.714695 36.768284 36.813313 36.484215 36.547585
 36.61446  36.645916 36.70423  36.688904 36.72494  36.74062  36.41914
 36.461193 36.469765 36.50664  36.584084 36.588047 36.651207 36.69999
 36.39729  36.444244 36.479183 36.544697 36.627945 36.617027 36.664997
 36.699593 36.395775 36.451782 36.465816 36.487537 36.561916 36.558224
 36.605164 36.64809  36.455654 36.503033 36.53322  36.55689  36.614952
 36.603363 36.678    36.71827  36.5      36.54164  36.58022  36.59727
 36.65292  36.659504 36.727596 36.783253 36.559074 36.59258  36.643238
 36.66804  36.72925  36.717064 36.80506  36.831852 36.692543 36.75117
 36.766933 36.773254 36.829826 36.828884 36.881584 36.905632 36.722706
 36.790615 36.831814 36.837437 36.837708 36.806396 36.866554 36.896854
 36.646015 36.65439  36.66463  36.649845 36.666576 36.63846  36.691765
 36.723236 36.636906 36.677296 36.71177  36.735813 36.773617 36.727646
 36.766003 36.797337 36.605457 36.635494 36.66693  36.638912 36.62719
 36.57979  36.637157 36.66609  36.470787 36.472973 36.51644  36.508537
 36.51377  36.486866 36.55275  36.592674 36.34474  36.358494 36.411144
 36.40726  36.41123  36.388805 36.45598  36.52577  36.3821   36.409294
 36.41278  36.397423 36.394566 36.3355   36.373425 36.42377  36.222755
 36.25008  36.298794 36.313107 36.350456 36.33564  36.394974 36.465263
 36.399834 36.435875 36.484177 36.489464 36.490685 36.41801  36.473522
 36.51444  36.236237 36.2666   36.3344   36.327976 36.378746 36.364285
 36.43879  36.483875 36.50947  36.571922 36.600166 36.580128 36.630947
 36.643467 36.74666  36.819706 36.651115 36.725258 36.819714 36.850338
 36.87909 ]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 6.105177879333496
Epoch: 1, Steps: 79 | Train Loss: 1.0386865 Vali Loss: 0.8791679 Test Loss: 0.7977862
Validation loss decreased (inf --> 0.879168).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 5.681067943572998
Epoch: 2, Steps: 79 | Train Loss: 0.9111849 Vali Loss: 0.8099778 Test Loss: 0.7440124
Validation loss decreased (0.879168 --> 0.809978).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 6.209219455718994
Epoch: 3, Steps: 79 | Train Loss: 0.7776750 Vali Loss: 0.4973303 Test Loss: 0.4622048
Validation loss decreased (0.809978 --> 0.497330).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 6.037156343460083
Epoch: 4, Steps: 79 | Train Loss: 0.5527378 Vali Loss: 0.4438974 Test Loss: 0.4188807
Validation loss decreased (0.497330 --> 0.443897).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 5.493608713150024
Epoch: 5, Steps: 79 | Train Loss: 0.4718660 Vali Loss: 0.3930681 Test Loss: 0.3791704
Validation loss decreased (0.443897 --> 0.393068).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 5.82699990272522
Epoch: 6, Steps: 79 | Train Loss: 0.4259602 Vali Loss: 0.3319472 Test Loss: 0.3224852
Validation loss decreased (0.393068 --> 0.331947).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 6.189303874969482
Epoch: 7, Steps: 79 | Train Loss: 0.3574951 Vali Loss: 0.2293546 Test Loss: 0.2202962
Validation loss decreased (0.331947 --> 0.229355).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 5.748723983764648
Epoch: 8, Steps: 79 | Train Loss: 0.2754866 Vali Loss: 0.1739877 Test Loss: 0.1832621
Validation loss decreased (0.229355 --> 0.173988).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 5.691362380981445
Epoch: 9, Steps: 79 | Train Loss: 0.2457133 Vali Loss: 0.1578536 Test Loss: 0.1688799
Validation loss decreased (0.173988 --> 0.157854).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 6.409038305282593
Epoch: 10, Steps: 79 | Train Loss: 0.2319545 Vali Loss: 0.1403721 Test Loss: 0.1567074
Validation loss decreased (0.157854 --> 0.140372).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 6.238499402999878
Epoch: 11, Steps: 79 | Train Loss: 0.2207823 Vali Loss: 0.1320585 Test Loss: 0.1478646
Validation loss decreased (0.140372 --> 0.132058).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 5.525646448135376
Epoch: 12, Steps: 79 | Train Loss: 0.2113752 Vali Loss: 0.1265739 Test Loss: 0.1436503
Validation loss decreased (0.132058 --> 0.126574).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 6.2322118282318115
Epoch: 13, Steps: 79 | Train Loss: 0.2047779 Vali Loss: 0.1223606 Test Loss: 0.1391622
Validation loss decreased (0.126574 --> 0.122361).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 6.222926378250122
Epoch: 14, Steps: 79 | Train Loss: 0.1986360 Vali Loss: 0.1198059 Test Loss: 0.1376325
Validation loss decreased (0.122361 --> 0.119806).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 5.922796249389648
Epoch: 15, Steps: 79 | Train Loss: 0.1932358 Vali Loss: 0.1178548 Test Loss: 0.1348649
Validation loss decreased (0.119806 --> 0.117855).  Saving model ...
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 6.13176703453064
Epoch: 16, Steps: 79 | Train Loss: 0.1883297 Vali Loss: 0.1209397 Test Loss: 0.1339282
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 6.154799461364746
Epoch: 17, Steps: 79 | Train Loss: 0.1843631 Vali Loss: 0.1163418 Test Loss: 0.1311949
Validation loss decreased (0.117855 --> 0.116342).  Saving model ...
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 5.684173345565796
Epoch: 18, Steps: 79 | Train Loss: 0.1795000 Vali Loss: 0.1162260 Test Loss: 0.1286465
Validation loss decreased (0.116342 --> 0.116226).  Saving model ...
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 5.8646559715271
Epoch: 19, Steps: 79 | Train Loss: 0.1767753 Vali Loss: 0.1157398 Test Loss: 0.1278404
Validation loss decreased (0.116226 --> 0.115740).  Saving model ...
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 6.241825342178345
Epoch: 20, Steps: 79 | Train Loss: 0.1729793 Vali Loss: 0.1162499 Test Loss: 0.1216068
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 5.986638307571411
Epoch: 21, Steps: 79 | Train Loss: 0.1690139 Vali Loss: 0.1118824 Test Loss: 0.1226775
Validation loss decreased (0.115740 --> 0.111882).  Saving model ...
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 5.825083494186401
Epoch: 22, Steps: 79 | Train Loss: 0.1676335 Vali Loss: 0.1171313 Test Loss: 0.1202391
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 6.182210683822632
Epoch: 23, Steps: 79 | Train Loss: 0.1624471 Vali Loss: 0.1135192 Test Loss: 0.1205737
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 5.9134039878845215
Epoch: 24, Steps: 79 | Train Loss: 0.1602764 Vali Loss: 0.1147042 Test Loss: 0.1229490
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 5.825085639953613
Epoch: 25, Steps: 79 | Train Loss: 0.1596501 Vali Loss: 0.1163656 Test Loss: 0.1176257
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 6.186644077301025
Epoch: 26, Steps: 79 | Train Loss: 0.1561122 Vali Loss: 0.1133563 Test Loss: 0.1159538
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 5.939167737960815
Epoch: 27, Steps: 79 | Train Loss: 0.1533423 Vali Loss: 0.1066092 Test Loss: 0.1158341
Validation loss decreased (0.111882 --> 0.106609).  Saving model ...
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 5.682462215423584
Epoch: 28, Steps: 79 | Train Loss: 0.1503306 Vali Loss: 0.1099532 Test Loss: 0.1119276
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 6.067643880844116
Epoch: 29, Steps: 79 | Train Loss: 0.1484878 Vali Loss: 0.1064992 Test Loss: 0.1065103
Validation loss decreased (0.106609 --> 0.106499).  Saving model ...
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 6.111248731613159
Epoch: 30, Steps: 79 | Train Loss: 0.1444568 Vali Loss: 0.1086848 Test Loss: 0.1056784
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 6.05646276473999
Epoch: 31, Steps: 79 | Train Loss: 0.1430733 Vali Loss: 0.1018724 Test Loss: 0.1037693
Validation loss decreased (0.106499 --> 0.101872).  Saving model ...
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 6.025850057601929
Epoch: 32, Steps: 79 | Train Loss: 0.1399743 Vali Loss: 0.0989533 Test Loss: 0.1003184
Validation loss decreased (0.101872 --> 0.098953).  Saving model ...
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 6.1665284633636475
Epoch: 33, Steps: 79 | Train Loss: 0.1365761 Vali Loss: 0.1024329 Test Loss: 0.1013575
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 5.750046730041504
Epoch: 34, Steps: 79 | Train Loss: 0.1353016 Vali Loss: 0.0983532 Test Loss: 0.0985058
Validation loss decreased (0.098953 --> 0.098353).  Saving model ...
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 5.940570116043091
Epoch: 35, Steps: 79 | Train Loss: 0.1339044 Vali Loss: 0.0945065 Test Loss: 0.0981239
Validation loss decreased (0.098353 --> 0.094506).  Saving model ...
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 6.150527238845825
Epoch: 36, Steps: 79 | Train Loss: 0.1311983 Vali Loss: 0.0998282 Test Loss: 0.0992388
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 5.989073276519775
Epoch: 37, Steps: 79 | Train Loss: 0.1293552 Vali Loss: 0.0959814 Test Loss: 0.0956212
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 5.793778896331787
Epoch: 38, Steps: 79 | Train Loss: 0.1268481 Vali Loss: 0.0930012 Test Loss: 0.0939012
Validation loss decreased (0.094506 --> 0.093001).  Saving model ...
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 6.38134503364563
Epoch: 39, Steps: 79 | Train Loss: 0.1247271 Vali Loss: 0.0976515 Test Loss: 0.0936911
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 5.882923364639282
Epoch: 40, Steps: 79 | Train Loss: 0.1236762 Vali Loss: 0.0997220 Test Loss: 0.0941923
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 5.451890707015991
Epoch: 41, Steps: 79 | Train Loss: 0.1215817 Vali Loss: 0.0967280 Test Loss: 0.0888708
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009402
Epoch: 42 cost time: 5.989652872085571
Epoch: 42, Steps: 79 | Train Loss: 0.1190011 Vali Loss: 0.0918924 Test Loss: 0.0881802
Validation loss decreased (0.093001 --> 0.091892).  Saving model ...
Adjusting learning rate to: 0.0009291
Epoch: 43 cost time: 6.081289052963257
Epoch: 43, Steps: 79 | Train Loss: 0.1174690 Vali Loss: 0.0972558 Test Loss: 0.0898501
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009171
Epoch: 44 cost time: 5.568699359893799
Epoch: 44, Steps: 79 | Train Loss: 0.1150396 Vali Loss: 0.0956135 Test Loss: 0.0869968
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009043
Epoch: 45 cost time: 6.068511247634888
Epoch: 45, Steps: 79 | Train Loss: 0.1138639 Vali Loss: 0.0963923 Test Loss: 0.0870238
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0008907
Epoch: 46 cost time: 6.165467262268066
Epoch: 46, Steps: 79 | Train Loss: 0.1126544 Vali Loss: 0.0904252 Test Loss: 0.0853186
Validation loss decreased (0.091892 --> 0.090425).  Saving model ...
Adjusting learning rate to: 0.0008763
Epoch: 47 cost time: 5.752388000488281
Epoch: 47, Steps: 79 | Train Loss: 0.1108945 Vali Loss: 0.0870827 Test Loss: 0.0862891
Validation loss decreased (0.090425 --> 0.087083).  Saving model ...
Adjusting learning rate to: 0.0008612
Epoch: 48 cost time: 6.014719009399414
Epoch: 48, Steps: 79 | Train Loss: 0.1099432 Vali Loss: 0.0906669 Test Loss: 0.0849385
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008453
Epoch: 49 cost time: 6.122043132781982
Epoch: 49, Steps: 79 | Train Loss: 0.1089833 Vali Loss: 0.0904746 Test Loss: 0.0848027
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0008288
Epoch: 50 cost time: 6.105121612548828
Epoch: 50, Steps: 79 | Train Loss: 0.1076290 Vali Loss: 0.0908578 Test Loss: 0.0833866
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0008115
Epoch: 51 cost time: 5.976978302001953
Epoch: 51, Steps: 79 | Train Loss: 0.1065558 Vali Loss: 0.0947139 Test Loss: 0.0846691
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0007937
Epoch: 52 cost time: 6.0760133266448975
Epoch: 52, Steps: 79 | Train Loss: 0.1058431 Vali Loss: 0.0893438 Test Loss: 0.0836489
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0007752
Epoch: 53 cost time: 6.217424392700195
Epoch: 53, Steps: 79 | Train Loss: 0.1046124 Vali Loss: 0.0938106 Test Loss: 0.0868687
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0007562
Epoch: 54 cost time: 5.818825006484985
Epoch: 54, Steps: 79 | Train Loss: 0.1039712 Vali Loss: 0.0936867 Test Loss: 0.0860106
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0007367
Epoch: 55 cost time: 6.0990800857543945
Epoch: 55, Steps: 79 | Train Loss: 0.1029217 Vali Loss: 0.0915438 Test Loss: 0.0839172
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0007167
Epoch: 56 cost time: 6.006211280822754
Epoch: 56, Steps: 79 | Train Loss: 0.1025896 Vali Loss: 0.0901984 Test Loss: 0.0833684
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0006963
Epoch: 57 cost time: 5.754416465759277
Epoch: 57, Steps: 79 | Train Loss: 0.1012829 Vali Loss: 0.0895530 Test Loss: 0.0845455
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0006754
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.1842559278011322, mae:0.33281540870666504, rmse:0.42925041913986206, mape:0.011898364871740341, mspe:0.00023770355619490147, rse:0.2958652377128601, r2_score:0.9089003269928737, acc:0.9881016351282597
corr: [38.076164 38.048054 38.03834  38.03553  37.96348  37.86877  37.804405
 37.75569  37.684196 37.974354 37.913643 37.88664  37.840866 37.7664
 37.67635  37.642902 37.627777 37.575825 37.7372   37.705692 37.72405
 37.7418   37.707195 37.67371  37.661816 37.64348  37.58358  37.650257
 37.601524 37.562374 37.5599   37.51442  37.47251  37.450245 37.448578
 37.419926 37.71184  37.67246  37.677975 37.701244 37.684216 37.652824
 37.624195 37.601974 37.549587 37.789955 37.736504 37.697685 37.661175
 37.630672 37.576042 37.53594  37.481907 37.4184   37.811893 37.75379
 37.710026 37.667976 37.608196 37.559013 37.523235 37.456753 37.359253
 37.58962  37.53427  37.488117 37.454712 37.4156   37.380062 37.323906
 37.273785 37.210804 37.453938 37.3744   37.333824 37.312473 37.249428
 37.20367  37.175404 37.135857 37.05624  37.231274 37.184135 37.146538
 37.138645 37.112118 37.085484 37.07335  37.054108 36.980484 37.15384
 37.09049  37.073563 37.073017 37.04129  37.020943 37.02139  37.02919
 36.99117  37.12761  37.10385  37.083908 37.074238 37.041004 37.025436
 37.02736  37.011024 36.928112 37.058014 37.000237 36.96457  36.954865
 36.932735 36.8924   36.89019  36.877853 36.820515 36.964127 36.921883
 36.89664  36.88975  36.83608  36.77474  36.74415  36.7378   36.6805
 36.786266 36.764313 36.744904 36.734615 36.69873  36.6508   36.59552
 36.54282  36.474716 36.69327  36.687862 36.684155 36.691204 36.657276
 36.61927  36.595604 36.569958 36.500244 36.56011  36.537586 36.519825
 36.51369  36.458763 36.41047  36.37784  36.352196 36.304737 36.463238
 36.415874 36.406845 36.39615  36.36706  36.342182 36.312927 36.31032
 36.290062 36.33962  36.347866 36.38017  36.39682  36.381798 36.37692
 36.37104  36.37911  36.371105 36.42133  36.463345 36.497128 36.5265
 36.532925 36.54161  36.544094 36.518616 36.49632  36.49736  36.49865
 36.520206 36.56075  36.573788 36.576305 36.568745 36.54733  36.502586
 36.470352 36.479076 36.504055 36.53654  36.54498  36.528313 36.519997
 36.49175  36.444946 36.56662  36.577545 36.58747  36.6074   36.589672
 36.55391  36.51875  36.478676 36.419167 36.54685  36.55663  36.569805
 36.58856  36.5894   36.57937  36.559917 36.536083 36.488045 36.54553
 36.514885 36.519974 36.538097 36.524857 36.53393  36.543797 36.548653
 36.52398  36.4671   36.46363  36.48088  36.51456  36.526066 36.53409
 36.522755 36.517803 36.46098  36.47188  36.438534 36.425587 36.44461
 36.42348  36.407467 36.411644 36.434616 36.418644 36.452408 36.412277
 36.413624 36.42248  36.446552 36.474842 36.476223 36.472813 36.46065
 36.490967 36.452145 36.463825 36.492233 36.488663 36.485256 36.475292
 36.46395  36.436077 36.522114 36.520565 36.54324  36.519142 36.49883
 36.4939   36.50593  36.521633 36.508728 36.610527 36.61876  36.63957
 36.63692  36.60465  36.586815 36.575073 36.59168  36.595734 36.671684
 36.650963 36.626343 36.619396 36.591    36.544655 36.52271  36.519478
 36.507484 36.626705 36.584446 36.599133 36.600212 36.53323  36.48317
 36.44175  36.437237 36.425472 36.532234 36.49479  36.475887 36.45543
 36.423096 36.366467 36.326706 36.31709  36.288696 36.455193 36.413116
 36.406456 36.401802 36.375202 36.331608 36.314587 36.320995 36.323368
 36.457367 36.436214 36.440243 36.436714 36.389233 36.343937 36.30044
 36.29073  36.28301  36.531334 36.504105 36.49926  36.490444 36.447292
 36.400005 36.37389  36.377316 36.3877   36.582024 36.5657   36.57098
 36.55647  36.50794  36.451492 36.39708  36.38598  36.379326 36.683884
 36.679317 36.684803 36.670166 36.611263 36.55936  36.518864 36.489574
 36.446392 36.730415 36.719006 36.734726 36.72599  36.689163 36.642155
 36.613895 36.57961  36.532043 36.7743   36.784325 36.815514 36.84674
 36.83445 ]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 5.281720161437988
Epoch: 1, Steps: 79 | Train Loss: 1.0672501 Vali Loss: 0.8840888 Test Loss: 0.8211380
Validation loss decreased (inf --> 0.884089).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 5.778536319732666
Epoch: 2, Steps: 79 | Train Loss: 0.9220629 Vali Loss: 0.8455797 Test Loss: 0.7623756
Validation loss decreased (0.884089 --> 0.845580).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 5.711291074752808
Epoch: 3, Steps: 79 | Train Loss: 0.8263902 Vali Loss: 0.6923735 Test Loss: 0.6227625
Validation loss decreased (0.845580 --> 0.692374).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 5.53424859046936
Epoch: 4, Steps: 79 | Train Loss: 0.6314891 Vali Loss: 0.4643104 Test Loss: 0.4354182
Validation loss decreased (0.692374 --> 0.464310).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 6.266961574554443
Epoch: 5, Steps: 79 | Train Loss: 0.4912600 Vali Loss: 0.3869063 Test Loss: 0.3767777
Validation loss decreased (0.464310 --> 0.386906).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 6.074533462524414
Epoch: 6, Steps: 79 | Train Loss: 0.4193129 Vali Loss: 0.3129481 Test Loss: 0.3047208
Validation loss decreased (0.386906 --> 0.312948).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 5.670489311218262
Epoch: 7, Steps: 79 | Train Loss: 0.3242073 Vali Loss: 0.2445018 Test Loss: 0.2332538
Validation loss decreased (0.312948 --> 0.244502).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 6.118314266204834
Epoch: 8, Steps: 79 | Train Loss: 0.2802041 Vali Loss: 0.2010129 Test Loss: 0.1975693
Validation loss decreased (0.244502 --> 0.201013).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 6.152306079864502
Epoch: 9, Steps: 79 | Train Loss: 0.2511377 Vali Loss: 0.1583284 Test Loss: 0.1693578
Validation loss decreased (0.201013 --> 0.158328).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 5.55053973197937
Epoch: 10, Steps: 79 | Train Loss: 0.2337252 Vali Loss: 0.1433242 Test Loss: 0.1565964
Validation loss decreased (0.158328 --> 0.143324).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 5.294950723648071
Epoch: 11, Steps: 79 | Train Loss: 0.2217494 Vali Loss: 0.1360996 Test Loss: 0.1534419
Validation loss decreased (0.143324 --> 0.136100).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 6.021258354187012
Epoch: 12, Steps: 79 | Train Loss: 0.2122244 Vali Loss: 0.1285481 Test Loss: 0.1469448
Validation loss decreased (0.136100 --> 0.128548).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 6.034498929977417
Epoch: 13, Steps: 79 | Train Loss: 0.2054340 Vali Loss: 0.1214700 Test Loss: 0.1406950
Validation loss decreased (0.128548 --> 0.121470).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 5.265771865844727
Epoch: 14, Steps: 79 | Train Loss: 0.1981480 Vali Loss: 0.1228902 Test Loss: 0.1398934
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 5.796519041061401
Epoch: 15, Steps: 79 | Train Loss: 0.1923347 Vali Loss: 0.1227739 Test Loss: 0.1342765
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 5.781283617019653
Epoch: 16, Steps: 79 | Train Loss: 0.1884901 Vali Loss: 0.1271677 Test Loss: 0.1341717
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 5.294463157653809
Epoch: 17, Steps: 79 | Train Loss: 0.1823759 Vali Loss: 0.1211088 Test Loss: 0.1272007
Validation loss decreased (0.121470 --> 0.121109).  Saving model ...
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 5.334582567214966
Epoch: 18, Steps: 79 | Train Loss: 0.1789677 Vali Loss: 0.1225249 Test Loss: 0.1279791
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 6.12727427482605
Epoch: 19, Steps: 79 | Train Loss: 0.1760256 Vali Loss: 0.1257710 Test Loss: 0.1278967
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 5.775368690490723
Epoch: 20, Steps: 79 | Train Loss: 0.1720621 Vali Loss: 0.1189966 Test Loss: 0.1256468
Validation loss decreased (0.121109 --> 0.118997).  Saving model ...
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 5.347636461257935
Epoch: 21, Steps: 79 | Train Loss: 0.1687617 Vali Loss: 0.1149248 Test Loss: 0.1212315
Validation loss decreased (0.118997 --> 0.114925).  Saving model ...
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 5.710422039031982
Epoch: 22, Steps: 79 | Train Loss: 0.1667183 Vali Loss: 0.1186737 Test Loss: 0.1210923
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 5.543670415878296
Epoch: 23, Steps: 79 | Train Loss: 0.1642823 Vali Loss: 0.1257056 Test Loss: 0.1262965
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 5.172025203704834
Epoch: 24, Steps: 79 | Train Loss: 0.1618129 Vali Loss: 0.1175465 Test Loss: 0.1169760
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 5.65561842918396
Epoch: 25, Steps: 79 | Train Loss: 0.1587322 Vali Loss: 0.1224454 Test Loss: 0.1167044
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 5.682267427444458
Epoch: 26, Steps: 79 | Train Loss: 0.1566090 Vali Loss: 0.1181597 Test Loss: 0.1181261
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 5.250685930252075
Epoch: 27, Steps: 79 | Train Loss: 0.1546115 Vali Loss: 0.1190283 Test Loss: 0.1183046
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 5.761674880981445
Epoch: 28, Steps: 79 | Train Loss: 0.1523256 Vali Loss: 0.1171931 Test Loss: 0.1148744
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 6.10669207572937
Epoch: 29, Steps: 79 | Train Loss: 0.1506634 Vali Loss: 0.1134100 Test Loss: 0.1150908
Validation loss decreased (0.114925 --> 0.113410).  Saving model ...
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 5.961122989654541
Epoch: 30, Steps: 79 | Train Loss: 0.1486141 Vali Loss: 0.1128583 Test Loss: 0.1157920
Validation loss decreased (0.113410 --> 0.112858).  Saving model ...
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 6.0926430225372314
Epoch: 31, Steps: 79 | Train Loss: 0.1469759 Vali Loss: 0.1163490 Test Loss: 0.1131243
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 6.022887945175171
Epoch: 32, Steps: 79 | Train Loss: 0.1453225 Vali Loss: 0.1110716 Test Loss: 0.1123167
Validation loss decreased (0.112858 --> 0.111072).  Saving model ...
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 5.427251577377319
Epoch: 33, Steps: 79 | Train Loss: 0.1445396 Vali Loss: 0.1114640 Test Loss: 0.1110494
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 5.882220029830933
Epoch: 34, Steps: 79 | Train Loss: 0.1419247 Vali Loss: 0.1165843 Test Loss: 0.1126294
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 6.557435750961304
Epoch: 35, Steps: 79 | Train Loss: 0.1403628 Vali Loss: 0.1139766 Test Loss: 0.1103229
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 6.550270080566406
Epoch: 36, Steps: 79 | Train Loss: 0.1387988 Vali Loss: 0.1145525 Test Loss: 0.1128350
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 6.398401260375977
Epoch: 37, Steps: 79 | Train Loss: 0.1375033 Vali Loss: 0.1158650 Test Loss: 0.1120762
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 6.411223649978638
Epoch: 38, Steps: 79 | Train Loss: 0.1361339 Vali Loss: 0.1140202 Test Loss: 0.1147583
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 6.257567405700684
Epoch: 39, Steps: 79 | Train Loss: 0.1342091 Vali Loss: 0.1076311 Test Loss: 0.1065312
Validation loss decreased (0.111072 --> 0.107631).  Saving model ...
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 6.379746675491333
Epoch: 40, Steps: 79 | Train Loss: 0.1321880 Vali Loss: 0.1167108 Test Loss: 0.1117318
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 5.914421319961548
Epoch: 41, Steps: 79 | Train Loss: 0.1304077 Vali Loss: 0.1132389 Test Loss: 0.1088260
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009402
Epoch: 42 cost time: 6.1136744022369385
Epoch: 42, Steps: 79 | Train Loss: 0.1286148 Vali Loss: 0.1151268 Test Loss: 0.1063611
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009291
Epoch: 43 cost time: 6.332270860671997
Epoch: 43, Steps: 79 | Train Loss: 0.1265478 Vali Loss: 0.1122420 Test Loss: 0.1052125
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009171
Epoch: 44 cost time: 6.448601722717285
Epoch: 44, Steps: 79 | Train Loss: 0.1262388 Vali Loss: 0.1131926 Test Loss: 0.1066050
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009043
Epoch: 45 cost time: 6.368414402008057
Epoch: 45, Steps: 79 | Train Loss: 0.1235293 Vali Loss: 0.1107572 Test Loss: 0.1039060
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0008907
Epoch: 46 cost time: 6.280468463897705
Epoch: 46, Steps: 79 | Train Loss: 0.1225261 Vali Loss: 0.1147583 Test Loss: 0.1053689
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0008763
Epoch: 47 cost time: 4.798632860183716
Epoch: 47, Steps: 79 | Train Loss: 0.1206476 Vali Loss: 0.1111769 Test Loss: 0.1034704
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0008612
Epoch: 48 cost time: 4.766756534576416
Epoch: 48, Steps: 79 | Train Loss: 0.1199402 Vali Loss: 0.1149517 Test Loss: 0.1040428
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0008453
Epoch: 49 cost time: 4.8296709060668945
Epoch: 49, Steps: 79 | Train Loss: 0.1178642 Vali Loss: 0.1127641 Test Loss: 0.1027272
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0008288
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.2255430668592453, mae:0.37218019366264343, rmse:0.47491374611854553, mape:0.013302123174071312, mspe:0.00029015366453677416, rse:0.3273391127586365, r2_score:0.883863321717216, acc:0.9866978768259287
corr: [37.766933 37.75925  37.79103  37.78158  37.816593 37.859188 37.885674
 37.924046 37.98648  38.07327  37.6158   37.602787 37.636566 37.67619
 37.712803 37.758705 37.799294 37.845043 37.91206  37.981243 37.500065
 37.508675 37.55788  37.570644 37.62116  37.69932  37.746166 37.7916
 37.872784 37.940754 37.44886  37.477005 37.514366 37.53705  37.585526
 37.660973 37.709393 37.762962 37.824306 37.881638 37.593056 37.58581
 37.614166 37.62445  37.643494 37.67713  37.71728  37.763603 37.77517
 37.838295 37.537193 37.530968 37.559227 37.55945  37.61567  37.702705
 37.763927 37.81391  37.85412  37.885376 37.572884 37.590927 37.604866
 37.60183  37.60462  37.63932  37.659645 37.704685 37.7337   37.77381
 37.395718 37.367676 37.356754 37.359207 37.38276  37.454647 37.491726
 37.51836  37.518818 37.530174 37.178314 37.16044  37.165688 37.186054
 37.21778  37.27848  37.329334 37.338608 37.343372 37.34506  36.950607
 36.96104  36.996857 37.016384 37.055996 37.13265  37.182674 37.231728
 37.294468 37.34889  36.895317 36.94036  36.994038 37.031433 37.0875
 37.15899  37.184113 37.1875   37.21398  37.250275 36.77584  36.78559
 36.82781  36.84478  36.892525 36.947956 36.95534  36.96325  36.984596
 37.011578 36.71782  36.77639  36.812572 36.80091  36.83008  36.86666
 36.891453 36.917027 36.959743 36.98835  36.50668  36.540096 36.579185
 36.581097 36.617676 36.69281  36.70576  36.727238 36.76992  36.803905
 36.44772  36.466763 36.522827 36.537758 36.567696 36.63079  36.69357
 36.717304 36.741398 36.74854  36.263912 36.297363 36.346443 36.386215
 36.45556  36.532066 36.618668 36.673393 36.693737 36.717686 36.13628
 36.181503 36.225548 36.267128 36.350018 36.42864  36.527496 36.589066
 36.6754   36.74791  36.221085 36.247204 36.3428   36.391514 36.478146
 36.547573 36.609196 36.645607 36.706745 36.739098 36.208263 36.26725
 36.35343  36.40324  36.484524 36.55876  36.6111   36.659275 36.693455
 36.733994 36.24868  36.294827 36.376446 36.45493  36.56532  36.64971
 36.713837 36.743923 36.78194  36.810505 36.283165 36.358616 36.438145
 36.4964   36.567486 36.65928  36.738594 36.784996 36.850143 36.910282
 36.326572 36.38851  36.49265  36.570164 36.666817 36.753586 36.817883
 36.876778 36.935646 36.972904 36.412243 36.46561  36.560722 36.615517
 36.693237 36.787262 36.888794 36.96813  37.046085 37.122692 36.50463
 36.57759  36.670593 36.705822 36.768143 36.82257  36.885258 36.915962
 36.9717   37.04968  36.497974 36.550327 36.6442   36.71824  36.789337
 36.868877 36.92793  36.97926  37.04193  37.117313 36.419224 36.48217
 36.571476 36.638443 36.736305 36.822586 36.932293 37.00085  37.058403
 37.127415 36.393127 36.479736 36.594917 36.68229  36.77082  36.86162
 36.96889  37.02953  37.079834 37.132484 36.52544  36.569145 36.62706
 36.69136  36.78152  36.852528 36.90115  36.973305 37.05128  37.1243
 36.57148  36.634495 36.712677 36.768658 36.84571  36.917126 36.981056
 37.029022 37.113087 37.209927 36.658215 36.720875 36.81306  36.838615
 36.876457 36.911785 36.95892  37.019592 37.082726 37.149452 36.6765
 36.694984 36.727047 36.73866  36.76669  36.821133 36.888985 36.94802
 37.02457  37.11874  36.615337 36.66068  36.729317 36.761757 36.792953
 36.835976 36.873158 36.91561  36.990116 37.08088  36.596195 36.617435
 36.666317 36.678608 36.705208 36.729595 36.78522  36.884    36.988697
 37.093132 36.612404 36.63653  36.684143 36.71758  36.76324  36.79246
 36.838047 36.89914  36.990326 37.1276   36.84987  36.872673 36.913593
 36.91011  36.91602  36.919468 36.954163 36.992252 37.07411  37.20047
 36.90986  36.943626 36.96833  36.955105 36.974342 37.013817 37.066837
 37.130463 37.21587  37.344654 37.004105 37.069332 37.144646 37.18205
 37.23285 ]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 4.690270900726318
Epoch: 1, Steps: 79 | Train Loss: 1.0215074 Vali Loss: 0.8758083 Test Loss: 0.7904013
Validation loss decreased (inf --> 0.875808).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 4.6429619789123535
Epoch: 2, Steps: 79 | Train Loss: 0.8985906 Vali Loss: 0.8104426 Test Loss: 0.7349176
Validation loss decreased (0.875808 --> 0.810443).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 4.598228693008423
Epoch: 3, Steps: 79 | Train Loss: 0.7789696 Vali Loss: 0.6049217 Test Loss: 0.5428135
Validation loss decreased (0.810443 --> 0.604922).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 4.59056282043457
Epoch: 4, Steps: 79 | Train Loss: 0.5859872 Vali Loss: 0.5007226 Test Loss: 0.4749855
Validation loss decreased (0.604922 --> 0.500723).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 4.6657021045684814
Epoch: 5, Steps: 79 | Train Loss: 0.4830294 Vali Loss: 0.3923651 Test Loss: 0.3783010
Validation loss decreased (0.500723 --> 0.392365).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 4.678537368774414
Epoch: 6, Steps: 79 | Train Loss: 0.4043582 Vali Loss: 0.3101940 Test Loss: 0.2962725
Validation loss decreased (0.392365 --> 0.310194).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 4.714283227920532
Epoch: 7, Steps: 79 | Train Loss: 0.3148913 Vali Loss: 0.2061541 Test Loss: 0.2048341
Validation loss decreased (0.310194 --> 0.206154).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 4.639441967010498
Epoch: 8, Steps: 79 | Train Loss: 0.2621594 Vali Loss: 0.1651131 Test Loss: 0.1723343
Validation loss decreased (0.206154 --> 0.165113).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 4.559575080871582
Epoch: 9, Steps: 79 | Train Loss: 0.2393822 Vali Loss: 0.1481640 Test Loss: 0.1594348
Validation loss decreased (0.165113 --> 0.148164).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 4.59218692779541
Epoch: 10, Steps: 79 | Train Loss: 0.2248998 Vali Loss: 0.1378742 Test Loss: 0.1497648
Validation loss decreased (0.148164 --> 0.137874).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 4.603370904922485
Epoch: 11, Steps: 79 | Train Loss: 0.2145613 Vali Loss: 0.1286073 Test Loss: 0.1425950
Validation loss decreased (0.137874 --> 0.128607).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 4.581876277923584
Epoch: 12, Steps: 79 | Train Loss: 0.2061728 Vali Loss: 0.1314344 Test Loss: 0.1399303
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 4.570400714874268
Epoch: 13, Steps: 79 | Train Loss: 0.1989498 Vali Loss: 0.1253128 Test Loss: 0.1390124
Validation loss decreased (0.128607 --> 0.125313).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 4.594644069671631
Epoch: 14, Steps: 79 | Train Loss: 0.1918741 Vali Loss: 0.1178695 Test Loss: 0.1320670
Validation loss decreased (0.125313 --> 0.117870).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 4.617609739303589
Epoch: 15, Steps: 79 | Train Loss: 0.1870043 Vali Loss: 0.1160891 Test Loss: 0.1263741
Validation loss decreased (0.117870 --> 0.116089).  Saving model ...
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 4.6261491775512695
Epoch: 16, Steps: 79 | Train Loss: 0.1823876 Vali Loss: 0.1106375 Test Loss: 0.1240855
Validation loss decreased (0.116089 --> 0.110637).  Saving model ...
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 4.690911293029785
Epoch: 17, Steps: 79 | Train Loss: 0.1767427 Vali Loss: 0.1102333 Test Loss: 0.1222885
Validation loss decreased (0.110637 --> 0.110233).  Saving model ...
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 4.6534318923950195
Epoch: 18, Steps: 79 | Train Loss: 0.1724920 Vali Loss: 0.1097784 Test Loss: 0.1197432
Validation loss decreased (0.110233 --> 0.109778).  Saving model ...
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 4.638896465301514
Epoch: 19, Steps: 79 | Train Loss: 0.1687861 Vali Loss: 0.1179155 Test Loss: 0.1180715
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 4.69339394569397
Epoch: 20, Steps: 79 | Train Loss: 0.1661129 Vali Loss: 0.1111944 Test Loss: 0.1188092
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 4.645765066146851
Epoch: 21, Steps: 79 | Train Loss: 0.1632851 Vali Loss: 0.1097168 Test Loss: 0.1115953
Validation loss decreased (0.109778 --> 0.109717).  Saving model ...
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 4.6880388259887695
Epoch: 22, Steps: 79 | Train Loss: 0.1599588 Vali Loss: 0.1065827 Test Loss: 0.1147176
Validation loss decreased (0.109717 --> 0.106583).  Saving model ...
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 4.657702445983887
Epoch: 23, Steps: 79 | Train Loss: 0.1573772 Vali Loss: 0.1086255 Test Loss: 0.1112339
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 4.657605171203613
Epoch: 24, Steps: 79 | Train Loss: 0.1534068 Vali Loss: 0.1032071 Test Loss: 0.1089691
Validation loss decreased (0.106583 --> 0.103207).  Saving model ...
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 4.7009193897247314
Epoch: 25, Steps: 79 | Train Loss: 0.1505178 Vali Loss: 0.1042411 Test Loss: 0.1064321
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 4.6550164222717285
Epoch: 26, Steps: 79 | Train Loss: 0.1497706 Vali Loss: 0.1074844 Test Loss: 0.1062099
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 4.664497137069702
Epoch: 27, Steps: 79 | Train Loss: 0.1456273 Vali Loss: 0.1049730 Test Loss: 0.1048138
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 4.673788785934448
Epoch: 28, Steps: 79 | Train Loss: 0.1438608 Vali Loss: 0.0972920 Test Loss: 0.0999225
Validation loss decreased (0.103207 --> 0.097292).  Saving model ...
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 4.63858437538147
Epoch: 29, Steps: 79 | Train Loss: 0.1426050 Vali Loss: 0.1060853 Test Loss: 0.1022676
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 4.686050653457642
Epoch: 30, Steps: 79 | Train Loss: 0.1397273 Vali Loss: 0.1031079 Test Loss: 0.1040807
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 4.667476177215576
Epoch: 31, Steps: 79 | Train Loss: 0.1376579 Vali Loss: 0.0991349 Test Loss: 0.1012364
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 4.6252641677856445
Epoch: 32, Steps: 79 | Train Loss: 0.1359958 Vali Loss: 0.1046081 Test Loss: 0.1001459
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 4.656811237335205
Epoch: 33, Steps: 79 | Train Loss: 0.1353190 Vali Loss: 0.1044120 Test Loss: 0.1019927
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 4.659525394439697
Epoch: 34, Steps: 79 | Train Loss: 0.1327387 Vali Loss: 0.1093022 Test Loss: 0.1036484
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 4.622060298919678
Epoch: 35, Steps: 79 | Train Loss: 0.1313541 Vali Loss: 0.1035863 Test Loss: 0.1019495
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 4.621060609817505
Epoch: 36, Steps: 79 | Train Loss: 0.1288559 Vali Loss: 0.1077715 Test Loss: 0.0979822
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 4.701971054077148
Epoch: 37, Steps: 79 | Train Loss: 0.1282406 Vali Loss: 0.1036831 Test Loss: 0.0977903
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 4.6451520919799805
Epoch: 38, Steps: 79 | Train Loss: 0.1263282 Vali Loss: 0.1087278 Test Loss: 0.1008898
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0009680
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.22669179737567902, mae:0.3704336881637573, rmse:0.4761216342449188, mape:0.013257513754069805, mspe:0.00029349778196774423, rse:0.32817167043685913, r2_score:0.883190713321321, acc:0.9867424862459302
corr: [38.32883  38.39423  38.42739  38.410965 38.346283 38.29     38.254093
 38.241703 38.220837 38.160095 38.068817 38.34788  38.361797 38.34738
 38.29689  38.24462  38.15111  38.13399  38.113243 38.12022  38.072166
 38.007095 38.175236 38.207607 38.189835 38.148712 38.11304  38.05379
 38.058334 38.07639  38.08128  38.030163 37.976967 38.130836 38.12076
 38.107574 38.06957  38.06155  37.990284 37.995285 38.040657 38.049908
 38.017952 37.95353  38.172977 38.164017 38.11581  38.064552 38.016865
 37.94968  37.933098 37.963326 37.975464 37.95115  37.896534 38.06943
 38.05461  38.022156 37.977608 37.931274 37.84198  37.822132 37.858543
 37.873207 37.849564 37.75424  37.827175 37.802864 37.78466  37.73533
 37.692307 37.60889  37.590664 37.63299  37.62212  37.588444 37.52593
 37.574047 37.551056 37.503735 37.424923 37.35464  37.2599   37.2313
 37.266884 37.30179  37.287262 37.20892  37.373535 37.38616  37.370804
 37.353443 37.29391  37.204887 37.178787 37.216007 37.2269   37.203625
 37.133427 37.229218 37.23129  37.20193  37.158615 37.125195 37.088203
 37.09431  37.151848 37.207973 37.200943 37.13039  37.320206 37.335155
 37.286648 37.234264 37.192562 37.124523 37.073246 37.10531  37.115047
 37.09286  36.995995 37.21726  37.22503  37.18956  37.152496 37.117973
 37.015842 36.975193 37.024487 37.04273  36.993843 36.89761  37.0067
 37.001812 36.968212 36.92355  36.866844 36.78605  36.732952 36.779804
 36.776997 36.73208  36.64542  36.815258 36.852886 36.825954 36.777725
 36.716946 36.646442 36.613174 36.680454 36.700737 36.694374 36.61475
 36.54881  36.581116 36.575268 36.525017 36.501106 36.462524 36.446568
 36.535316 36.567574 36.59845  36.54856  36.525734 36.595497 36.620804
 36.645058 36.68032  36.674057 36.654808 36.75005  36.783623 36.77651
 36.70824  36.566845 36.62607  36.6475   36.655354 36.65655  36.63058
 36.63996  36.741257 36.75727  36.758366 36.704533 36.642487 36.70035
 36.70054  36.685287 36.652363 36.63808  36.61517  36.69429  36.72474
 36.74598  36.693142 36.544    36.62548  36.650257 36.662704 36.67546
 36.68336  36.679226 36.779232 36.831825 36.856323 36.818756 36.53059
 36.59304  36.600346 36.597515 36.581493 36.57335  36.561977 36.657463
 36.716557 36.7676   36.778206 36.484863 36.556522 36.580467 36.54954
 36.56347  36.594315 36.64038  36.78204  36.832394 36.884445 36.90674
 36.602993 36.670753 36.697697 36.67644  36.646015 36.634    36.6678
 36.802086 36.863033 36.90483  36.930904 36.589176 36.677372 36.71623
 36.70477  36.726276 36.73048  36.757126 36.87731  36.9295   36.96601
 36.94971  36.53502  36.61942  36.686283 36.70055  36.714325 36.720333
 36.74348  36.848366 36.870953 36.89065  36.904854 36.589664 36.691498
 36.727703 36.744198 36.765423 36.781967 36.802826 36.907524 36.91836
 36.895752 36.884453 36.501225 36.57544  36.60341  36.59335  36.572998
 36.59013  36.635323 36.72179  36.749573 36.764553 36.739265 36.5557
 36.611073 36.63771  36.61697  36.611603 36.57936  36.603    36.68418
 36.71743  36.713078 36.71649  36.442455 36.530415 36.566444 36.56494
 36.596046 36.576656 36.555004 36.594646 36.59392  36.556324 36.511974
 36.356445 36.411274 36.447994 36.431496 36.43142  36.414566 36.41391
 36.4985   36.508636 36.49717  36.442997 36.43782  36.518368 36.549458
 36.51168  36.48219  36.43801  36.437687 36.48019  36.47634  36.44767
 36.38581  36.430973 36.52257  36.5486   36.522335 36.546005 36.521168
 36.503826 36.576874 36.577328 36.546352 36.505573 36.55877  36.638325
 36.667732 36.68629  36.68297  36.63518  36.62305  36.61873  36.604473
 36.573933 36.49269  36.76458  36.88155  36.96512  37.010605 37.02822
 37.018547 37.029995 37.06313  37.10261  37.110214 37.054535 37.02892
 37.15233 ]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 4.369652986526489
Epoch: 1, Steps: 79 | Train Loss: 1.0548008 Vali Loss: 0.8937390 Test Loss: 0.8185552
Validation loss decreased (inf --> 0.893739).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 4.39458966255188
Epoch: 2, Steps: 79 | Train Loss: 0.9176878 Vali Loss: 0.8242269 Test Loss: 0.7614638
Validation loss decreased (0.893739 --> 0.824227).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 4.431485176086426
Epoch: 3, Steps: 79 | Train Loss: 0.8149332 Vali Loss: 0.6677952 Test Loss: 0.6021004
Validation loss decreased (0.824227 --> 0.667795).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 4.36988639831543
Epoch: 4, Steps: 79 | Train Loss: 0.5984326 Vali Loss: 0.4126804 Test Loss: 0.4089223
Validation loss decreased (0.667795 --> 0.412680).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 4.38101863861084
Epoch: 5, Steps: 79 | Train Loss: 0.4743056 Vali Loss: 0.3741919 Test Loss: 0.3549943
Validation loss decreased (0.412680 --> 0.374192).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 4.397858381271362
Epoch: 6, Steps: 79 | Train Loss: 0.4057346 Vali Loss: 0.3086756 Test Loss: 0.2971782
Validation loss decreased (0.374192 --> 0.308676).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 4.453089714050293
Epoch: 7, Steps: 79 | Train Loss: 0.3205292 Vali Loss: 0.2191496 Test Loss: 0.2114291
Validation loss decreased (0.308676 --> 0.219150).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 4.38162636756897
Epoch: 8, Steps: 79 | Train Loss: 0.2701975 Vali Loss: 0.1647751 Test Loss: 0.1698181
Validation loss decreased (0.219150 --> 0.164775).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 4.349340200424194
Epoch: 9, Steps: 79 | Train Loss: 0.2449686 Vali Loss: 0.1452638 Test Loss: 0.1558456
Validation loss decreased (0.164775 --> 0.145264).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 4.40976357460022
Epoch: 10, Steps: 79 | Train Loss: 0.2307265 Vali Loss: 0.1350599 Test Loss: 0.1480802
Validation loss decreased (0.145264 --> 0.135060).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 4.345199346542358
Epoch: 11, Steps: 79 | Train Loss: 0.2199409 Vali Loss: 0.1324299 Test Loss: 0.1449134
Validation loss decreased (0.135060 --> 0.132430).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 4.383877515792847
Epoch: 12, Steps: 79 | Train Loss: 0.2110456 Vali Loss: 0.1271951 Test Loss: 0.1431403
Validation loss decreased (0.132430 --> 0.127195).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 4.406247138977051
Epoch: 13, Steps: 79 | Train Loss: 0.2051328 Vali Loss: 0.1273217 Test Loss: 0.1369747
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 4.391577482223511
Epoch: 14, Steps: 79 | Train Loss: 0.1990464 Vali Loss: 0.1261698 Test Loss: 0.1377560
Validation loss decreased (0.127195 --> 0.126170).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 4.406261920928955
Epoch: 15, Steps: 79 | Train Loss: 0.1947590 Vali Loss: 0.1286236 Test Loss: 0.1360948
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 4.427814483642578
Epoch: 16, Steps: 79 | Train Loss: 0.1883199 Vali Loss: 0.1198910 Test Loss: 0.1342592
Validation loss decreased (0.126170 --> 0.119891).  Saving model ...
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 4.401974201202393
Epoch: 17, Steps: 79 | Train Loss: 0.1854186 Vali Loss: 0.1304451 Test Loss: 0.1316588
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 4.405482292175293
Epoch: 18, Steps: 79 | Train Loss: 0.1815823 Vali Loss: 0.1203213 Test Loss: 0.1287117
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 4.388932943344116
Epoch: 19, Steps: 79 | Train Loss: 0.1771311 Vali Loss: 0.1188081 Test Loss: 0.1272247
Validation loss decreased (0.119891 --> 0.118808).  Saving model ...
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 4.356738328933716
Epoch: 20, Steps: 79 | Train Loss: 0.1741904 Vali Loss: 0.1168975 Test Loss: 0.1303755
Validation loss decreased (0.118808 --> 0.116898).  Saving model ...
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 4.411039113998413
Epoch: 21, Steps: 79 | Train Loss: 0.1712603 Vali Loss: 0.1170760 Test Loss: 0.1261220
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 4.440703630447388
Epoch: 22, Steps: 79 | Train Loss: 0.1685669 Vali Loss: 0.1162955 Test Loss: 0.1246917
Validation loss decreased (0.116898 --> 0.116295).  Saving model ...
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 4.43730092048645
Epoch: 23, Steps: 79 | Train Loss: 0.1638358 Vali Loss: 0.1148639 Test Loss: 0.1218042
Validation loss decreased (0.116295 --> 0.114864).  Saving model ...
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 4.406850099563599
Epoch: 24, Steps: 79 | Train Loss: 0.1616113 Vali Loss: 0.1085731 Test Loss: 0.1166303
Validation loss decreased (0.114864 --> 0.108573).  Saving model ...
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 4.3911378383636475
Epoch: 25, Steps: 79 | Train Loss: 0.1589185 Vali Loss: 0.1226636 Test Loss: 0.1203168
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 4.375574588775635
Epoch: 26, Steps: 79 | Train Loss: 0.1556371 Vali Loss: 0.1156673 Test Loss: 0.1157347
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 4.381556272506714
Epoch: 27, Steps: 79 | Train Loss: 0.1537071 Vali Loss: 0.1171402 Test Loss: 0.1158172
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 4.394014835357666
Epoch: 28, Steps: 79 | Train Loss: 0.1511750 Vali Loss: 0.1115060 Test Loss: 0.1119666
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 4.433141708374023
Epoch: 29, Steps: 79 | Train Loss: 0.1498404 Vali Loss: 0.1096232 Test Loss: 0.1130719
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 4.367221117019653
Epoch: 30, Steps: 79 | Train Loss: 0.1473375 Vali Loss: 0.1136503 Test Loss: 0.1111774
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 4.416543006896973
Epoch: 31, Steps: 79 | Train Loss: 0.1449260 Vali Loss: 0.1125160 Test Loss: 0.1090310
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 4.440558910369873
Epoch: 32, Steps: 79 | Train Loss: 0.1423618 Vali Loss: 0.1155974 Test Loss: 0.1078206
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 4.432319641113281
Epoch: 33, Steps: 79 | Train Loss: 0.1400518 Vali Loss: 0.1068656 Test Loss: 0.1074006
Validation loss decreased (0.108573 --> 0.106866).  Saving model ...
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 4.342663288116455
Epoch: 34, Steps: 79 | Train Loss: 0.1381694 Vali Loss: 0.1112473 Test Loss: 0.1062019
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 4.420466899871826
Epoch: 35, Steps: 79 | Train Loss: 0.1372362 Vali Loss: 0.1124521 Test Loss: 0.1071649
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 5.409101247787476
Epoch: 36, Steps: 79 | Train Loss: 0.1346459 Vali Loss: 0.1100528 Test Loss: 0.1041090
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 5.168245792388916
Epoch: 37, Steps: 79 | Train Loss: 0.1335782 Vali Loss: 0.1114982 Test Loss: 0.1066629
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 5.072035312652588
Epoch: 38, Steps: 79 | Train Loss: 0.1323390 Vali Loss: 0.1079188 Test Loss: 0.1050968
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 5.147761106491089
Epoch: 39, Steps: 79 | Train Loss: 0.1302303 Vali Loss: 0.1090620 Test Loss: 0.1048803
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 5.1491265296936035
Epoch: 40, Steps: 79 | Train Loss: 0.1287741 Vali Loss: 0.1096401 Test Loss: 0.1045340
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 5.3364880084991455
Epoch: 41, Steps: 79 | Train Loss: 0.1266933 Vali Loss: 0.1163224 Test Loss: 0.1060119
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0009402
Epoch: 42 cost time: 5.055028438568115
Epoch: 42, Steps: 79 | Train Loss: 0.1261520 Vali Loss: 0.1122586 Test Loss: 0.1065842
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0009291
Epoch: 43 cost time: 4.759486675262451
Epoch: 43, Steps: 79 | Train Loss: 0.1240194 Vali Loss: 0.1087371 Test Loss: 0.1047570
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0009171
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.2308465838432312, mae:0.37523695826530457, rmse:0.48046496510505676, mape:0.013406164012849331, mspe:0.0002960980636999011, rse:0.3311653733253479, r2_score:0.8846013062824332, acc:0.9865938359871507
corr: [37.686043 37.761642 37.808796 37.870525 37.90542  37.866592 37.864513
 37.856987 37.80499  37.86406  37.91498  37.98649  37.592346 37.649277
 37.703205 37.75491  37.78783  37.757122 37.736057 37.752254 37.747417
 37.838894 37.92303  38.00856  37.80112  37.8595   37.899082 37.929718
 37.945946 37.90957  37.910034 37.922188 37.899788 37.969173 38.020634
 38.102695 37.786407 37.87116  37.915207 37.946033 37.95154  37.92314
 37.927574 37.96381  37.949455 38.00664  38.04032  38.087902 37.88708
 37.91539  37.952076 37.99836  37.999203 37.975746 37.96912  37.97312
 37.954666 38.009315 38.048347 38.09625  37.910065 37.94856  37.93364
 37.943222 37.934486 37.8901   37.84898  37.83823  37.810745 37.85765
 37.888096 37.924545 37.64391  37.679596 37.68958  37.669197 37.679417
 37.669067 37.651962 37.66351  37.611263 37.63525  37.69027  37.73629
 37.469048 37.50022  37.489967 37.489796 37.471607 37.439236 37.4389
 37.46662  37.441967 37.46747  37.49528  37.53333  37.335953 37.36577
 37.399334 37.43803  37.452106 37.45367  37.47206  37.511105 37.488384
 37.53     37.57218  37.605915 37.231674 37.27152  37.304794 37.315098
 37.333763 37.321636 37.32099  37.363342 37.35297  37.384182 37.42149
 37.429096 37.13992  37.16212  37.19842  37.236492 37.272797 37.263607
 37.262695 37.274628 37.243618 37.295147 37.354027 37.379604 36.97281
 36.997036 37.026665 37.075428 37.097565 37.075035 37.106922 37.113907
 37.0725   37.117096 37.150402 37.20423  36.850464 36.901268 36.92809
 36.9767   37.01824  36.98353  36.981155 37.02533  37.01376  37.03755
 37.07418  37.048    36.588646 36.61739  36.66776  36.70923  36.76191
 36.779118 36.799404 36.819508 36.78472  36.79737  36.840706 36.89269
 36.63552  36.692448 36.752914 36.81256  36.840252 36.833965 36.865974
 36.879147 36.837757 36.8569   36.89882  36.92531  36.69817  36.74087
 36.76748  36.810036 36.875328 36.84085  36.833523 36.856956 36.832397
 36.881065 36.93499  36.96469  36.77598  36.81982  36.889214 36.94288
 36.960228 36.909428 36.8988   36.910526 36.876213 36.90791  36.943195
 36.978058 36.774246 36.792053 36.82137  36.87129  36.919144 36.90219
 36.898563 36.94178  36.93088  36.98031  37.03616  37.053722 36.775665
 36.793564 36.83786  36.893005 36.921448 36.887894 36.88205  36.903538
 36.89538  36.928513 36.99238  37.03137  36.73768  36.779156 36.820244
 36.87339  36.944653 36.919224 36.893085 36.872322 36.807808 36.814377
 36.832542 36.856274 36.606297 36.64212  36.681698 36.733883 36.800404
 36.79407  36.791748 36.796032 36.78259  36.82644  36.85332  36.92383
 36.64014  36.680805 36.741726 36.787773 36.831306 36.79519  36.792847
 36.807457 36.785233 36.812046 36.853058 36.886456 36.485493 36.546844
 36.619164 36.699886 36.751957 36.70558  36.716885 36.739006 36.71101
 36.753223 36.773155 36.81031  36.522873 36.565414 36.621185 36.677517
 36.739315 36.692677 36.663826 36.68093  36.654537 36.676502 36.716457
 36.739616 36.414757 36.465027 36.521065 36.555    36.588898 36.5334
 36.52155  36.527054 36.506348 36.535954 36.581757 36.62849  36.315693
 36.363537 36.43345  36.466652 36.507584 36.46764  36.453903 36.47419
 36.443214 36.485638 36.55801  36.647175 36.450603 36.5232   36.60868
 36.635796 36.659294 36.615364 36.577965 36.566177 36.50976  36.513374
 36.55952  36.642815 36.38035  36.41148  36.466774 36.52509  36.592518
 36.56706  36.5692   36.586273 36.558125 36.586617 36.635    36.70971
 36.462353 36.538074 36.60822  36.66742  36.70436  36.66028  36.619427
 36.61429  36.562477 36.610313 36.671833 36.748703 36.69129  36.759254
 36.826622 36.89067  36.96543  36.90516  36.89131  36.877396 36.840614
 36.883377 36.93121  37.017635 36.9337   37.035316 37.136967 37.234818
 37.328213]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 4.9511878490448
Epoch: 1, Steps: 79 | Train Loss: 1.0210722 Vali Loss: 0.8729444 Test Loss: 0.8019893
Validation loss decreased (inf --> 0.872944).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 5.561912775039673
Epoch: 2, Steps: 79 | Train Loss: 0.8927576 Vali Loss: 0.7805816 Test Loss: 0.7167954
Validation loss decreased (0.872944 --> 0.780582).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 5.078236103057861
Epoch: 3, Steps: 79 | Train Loss: 0.7385785 Vali Loss: 0.5147574 Test Loss: 0.4711900
Validation loss decreased (0.780582 --> 0.514757).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 5.2263078689575195
Epoch: 4, Steps: 79 | Train Loss: 0.5220501 Vali Loss: 0.4147030 Test Loss: 0.3820289
Validation loss decreased (0.514757 --> 0.414703).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 5.382614612579346
Epoch: 5, Steps: 79 | Train Loss: 0.4447990 Vali Loss: 0.3531404 Test Loss: 0.3331101
Validation loss decreased (0.414703 --> 0.353140).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 5.065520763397217
Epoch: 6, Steps: 79 | Train Loss: 0.3860858 Vali Loss: 0.3024437 Test Loss: 0.2908936
Validation loss decreased (0.353140 --> 0.302444).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 4.904093265533447
Epoch: 7, Steps: 79 | Train Loss: 0.3351472 Vali Loss: 0.2332281 Test Loss: 0.2172927
Validation loss decreased (0.302444 --> 0.233228).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 5.71544075012207
Epoch: 8, Steps: 79 | Train Loss: 0.2713897 Vali Loss: 0.1753961 Test Loss: 0.1756155
Validation loss decreased (0.233228 --> 0.175396).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 5.483267545700073
Epoch: 9, Steps: 79 | Train Loss: 0.2402988 Vali Loss: 0.1560079 Test Loss: 0.1623641
Validation loss decreased (0.175396 --> 0.156008).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 5.2254555225372314
Epoch: 10, Steps: 79 | Train Loss: 0.2251871 Vali Loss: 0.1402373 Test Loss: 0.1489823
Validation loss decreased (0.156008 --> 0.140237).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 5.015819787979126
Epoch: 11, Steps: 79 | Train Loss: 0.2158628 Vali Loss: 0.1329809 Test Loss: 0.1434140
Validation loss decreased (0.140237 --> 0.132981).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 5.392405271530151
Epoch: 12, Steps: 79 | Train Loss: 0.2089909 Vali Loss: 0.1303959 Test Loss: 0.1412511
Validation loss decreased (0.132981 --> 0.130396).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 5.174909353256226
Epoch: 13, Steps: 79 | Train Loss: 0.1995139 Vali Loss: 0.1238121 Test Loss: 0.1335195
Validation loss decreased (0.130396 --> 0.123812).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 5.307316780090332
Epoch: 14, Steps: 79 | Train Loss: 0.1942461 Vali Loss: 0.1330153 Test Loss: 0.1405539
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 5.232168197631836
Epoch: 15, Steps: 79 | Train Loss: 0.1884762 Vali Loss: 0.1238643 Test Loss: 0.1342550
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 5.427700757980347
Epoch: 16, Steps: 79 | Train Loss: 0.1836443 Vali Loss: 0.1227895 Test Loss: 0.1318204
Validation loss decreased (0.123812 --> 0.122789).  Saving model ...
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 5.442563056945801
Epoch: 17, Steps: 79 | Train Loss: 0.1780305 Vali Loss: 0.1207015 Test Loss: 0.1251724
Validation loss decreased (0.122789 --> 0.120701).  Saving model ...
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 5.316033124923706
Epoch: 18, Steps: 79 | Train Loss: 0.1758791 Vali Loss: 0.1166528 Test Loss: 0.1214150
Validation loss decreased (0.120701 --> 0.116653).  Saving model ...
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 5.250605583190918
Epoch: 19, Steps: 79 | Train Loss: 0.1715408 Vali Loss: 0.1106210 Test Loss: 0.1187785
Validation loss decreased (0.116653 --> 0.110621).  Saving model ...
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 5.0034019947052
Epoch: 20, Steps: 79 | Train Loss: 0.1682201 Vali Loss: 0.1117834 Test Loss: 0.1147500
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 5.257987022399902
Epoch: 21, Steps: 79 | Train Loss: 0.1654456 Vali Loss: 0.1115788 Test Loss: 0.1137332
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 5.026396989822388
Epoch: 22, Steps: 79 | Train Loss: 0.1616456 Vali Loss: 0.1130205 Test Loss: 0.1182753
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 5.049856185913086
Epoch: 23, Steps: 79 | Train Loss: 0.1592178 Vali Loss: 0.1135790 Test Loss: 0.1174109
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 5.138375520706177
Epoch: 24, Steps: 79 | Train Loss: 0.1569728 Vali Loss: 0.1094967 Test Loss: 0.1077569
Validation loss decreased (0.110621 --> 0.109497).  Saving model ...
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 5.001774072647095
Epoch: 25, Steps: 79 | Train Loss: 0.1537285 Vali Loss: 0.1096716 Test Loss: 0.1083848
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 5.357219457626343
Epoch: 26, Steps: 79 | Train Loss: 0.1518375 Vali Loss: 0.1081653 Test Loss: 0.1087011
Validation loss decreased (0.109497 --> 0.108165).  Saving model ...
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 5.5207178592681885
Epoch: 27, Steps: 79 | Train Loss: 0.1491620 Vali Loss: 0.1033983 Test Loss: 0.1056355
Validation loss decreased (0.108165 --> 0.103398).  Saving model ...
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 5.790386438369751
Epoch: 28, Steps: 79 | Train Loss: 0.1468330 Vali Loss: 0.1092338 Test Loss: 0.1051137
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 5.1296586990356445
Epoch: 29, Steps: 79 | Train Loss: 0.1444398 Vali Loss: 0.1063794 Test Loss: 0.1042447
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 5.001589059829712
Epoch: 30, Steps: 79 | Train Loss: 0.1424854 Vali Loss: 0.1039663 Test Loss: 0.1040308
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 5.811825275421143
Epoch: 31, Steps: 79 | Train Loss: 0.1404946 Vali Loss: 0.1054224 Test Loss: 0.1009652
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 5.0281689167022705
Epoch: 32, Steps: 79 | Train Loss: 0.1386059 Vali Loss: 0.1089924 Test Loss: 0.1011631
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 5.373919248580933
Epoch: 33, Steps: 79 | Train Loss: 0.1362539 Vali Loss: 0.1018878 Test Loss: 0.0994243
Validation loss decreased (0.103398 --> 0.101888).  Saving model ...
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 5.170109987258911
Epoch: 34, Steps: 79 | Train Loss: 0.1347307 Vali Loss: 0.1077791 Test Loss: 0.1008791
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 4.971423625946045
Epoch: 35, Steps: 79 | Train Loss: 0.1324713 Vali Loss: 0.1008065 Test Loss: 0.0972178
Validation loss decreased (0.101888 --> 0.100806).  Saving model ...
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 5.0026750564575195
Epoch: 36, Steps: 79 | Train Loss: 0.1298928 Vali Loss: 0.0988650 Test Loss: 0.0934360
Validation loss decreased (0.100806 --> 0.098865).  Saving model ...
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 4.891557693481445
Epoch: 37, Steps: 79 | Train Loss: 0.1280099 Vali Loss: 0.0979061 Test Loss: 0.0934968
Validation loss decreased (0.098865 --> 0.097906).  Saving model ...
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 4.931134223937988
Epoch: 38, Steps: 79 | Train Loss: 0.1261032 Vali Loss: 0.1008513 Test Loss: 0.0938795
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 5.2046544551849365
Epoch: 39, Steps: 79 | Train Loss: 0.1237804 Vali Loss: 0.1027755 Test Loss: 0.0959863
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 5.0424275398254395
Epoch: 40, Steps: 79 | Train Loss: 0.1224303 Vali Loss: 0.1005423 Test Loss: 0.0932342
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 5.595086097717285
Epoch: 41, Steps: 79 | Train Loss: 0.1211338 Vali Loss: 0.1049220 Test Loss: 0.0943251
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009402
Epoch: 42 cost time: 5.088312864303589
Epoch: 42, Steps: 79 | Train Loss: 0.1193978 Vali Loss: 0.1000861 Test Loss: 0.0924520
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009291
Epoch: 43 cost time: 5.640784025192261
Epoch: 43, Steps: 79 | Train Loss: 0.1186682 Vali Loss: 0.1020715 Test Loss: 0.0916186
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009171
Epoch: 44 cost time: 5.11993408203125
Epoch: 44, Steps: 79 | Train Loss: 0.1166862 Vali Loss: 0.1023993 Test Loss: 0.0958710
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0009043
Epoch: 45 cost time: 5.344768762588501
Epoch: 45, Steps: 79 | Train Loss: 0.1163077 Vali Loss: 0.1017425 Test Loss: 0.0924718
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0008907
Epoch: 46 cost time: 5.740663766860962
Epoch: 46, Steps: 79 | Train Loss: 0.1147020 Vali Loss: 0.0969063 Test Loss: 0.0907889
Validation loss decreased (0.097906 --> 0.096906).  Saving model ...
Adjusting learning rate to: 0.0008763
Epoch: 47 cost time: 4.872369289398193
Epoch: 47, Steps: 79 | Train Loss: 0.1137863 Vali Loss: 0.0988880 Test Loss: 0.0917102
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008612
Epoch: 48 cost time: 4.905579328536987
Epoch: 48, Steps: 79 | Train Loss: 0.1124189 Vali Loss: 0.1023778 Test Loss: 0.0942306
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0008453
Epoch: 49 cost time: 5.223684787750244
Epoch: 49, Steps: 79 | Train Loss: 0.1123752 Vali Loss: 0.0995068 Test Loss: 0.0931915
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0008288
Epoch: 50 cost time: 4.945246934890747
Epoch: 50, Steps: 79 | Train Loss: 0.1111915 Vali Loss: 0.1035604 Test Loss: 0.0936587
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0008115
Epoch: 51 cost time: 5.330023288726807
Epoch: 51, Steps: 79 | Train Loss: 0.1099512 Vali Loss: 0.0982394 Test Loss: 0.0906698
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0007937
Epoch: 52 cost time: 5.714650869369507
Epoch: 52, Steps: 79 | Train Loss: 0.1092056 Vali Loss: 0.0983986 Test Loss: 0.0925605
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0007752
Epoch: 53 cost time: 5.148559093475342
Epoch: 53, Steps: 79 | Train Loss: 0.1081166 Vali Loss: 0.1029242 Test Loss: 0.0971402
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0007562
Epoch: 54 cost time: 5.2060019969940186
Epoch: 54, Steps: 79 | Train Loss: 0.1074955 Vali Loss: 0.0988791 Test Loss: 0.0929513
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0007367
Epoch: 55 cost time: 5.092763662338257
Epoch: 55, Steps: 79 | Train Loss: 0.1063413 Vali Loss: 0.0989828 Test Loss: 0.0930852
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0007167
Epoch: 56 cost time: 4.758660793304443
Epoch: 56, Steps: 79 | Train Loss: 0.1056464 Vali Loss: 0.1010216 Test Loss: 0.0946113
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0006963
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.20099379122257233, mae:0.3484036326408386, rmse:0.4483233094215393, mape:0.012452851980924606, mspe:0.0002588059869594872, rse:0.30901139974594116, r2_score:0.9013574179103624, acc:0.9875471480190754
corr: [37.89934  37.826206 37.749756 37.684856 37.722897 37.711494 37.743584
 37.76713  37.806847 37.836723 37.832554 37.79749  37.78045  37.902866
 37.822266 37.72535  37.680172 37.69851  37.68721  37.71875  37.73908
 37.7829   37.802074 37.81433  37.7814   37.78863  37.846275 37.723473
 37.635365 37.585674 37.617294 37.61429  37.62655  37.631332 37.674694
 37.7047   37.69924  37.677563 37.666767 37.87848  37.763702 37.670963
 37.614666 37.62056  37.600426 37.605545 37.60459  37.631664 37.666912
 37.6961   37.674065 37.635178 37.94253  37.825527 37.74022  37.695854
 37.738644 37.74087  37.761208 37.785156 37.80579  37.803787 37.80725
 37.768814 37.713287 37.952614 37.857937 37.76372  37.685856 37.666725
 37.62199  37.63778  37.628487 37.62054  37.635166 37.62145  37.543217
 37.464928 37.691933 37.58885  37.49242  37.403767 37.40491  37.38364
 37.38677  37.369755 37.388313 37.392963 37.365128 37.295002 37.202698
 37.554573 37.4523   37.345318 37.266853 37.26104  37.215702 37.203144
 37.19726  37.204483 37.217567 37.226604 37.19285  37.160664 37.367283
 37.293346 37.209705 37.14676  37.15478  37.10837  37.0909   37.073807
 37.07357  37.068462 37.07132  37.01964  36.96416  37.210495 37.113697
 37.01027  36.929203 36.88728  36.844646 36.836018 36.856895 36.870605
 36.86078  36.850075 36.807686 36.784584 37.070107 36.958878 36.845592
 36.73547  36.69033  36.632607 36.62039  36.59719  36.605553 36.60373
 36.60562  36.554688 36.495506 36.849678 36.7423   36.649162 36.579384
 36.575985 36.541836 36.537586 36.510624 36.504932 36.46464  36.442154
 36.40178  36.365173 36.700462 36.608204 36.50153  36.424816 36.37178
 36.33576  36.327503 36.324554 36.326042 36.320477 36.340847 36.309002
 36.275883 36.64409  36.554646 36.477375 36.410686 36.414318 36.385014
 36.378456 36.373688 36.385246 36.3834   36.376987 36.349915 36.320023
 36.644962 36.565693 36.480755 36.435417 36.43587  36.383957 36.35157
 36.32631  36.326603 36.329025 36.32691  36.310314 36.281643 36.667564
 36.578007 36.482903 36.403942 36.395233 36.362465 36.37251  36.379356
 36.401226 36.411625 36.422825 36.39975  36.38322  36.675346 36.60219
 36.53605  36.50362  36.516033 36.465847 36.434402 36.430447 36.444687
 36.45709  36.45359  36.454315 36.434174 36.636505 36.564735 36.497765
 36.45398  36.471977 36.45867  36.455082 36.442673 36.44929  36.474194
 36.507153 36.452507 36.42179  36.623928 36.538475 36.46318  36.382603
 36.36971  36.349483 36.361256 36.353455 36.381294 36.394287 36.433426
 36.452347 36.436897 36.742775 36.672344 36.588917 36.53677  36.534264
 36.500202 36.48906  36.48329  36.506664 36.49812  36.48668  36.444885
 36.40385  36.70099  36.60415  36.505474 36.437347 36.432095 36.417763
 36.434055 36.43019  36.46986  36.49471  36.51786  36.51391  36.50361
 36.75246  36.676876 36.58946  36.50809  36.5128   36.488464 36.489506
 36.494156 36.53307  36.53657  36.54265  36.50975  36.471405 36.782307
 36.710114 36.59296  36.49128  36.486122 36.453636 36.451942 36.444378
 36.46555  36.465794 36.478806 36.460133 36.448475 36.73001  36.64026
 36.54794  36.453037 36.45197  36.40617  36.409832 36.39653  36.414356
 36.426113 36.45463  36.455765 36.47359  36.674915 36.603405 36.53023
 36.479008 36.508125 36.523567 36.545284 36.569824 36.611267 36.661743
 36.70596  36.698563 36.691734 36.87837  36.752132 36.6366   36.582455
 36.59397  36.597633 36.63347  36.636475 36.67897  36.706882 36.771595
 36.789207 36.788128 36.935684 36.871635 36.76558  36.70755  36.70946
 36.698277 36.718727 36.73897  36.77799  36.819504 36.87219  36.868774
 36.87954  37.106064 36.98582  36.866238 36.78776  36.820194 36.828575
 36.86325  36.908855 36.98387  37.081944 37.158306 37.168385 37.195393
 37.357372]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 5.130498647689819
Epoch: 1, Steps: 79 | Train Loss: 1.0149996 Vali Loss: 0.8644215 Test Loss: 0.7907927
Validation loss decreased (inf --> 0.864421).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 5.446431636810303
Epoch: 2, Steps: 79 | Train Loss: 0.8883433 Vali Loss: 0.7903299 Test Loss: 0.7214576
Validation loss decreased (0.864421 --> 0.790330).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 5.088147878646851
Epoch: 3, Steps: 79 | Train Loss: 0.7806942 Vali Loss: 0.6101541 Test Loss: 0.5399421
Validation loss decreased (0.790330 --> 0.610154).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 5.369790077209473
Epoch: 4, Steps: 79 | Train Loss: 0.5751436 Vali Loss: 0.4502727 Test Loss: 0.4198397
Validation loss decreased (0.610154 --> 0.450273).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 5.9030091762542725
Epoch: 5, Steps: 79 | Train Loss: 0.4654054 Vali Loss: 0.3578834 Test Loss: 0.3479971
Validation loss decreased (0.450273 --> 0.357883).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 5.466491222381592
Epoch: 6, Steps: 79 | Train Loss: 0.3915677 Vali Loss: 0.2864410 Test Loss: 0.2753359
Validation loss decreased (0.357883 --> 0.286441).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 4.960073232650757
Epoch: 7, Steps: 79 | Train Loss: 0.3124286 Vali Loss: 0.2108586 Test Loss: 0.2065115
Validation loss decreased (0.286441 --> 0.210859).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 5.11575174331665
Epoch: 8, Steps: 79 | Train Loss: 0.2697761 Vali Loss: 0.1719235 Test Loss: 0.1773015
Validation loss decreased (0.210859 --> 0.171923).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 5.138880491256714
Epoch: 9, Steps: 79 | Train Loss: 0.2432015 Vali Loss: 0.1545325 Test Loss: 0.1590270
Validation loss decreased (0.171923 --> 0.154533).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 5.408674240112305
Epoch: 10, Steps: 79 | Train Loss: 0.2279225 Vali Loss: 0.1372308 Test Loss: 0.1477568
Validation loss decreased (0.154533 --> 0.137231).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 5.447612762451172
Epoch: 11, Steps: 79 | Train Loss: 0.2164910 Vali Loss: 0.1438886 Test Loss: 0.1459318
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 4.923051595687866
Epoch: 12, Steps: 79 | Train Loss: 0.2089753 Vali Loss: 0.1314847 Test Loss: 0.1367733
Validation loss decreased (0.137231 --> 0.131485).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 5.915598630905151
Epoch: 13, Steps: 79 | Train Loss: 0.2014245 Vali Loss: 0.1265407 Test Loss: 0.1347912
Validation loss decreased (0.131485 --> 0.126541).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 4.774157524108887
Epoch: 14, Steps: 79 | Train Loss: 0.1933437 Vali Loss: 0.1295747 Test Loss: 0.1347639
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 5.528165578842163
Epoch: 15, Steps: 79 | Train Loss: 0.1885036 Vali Loss: 0.1208877 Test Loss: 0.1327090
Validation loss decreased (0.126541 --> 0.120888).  Saving model ...
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 5.244189023971558
Epoch: 16, Steps: 79 | Train Loss: 0.1842566 Vali Loss: 0.1152326 Test Loss: 0.1264473
Validation loss decreased (0.120888 --> 0.115233).  Saving model ...
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 5.182379722595215
Epoch: 17, Steps: 79 | Train Loss: 0.1794577 Vali Loss: 0.1174123 Test Loss: 0.1256751
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 5.291482925415039
Epoch: 18, Steps: 79 | Train Loss: 0.1764596 Vali Loss: 0.1181060 Test Loss: 0.1273278
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 5.3826003074646
Epoch: 19, Steps: 79 | Train Loss: 0.1713832 Vali Loss: 0.1116170 Test Loss: 0.1191538
Validation loss decreased (0.115233 --> 0.111617).  Saving model ...
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 4.847034454345703
Epoch: 20, Steps: 79 | Train Loss: 0.1686300 Vali Loss: 0.1106526 Test Loss: 0.1186730
Validation loss decreased (0.111617 --> 0.110653).  Saving model ...
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 4.8679399490356445
Epoch: 21, Steps: 79 | Train Loss: 0.1647615 Vali Loss: 0.1084100 Test Loss: 0.1187322
Validation loss decreased (0.110653 --> 0.108410).  Saving model ...
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 4.953893184661865
Epoch: 22, Steps: 79 | Train Loss: 0.1639048 Vali Loss: 0.1138876 Test Loss: 0.1176511
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 4.432650089263916
Epoch: 23, Steps: 79 | Train Loss: 0.1606428 Vali Loss: 0.1055257 Test Loss: 0.1163942
Validation loss decreased (0.108410 --> 0.105526).  Saving model ...
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 4.091606855392456
Epoch: 24, Steps: 79 | Train Loss: 0.1580560 Vali Loss: 0.1083423 Test Loss: 0.1141836
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 4.0992841720581055
Epoch: 25, Steps: 79 | Train Loss: 0.1551092 Vali Loss: 0.1092050 Test Loss: 0.1140993
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 4.135290622711182
Epoch: 26, Steps: 79 | Train Loss: 0.1528774 Vali Loss: 0.1044534 Test Loss: 0.1107210
Validation loss decreased (0.105526 --> 0.104453).  Saving model ...
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 4.116052150726318
Epoch: 27, Steps: 79 | Train Loss: 0.1495429 Vali Loss: 0.1066082 Test Loss: 0.1119249
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 4.093719720840454
Epoch: 28, Steps: 79 | Train Loss: 0.1478319 Vali Loss: 0.1107277 Test Loss: 0.1089232
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 4.099709510803223
Epoch: 29, Steps: 79 | Train Loss: 0.1456533 Vali Loss: 0.1067787 Test Loss: 0.1100115
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 4.206267833709717
Epoch: 30, Steps: 79 | Train Loss: 0.1428443 Vali Loss: 0.1044724 Test Loss: 0.1070111
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 4.287280082702637
Epoch: 31, Steps: 79 | Train Loss: 0.1408454 Vali Loss: 0.1138308 Test Loss: 0.1054541
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 4.18670916557312
Epoch: 32, Steps: 79 | Train Loss: 0.1391152 Vali Loss: 0.1006658 Test Loss: 0.1019034
Validation loss decreased (0.104453 --> 0.100666).  Saving model ...
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 4.200188875198364
Epoch: 33, Steps: 79 | Train Loss: 0.1379033 Vali Loss: 0.1161107 Test Loss: 0.1078659
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 4.1306538581848145
Epoch: 34, Steps: 79 | Train Loss: 0.1360882 Vali Loss: 0.1082103 Test Loss: 0.1048757
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 4.154348850250244
Epoch: 35, Steps: 79 | Train Loss: 0.1341450 Vali Loss: 0.1065092 Test Loss: 0.1067753
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 4.145495653152466
Epoch: 36, Steps: 79 | Train Loss: 0.1324087 Vali Loss: 0.1077131 Test Loss: 0.1029828
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 4.134255886077881
Epoch: 37, Steps: 79 | Train Loss: 0.1303662 Vali Loss: 0.1038683 Test Loss: 0.1002451
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 4.191544771194458
Epoch: 38, Steps: 79 | Train Loss: 0.1285641 Vali Loss: 0.1100560 Test Loss: 0.1003526
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 4.150872468948364
Epoch: 39, Steps: 79 | Train Loss: 0.1263622 Vali Loss: 0.1109189 Test Loss: 0.0986263
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 4.177327394485474
Epoch: 40, Steps: 79 | Train Loss: 0.1246379 Vali Loss: 0.1047827 Test Loss: 0.0971954
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 4.248620271682739
Epoch: 41, Steps: 79 | Train Loss: 0.1229385 Vali Loss: 0.0998609 Test Loss: 0.0934095
Validation loss decreased (0.100666 --> 0.099861).  Saving model ...
Adjusting learning rate to: 0.0009402
Epoch: 42 cost time: 4.2408366203308105
Epoch: 42, Steps: 79 | Train Loss: 0.1220909 Vali Loss: 0.1021926 Test Loss: 0.0939136
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009291
Epoch: 43 cost time: 4.1592888832092285
Epoch: 43, Steps: 79 | Train Loss: 0.1203867 Vali Loss: 0.1015105 Test Loss: 0.0932816
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009171
Epoch: 44 cost time: 4.164383888244629
Epoch: 44, Steps: 79 | Train Loss: 0.1193360 Vali Loss: 0.1050757 Test Loss: 0.0945133
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009043
Epoch: 45 cost time: 4.191346645355225
Epoch: 45, Steps: 79 | Train Loss: 0.1177062 Vali Loss: 0.1092610 Test Loss: 0.0973940
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0008907
Epoch: 46 cost time: 4.137045621871948
Epoch: 46, Steps: 79 | Train Loss: 0.1163125 Vali Loss: 0.1005822 Test Loss: 0.0949849
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0008763
Epoch: 47 cost time: 4.189364910125732
Epoch: 47, Steps: 79 | Train Loss: 0.1158967 Vali Loss: 0.1045018 Test Loss: 0.0932602
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0008612
Epoch: 48 cost time: 4.221548795700073
Epoch: 48, Steps: 79 | Train Loss: 0.1145368 Vali Loss: 0.1028904 Test Loss: 0.0922633
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0008453
Epoch: 49 cost time: 4.183712482452393
Epoch: 49, Steps: 79 | Train Loss: 0.1135944 Vali Loss: 0.1031664 Test Loss: 0.0921589
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0008288
Epoch: 50 cost time: 4.2915732860565186
Epoch: 50, Steps: 79 | Train Loss: 0.1128285 Vali Loss: 0.0997813 Test Loss: 0.0920662
Validation loss decreased (0.099861 --> 0.099781).  Saving model ...
Adjusting learning rate to: 0.0008115
Epoch: 51 cost time: 4.139490365982056
Epoch: 51, Steps: 79 | Train Loss: 0.1117154 Vali Loss: 0.0970113 Test Loss: 0.0910208
Validation loss decreased (0.099781 --> 0.097011).  Saving model ...
Adjusting learning rate to: 0.0007937
Epoch: 52 cost time: 4.126247882843018
Epoch: 52, Steps: 79 | Train Loss: 0.1110650 Vali Loss: 0.1005310 Test Loss: 0.0946126
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0007752
Epoch: 53 cost time: 4.129652500152588
Epoch: 53, Steps: 79 | Train Loss: 0.1101949 Vali Loss: 0.1025019 Test Loss: 0.0953798
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0007562
Epoch: 54 cost time: 4.161834239959717
Epoch: 54, Steps: 79 | Train Loss: 0.1093565 Vali Loss: 0.1023093 Test Loss: 0.0936745
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0007367
Epoch: 55 cost time: 4.158177614212036
Epoch: 55, Steps: 79 | Train Loss: 0.1090245 Vali Loss: 0.0999409 Test Loss: 0.0952442
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0007167
Epoch: 56 cost time: 4.172881126403809
Epoch: 56, Steps: 79 | Train Loss: 0.1079886 Vali Loss: 0.0997008 Test Loss: 0.0953669
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0006963
Epoch: 57 cost time: 4.098920106887817
Epoch: 57, Steps: 79 | Train Loss: 0.1072337 Vali Loss: 0.1046294 Test Loss: 0.0969830
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0006754
Epoch: 58 cost time: 4.123219728469849
Epoch: 58, Steps: 79 | Train Loss: 0.1068519 Vali Loss: 0.0968950 Test Loss: 0.0907859
Validation loss decreased (0.097011 --> 0.096895).  Saving model ...
Adjusting learning rate to: 0.0006542
Epoch: 59 cost time: 4.211243391036987
Epoch: 59, Steps: 79 | Train Loss: 0.1059666 Vali Loss: 0.1015678 Test Loss: 0.0937094
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0006327
Epoch: 60 cost time: 4.095677375793457
Epoch: 60, Steps: 79 | Train Loss: 0.1057462 Vali Loss: 0.1002008 Test Loss: 0.0915742
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0006110
Epoch: 61 cost time: 4.1406519412994385
Epoch: 61, Steps: 79 | Train Loss: 0.1047193 Vali Loss: 0.1020318 Test Loss: 0.0926033
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0005890
Epoch: 62 cost time: 4.141743421554565
Epoch: 62, Steps: 79 | Train Loss: 0.1036422 Vali Loss: 0.0991882 Test Loss: 0.0931099
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0005668
Epoch: 63 cost time: 4.162205219268799
Epoch: 63, Steps: 79 | Train Loss: 0.1036302 Vali Loss: 0.0979187 Test Loss: 0.0933994
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0005445
Epoch: 64 cost time: 4.177207708358765
Epoch: 64, Steps: 79 | Train Loss: 0.1032353 Vali Loss: 0.1030865 Test Loss: 0.0960679
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0005222
Epoch: 65 cost time: 4.290808200836182
Epoch: 65, Steps: 79 | Train Loss: 0.1024406 Vali Loss: 0.0973798 Test Loss: 0.0922718
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0004997
Epoch: 66 cost time: 4.134138345718384
Epoch: 66, Steps: 79 | Train Loss: 0.1018111 Vali Loss: 0.1006933 Test Loss: 0.0943040
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0004773
Epoch: 67 cost time: 4.199395418167114
Epoch: 67, Steps: 79 | Train Loss: 0.1011226 Vali Loss: 0.1003966 Test Loss: 0.0938240
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0004549
Epoch: 68 cost time: 4.78226900100708
Epoch: 68, Steps: 79 | Train Loss: 0.1005417 Vali Loss: 0.1012868 Test Loss: 0.0956924
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0004326
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.20320406556129456, mae:0.3505037724971771, rmse:0.45078161358833313, mape:0.012513088062405586, mspe:0.00026059430092573166, rse:0.3107058107852936, r2_score:0.8974869347689975, acc:0.9874869119375944
corr: [37.80228  37.800915 37.81832  37.830185 37.888676 37.90609  37.874084
 37.90559  37.91697  37.905197 37.88175  37.884357 37.897125 37.93079
 37.613983 37.55037  37.523193 37.52258  37.571407 37.62898  37.626587
 37.70154  37.727062 37.730797 37.765705 37.815144 37.855904 37.922924
 37.55318  37.49351  37.468727 37.480213 37.539562 37.5806   37.55283
 37.62883  37.671944 37.710907 37.730457 37.744637 37.769627 37.826958
 37.618263 37.556725 37.49385  37.47607  37.48695  37.48797  37.463104
 37.532337 37.549393 37.54299  37.541115 37.54659  37.579247 37.627335
 37.43061  37.374817 37.370026 37.35579  37.39156  37.43074  37.390785
 37.438828 37.469593 37.479248 37.490543 37.485886 37.509056 37.568073
 37.24422  37.17684  37.141632 37.120163 37.138927 37.141857 37.117577
 37.161106 37.1843   37.185097 37.19438  37.20207  37.251972 37.34284
 37.12024  37.070843 37.040657 37.004932 37.032776 37.054745 37.020462
 37.03396  37.02343  37.028748 37.034122 37.05238  37.099503 37.176945
 36.868694 36.829655 36.82897  36.813072 36.863712 36.915665 36.913826
 36.973446 36.974987 36.975906 36.953186 36.92695  36.96157  37.037945
 36.79134  36.72779  36.718918 36.715054 36.77689  36.832775 36.83323
 36.892715 36.912453 36.912136 36.868637 36.858635 36.87974  36.942783
 36.547787 36.47803  36.46876  36.473793 36.541042 36.61326  36.617645
 36.666103 36.675938 36.664253 36.645008 36.644165 36.659004 36.684902
 36.359566 36.339794 36.358257 36.368332 36.430485 36.47971  36.4477
 36.492466 36.50892  36.471657 36.41519  36.386177 36.383698 36.43412
 36.189392 36.151142 36.159897 36.17391  36.267654 36.33982  36.354713
 36.411224 36.442738 36.446953 36.41564  36.385807 36.372215 36.397747
 36.167305 36.15739  36.197426 36.259823 36.359894 36.42288  36.416634
 36.45934  36.4726   36.47399  36.431274 36.406002 36.401722 36.4237
 36.27749  36.230137 36.23196  36.241573 36.284023 36.33082  36.319504
 36.368137 36.37587  36.35604  36.330635 36.29459  36.28558  36.31788
 36.275127 36.254593 36.25181  36.253593 36.317833 36.366695 36.344418
 36.39742  36.39972  36.384617 36.365177 36.338768 36.346367 36.371407
 36.255592 36.21871  36.21565  36.1994   36.250626 36.275227 36.252876
 36.303944 36.306416 36.292305 36.289413 36.282593 36.27892  36.322136
 36.121006 36.09511  36.1007   36.09725  36.175182 36.239426 36.24596
 36.300613 36.30552  36.29507  36.27009  36.250484 36.234337 36.255898
 36.121437 36.09571  36.111485 36.123142 36.172634 36.26318  36.293102
 36.34096  36.34199  36.336475 36.313354 36.290703 36.256428 36.273445
 36.138638 36.133278 36.150196 36.172585 36.220856 36.297894 36.300083
 36.363117 36.373695 36.35078  36.28988  36.242046 36.22404  36.246643
 36.27129  36.2447   36.26075  36.268085 36.342567 36.416695 36.420162
 36.462467 36.463203 36.424644 36.394814 36.388397 36.365437 36.374672
 36.32126  36.30634  36.36323  36.395466 36.464745 36.51665  36.525265
 36.596863 36.610558 36.58599  36.53465  36.508247 36.507748 36.51626
 36.46655  36.44042  36.473625 36.491653 36.56033  36.61003  36.58444
 36.639927 36.656143 36.64679  36.616035 36.572468 36.576492 36.60919
 36.45307  36.411083 36.415745 36.420906 36.49227  36.535755 36.539383
 36.619434 36.607685 36.594425 36.562992 36.53287  36.521233 36.56374
 36.484726 36.465923 36.494263 36.50927  36.588436 36.628395 36.612896
 36.653275 36.653706 36.642666 36.622383 36.61128  36.637917 36.696552
 36.637066 36.605484 36.582123 36.582344 36.652424 36.68266  36.6409
 36.667225 36.653423 36.66726  36.672604 36.684895 36.729294 36.79531
 36.80632  36.770023 36.77939  36.77217  36.806255 36.811405 36.790287
 36.862694 36.87036  36.881744 36.904808 36.94328  36.996696 37.021694
 36.98964 ]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 5.233130931854248
Epoch: 1, Steps: 79 | Train Loss: 1.0599911 Vali Loss: 0.8917091 Test Loss: 0.8207643
Validation loss decreased (inf --> 0.891709).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 5.00620174407959
Epoch: 2, Steps: 79 | Train Loss: 0.9111971 Vali Loss: 0.7901058 Test Loss: 0.7198255
Validation loss decreased (0.891709 --> 0.790106).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 5.473777532577515
Epoch: 3, Steps: 79 | Train Loss: 0.7840223 Vali Loss: 0.5492835 Test Loss: 0.5002801
Validation loss decreased (0.790106 --> 0.549284).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 5.483316421508789
Epoch: 4, Steps: 79 | Train Loss: 0.5537517 Vali Loss: 0.3883717 Test Loss: 0.3677118
Validation loss decreased (0.549284 --> 0.388372).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 5.0800862312316895
Epoch: 5, Steps: 79 | Train Loss: 0.4583442 Vali Loss: 0.3573220 Test Loss: 0.3279915
Validation loss decreased (0.388372 --> 0.357322).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 5.880394220352173
Epoch: 6, Steps: 79 | Train Loss: 0.3942617 Vali Loss: 0.2950464 Test Loss: 0.2844984
Validation loss decreased (0.357322 --> 0.295046).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 4.816195487976074
Epoch: 7, Steps: 79 | Train Loss: 0.3259953 Vali Loss: 0.2237348 Test Loss: 0.2137131
Validation loss decreased (0.295046 --> 0.223735).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 5.423689365386963
Epoch: 8, Steps: 79 | Train Loss: 0.2726882 Vali Loss: 0.1728389 Test Loss: 0.1789443
Validation loss decreased (0.223735 --> 0.172839).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 5.235480308532715
Epoch: 9, Steps: 79 | Train Loss: 0.2471578 Vali Loss: 0.1516765 Test Loss: 0.1636224
Validation loss decreased (0.172839 --> 0.151677).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 4.830167055130005
Epoch: 10, Steps: 79 | Train Loss: 0.2299505 Vali Loss: 0.1433136 Test Loss: 0.1498637
Validation loss decreased (0.151677 --> 0.143314).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 5.6075215339660645
Epoch: 11, Steps: 79 | Train Loss: 0.2203627 Vali Loss: 0.1363268 Test Loss: 0.1474518
Validation loss decreased (0.143314 --> 0.136327).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 5.571233510971069
Epoch: 12, Steps: 79 | Train Loss: 0.2110181 Vali Loss: 0.1299747 Test Loss: 0.1407236
Validation loss decreased (0.136327 --> 0.129975).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 4.031506061553955
Epoch: 13, Steps: 79 | Train Loss: 0.2028419 Vali Loss: 0.1234366 Test Loss: 0.1382151
Validation loss decreased (0.129975 --> 0.123437).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 4.041578054428101
Epoch: 14, Steps: 79 | Train Loss: 0.1983061 Vali Loss: 0.1194801 Test Loss: 0.1352679
Validation loss decreased (0.123437 --> 0.119480).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 4.070160150527954
Epoch: 15, Steps: 79 | Train Loss: 0.1919968 Vali Loss: 0.1184515 Test Loss: 0.1313841
Validation loss decreased (0.119480 --> 0.118452).  Saving model ...
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 4.034493446350098
Epoch: 16, Steps: 79 | Train Loss: 0.1878006 Vali Loss: 0.1187450 Test Loss: 0.1304404
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 4.035579204559326
Epoch: 17, Steps: 79 | Train Loss: 0.1836406 Vali Loss: 0.1174200 Test Loss: 0.1288351
Validation loss decreased (0.118452 --> 0.117420).  Saving model ...
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 4.028298616409302
Epoch: 18, Steps: 79 | Train Loss: 0.1796498 Vali Loss: 0.1183735 Test Loss: 0.1283278
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 4.119976043701172
Epoch: 19, Steps: 79 | Train Loss: 0.1760281 Vali Loss: 0.1202249 Test Loss: 0.1268463
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 4.077744245529175
Epoch: 20, Steps: 79 | Train Loss: 0.1737802 Vali Loss: 0.1158646 Test Loss: 0.1261608
Validation loss decreased (0.117420 --> 0.115865).  Saving model ...
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 4.071531534194946
Epoch: 21, Steps: 79 | Train Loss: 0.1710619 Vali Loss: 0.1147834 Test Loss: 0.1245807
Validation loss decreased (0.115865 --> 0.114783).  Saving model ...
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 4.159008502960205
Epoch: 22, Steps: 79 | Train Loss: 0.1675905 Vali Loss: 0.1157295 Test Loss: 0.1236674
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 4.132945537567139
Epoch: 23, Steps: 79 | Train Loss: 0.1649421 Vali Loss: 0.1197083 Test Loss: 0.1227179
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 4.154415607452393
Epoch: 24, Steps: 79 | Train Loss: 0.1632230 Vali Loss: 0.1232660 Test Loss: 0.1264777
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 4.049118757247925
Epoch: 25, Steps: 79 | Train Loss: 0.1624132 Vali Loss: 0.1088914 Test Loss: 0.1199342
Validation loss decreased (0.114783 --> 0.108891).  Saving model ...
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 4.137985467910767
Epoch: 26, Steps: 79 | Train Loss: 0.1595925 Vali Loss: 0.1204907 Test Loss: 0.1244809
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 4.090907096862793
Epoch: 27, Steps: 79 | Train Loss: 0.1569318 Vali Loss: 0.1120668 Test Loss: 0.1191379
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 4.133295297622681
Epoch: 28, Steps: 79 | Train Loss: 0.1553588 Vali Loss: 0.1149672 Test Loss: 0.1185009
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 4.194228172302246
Epoch: 29, Steps: 79 | Train Loss: 0.1527850 Vali Loss: 0.1155108 Test Loss: 0.1182260
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 4.04656982421875
Epoch: 30, Steps: 79 | Train Loss: 0.1506461 Vali Loss: 0.1216038 Test Loss: 0.1196616
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 4.148917198181152
Epoch: 31, Steps: 79 | Train Loss: 0.1495658 Vali Loss: 0.1215881 Test Loss: 0.1200268
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 4.096068859100342
Epoch: 32, Steps: 79 | Train Loss: 0.1483521 Vali Loss: 0.1164817 Test Loss: 0.1164270
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 4.04517126083374
Epoch: 33, Steps: 79 | Train Loss: 0.1462122 Vali Loss: 0.1138320 Test Loss: 0.1189184
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 4.106904983520508
Epoch: 34, Steps: 79 | Train Loss: 0.1438994 Vali Loss: 0.1136557 Test Loss: 0.1146551
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 4.150976657867432
Epoch: 35, Steps: 79 | Train Loss: 0.1430394 Vali Loss: 0.1126795 Test Loss: 0.1141931
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0009874
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.2829875946044922, mae:0.41425976157188416, rmse:0.5319657921791077, mape:0.014836330898106098, mspe:0.0003677626373246312, rse:0.3666628301143646, r2_score:0.8508249033302846, acc:0.9851636691018939
corr: [37.46764  37.577793 37.487186 37.48368  37.506817 37.39472  37.630577
 37.62689  37.630024 37.751354 37.840336 37.897495 37.926994 37.800198
 37.90795  37.39659  37.47965  37.376293 37.37571  37.423634 37.281784
 37.542446 37.6057   37.64895  37.822796 37.925144 38.004204 38.06033
 37.9468   38.018093 37.553524 37.634403 37.49923  37.455864 37.484226
 37.307335 37.535217 37.559414 37.546535 37.719345 37.821    37.891376
 37.92539  37.80904  37.898865 37.516865 37.61541  37.480522 37.410183
 37.43669  37.15833  37.403664 37.474926 37.452198 37.62524  37.743034
 37.766846 37.81805  37.65013  37.74429  37.569603 37.58984  37.423283
 37.310905 37.35775  37.06996  37.27291  37.34137  37.289177 37.43511
 37.58492  37.598995 37.644707 37.49178  37.549072 37.40905  37.48512
 37.35477  37.2396   37.222355 36.905434 37.099106 37.140293 37.089417
 37.203655 37.28573  37.287727 37.27265  37.14404  37.200157 37.09404
 37.151676 37.058025 36.935013 36.95329  36.661644 36.865047 36.914616
 36.855053 36.93559  37.018818 37.00553  37.01605  36.843273 36.936134
 37.09496  37.139145 37.073425 36.989452 37.0038   36.71791  36.877033
 36.943405 36.88906  36.975502 37.0598   37.039055 37.08651  36.86984
 36.964302 37.01079  37.07393  37.004456 36.940083 36.94844  36.705307
 36.928288 37.0042   36.930325 36.959835 37.00383  36.94152  36.94765
 36.71158  36.786278 36.68118  36.74407  36.659214 36.62119  36.692837
 36.480072 36.681667 36.733093 36.64731  36.7069   36.77845  36.76998
 36.785812 36.545887 36.60235  36.382328 36.513157 36.421852 36.38687
 36.497578 36.24412  36.51716  36.61905  36.560017 36.646454 36.776585
 36.709087 36.757942 36.517815 36.563168 36.299057 36.474552 36.388187
 36.347027 36.47277  36.189693 36.48453  36.57275  36.48228  36.56985
 36.706272 36.631584 36.690346 36.475037 36.520515 36.07246  36.35457
 36.200428 36.220688 36.449234 36.11124  36.54229  36.66017  36.60491
 36.77528  36.889297 36.84116  36.94869  36.710922 36.75776  36.138195
 36.360817 36.253246 36.21791  36.372368 36.09198  36.44979  36.518982
 36.435127 36.533897 36.647182 36.586662 36.731903 36.506573 36.545074
 36.091793 36.342392 36.211205 36.18927  36.36881  36.113483 36.442684
 36.52906  36.48607  36.605194 36.76615  36.750042 36.837345 36.661854
 36.694668 36.110153 36.33704  36.21632  36.219402 36.394623 36.140316
 36.437893 36.508484 36.467545 36.567715 36.6767   36.65065  36.723354
 36.560516 36.60387  36.043987 36.288246 36.147648 36.151512 36.349968
 36.06093  36.369446 36.411797 36.389965 36.507275 36.63448  36.614117
 36.76128  36.60312  36.68455  35.97794  36.23652  36.088715 36.11672
 36.352077 36.06448  36.3807   36.42521  36.405464 36.529255 36.645954
 36.6385   36.757545 36.609512 36.697735 36.01776  36.26215  36.168037
 36.19438  36.357895 36.10939  36.366943 36.415382 36.388947 36.495808
 36.589237 36.57533  36.725807 36.52264  36.597347 35.896103 36.151382
 36.081535 36.103355 36.283596 36.033257 36.360336 36.42297  36.389294
 36.49823  36.587517 36.559196 36.743618 36.52832  36.635788 36.003086
 36.207443 36.149902 36.1382   36.324745 36.08443  36.382805 36.41598
 36.388664 36.481266 36.60875  36.597828 36.791416 36.601063 36.69287
 36.10575  36.326347 36.316914 36.33739  36.517506 36.305576 36.618073
 36.615814 36.5752   36.64583  36.72415  36.710575 36.88681  36.64408
 36.71294  36.162537 36.39265  36.38866  36.425617 36.587803 36.345642
 36.698624 36.708065 36.7004   36.799908 36.879242 36.8668   37.045753
 36.809906 36.87296  36.399796 36.59333  36.58552  36.56755  36.7003
 36.56875  36.84841  36.902657 36.975975 37.066105 37.149887 37.19199
 37.289425 37.190613 37.306396 36.528908 36.77937  36.829033 36.908802
 37.05788 ]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 4.382749557495117
Epoch: 1, Steps: 79 | Train Loss: 1.0398965 Vali Loss: 0.8822777 Test Loss: 0.8052665
Validation loss decreased (inf --> 0.882278).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 3.9677929878234863
Epoch: 2, Steps: 79 | Train Loss: 0.9089729 Vali Loss: 0.7950206 Test Loss: 0.7331172
Validation loss decreased (0.882278 --> 0.795021).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 3.9690046310424805
Epoch: 3, Steps: 79 | Train Loss: 0.7846420 Vali Loss: 0.5932574 Test Loss: 0.5325582
Validation loss decreased (0.795021 --> 0.593257).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 3.9395334720611572
Epoch: 4, Steps: 79 | Train Loss: 0.5923372 Vali Loss: 0.4460118 Test Loss: 0.4250962
Validation loss decreased (0.593257 --> 0.446012).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 3.9687626361846924
Epoch: 5, Steps: 79 | Train Loss: 0.4787894 Vali Loss: 0.3513692 Test Loss: 0.3396731
Validation loss decreased (0.446012 --> 0.351369).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 5.866897821426392
Epoch: 6, Steps: 79 | Train Loss: 0.3956297 Vali Loss: 0.2936105 Test Loss: 0.2755412
Validation loss decreased (0.351369 --> 0.293611).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 3.9048914909362793
Epoch: 7, Steps: 79 | Train Loss: 0.3228585 Vali Loss: 0.2230465 Test Loss: 0.2106347
Validation loss decreased (0.293611 --> 0.223047).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 3.937704563140869
Epoch: 8, Steps: 79 | Train Loss: 0.2803927 Vali Loss: 0.1749377 Test Loss: 0.1830725
Validation loss decreased (0.223047 --> 0.174938).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 4.011017799377441
Epoch: 9, Steps: 79 | Train Loss: 0.2510731 Vali Loss: 0.1516451 Test Loss: 0.1637010
Validation loss decreased (0.174938 --> 0.151645).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 5.060120582580566
Epoch: 10, Steps: 79 | Train Loss: 0.2364741 Vali Loss: 0.1472740 Test Loss: 0.1565046
Validation loss decreased (0.151645 --> 0.147274).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 3.952427864074707
Epoch: 11, Steps: 79 | Train Loss: 0.2258548 Vali Loss: 0.1435609 Test Loss: 0.1505545
Validation loss decreased (0.147274 --> 0.143561).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 3.9245383739471436
Epoch: 12, Steps: 79 | Train Loss: 0.2159356 Vali Loss: 0.1368848 Test Loss: 0.1486190
Validation loss decreased (0.143561 --> 0.136885).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 3.8680028915405273
Epoch: 13, Steps: 79 | Train Loss: 0.2080021 Vali Loss: 0.1323917 Test Loss: 0.1441104
Validation loss decreased (0.136885 --> 0.132392).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 3.969226360321045
Epoch: 14, Steps: 79 | Train Loss: 0.2030717 Vali Loss: 0.1300577 Test Loss: 0.1443050
Validation loss decreased (0.132392 --> 0.130058).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 3.889047384262085
Epoch: 15, Steps: 79 | Train Loss: 0.1980781 Vali Loss: 0.1302486 Test Loss: 0.1358985
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 4.188483476638794
Epoch: 16, Steps: 79 | Train Loss: 0.1938408 Vali Loss: 0.1252705 Test Loss: 0.1347093
Validation loss decreased (0.130058 --> 0.125271).  Saving model ...
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 3.926812171936035
Epoch: 17, Steps: 79 | Train Loss: 0.1880330 Vali Loss: 0.1267793 Test Loss: 0.1309793
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 4.1375508308410645
Epoch: 18, Steps: 79 | Train Loss: 0.1837147 Vali Loss: 0.1229086 Test Loss: 0.1294268
Validation loss decreased (0.125271 --> 0.122909).  Saving model ...
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 3.9226202964782715
Epoch: 19, Steps: 79 | Train Loss: 0.1800807 Vali Loss: 0.1211119 Test Loss: 0.1312577
Validation loss decreased (0.122909 --> 0.121112).  Saving model ...
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 3.951997756958008
Epoch: 20, Steps: 79 | Train Loss: 0.1770006 Vali Loss: 0.1208253 Test Loss: 0.1247340
Validation loss decreased (0.121112 --> 0.120825).  Saving model ...
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 3.9001405239105225
Epoch: 21, Steps: 79 | Train Loss: 0.1713559 Vali Loss: 0.1198468 Test Loss: 0.1212675
Validation loss decreased (0.120825 --> 0.119847).  Saving model ...
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 3.985943555831909
Epoch: 22, Steps: 79 | Train Loss: 0.1699547 Vali Loss: 0.1143955 Test Loss: 0.1200148
Validation loss decreased (0.119847 --> 0.114396).  Saving model ...
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 4.044806241989136
Epoch: 23, Steps: 79 | Train Loss: 0.1674573 Vali Loss: 0.1088920 Test Loss: 0.1175095
Validation loss decreased (0.114396 --> 0.108892).  Saving model ...
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 4.174316167831421
Epoch: 24, Steps: 79 | Train Loss: 0.1647018 Vali Loss: 0.1098463 Test Loss: 0.1173202
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 4.244781732559204
Epoch: 25, Steps: 79 | Train Loss: 0.1622691 Vali Loss: 0.1141889 Test Loss: 0.1159381
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 3.931495428085327
Epoch: 26, Steps: 79 | Train Loss: 0.1585134 Vali Loss: 0.1123282 Test Loss: 0.1153955
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 3.9235751628875732
Epoch: 27, Steps: 79 | Train Loss: 0.1567369 Vali Loss: 0.1233334 Test Loss: 0.1172543
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 3.94873309135437
Epoch: 28, Steps: 79 | Train Loss: 0.1549882 Vali Loss: 0.1152726 Test Loss: 0.1131867
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 3.923494338989258
Epoch: 29, Steps: 79 | Train Loss: 0.1515603 Vali Loss: 0.1118426 Test Loss: 0.1124335
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 4.106358766555786
Epoch: 30, Steps: 79 | Train Loss: 0.1508004 Vali Loss: 0.1158170 Test Loss: 0.1096161
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 4.551586389541626
Epoch: 31, Steps: 79 | Train Loss: 0.1485797 Vali Loss: 0.1151414 Test Loss: 0.1099644
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 5.247280836105347
Epoch: 32, Steps: 79 | Train Loss: 0.1464126 Vali Loss: 0.1068346 Test Loss: 0.1107976
Validation loss decreased (0.108892 --> 0.106835).  Saving model ...
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 5.090932130813599
Epoch: 33, Steps: 79 | Train Loss: 0.1430615 Vali Loss: 0.1170814 Test Loss: 0.1071145
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 4.831942558288574
Epoch: 34, Steps: 79 | Train Loss: 0.1413011 Vali Loss: 0.1232377 Test Loss: 0.1122010
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 4.389221429824829
Epoch: 35, Steps: 79 | Train Loss: 0.1402326 Vali Loss: 0.1157081 Test Loss: 0.1077202
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 5.167529106140137
Epoch: 36, Steps: 79 | Train Loss: 0.1384232 Vali Loss: 0.1164711 Test Loss: 0.1061570
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 5.458035469055176
Epoch: 37, Steps: 79 | Train Loss: 0.1362197 Vali Loss: 0.1134453 Test Loss: 0.1091761
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 4.652662515640259
Epoch: 38, Steps: 79 | Train Loss: 0.1340394 Vali Loss: 0.1102119 Test Loss: 0.1046704
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 4.6706366539001465
Epoch: 39, Steps: 79 | Train Loss: 0.1317105 Vali Loss: 0.1059650 Test Loss: 0.1015870
Validation loss decreased (0.106835 --> 0.105965).  Saving model ...
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 5.550820827484131
Epoch: 40, Steps: 79 | Train Loss: 0.1299177 Vali Loss: 0.1086256 Test Loss: 0.0993451
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 4.78098464012146
Epoch: 41, Steps: 79 | Train Loss: 0.1288387 Vali Loss: 0.1085403 Test Loss: 0.1039920
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009402
Epoch: 42 cost time: 4.496156930923462
Epoch: 42, Steps: 79 | Train Loss: 0.1272820 Vali Loss: 0.1100938 Test Loss: 0.1020948
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009291
Epoch: 43 cost time: 5.206057071685791
Epoch: 43, Steps: 79 | Train Loss: 0.1253442 Vali Loss: 0.1137960 Test Loss: 0.1036608
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009171
Epoch: 44 cost time: 4.8264923095703125
Epoch: 44, Steps: 79 | Train Loss: 0.1243434 Vali Loss: 0.1094358 Test Loss: 0.1015141
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009043
Epoch: 45 cost time: 5.3365020751953125
Epoch: 45, Steps: 79 | Train Loss: 0.1226393 Vali Loss: 0.1093287 Test Loss: 0.1016264
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0008907
Epoch: 46 cost time: 5.709265232086182
Epoch: 46, Steps: 79 | Train Loss: 0.1226728 Vali Loss: 0.1087169 Test Loss: 0.0991891
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0008763
Epoch: 47 cost time: 4.87185263633728
Epoch: 47, Steps: 79 | Train Loss: 0.1208788 Vali Loss: 0.1058142 Test Loss: 0.0979164
Validation loss decreased (0.105965 --> 0.105814).  Saving model ...
Adjusting learning rate to: 0.0008612
Epoch: 48 cost time: 4.973341464996338
Epoch: 48, Steps: 79 | Train Loss: 0.1190193 Vali Loss: 0.1011991 Test Loss: 0.0974157
Validation loss decreased (0.105814 --> 0.101199).  Saving model ...
Adjusting learning rate to: 0.0008453
Epoch: 49 cost time: 5.799831390380859
Epoch: 49, Steps: 79 | Train Loss: 0.1181938 Vali Loss: 0.1086507 Test Loss: 0.0985937
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008288
Epoch: 50 cost time: 4.790091037750244
Epoch: 50, Steps: 79 | Train Loss: 0.1175403 Vali Loss: 0.1094802 Test Loss: 0.1008879
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0008115
Epoch: 51 cost time: 5.711153030395508
Epoch: 51, Steps: 79 | Train Loss: 0.1164050 Vali Loss: 0.1091637 Test Loss: 0.1001143
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0007937
Epoch: 52 cost time: 5.664933204650879
Epoch: 52, Steps: 79 | Train Loss: 0.1158240 Vali Loss: 0.1135844 Test Loss: 0.1017289
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0007752
Epoch: 53 cost time: 4.596868515014648
Epoch: 53, Steps: 79 | Train Loss: 0.1147514 Vali Loss: 0.1069454 Test Loss: 0.0999749
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0007562
Epoch: 54 cost time: 5.525212526321411
Epoch: 54, Steps: 79 | Train Loss: 0.1134704 Vali Loss: 0.1068565 Test Loss: 0.0990834
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0007367
Epoch: 55 cost time: 4.726759672164917
Epoch: 55, Steps: 79 | Train Loss: 0.1129922 Vali Loss: 0.1078125 Test Loss: 0.0981415
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0007167
Epoch: 56 cost time: 4.587756633758545
Epoch: 56, Steps: 79 | Train Loss: 0.1122977 Vali Loss: 0.1076139 Test Loss: 0.0964253
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0006963
Epoch: 57 cost time: 5.663602590560913
Epoch: 57, Steps: 79 | Train Loss: 0.1117113 Vali Loss: 0.1114597 Test Loss: 0.1008569
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0006754
Epoch: 58 cost time: 4.895982027053833
Epoch: 58, Steps: 79 | Train Loss: 0.1105985 Vali Loss: 0.1109773 Test Loss: 0.0993438
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0006542
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.21055445075035095, mae:0.3587137460708618, rmse:0.4588621258735657, mape:0.012814793735742569, mspe:0.00027060043066740036, rse:0.31627538800239563, r2_score:0.8909726934906034, acc:0.9871852062642574
corr: [37.968845 38.016796 38.051064 38.056763 38.075912 38.10336  38.090702
 38.091873 38.023495 37.977562 37.960766 37.892616 37.84036  37.80699
 37.761997 37.74805  37.63752  37.653816 37.666317 37.67628  37.669865
 37.682674 37.71687  37.727737 37.66602  37.62256  37.609795 37.58733
 37.55343  37.55382  37.545055 37.538834 37.65291  37.65315  37.642624
 37.62644  37.626312 37.658634 37.674694 37.67557  37.636673 37.64612
 37.667137 37.64422  37.63461  37.64509  37.625095 37.650963 37.63175
 37.588615 37.579834 37.560547 37.537743 37.516438 37.498337 37.490906
 37.437252 37.4243   37.44557  37.436214 37.4316   37.408348 37.37475
 37.370335 37.45378  37.430138 37.410057 37.392513 37.38437  37.399773
 37.41129  37.40985  37.348106 37.30775  37.29463  37.25711  37.219597
 37.188366 37.170124 37.17601  37.270412 37.218372 37.17736  37.15498
 37.1376   37.103764 37.126038 37.159428 37.12118  37.097027 37.09723
 37.066883 37.054916 37.043537 37.04337  37.034794 37.001152 36.965313
 36.95432  36.938354 36.96048  36.956768 36.97124  36.99623  36.960526
 36.950592 36.992935 36.98646  36.980206 36.955757 36.928787 36.913998
 36.75586  36.72123  36.700504 36.687992 36.671703 36.662342 36.68388
 36.698147 36.671368 36.647152 36.67926  36.679604 36.67555  36.68324
 36.67815  36.704475 36.51737  36.469406 36.433407 36.39872  36.39889
 36.40118  36.43487  36.47465  36.46071  36.468884 36.52335  36.515465
 36.543976 36.51611  36.499527 36.504307 36.4324   36.376926 36.35813
 36.352173 36.33687  36.33779  36.376015 36.408733 36.39935  36.402508
 36.42463  36.41991  36.412422 36.38531  36.338367 36.34399  36.168926
 36.148758 36.137463 36.13979  36.131145 36.1524   36.2091   36.237255
 36.244385 36.26566  36.30463  36.29424  36.312233 36.30969  36.301033
 36.31407  36.20711  36.20197  36.220432 36.218304 36.2348   36.257076
 36.302822 36.34754  36.333935 36.35427  36.39992  36.40091  36.40398
 36.402622 36.399376 36.420326 36.309338 36.28636  36.270355 36.274216
 36.27975  36.297585 36.324184 36.338802 36.312477 36.286057 36.31482
 36.31216  36.32515  36.3071   36.269775 36.280796 36.23633  36.225883
 36.243393 36.26891  36.29954  36.321087 36.350582 36.3576   36.34012
 36.311516 36.332226 36.294746 36.275696 36.271122 36.295586 36.33585
 36.249428 36.248074 36.284264 36.322845 36.349617 36.372    36.4115
 36.443775 36.436752 36.41982  36.416412 36.40769  36.395805 36.393715
 36.39668  36.421047 36.40977  36.397655 36.386772 36.38107  36.40171
 36.441814 36.47241  36.49179  36.45965  36.437126 36.464443 36.43524
 36.416286 36.418446 36.435364 36.47923  36.377777 36.375362 36.39249
 36.406284 36.435673 36.49729  36.529663 36.55434  36.526924 36.522686
 36.541065 36.544735 36.56136  36.564068 36.571568 36.61199  36.509254
 36.52625  36.545338 36.582085 36.582825 36.60061  36.638535 36.644382
 36.611378 36.606728 36.619534 36.610783 36.597115 36.60522  36.614296
 36.64785  36.42263  36.41476  36.41318  36.42622  36.429493 36.443073
 36.468372 36.50705  36.497128 36.5006   36.53284  36.546947 36.560856
 36.570934 36.55722  36.584644 36.51031  36.4634   36.44468  36.460644
 36.47495  36.497643 36.549587 36.56407  36.52356  36.51805  36.55478
 36.551006 36.568066 36.577415 36.60138  36.655544 36.400696 36.37861
 36.379185 36.378853 36.379784 36.36463  36.379482 36.398384 36.36211
 36.35428  36.407482 36.447746 36.489925 36.52671  36.543217 36.582985
 36.39917  36.405235 36.42994  36.475594 36.488876 36.502865 36.519363
 36.53684  36.522747 36.529217 36.583817 36.63438  36.69976  36.749767
 36.783195 36.829655 36.615692 36.63021  36.66979  36.71247  36.76701
 36.81595  36.87579  36.94118  36.93245  36.985416 37.07875  37.13671
 37.182766]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 5.096333026885986
Epoch: 1, Steps: 79 | Train Loss: 1.0291071 Vali Loss: 0.8792000 Test Loss: 0.8105974
Validation loss decreased (inf --> 0.879200).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 5.2152299880981445
Epoch: 2, Steps: 79 | Train Loss: 0.8940469 Vali Loss: 0.7880399 Test Loss: 0.7149469
Validation loss decreased (0.879200 --> 0.788040).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 5.0078747272491455
Epoch: 3, Steps: 79 | Train Loss: 0.7421774 Vali Loss: 0.4950986 Test Loss: 0.4441636
Validation loss decreased (0.788040 --> 0.495099).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 5.523878574371338
Epoch: 4, Steps: 79 | Train Loss: 0.5256941 Vali Loss: 0.3967745 Test Loss: 0.3788480
Validation loss decreased (0.495099 --> 0.396775).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 5.548785924911499
Epoch: 5, Steps: 79 | Train Loss: 0.4499282 Vali Loss: 0.3469243 Test Loss: 0.3375061
Validation loss decreased (0.396775 --> 0.346924).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 4.431745529174805
Epoch: 6, Steps: 79 | Train Loss: 0.3990776 Vali Loss: 0.3074161 Test Loss: 0.2902754
Validation loss decreased (0.346924 --> 0.307416).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 5.0729079246521
Epoch: 7, Steps: 79 | Train Loss: 0.3433288 Vali Loss: 0.2351197 Test Loss: 0.2251602
Validation loss decreased (0.307416 --> 0.235120).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 5.406181812286377
Epoch: 8, Steps: 79 | Train Loss: 0.2775851 Vali Loss: 0.1910892 Test Loss: 0.1883250
Validation loss decreased (0.235120 --> 0.191089).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 4.758501052856445
Epoch: 9, Steps: 79 | Train Loss: 0.2481907 Vali Loss: 0.1593225 Test Loss: 0.1606724
Validation loss decreased (0.191089 --> 0.159322).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 5.597100019454956
Epoch: 10, Steps: 79 | Train Loss: 0.2306560 Vali Loss: 0.1458127 Test Loss: 0.1511514
Validation loss decreased (0.159322 --> 0.145813).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 5.818542242050171
Epoch: 11, Steps: 79 | Train Loss: 0.2203510 Vali Loss: 0.1406787 Test Loss: 0.1477102
Validation loss decreased (0.145813 --> 0.140679).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 4.7266480922698975
Epoch: 12, Steps: 79 | Train Loss: 0.2115290 Vali Loss: 0.1348458 Test Loss: 0.1389579
Validation loss decreased (0.140679 --> 0.134846).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 5.557907342910767
Epoch: 13, Steps: 79 | Train Loss: 0.2041349 Vali Loss: 0.1314409 Test Loss: 0.1406276
Validation loss decreased (0.134846 --> 0.131441).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 5.137371778488159
Epoch: 14, Steps: 79 | Train Loss: 0.1977705 Vali Loss: 0.1275291 Test Loss: 0.1341547
Validation loss decreased (0.131441 --> 0.127529).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 5.2372283935546875
Epoch: 15, Steps: 79 | Train Loss: 0.1932568 Vali Loss: 0.1251009 Test Loss: 0.1340877
Validation loss decreased (0.127529 --> 0.125101).  Saving model ...
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 5.4000232219696045
Epoch: 16, Steps: 79 | Train Loss: 0.1888548 Vali Loss: 0.1213839 Test Loss: 0.1277737
Validation loss decreased (0.125101 --> 0.121384).  Saving model ...
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 4.638562917709351
Epoch: 17, Steps: 79 | Train Loss: 0.1845213 Vali Loss: 0.1193025 Test Loss: 0.1291368
Validation loss decreased (0.121384 --> 0.119302).  Saving model ...
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 5.31115198135376
Epoch: 18, Steps: 79 | Train Loss: 0.1795760 Vali Loss: 0.1162896 Test Loss: 0.1203133
Validation loss decreased (0.119302 --> 0.116290).  Saving model ...
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 5.489904403686523
Epoch: 19, Steps: 79 | Train Loss: 0.1750246 Vali Loss: 0.1185797 Test Loss: 0.1208493
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 5.175662279129028
Epoch: 20, Steps: 79 | Train Loss: 0.1719710 Vali Loss: 0.1138880 Test Loss: 0.1225857
Validation loss decreased (0.116290 --> 0.113888).  Saving model ...
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 5.710522174835205
Epoch: 21, Steps: 79 | Train Loss: 0.1691404 Vali Loss: 0.1117487 Test Loss: 0.1185365
Validation loss decreased (0.113888 --> 0.111749).  Saving model ...
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 5.496032953262329
Epoch: 22, Steps: 79 | Train Loss: 0.1654597 Vali Loss: 0.1201693 Test Loss: 0.1212308
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 5.239514589309692
Epoch: 23, Steps: 79 | Train Loss: 0.1632635 Vali Loss: 0.1108870 Test Loss: 0.1195048
Validation loss decreased (0.111749 --> 0.110887).  Saving model ...
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 5.0071258544921875
Epoch: 24, Steps: 79 | Train Loss: 0.1587627 Vali Loss: 0.1129546 Test Loss: 0.1148925
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 5.528496265411377
Epoch: 25, Steps: 79 | Train Loss: 0.1579629 Vali Loss: 0.1141668 Test Loss: 0.1149255
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 4.4131200313568115
Epoch: 26, Steps: 79 | Train Loss: 0.1559471 Vali Loss: 0.1112559 Test Loss: 0.1157554
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 5.420295238494873
Epoch: 27, Steps: 79 | Train Loss: 0.1534172 Vali Loss: 0.1144275 Test Loss: 0.1146067
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 5.556340932846069
Epoch: 28, Steps: 79 | Train Loss: 0.1507295 Vali Loss: 0.1080837 Test Loss: 0.1109007
Validation loss decreased (0.110887 --> 0.108084).  Saving model ...
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 5.02215576171875
Epoch: 29, Steps: 79 | Train Loss: 0.1476075 Vali Loss: 0.1093722 Test Loss: 0.1103572
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 5.7085771560668945
Epoch: 30, Steps: 79 | Train Loss: 0.1462197 Vali Loss: 0.1142366 Test Loss: 0.1082065
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 5.0366456508636475
Epoch: 31, Steps: 79 | Train Loss: 0.1436925 Vali Loss: 0.1080070 Test Loss: 0.1049236
Validation loss decreased (0.108084 --> 0.108007).  Saving model ...
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 4.806312322616577
Epoch: 32, Steps: 79 | Train Loss: 0.1422222 Vali Loss: 0.1154285 Test Loss: 0.1074482
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 5.453973293304443
Epoch: 33, Steps: 79 | Train Loss: 0.1403368 Vali Loss: 0.1113510 Test Loss: 0.1030096
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 4.593451738357544
Epoch: 34, Steps: 79 | Train Loss: 0.1384178 Vali Loss: 0.1016523 Test Loss: 0.1015629
Validation loss decreased (0.108007 --> 0.101652).  Saving model ...
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 4.696701526641846
Epoch: 35, Steps: 79 | Train Loss: 0.1357915 Vali Loss: 0.1122709 Test Loss: 0.1024751
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 5.197796821594238
Epoch: 36, Steps: 79 | Train Loss: 0.1341375 Vali Loss: 0.1117260 Test Loss: 0.1005446
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 5.1927266120910645
Epoch: 37, Steps: 79 | Train Loss: 0.1326873 Vali Loss: 0.1034546 Test Loss: 0.0998501
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 4.512175559997559
Epoch: 38, Steps: 79 | Train Loss: 0.1314694 Vali Loss: 0.1070456 Test Loss: 0.0985743
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 5.170716285705566
Epoch: 39, Steps: 79 | Train Loss: 0.1299839 Vali Loss: 0.1048331 Test Loss: 0.1000125
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 5.397706747055054
Epoch: 40, Steps: 79 | Train Loss: 0.1289347 Vali Loss: 0.1126652 Test Loss: 0.1026539
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 4.394191265106201
Epoch: 41, Steps: 79 | Train Loss: 0.1269580 Vali Loss: 0.1038394 Test Loss: 0.0972301
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0009402
Epoch: 42 cost time: 5.097527265548706
Epoch: 42, Steps: 79 | Train Loss: 0.1259613 Vali Loss: 0.1028374 Test Loss: 0.0967853
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0009291
Epoch: 43 cost time: 5.54536509513855
Epoch: 43, Steps: 79 | Train Loss: 0.1242086 Vali Loss: 0.1107301 Test Loss: 0.0977272
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0009171
Epoch: 44 cost time: 4.916112422943115
Epoch: 44, Steps: 79 | Train Loss: 0.1230037 Vali Loss: 0.1022431 Test Loss: 0.0953578
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0009043
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.22916629910469055, mae:0.37405821681022644, rmse:0.47871318459510803, mape:0.01338154822587967, mspe:0.0002961638383567333, rse:0.32995790243148804, r2_score:0.8802851680343643, acc:0.9866184517741203
corr: [37.6623   37.710976 37.676    37.62295  37.607502 37.62432  37.64408
 37.70126  37.740612 37.81855  37.837902 37.85459  37.809338 37.823963
 37.89652  37.894173 37.852745 37.56117  37.575115 37.50771  37.46355
 37.426067 37.44637  37.491997 37.573227 37.59951  37.64925  37.68623
 37.725746 37.70966  37.72016  37.79924  37.805653 37.803444 37.54733
 37.55196  37.491467 37.48053  37.47339  37.492245 37.515987 37.57464
 37.583344 37.62752  37.6504   37.688385 37.676453 37.708344 37.78581
 37.80186  37.74902  37.44601  37.434303 37.34304  37.295753 37.301643
 37.33184  37.37488  37.42977  37.429718 37.465878 37.45502  37.460464
 37.43719  37.46814  37.552773 37.56314  37.52566  37.19426  37.186394
 37.140438 37.12373  37.123672 37.157047 37.1905   37.230694 37.22684
 37.263428 37.269688 37.25604  37.18391  37.20358  37.28342  37.309437
 37.26843  36.901577 36.911995 36.85142  36.826405 36.83107  36.867836
 36.918102 36.98112  36.983997 37.0132   37.03914  37.04607  37.03688
 37.080597 37.205418 37.233166 37.212082 36.79379  36.809723 36.78274
 36.76722  36.78434  36.819126 36.855885 36.924793 36.922657 36.969326
 36.99862  36.985027 36.96352  36.9819   37.072563 37.09174  37.069386
 36.728786 36.71394  36.66231  36.645943 36.64178  36.65943  36.67462
 36.71362  36.73274  36.749413 36.76713  36.79605  36.77991  36.8275
 36.925243 36.91629  36.90617  36.42527  36.438755 36.378155 36.383816
 36.390408 36.404793 36.44121  36.49181  36.5153   36.55918  36.559288
 36.566902 36.547737 36.599358 36.690887 36.707504 36.704727 36.325603
 36.35467  36.31996  36.30498  36.307514 36.323036 36.349976 36.40635
 36.427563 36.467525 36.49173  36.529224 36.510353 36.56412  36.634396
 36.65283  36.66285  36.22104  36.246845 36.226974 36.24437  36.266926
 36.30231  36.347954 36.408794 36.430595 36.486206 36.53431  36.558773
 36.543743 36.592342 36.68443  36.702335 36.711197 36.377735 36.37822
 36.364326 36.36068  36.371002 36.41434  36.43305  36.490955 36.510647
 36.565475 36.602608 36.61223  36.6095   36.64232  36.711395 36.747425
 36.71517  36.249912 36.27748  36.25655  36.26199  36.290478 36.349064
 36.391346 36.43429  36.472763 36.524822 36.580578 36.618465 36.60365
 36.65762  36.750317 36.752583 36.71827  36.372993 36.40857  36.41418
 36.427788 36.432343 36.468227 36.518196 36.570866 36.5539   36.596718
 36.613434 36.639423 36.61452  36.628784 36.677162 36.66242  36.622295
 36.47216  36.48659  36.46264  36.448902 36.45483  36.494854 36.544975
 36.61317  36.63607  36.682655 36.70493  36.731567 36.697136 36.749264
 36.828335 36.836166 36.7959   36.568817 36.61954  36.596565 36.58497
 36.59562  36.631073 36.66317  36.73141  36.767303 36.810215 36.81816
 36.856335 36.853645 36.892895 36.93857  36.933575 36.91942  36.55995
 36.56748  36.52669  36.538574 36.535767 36.558334 36.59448  36.657852
 36.68424  36.734993 36.767097 36.786297 36.754936 36.78008  36.85341
 36.87614  36.87179  36.561287 36.589245 36.521393 36.503227 36.48256
 36.504986 36.515118 36.54452  36.563824 36.622486 36.668003 36.679398
 36.651314 36.676147 36.716965 36.72373  36.708023 36.374428 36.39524
 36.36243  36.3266   36.30039  36.311573 36.32655  36.37252  36.399376
 36.48957  36.545975 36.565163 36.54214  36.56498  36.66418  36.720543
 36.748943 36.405178 36.427563 36.363075 36.320488 36.29597  36.331642
 36.341835 36.398235 36.45107  36.552605 36.602642 36.626957 36.60964
 36.636246 36.729115 36.769054 36.82223  36.531986 36.549053 36.505733
 36.483635 36.45945  36.476177 36.487034 36.56629  36.627563 36.737953
 36.801865 36.85183  36.829315 36.87649  36.971405 37.043743 37.078323
 36.761734 36.86116  36.9033   36.93651  36.92217  36.974365 37.02322
 37.128696]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 4.880615234375
Epoch: 1, Steps: 79 | Train Loss: 1.0376793 Vali Loss: 0.8857679 Test Loss: 0.7988986
Validation loss decreased (inf --> 0.885768).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 5.1767637729644775
Epoch: 2, Steps: 79 | Train Loss: 0.8996441 Vali Loss: 0.8060799 Test Loss: 0.7262261
Validation loss decreased (0.885768 --> 0.806080).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 5.487825870513916
Epoch: 3, Steps: 79 | Train Loss: 0.7975790 Vali Loss: 0.6243452 Test Loss: 0.5506359
Validation loss decreased (0.806080 --> 0.624345).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 4.711909770965576
Epoch: 4, Steps: 79 | Train Loss: 0.5786156 Vali Loss: 0.4262607 Test Loss: 0.4002108
Validation loss decreased (0.624345 --> 0.426261).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 5.005694150924683
Epoch: 5, Steps: 79 | Train Loss: 0.4588210 Vali Loss: 0.3592536 Test Loss: 0.3394214
Validation loss decreased (0.426261 --> 0.359254).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 4.887606143951416
Epoch: 6, Steps: 79 | Train Loss: 0.4044556 Vali Loss: 0.3135254 Test Loss: 0.2878516
Validation loss decreased (0.359254 --> 0.313525).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 5.46461820602417
Epoch: 7, Steps: 79 | Train Loss: 0.3519647 Vali Loss: 0.2580891 Test Loss: 0.2472589
Validation loss decreased (0.313525 --> 0.258089).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 5.491661310195923
Epoch: 8, Steps: 79 | Train Loss: 0.2905850 Vali Loss: 0.1929710 Test Loss: 0.1901917
Validation loss decreased (0.258089 --> 0.192971).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 4.336827516555786
Epoch: 9, Steps: 79 | Train Loss: 0.2531525 Vali Loss: 0.1596193 Test Loss: 0.1674489
Validation loss decreased (0.192971 --> 0.159619).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 5.371763706207275
Epoch: 10, Steps: 79 | Train Loss: 0.2336703 Vali Loss: 0.1452821 Test Loss: 0.1522003
Validation loss decreased (0.159619 --> 0.145282).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 5.279525995254517
Epoch: 11, Steps: 79 | Train Loss: 0.2206935 Vali Loss: 0.1406329 Test Loss: 0.1465942
Validation loss decreased (0.145282 --> 0.140633).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 4.3453168869018555
Epoch: 12, Steps: 79 | Train Loss: 0.2121709 Vali Loss: 0.1339974 Test Loss: 0.1400090
Validation loss decreased (0.140633 --> 0.133997).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 5.179476261138916
Epoch: 13, Steps: 79 | Train Loss: 0.2063193 Vali Loss: 0.1333802 Test Loss: 0.1383555
Validation loss decreased (0.133997 --> 0.133380).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 4.5827977657318115
Epoch: 14, Steps: 79 | Train Loss: 0.1994771 Vali Loss: 0.1274570 Test Loss: 0.1364708
Validation loss decreased (0.133380 --> 0.127457).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 4.558377027511597
Epoch: 15, Steps: 79 | Train Loss: 0.1952487 Vali Loss: 0.1257223 Test Loss: 0.1339062
Validation loss decreased (0.127457 --> 0.125722).  Saving model ...
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 5.413207054138184
Epoch: 16, Steps: 79 | Train Loss: 0.1904160 Vali Loss: 0.1220580 Test Loss: 0.1310891
Validation loss decreased (0.125722 --> 0.122058).  Saving model ...
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 5.429028272628784
Epoch: 17, Steps: 79 | Train Loss: 0.1869570 Vali Loss: 0.1262055 Test Loss: 0.1320318
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 4.555134534835815
Epoch: 18, Steps: 79 | Train Loss: 0.1823893 Vali Loss: 0.1223009 Test Loss: 0.1302986
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 5.599792003631592
Epoch: 19, Steps: 79 | Train Loss: 0.1785104 Vali Loss: 0.1174289 Test Loss: 0.1271699
Validation loss decreased (0.122058 --> 0.117429).  Saving model ...
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 4.893479108810425
Epoch: 20, Steps: 79 | Train Loss: 0.1740104 Vali Loss: 0.1211182 Test Loss: 0.1255945
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 5.482630252838135
Epoch: 21, Steps: 79 | Train Loss: 0.1728749 Vali Loss: 0.1227541 Test Loss: 0.1245584
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 5.411815881729126
Epoch: 22, Steps: 79 | Train Loss: 0.1715068 Vali Loss: 0.1119549 Test Loss: 0.1238073
Validation loss decreased (0.117429 --> 0.111955).  Saving model ...
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 4.582993268966675
Epoch: 23, Steps: 79 | Train Loss: 0.1672910 Vali Loss: 0.1210346 Test Loss: 0.1238248
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 4.880282878875732
Epoch: 24, Steps: 79 | Train Loss: 0.1638418 Vali Loss: 0.1160966 Test Loss: 0.1220059
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 5.112854242324829
Epoch: 25, Steps: 79 | Train Loss: 0.1608303 Vali Loss: 0.1154216 Test Loss: 0.1201856
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 4.550400733947754
Epoch: 26, Steps: 79 | Train Loss: 0.1603092 Vali Loss: 0.1156338 Test Loss: 0.1194720
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 5.2028162479400635
Epoch: 27, Steps: 79 | Train Loss: 0.1565817 Vali Loss: 0.1118728 Test Loss: 0.1174536
Validation loss decreased (0.111955 --> 0.111873).  Saving model ...
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 5.662536144256592
Epoch: 28, Steps: 79 | Train Loss: 0.1546119 Vali Loss: 0.1156530 Test Loss: 0.1169837
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 5.040807008743286
Epoch: 29, Steps: 79 | Train Loss: 0.1524618 Vali Loss: 0.1156201 Test Loss: 0.1155200
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 5.319656133651733
Epoch: 30, Steps: 79 | Train Loss: 0.1509174 Vali Loss: 0.1186029 Test Loss: 0.1175976
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 5.807856798171997
Epoch: 31, Steps: 79 | Train Loss: 0.1483238 Vali Loss: 0.1149754 Test Loss: 0.1141480
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 4.564224004745483
Epoch: 32, Steps: 79 | Train Loss: 0.1471197 Vali Loss: 0.1092663 Test Loss: 0.1159998
Validation loss decreased (0.111873 --> 0.109266).  Saving model ...
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 5.4453284740448
Epoch: 33, Steps: 79 | Train Loss: 0.1450328 Vali Loss: 0.1106158 Test Loss: 0.1130204
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 5.363901376724243
Epoch: 34, Steps: 79 | Train Loss: 0.1433925 Vali Loss: 0.1143523 Test Loss: 0.1147073
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 4.414279460906982
Epoch: 35, Steps: 79 | Train Loss: 0.1416075 Vali Loss: 0.1176762 Test Loss: 0.1116631
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 5.816230773925781
Epoch: 36, Steps: 79 | Train Loss: 0.1402465 Vali Loss: 0.1128329 Test Loss: 0.1149203
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 5.170790433883667
Epoch: 37, Steps: 79 | Train Loss: 0.1385104 Vali Loss: 0.1125434 Test Loss: 0.1121229
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 4.78843879699707
Epoch: 38, Steps: 79 | Train Loss: 0.1366613 Vali Loss: 0.1171676 Test Loss: 0.1096849
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 5.162826061248779
Epoch: 39, Steps: 79 | Train Loss: 0.1350778 Vali Loss: 0.1081854 Test Loss: 0.1049339
Validation loss decreased (0.109266 --> 0.108185).  Saving model ...
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 5.031340599060059
Epoch: 40, Steps: 79 | Train Loss: 0.1343293 Vali Loss: 0.1121066 Test Loss: 0.1090449
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 5.760670185089111
Epoch: 41, Steps: 79 | Train Loss: 0.1324505 Vali Loss: 0.1152019 Test Loss: 0.1078576
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009402
Epoch: 42 cost time: 5.468067407608032
Epoch: 42, Steps: 79 | Train Loss: 0.1308330 Vali Loss: 0.1112837 Test Loss: 0.1047845
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009291
Epoch: 43 cost time: 4.556766510009766
Epoch: 43, Steps: 79 | Train Loss: 0.1295779 Vali Loss: 0.1132564 Test Loss: 0.1051950
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009171
Epoch: 44 cost time: 5.390954256057739
Epoch: 44, Steps: 79 | Train Loss: 0.1281164 Vali Loss: 0.1161232 Test Loss: 0.1068524
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009043
Epoch: 45 cost time: 5.28120756149292
Epoch: 45, Steps: 79 | Train Loss: 0.1266374 Vali Loss: 0.1117902 Test Loss: 0.1064763
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0008907
Epoch: 46 cost time: 4.597548246383667
Epoch: 46, Steps: 79 | Train Loss: 0.1255004 Vali Loss: 0.1087134 Test Loss: 0.1057331
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0008763
Epoch: 47 cost time: 5.6691131591796875
Epoch: 47, Steps: 79 | Train Loss: 0.1245045 Vali Loss: 0.1121233 Test Loss: 0.1092756
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0008612
Epoch: 48 cost time: 4.997990608215332
Epoch: 48, Steps: 79 | Train Loss: 0.1235549 Vali Loss: 0.1098398 Test Loss: 0.1045152
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0008453
Epoch: 49 cost time: 5.521466493606567
Epoch: 49, Steps: 79 | Train Loss: 0.1229808 Vali Loss: 0.1154568 Test Loss: 0.1073181
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0008288
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.23233474791049957, mae:0.37673503160476685, rmse:0.4820111393928528, mape:0.013487924821674824, mspe:0.0003009759820997715, rse:0.3322310745716095, r2_score:0.8735803568305455, acc:0.9865120751783252
corr: [38.035492 38.093594 38.15085  38.20005  38.243027 38.232246 38.23507
 38.219032 38.157795 38.216457 38.2187   38.195908 38.083458 38.01327
 38.00625  38.008434 38.04648  38.03315  37.833725 37.83448  37.837032
 37.868134 37.89238  37.870865 37.908726 37.939068 37.897575 37.960987
 37.946777 37.961193 37.92759  37.90377  37.921047 37.967514 38.055943
 38.09284  37.73309  37.739666 37.742165 37.770622 37.8137   37.832054
 37.839954 37.84941  37.806835 37.886265 37.874187 37.891666 37.854774
 37.825966 37.860584 37.9123   38.006622 38.040943 37.918644 37.90867
 37.907112 37.90906  37.91034  37.889652 37.88949  37.88475  37.851665
 37.911144 37.87978  37.85764  37.807407 37.758327 37.765667 37.79785
 37.869164 37.873882 37.64734  37.599705 37.578907 37.565125 37.590973
 37.585854 37.617424 37.622643 37.58698  37.61984  37.584515 37.55985
 37.506874 37.42653  37.40352  37.423878 37.467632 37.466873 37.285465
 37.240128 37.2194   37.252476 37.26873  37.2724   37.279064 37.290073
 37.24867  37.29467  37.26443  37.25733  37.22854  37.196198 37.202103
 37.230953 37.267605 37.26826  37.25175  37.18834  37.182713 37.190838
 37.214912 37.20457  37.19866  37.20989  37.128387 37.138096 37.08939
 37.07697  36.997578 36.928165 36.93879  36.980312 37.02315  37.035522
 37.104977 37.037094 37.01644  37.014446 37.020977 37.004295 36.97529
 36.968285 36.916786 36.958572 36.90917  36.89555  36.854816 36.83958
 36.8417   36.855404 36.920944 36.917114 36.715237 36.678867 36.669212
 36.677113 36.701744 36.695072 36.73028  36.765503 36.725433 36.76169
 36.742523 36.7441   36.655514 36.61306  36.62775  36.644894 36.68635
 36.696075 36.42362  36.41504  36.444813 36.494675 36.567875 36.63632
 36.676914 36.693615 36.666515 36.72911  36.75631  36.81204  36.79442
 36.78785  36.834053 36.8904   36.96683  36.98444  36.614067 36.60967
 36.6409   36.68078  36.74948  36.77537  36.813988 36.82629  36.788334
 36.84063  36.830692 36.8701   36.825584 36.787468 36.817204 36.837406
 36.930923 36.966778 36.623108 36.565838 36.577274 36.63702  36.705112
 36.768623 36.793003 36.850388 36.84109  36.86682  36.85043  36.869938
 36.82886  36.79895  36.823193 36.86341  36.933113 36.96049  36.62488
 36.62251  36.65859  36.698032 36.74842  36.81167  36.848965 36.878155
 36.841717 36.883533 36.89993  36.905876 36.8562   36.80089  36.84107
 36.899086 36.98359  37.013218 36.668262 36.645866 36.68861  36.74886
 36.815945 36.85054  36.858387 36.862144 36.827435 36.88172  36.89006
 36.89958  36.879913 36.87589  36.93316  37.00881  37.093998 37.131233
 36.81246  36.821598 36.87076  36.875584 36.907375 36.948837 36.968678
 36.998817 36.98404  36.995842 36.960518 36.961884 36.953156 36.908283
 36.94008  37.00475  37.09125  37.1252   36.864044 36.861984 36.87074
 36.8895   36.9191   36.926685 36.9081   36.9474   36.94338  37.005795
 37.025715 37.03416  36.98613  36.934925 36.949665 36.96239  37.020454
 37.05331  36.895325 36.88988  36.90635  36.93378  36.987812 37.02627
 37.033115 37.046032 37.012592 37.06386  37.082283 37.09053  37.051384
 37.031094 37.05482  37.092804 37.1295   37.13673  36.787277 36.778046
 36.801506 36.834385 36.88579  36.91198  36.941822 37.00869  36.998966
 37.060085 37.06543  37.099606 37.076256 37.012024 37.008232 37.07932
 37.143013 37.170933 36.840702 36.861603 36.88896  36.93338  36.954422
 36.973106 37.00602  37.03195  37.0022   37.039303 37.015495 37.027752
 37.01501  37.016148 37.028908 37.05363  37.121414 37.18887  36.842228
 36.827095 36.852528 36.892155 36.956474 36.97511  37.016632 37.054234
 37.063152 37.133537 37.157825 37.19044  37.15785  37.16028  37.21824
 37.316933 37.466183 37.56906  37.28061  37.33317  37.415565 37.516945
 37.650707]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 5.6081883907318115
Epoch: 1, Steps: 79 | Train Loss: 1.0188057 Vali Loss: 0.8618407 Test Loss: 0.8001867
Validation loss decreased (inf --> 0.861841).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 4.713984251022339
Epoch: 2, Steps: 79 | Train Loss: 0.8905524 Vali Loss: 0.7678266 Test Loss: 0.7062038
Validation loss decreased (0.861841 --> 0.767827).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 4.744522571563721
Epoch: 3, Steps: 79 | Train Loss: 0.7713608 Vali Loss: 0.5937762 Test Loss: 0.5143803
Validation loss decreased (0.767827 --> 0.593776).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 5.622682571411133
Epoch: 4, Steps: 79 | Train Loss: 0.5636791 Vali Loss: 0.4093930 Test Loss: 0.3865374
Validation loss decreased (0.593776 --> 0.409393).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 5.515969276428223
Epoch: 5, Steps: 79 | Train Loss: 0.4519231 Vali Loss: 0.3465040 Test Loss: 0.3278425
Validation loss decreased (0.409393 --> 0.346504).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 4.607431411743164
Epoch: 6, Steps: 79 | Train Loss: 0.4011625 Vali Loss: 0.2967976 Test Loss: 0.2886411
Validation loss decreased (0.346504 --> 0.296798).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 5.198679208755493
Epoch: 7, Steps: 79 | Train Loss: 0.3500279 Vali Loss: 0.2547947 Test Loss: 0.2388302
Validation loss decreased (0.296798 --> 0.254795).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 5.1791346073150635
Epoch: 8, Steps: 79 | Train Loss: 0.2910656 Vali Loss: 0.1958178 Test Loss: 0.1930498
Validation loss decreased (0.254795 --> 0.195818).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 5.721510887145996
Epoch: 9, Steps: 79 | Train Loss: 0.2542824 Vali Loss: 0.1664120 Test Loss: 0.1642617
Validation loss decreased (0.195818 --> 0.166412).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 5.560627222061157
Epoch: 10, Steps: 79 | Train Loss: 0.2320760 Vali Loss: 0.1553238 Test Loss: 0.1573075
Validation loss decreased (0.166412 --> 0.155324).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 4.281017065048218
Epoch: 11, Steps: 79 | Train Loss: 0.2222765 Vali Loss: 0.1459869 Test Loss: 0.1514011
Validation loss decreased (0.155324 --> 0.145987).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 5.336362838745117
Epoch: 12, Steps: 79 | Train Loss: 0.2121791 Vali Loss: 0.1344696 Test Loss: 0.1451728
Validation loss decreased (0.145987 --> 0.134470).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 4.900900602340698
Epoch: 13, Steps: 79 | Train Loss: 0.2056668 Vali Loss: 0.1339968 Test Loss: 0.1403128
Validation loss decreased (0.134470 --> 0.133997).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 4.5021209716796875
Epoch: 14, Steps: 79 | Train Loss: 0.1991272 Vali Loss: 0.1294210 Test Loss: 0.1350641
Validation loss decreased (0.133997 --> 0.129421).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 5.38949728012085
Epoch: 15, Steps: 79 | Train Loss: 0.1942976 Vali Loss: 0.1288204 Test Loss: 0.1345166
Validation loss decreased (0.129421 --> 0.128820).  Saving model ...
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 5.624772548675537
Epoch: 16, Steps: 79 | Train Loss: 0.1884889 Vali Loss: 0.1307284 Test Loss: 0.1351148
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 4.22290825843811
Epoch: 17, Steps: 79 | Train Loss: 0.1843486 Vali Loss: 0.1257426 Test Loss: 0.1309431
Validation loss decreased (0.128820 --> 0.125743).  Saving model ...
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 5.615617275238037
Epoch: 18, Steps: 79 | Train Loss: 0.1808980 Vali Loss: 0.1209268 Test Loss: 0.1283048
Validation loss decreased (0.125743 --> 0.120927).  Saving model ...
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 5.127853631973267
Epoch: 19, Steps: 79 | Train Loss: 0.1780229 Vali Loss: 0.1203463 Test Loss: 0.1283678
Validation loss decreased (0.120927 --> 0.120346).  Saving model ...
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 4.496128082275391
Epoch: 20, Steps: 79 | Train Loss: 0.1726864 Vali Loss: 0.1184475 Test Loss: 0.1231371
Validation loss decreased (0.120346 --> 0.118448).  Saving model ...
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 5.680785655975342
Epoch: 21, Steps: 79 | Train Loss: 0.1700852 Vali Loss: 0.1157228 Test Loss: 0.1280155
Validation loss decreased (0.118448 --> 0.115723).  Saving model ...
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 5.045746564865112
Epoch: 22, Steps: 79 | Train Loss: 0.1674434 Vali Loss: 0.1119282 Test Loss: 0.1224291
Validation loss decreased (0.115723 --> 0.111928).  Saving model ...
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 4.621063947677612
Epoch: 23, Steps: 79 | Train Loss: 0.1642468 Vali Loss: 0.1168001 Test Loss: 0.1192315
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 5.373444080352783
Epoch: 24, Steps: 79 | Train Loss: 0.1627993 Vali Loss: 0.1182421 Test Loss: 0.1253825
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 4.970498561859131
Epoch: 25, Steps: 79 | Train Loss: 0.1600251 Vali Loss: 0.1167976 Test Loss: 0.1202246
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 5.589190244674683
Epoch: 26, Steps: 79 | Train Loss: 0.1585326 Vali Loss: 0.1160664 Test Loss: 0.1230049
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 5.243294715881348
Epoch: 27, Steps: 79 | Train Loss: 0.1557225 Vali Loss: 0.1137829 Test Loss: 0.1192020
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 4.949323892593384
Epoch: 28, Steps: 79 | Train Loss: 0.1538680 Vali Loss: 0.1051076 Test Loss: 0.1130933
Validation loss decreased (0.111928 --> 0.105108).  Saving model ...
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 5.1369242668151855
Epoch: 29, Steps: 79 | Train Loss: 0.1496037 Vali Loss: 0.1203370 Test Loss: 0.1135413
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 5.0089380741119385
Epoch: 30, Steps: 79 | Train Loss: 0.1488936 Vali Loss: 0.1198256 Test Loss: 0.1109904
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 5.62579607963562
Epoch: 31, Steps: 79 | Train Loss: 0.1455293 Vali Loss: 0.1225398 Test Loss: 0.1150682
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 5.858820676803589
Epoch: 32, Steps: 79 | Train Loss: 0.1447869 Vali Loss: 0.1129721 Test Loss: 0.1118061
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 4.898927450180054
Epoch: 33, Steps: 79 | Train Loss: 0.1418706 Vali Loss: 0.1161124 Test Loss: 0.1082645
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 4.805312156677246
Epoch: 34, Steps: 79 | Train Loss: 0.1413897 Vali Loss: 0.1104191 Test Loss: 0.1089316
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 5.781072616577148
Epoch: 35, Steps: 79 | Train Loss: 0.1394933 Vali Loss: 0.1141520 Test Loss: 0.1070785
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 4.543900012969971
Epoch: 36, Steps: 79 | Train Loss: 0.1370323 Vali Loss: 0.1109331 Test Loss: 0.1067730
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 5.4002580642700195
Epoch: 37, Steps: 79 | Train Loss: 0.1360466 Vali Loss: 0.1141833 Test Loss: 0.1086619
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 5.571254253387451
Epoch: 38, Steps: 79 | Train Loss: 0.1355888 Vali Loss: 0.1139360 Test Loss: 0.1067999
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0009680
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.25050875544548035, mae:0.38881245255470276, rmse:0.5005084872245789, mape:0.013912374153733253, mspe:0.00032391693093813956, rse:0.3449805676937103, r2_score:0.8635545137934579, acc:0.9860876258462667
corr: [37.805416 37.855934 37.8781   37.8753   37.92911  37.939545 37.959972
 37.998833 37.94128  37.88776  37.98687  37.97466  37.959797 37.865643
 37.951855 37.968147 38.02441  38.145638 38.214027 38.052673 38.05855
 38.04929  38.00461  38.00798  37.943817 37.917984 37.9035   37.844784
 37.789307 37.857597 37.88339  37.879265 37.820538 37.882137 37.898266
 37.95385  38.052757 38.11741  37.919567 37.913113 37.89508  37.858963
 37.826096 37.772423 37.74407  37.74867  37.70799  37.632957 37.71805
 37.753265 37.741302 37.666157 37.740917 37.729656 37.735554 37.8326
 37.90261  37.9116   37.880424 37.84561  37.810585 37.763332 37.684525
 37.6681   37.660263 37.579266 37.48017  37.540066 37.54413  37.551323
 37.458824 37.49177  37.51179  37.52524  37.578182 37.62036  37.483784
 37.460552 37.431686 37.4091   37.39229  37.304306 37.2619   37.25012
 37.179897 37.114754 37.199333 37.18939  37.13373  37.041954 37.07354
 37.114708 37.1229   37.176563 37.23399  37.184338 37.15844  37.119446
 37.10317  37.06982  36.996094 36.966618 36.97771  36.93702  36.861095
 36.94156  36.873013 36.771805 36.70985  36.772003 36.82168  36.845787
 36.873608 36.939587 36.922276 36.863266 36.8407   36.822853 36.802803
 36.756916 36.725655 36.695293 36.65913  36.597088 36.731777 36.708973
 36.64102  36.58922  36.62253  36.684265 36.7194   36.72274  36.765236
 36.6084   36.59353  36.61161  36.585354 36.58708  36.519356 36.50419
 36.487415 36.46669  36.43586  36.544365 36.505733 36.440712 36.425587
 36.50063  36.568985 36.616768 36.663868 36.704098 36.53309  36.505947
 36.526115 36.508865 36.51717  36.449554 36.436672 36.459576 36.454567
 36.45779  36.56458  36.55017  36.52582  36.551975 36.614925 36.64824
 36.674957 36.729538 36.77588  36.392708 36.3839   36.42203  36.4615
 36.50211  36.46451  36.477062 36.505894 36.48863  36.484596 36.590992
 36.579815 36.554424 36.538048 36.60425  36.672127 36.722572 36.7919
 36.81825  36.505863 36.51718  36.520557 36.49956  36.52037  36.501774
 36.519943 36.55985  36.5216   36.534393 36.63678  36.610504 36.583702
 36.547333 36.555847 36.619736 36.6436   36.7297   36.80087  36.620483
 36.612434 36.639034 36.590733 36.591057 36.541996 36.524136 36.565884
 36.511032 36.47378  36.619602 36.608337 36.626682 36.635082 36.669353
 36.702316 36.75305  36.831287 36.88578  36.71083  36.673923 36.69611
 36.661396 36.692253 36.613987 36.606327 36.621716 36.565655 36.557724
 36.66325  36.599125 36.578487 36.520424 36.538498 36.61493  36.69824
 36.793438 36.84794  36.679543 36.702015 36.77084  36.7596   36.802433
 36.751583 36.756954 36.793453 36.7469   36.736534 36.852913 36.80777
 36.79856  36.783318 36.799088 36.833523 36.88376  36.962284 36.99285
 36.748627 36.756367 36.77469  36.778168 36.83143  36.78862  36.71577
 36.771614 36.73598  36.70745  36.826797 36.77388  36.774902 36.74969
 36.773567 36.806686 36.856106 36.95577  37.003933 36.703854 36.695423
 36.69599  36.67033  36.716812 36.665558 36.67567  36.7275   36.66234
 36.66364  36.781666 36.762524 36.786045 36.73455  36.75536  36.787857
 36.810356 36.89376  36.91314  36.619873 36.664825 36.676533 36.619873
 36.643383 36.59215  36.584396 36.6222   36.57823  36.59684  36.73784
 36.7245   36.756126 36.699955 36.764366 36.81862  36.833298 36.957527
 36.952843 36.651737 36.702587 36.72404  36.68439  36.732403 36.68383
 36.694695 36.7245   36.678814 36.66703  36.792427 36.817837 36.858543
 36.85622  36.947857 36.972652 36.996593 37.11557  37.168404 37.046715
 37.064182 37.0896   37.06124  37.11435  37.073982 37.073757 37.12607
 37.13556  37.155132 37.26757  37.29864  37.378696 37.39694  37.495365
 37.5457   37.612465 37.765556 37.8234   37.40896  37.501003 37.579624
 37.58888 ]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 5.0888142585754395
Epoch: 1, Steps: 79 | Train Loss: 1.0358853 Vali Loss: 0.8767685 Test Loss: 0.8075097
Validation loss decreased (inf --> 0.876769).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 5.111047744750977
Epoch: 2, Steps: 79 | Train Loss: 0.8921789 Vali Loss: 0.7860094 Test Loss: 0.7179601
Validation loss decreased (0.876769 --> 0.786009).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 5.500326871871948
Epoch: 3, Steps: 79 | Train Loss: 0.7763771 Vali Loss: 0.6168335 Test Loss: 0.5393332
Validation loss decreased (0.786009 --> 0.616833).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 5.483896493911743
Epoch: 4, Steps: 79 | Train Loss: 0.5680717 Vali Loss: 0.3658573 Test Loss: 0.3423989
Validation loss decreased (0.616833 --> 0.365857).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 5.105280637741089
Epoch: 5, Steps: 79 | Train Loss: 0.4436513 Vali Loss: 0.3431785 Test Loss: 0.3170803
Validation loss decreased (0.365857 --> 0.343178).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 5.841385841369629
Epoch: 6, Steps: 79 | Train Loss: 0.4018999 Vali Loss: 0.3061412 Test Loss: 0.2864293
Validation loss decreased (0.343178 --> 0.306141).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 5.009922742843628
Epoch: 7, Steps: 79 | Train Loss: 0.3517295 Vali Loss: 0.2573392 Test Loss: 0.2395883
Validation loss decreased (0.306141 --> 0.257339).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 5.078485727310181
Epoch: 8, Steps: 79 | Train Loss: 0.2952193 Vali Loss: 0.1970502 Test Loss: 0.1887740
Validation loss decreased (0.257339 --> 0.197050).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 5.333860158920288
Epoch: 9, Steps: 79 | Train Loss: 0.2607501 Vali Loss: 0.1615326 Test Loss: 0.1623302
Validation loss decreased (0.197050 --> 0.161533).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 4.640130996704102
Epoch: 10, Steps: 79 | Train Loss: 0.2387269 Vali Loss: 0.1472962 Test Loss: 0.1506878
Validation loss decreased (0.161533 --> 0.147296).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 5.478940486907959
Epoch: 11, Steps: 79 | Train Loss: 0.2230927 Vali Loss: 0.1384914 Test Loss: 0.1456515
Validation loss decreased (0.147296 --> 0.138491).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 5.5003650188446045
Epoch: 12, Steps: 79 | Train Loss: 0.2147140 Vali Loss: 0.1345536 Test Loss: 0.1419759
Validation loss decreased (0.138491 --> 0.134554).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 4.490683078765869
Epoch: 13, Steps: 79 | Train Loss: 0.2060611 Vali Loss: 0.1322981 Test Loss: 0.1360562
Validation loss decreased (0.134554 --> 0.132298).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 5.051099061965942
Epoch: 14, Steps: 79 | Train Loss: 0.2017316 Vali Loss: 0.1255158 Test Loss: 0.1366702
Validation loss decreased (0.132298 --> 0.125516).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 5.316997051239014
Epoch: 15, Steps: 79 | Train Loss: 0.1955784 Vali Loss: 0.1277662 Test Loss: 0.1350840
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 5.558438062667847
Epoch: 16, Steps: 79 | Train Loss: 0.1891506 Vali Loss: 0.1256553 Test Loss: 0.1351958
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 5.3424553871154785
Epoch: 17, Steps: 79 | Train Loss: 0.1866986 Vali Loss: 0.1256727 Test Loss: 0.1321812
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 4.617210388183594
Epoch: 18, Steps: 79 | Train Loss: 0.1826207 Vali Loss: 0.1281709 Test Loss: 0.1295928
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 5.232794284820557
Epoch: 19, Steps: 79 | Train Loss: 0.1782381 Vali Loss: 0.1122592 Test Loss: 0.1265262
Validation loss decreased (0.125516 --> 0.112259).  Saving model ...
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 5.5089333057403564
Epoch: 20, Steps: 79 | Train Loss: 0.1739976 Vali Loss: 0.1202261 Test Loss: 0.1243922
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 4.425737619400024
Epoch: 21, Steps: 79 | Train Loss: 0.1703110 Vali Loss: 0.1135112 Test Loss: 0.1234788
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 5.1273980140686035
Epoch: 22, Steps: 79 | Train Loss: 0.1675684 Vali Loss: 0.1179207 Test Loss: 0.1179696
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 4.848415374755859
Epoch: 23, Steps: 79 | Train Loss: 0.1644163 Vali Loss: 0.1156262 Test Loss: 0.1182028
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 5.4287261962890625
Epoch: 24, Steps: 79 | Train Loss: 0.1608051 Vali Loss: 0.1141880 Test Loss: 0.1183411
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 5.701792478561401
Epoch: 25, Steps: 79 | Train Loss: 0.1585817 Vali Loss: 0.1175904 Test Loss: 0.1184556
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 4.5990495681762695
Epoch: 26, Steps: 79 | Train Loss: 0.1566958 Vali Loss: 0.1220231 Test Loss: 0.1146679
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 4.862597942352295
Epoch: 27, Steps: 79 | Train Loss: 0.1544488 Vali Loss: 0.1078218 Test Loss: 0.1109420
Validation loss decreased (0.112259 --> 0.107822).  Saving model ...
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 5.149165153503418
Epoch: 28, Steps: 79 | Train Loss: 0.1524642 Vali Loss: 0.1225167 Test Loss: 0.1142401
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 4.414731502532959
Epoch: 29, Steps: 79 | Train Loss: 0.1493113 Vali Loss: 0.1137037 Test Loss: 0.1135442
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 5.428497552871704
Epoch: 30, Steps: 79 | Train Loss: 0.1483888 Vali Loss: 0.1102682 Test Loss: 0.1080588
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 5.557505130767822
Epoch: 31, Steps: 79 | Train Loss: 0.1456333 Vali Loss: 0.1058978 Test Loss: 0.1067398
Validation loss decreased (0.107822 --> 0.105898).  Saving model ...
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 5.816849708557129
Epoch: 32, Steps: 79 | Train Loss: 0.1435117 Vali Loss: 0.1186184 Test Loss: 0.1071072
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 5.562949180603027
Epoch: 33, Steps: 79 | Train Loss: 0.1423380 Vali Loss: 0.1137828 Test Loss: 0.1049106
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 4.665344953536987
Epoch: 34, Steps: 79 | Train Loss: 0.1396910 Vali Loss: 0.1089592 Test Loss: 0.1059310
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 5.7055792808532715
Epoch: 35, Steps: 79 | Train Loss: 0.1376003 Vali Loss: 0.1209551 Test Loss: 0.1066785
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 5.586182117462158
Epoch: 36, Steps: 79 | Train Loss: 0.1368470 Vali Loss: 0.1127236 Test Loss: 0.1060128
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 4.960433006286621
Epoch: 37, Steps: 79 | Train Loss: 0.1349895 Vali Loss: 0.1119989 Test Loss: 0.1004116
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 5.3437042236328125
Epoch: 38, Steps: 79 | Train Loss: 0.1332038 Vali Loss: 0.1126851 Test Loss: 0.1029379
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 5.068612098693848
Epoch: 39, Steps: 79 | Train Loss: 0.1315780 Vali Loss: 0.1039658 Test Loss: 0.0993553
Validation loss decreased (0.105898 --> 0.103966).  Saving model ...
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 4.872530460357666
Epoch: 40, Steps: 79 | Train Loss: 0.1292288 Vali Loss: 0.1081603 Test Loss: 0.1010149
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 5.291778802871704
Epoch: 41, Steps: 79 | Train Loss: 0.1276510 Vali Loss: 0.1036307 Test Loss: 0.0954092
Validation loss decreased (0.103966 --> 0.103631).  Saving model ...
Adjusting learning rate to: 0.0009402
Epoch: 42 cost time: 4.994564056396484
Epoch: 42, Steps: 79 | Train Loss: 0.1268453 Vali Loss: 0.1056173 Test Loss: 0.0979076
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009291
Epoch: 43 cost time: 5.780653953552246
Epoch: 43, Steps: 79 | Train Loss: 0.1244113 Vali Loss: 0.1069323 Test Loss: 0.0981149
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009171
Epoch: 44 cost time: 4.999207496643066
Epoch: 44, Steps: 79 | Train Loss: 0.1234448 Vali Loss: 0.1075852 Test Loss: 0.0967994
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009043
Epoch: 45 cost time: 4.709003448486328
Epoch: 45, Steps: 79 | Train Loss: 0.1218800 Vali Loss: 0.1050210 Test Loss: 0.0956033
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0008907
Epoch: 46 cost time: 5.484681844711304
Epoch: 46, Steps: 79 | Train Loss: 0.1206430 Vali Loss: 0.1106325 Test Loss: 0.0980605
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0008763
Epoch: 47 cost time: 5.4197211265563965
Epoch: 47, Steps: 79 | Train Loss: 0.1199502 Vali Loss: 0.1069105 Test Loss: 0.0944310
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0008612
Epoch: 48 cost time: 4.659204721450806
Epoch: 48, Steps: 79 | Train Loss: 0.1190408 Vali Loss: 0.1050752 Test Loss: 0.0952139
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0008453
Epoch: 49 cost time: 5.639360189437866
Epoch: 49, Steps: 79 | Train Loss: 0.1181720 Vali Loss: 0.1096621 Test Loss: 0.0966522
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0008288
Epoch: 50 cost time: 5.516649961471558
Epoch: 50, Steps: 79 | Train Loss: 0.1167180 Vali Loss: 0.0999452 Test Loss: 0.0935431
Validation loss decreased (0.103631 --> 0.099945).  Saving model ...
Adjusting learning rate to: 0.0008115
Epoch: 51 cost time: 4.241428852081299
Epoch: 51, Steps: 79 | Train Loss: 0.1162157 Vali Loss: 0.1057517 Test Loss: 0.0947022
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0007937
Epoch: 52 cost time: 5.736772298812866
Epoch: 52, Steps: 79 | Train Loss: 0.1149829 Vali Loss: 0.1047279 Test Loss: 0.0932342
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0007752
Epoch: 53 cost time: 4.540062665939331
Epoch: 53, Steps: 79 | Train Loss: 0.1138748 Vali Loss: 0.1083371 Test Loss: 0.0959033
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0007562
Epoch: 54 cost time: 4.87134575843811
Epoch: 54, Steps: 79 | Train Loss: 0.1133443 Vali Loss: 0.1106778 Test Loss: 0.0950321
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0007367
Epoch: 55 cost time: 5.123698949813843
Epoch: 55, Steps: 79 | Train Loss: 0.1124540 Vali Loss: 0.1060252 Test Loss: 0.0954008
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0007167
Epoch: 56 cost time: 4.642499923706055
Epoch: 56, Steps: 79 | Train Loss: 0.1117678 Vali Loss: 0.1116460 Test Loss: 0.0960856
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0006963
Epoch: 57 cost time: 5.783539772033691
Epoch: 57, Steps: 79 | Train Loss: 0.1103918 Vali Loss: 0.1108084 Test Loss: 0.0969012
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0006754
Epoch: 58 cost time: 4.866262435913086
Epoch: 58, Steps: 79 | Train Loss: 0.1101442 Vali Loss: 0.1079255 Test Loss: 0.0960659
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0006542
Epoch: 59 cost time: 4.703154802322388
Epoch: 59, Steps: 79 | Train Loss: 0.1095312 Vali Loss: 0.1079523 Test Loss: 0.0952675
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0006327
Epoch: 60 cost time: 5.465439796447754
Epoch: 60, Steps: 79 | Train Loss: 0.1087972 Vali Loss: 0.1069739 Test Loss: 0.0929829
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0006110
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.21293804049491882, mae:0.36154472827911377, rmse:0.46145209670066833, mape:0.012916088104248047, mspe:0.0002736588066909462, rse:0.3180605471134186, r2_score:0.8883996371032834, acc:0.987083911895752
corr: [38.149475 38.166573 38.146408 38.113525 38.147755 38.143986 38.123318
 38.040848 37.98414  37.93167  37.86972  37.80268  37.719513 37.576878
 37.468845 37.45047  37.496048 37.593586 37.642986 37.702335 37.752552
 37.7749   37.780537 37.795628 37.852177 37.853485 37.84417  37.788017
 37.754963 37.731716 37.71512  37.700584 37.68808  37.62513  37.58127
 37.586082 37.67299  37.794643 37.865593 37.948288 37.906776 37.937935
 37.958244 37.95946  38.009766 38.000717 37.979687 37.91302  37.857483
 37.832573 37.82355  37.811882 37.759995 37.68426  37.649147 37.681473
 37.763985 37.86241  37.912163 37.988274 37.692043 37.689907 37.70022
 37.695908 37.73749  37.74204  37.723137 37.6907   37.687347 37.685284
 37.656567 37.630497 37.586777 37.50855  37.413673 37.37622  37.414963
 37.527027 37.565002 37.633568 37.332386 37.334908 37.338844 37.326935
 37.357758 37.338875 37.32745  37.291195 37.25909  37.256035 37.236263
 37.23164  37.203766 37.118195 37.04316  37.004025 37.05902  37.14504
 37.17071  37.230408 37.070873 37.083538 37.118305 37.13135  37.167175
 37.200493 37.19622  37.151108 37.13764  37.12544  37.09194  37.069363
 37.03979  36.94583  36.84377  36.8058   36.841385 36.90629  36.942104
 36.992435 36.79664  36.792595 36.815903 36.82289  36.873043 36.885025
 36.883266 36.86626  36.85074  36.8425   36.806694 36.762634 36.71458
 36.62297  36.52588  36.50542  36.531685 36.58271  36.60621  36.66317
 36.556366 36.547344 36.543217 36.53779  36.560158 36.544086 36.563786
 36.585007 36.6113   36.596836 36.58942  36.57537  36.539024 36.453537
 36.36053  36.33063  36.359554 36.410057 36.407207 36.46047  36.35816
 36.3822   36.425983 36.42971  36.484837 36.515244 36.52402  36.518024
 36.525265 36.54049  36.556267 36.565563 36.536835 36.473164 36.415092
 36.407925 36.42685  36.508522 36.55022  36.628387 36.340874 36.348114
 36.37754  36.396347 36.439793 36.449474 36.494923 36.528774 36.546936
 36.553764 36.56191  36.555565 36.533012 36.443825 36.357925 36.342537
 36.39408  36.46141  36.48561  36.550056 36.28983  36.3077   36.340446
 36.37483  36.423817 36.424713 36.42435  36.42449  36.44144  36.467354
 36.475586 36.48382  36.45849  36.388905 36.31818  36.274372 36.310925
 36.388332 36.45626  36.55911  36.290726 36.30712  36.319736 36.321465
 36.393726 36.43366  36.457577 36.457634 36.452    36.46082  36.449715
 36.43171  36.41785  36.358166 36.277534 36.25971  36.31024  36.40334
 36.458675 36.52005  36.3758   36.382107 36.40497  36.411015 36.446327
 36.448887 36.428432 36.37251  36.372467 36.36789  36.368603 36.3506
 36.33636  36.2544   36.175957 36.17134  36.1937   36.272324 36.30559
 36.377495 36.36705  36.387356 36.402157 36.401184 36.453156 36.473923
 36.484646 36.482025 36.47238  36.466164 36.468376 36.475716 36.47673
 36.42134  36.357285 36.345306 36.394226 36.46362  36.491333 36.575115
 36.453506 36.4861   36.49929  36.4978   36.523952 36.53442  36.51518
 36.47761  36.4458   36.448875 36.44836  36.455704 36.450912 36.376823
 36.323624 36.353554 36.4067   36.497677 36.534428 36.628067 36.578728
 36.58747  36.5761   36.54473  36.564228 36.549065 36.522514 36.468666
 36.44255  36.413906 36.40378  36.393105 36.393585 36.315502 36.236565
 36.248413 36.301643 36.390503 36.43422  36.50134  36.52738  36.543583
 36.544228 36.55556  36.56203  36.551224 36.539284 36.495995 36.480328
 36.460407 36.444843 36.42235  36.405827 36.35852  36.31754  36.359882
 36.442383 36.554596 36.627945 36.726276 36.741585 36.764915 36.783737
 36.794727 36.83264  36.821754 36.829193 36.814636 36.808388 36.810833
 36.8378   36.85649  36.851147 36.776577 36.751705 36.800224 36.898083
 37.023525 37.12119  37.280148 37.254208 37.29813  37.360756 37.40841
 37.484573]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 5.812196254730225
Epoch: 1, Steps: 79 | Train Loss: 1.0308812 Vali Loss: 0.8711604 Test Loss: 0.7922369
Validation loss decreased (inf --> 0.871160).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 5.030139207839966
Epoch: 2, Steps: 79 | Train Loss: 0.8909165 Vali Loss: 0.7830913 Test Loss: 0.7264815
Validation loss decreased (0.871160 --> 0.783091).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 4.5843119621276855
Epoch: 3, Steps: 79 | Train Loss: 0.7932344 Vali Loss: 0.6168705 Test Loss: 0.5407772
Validation loss decreased (0.783091 --> 0.616870).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 5.310013294219971
Epoch: 4, Steps: 79 | Train Loss: 0.5930547 Vali Loss: 0.4305655 Test Loss: 0.4048597
Validation loss decreased (0.616870 --> 0.430566).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 5.332735061645508
Epoch: 5, Steps: 79 | Train Loss: 0.4719149 Vali Loss: 0.3768447 Test Loss: 0.3586309
Validation loss decreased (0.430566 --> 0.376845).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 4.5233683586120605
Epoch: 6, Steps: 79 | Train Loss: 0.4153197 Vali Loss: 0.3353067 Test Loss: 0.3134039
Validation loss decreased (0.376845 --> 0.335307).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 5.367632865905762
Epoch: 7, Steps: 79 | Train Loss: 0.3724706 Vali Loss: 0.2826208 Test Loss: 0.2680746
Validation loss decreased (0.335307 --> 0.282621).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 5.375963449478149
Epoch: 8, Steps: 79 | Train Loss: 0.3153680 Vali Loss: 0.2064451 Test Loss: 0.2000792
Validation loss decreased (0.282621 --> 0.206445).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 5.357582330703735
Epoch: 9, Steps: 79 | Train Loss: 0.2633858 Vali Loss: 0.1641119 Test Loss: 0.1659661
Validation loss decreased (0.206445 --> 0.164112).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 5.368922710418701
Epoch: 10, Steps: 79 | Train Loss: 0.2390871 Vali Loss: 0.1482625 Test Loss: 0.1568727
Validation loss decreased (0.164112 --> 0.148262).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 4.60161566734314
Epoch: 11, Steps: 79 | Train Loss: 0.2258517 Vali Loss: 0.1396490 Test Loss: 0.1472258
Validation loss decreased (0.148262 --> 0.139649).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 5.311493158340454
Epoch: 12, Steps: 79 | Train Loss: 0.2165792 Vali Loss: 0.1396132 Test Loss: 0.1468944
Validation loss decreased (0.139649 --> 0.139613).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 5.442452669143677
Epoch: 13, Steps: 79 | Train Loss: 0.2085743 Vali Loss: 0.1357427 Test Loss: 0.1438021
Validation loss decreased (0.139613 --> 0.135743).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 4.837276458740234
Epoch: 14, Steps: 79 | Train Loss: 0.2031737 Vali Loss: 0.1344964 Test Loss: 0.1397218
Validation loss decreased (0.135743 --> 0.134496).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 4.719072580337524
Epoch: 15, Steps: 79 | Train Loss: 0.1966312 Vali Loss: 0.1297751 Test Loss: 0.1362151
Validation loss decreased (0.134496 --> 0.129775).  Saving model ...
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 4.833709001541138
Epoch: 16, Steps: 79 | Train Loss: 0.1909926 Vali Loss: 0.1265251 Test Loss: 0.1349594
Validation loss decreased (0.129775 --> 0.126525).  Saving model ...
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 5.179037809371948
Epoch: 17, Steps: 79 | Train Loss: 0.1867811 Vali Loss: 0.1241860 Test Loss: 0.1314848
Validation loss decreased (0.126525 --> 0.124186).  Saving model ...
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 5.3420000076293945
Epoch: 18, Steps: 79 | Train Loss: 0.1841782 Vali Loss: 0.1197279 Test Loss: 0.1315288
Validation loss decreased (0.124186 --> 0.119728).  Saving model ...
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 4.458051919937134
Epoch: 19, Steps: 79 | Train Loss: 0.1798009 Vali Loss: 0.1174835 Test Loss: 0.1264756
Validation loss decreased (0.119728 --> 0.117484).  Saving model ...
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 5.489435195922852
Epoch: 20, Steps: 79 | Train Loss: 0.1764999 Vali Loss: 0.1202434 Test Loss: 0.1266395
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 4.897411584854126
Epoch: 21, Steps: 79 | Train Loss: 0.1706489 Vali Loss: 0.1194957 Test Loss: 0.1239893
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 5.36023211479187
Epoch: 22, Steps: 79 | Train Loss: 0.1682730 Vali Loss: 0.1169258 Test Loss: 0.1219617
Validation loss decreased (0.117484 --> 0.116926).  Saving model ...
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 6.169826984405518
Epoch: 23, Steps: 79 | Train Loss: 0.1652281 Vali Loss: 0.1164861 Test Loss: 0.1244256
Validation loss decreased (0.116926 --> 0.116486).  Saving model ...
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 5.775114297866821
Epoch: 24, Steps: 79 | Train Loss: 0.1614539 Vali Loss: 0.1147631 Test Loss: 0.1183231
Validation loss decreased (0.116486 --> 0.114763).  Saving model ...
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 6.093021869659424
Epoch: 25, Steps: 79 | Train Loss: 0.1604151 Vali Loss: 0.1139785 Test Loss: 0.1140206
Validation loss decreased (0.114763 --> 0.113979).  Saving model ...
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 5.984333038330078
Epoch: 26, Steps: 79 | Train Loss: 0.1577815 Vali Loss: 0.1085549 Test Loss: 0.1184504
Validation loss decreased (0.113979 --> 0.108555).  Saving model ...
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 5.200092077255249
Epoch: 27, Steps: 79 | Train Loss: 0.1552660 Vali Loss: 0.1096773 Test Loss: 0.1123700
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 5.6979289054870605
Epoch: 28, Steps: 79 | Train Loss: 0.1526380 Vali Loss: 0.1143058 Test Loss: 0.1131098
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 5.7850425243377686
Epoch: 29, Steps: 79 | Train Loss: 0.1509693 Vali Loss: 0.1179725 Test Loss: 0.1163662
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 5.320835590362549
Epoch: 30, Steps: 79 | Train Loss: 0.1487363 Vali Loss: 0.1202018 Test Loss: 0.1126733
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 5.521733999252319
Epoch: 31, Steps: 79 | Train Loss: 0.1472821 Vali Loss: 0.1070494 Test Loss: 0.1110227
Validation loss decreased (0.108555 --> 0.107049).  Saving model ...
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 5.715627670288086
Epoch: 32, Steps: 79 | Train Loss: 0.1457501 Vali Loss: 0.1299333 Test Loss: 0.1151682
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 4.917958974838257
Epoch: 33, Steps: 79 | Train Loss: 0.1430160 Vali Loss: 0.1136128 Test Loss: 0.1100823
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 3.709834337234497
Epoch: 34, Steps: 79 | Train Loss: 0.1419381 Vali Loss: 0.1120754 Test Loss: 0.1087167
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 3.818150520324707
Epoch: 35, Steps: 79 | Train Loss: 0.1403633 Vali Loss: 0.1179689 Test Loss: 0.1116968
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 3.8229939937591553
Epoch: 36, Steps: 79 | Train Loss: 0.1374971 Vali Loss: 0.1133106 Test Loss: 0.1077796
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 3.860340118408203
Epoch: 37, Steps: 79 | Train Loss: 0.1364667 Vali Loss: 0.1129849 Test Loss: 0.1084386
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 3.8089170455932617
Epoch: 38, Steps: 79 | Train Loss: 0.1346571 Vali Loss: 0.1132945 Test Loss: 0.1081672
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 3.9371535778045654
Epoch: 39, Steps: 79 | Train Loss: 0.1331521 Vali Loss: 0.1131426 Test Loss: 0.1093132
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 3.9527218341827393
Epoch: 40, Steps: 79 | Train Loss: 0.1311328 Vali Loss: 0.1160559 Test Loss: 0.1076446
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 3.8253259658813477
Epoch: 41, Steps: 79 | Train Loss: 0.1295436 Vali Loss: 0.1122719 Test Loss: 0.1058586
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0009402
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.25011661648750305, mae:0.3926147222518921, rmse:0.5001165866851807, mape:0.01401937659829855, mspe:0.00032062054378911853, rse:0.34471046924591064, r2_score:0.8704131814975332, acc:0.9859806234017015
corr: [37.92692  37.89372  37.87228  37.904705 37.85686  37.878387 37.886833
 37.821228 37.787796 37.821606 37.87365  37.905937 37.891678 37.86728
 37.84122  37.820274 37.802734 37.810238 37.811184 37.796356 37.75158
 37.747288 37.71259  37.678814 37.686413 37.6779   37.714123 37.756092
 37.725765 37.683002 37.742046 37.85794  37.90847  37.93828  37.93867
 37.92236  37.90884  37.893074 37.867542 37.875923 37.833115 37.7749
 37.661495 37.615017 37.596157 37.636288 37.624504 37.65804  37.711662
 37.683807 37.66748  37.731693 37.854843 37.894928 37.89431  37.863617
 37.844017 37.834755 37.82854  37.8154   37.816902 37.795155 37.769703
 37.53972  37.489723 37.457317 37.457348 37.434296 37.471523 37.507267
 37.438053 37.423275 37.488132 37.57786  37.602814 37.58863  37.576763
 37.53878  37.501045 37.440426 37.405106 37.418045 37.371742 37.31084
 37.13885  37.09369  37.04086  37.06519  37.016003 37.000793 37.039043
 37.034084 37.03117  37.073547 37.185257 37.21147  37.22724  37.213448
 37.16867  37.130573 37.0661   37.039566 37.034847 37.032864 37.01352
 36.991154 36.93426  36.91015  36.92662  36.898624 36.924152 36.932343
 36.911297 36.896854 36.93078  36.99514  36.980057 36.94187  36.913162
 36.90136  36.85127  36.82475  36.79852  36.777393 36.765854 36.74713
 36.768677 36.71881  36.704937 36.718197 36.672783 36.691082 36.70953
 36.66068  36.64427  36.661076 36.72286  36.717857 36.68097  36.621807
 36.568398 36.54801  36.490463 36.45672  36.449318 36.411728 36.39002
 36.502735 36.458637 36.46789  36.476593 36.41227  36.449444 36.475395
 36.414795 36.399296 36.414597 36.488777 36.49547  36.48453  36.47019
 36.440964 36.416355 36.38727  36.398487 36.382072 36.37653  36.35747
 36.382202 36.359932 36.366108 36.40811  36.38951  36.434223 36.436832
 36.39269  36.43448  36.479134 36.533627 36.571934 36.59163  36.605152
 36.609867 36.62744  36.6241   36.651894 36.641644 36.614605 36.582726
 36.563168 36.543434 36.528763 36.56229  36.5334   36.575653 36.608093
 36.589184 36.59984  36.6289   36.70778  36.71394  36.678513 36.689518
 36.663483 36.676487 36.65768  36.683533 36.66036  36.648033 36.62898
 36.60874  36.581066 36.56228  36.582222 36.5683   36.590782 36.632004
 36.609257 36.630234 36.652897 36.70641  36.735203 36.76239  36.77428
 36.776867 36.792763 36.7915   36.83463  36.846367 36.821255 36.80736
 36.643856 36.60921  36.578278 36.609783 36.60778  36.66484  36.713654
 36.69774  36.7105   36.74877  36.832397 36.84701  36.82487  36.80994
 36.780846 36.762535 36.73473  36.725372 36.72328  36.735508 36.75905
 36.59016  36.6038   36.641212 36.681744 36.672775 36.70556  36.71999
 36.692245 36.750668 36.749657 36.812103 36.846672 36.837124 36.82851
 36.853897 36.848015 36.844574 36.860157 36.849407 36.84906  36.848022
 36.690056 36.642216 36.63741  36.665977 36.643177 36.673553 36.705982
 36.676785 36.685944 36.70914  36.804646 36.817696 36.791367 36.779926
 36.76567  36.72811  36.68664  36.69612  36.69617  36.683575 36.65808
 36.64586  36.613857 36.599308 36.63493  36.614677 36.62763  36.667297
 36.63213  36.646458 36.672707 36.76445  36.79373  36.75393  36.73081
 36.702053 36.6931   36.67417  36.67128  36.64619  36.632923 36.59903
 36.63761  36.610027 36.601116 36.588924 36.544094 36.55174  36.570625
 36.546337 36.579494 36.587345 36.684784 36.738155 36.76034  36.755253
 36.765392 36.774372 36.779354 36.835712 36.85997  36.865616 36.865696
 36.706577 36.696423 36.713757 36.75379  36.734737 36.746807 36.77534
 36.73824  36.74349  36.788067 36.901352 36.928608 36.97208  36.973225
 36.98488  36.99622  37.018723 37.018223 37.02061  37.04587  37.05822
 36.90917  36.938454 36.98497  37.06191  37.091667 37.179325 37.27886
 37.30744 ]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 3.947181463241577
Epoch: 1, Steps: 79 | Train Loss: 1.0427596 Vali Loss: 0.8867256 Test Loss: 0.8103002
Validation loss decreased (inf --> 0.886726).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 3.8152122497558594
Epoch: 2, Steps: 79 | Train Loss: 0.8941620 Vali Loss: 0.7835728 Test Loss: 0.6901219
Validation loss decreased (0.886726 --> 0.783573).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 3.796135187149048
Epoch: 3, Steps: 79 | Train Loss: 0.7385297 Vali Loss: 0.5471994 Test Loss: 0.4807561
Validation loss decreased (0.783573 --> 0.547199).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 3.7135825157165527
Epoch: 4, Steps: 79 | Train Loss: 0.5596419 Vali Loss: 0.4435516 Test Loss: 0.4044186
Validation loss decreased (0.547199 --> 0.443552).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 3.8111164569854736
Epoch: 5, Steps: 79 | Train Loss: 0.4746496 Vali Loss: 0.3553039 Test Loss: 0.3380595
Validation loss decreased (0.443552 --> 0.355304).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 3.7649781703948975
Epoch: 6, Steps: 79 | Train Loss: 0.4133002 Vali Loss: 0.3219237 Test Loss: 0.3030790
Validation loss decreased (0.355304 --> 0.321924).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 3.8461742401123047
Epoch: 7, Steps: 79 | Train Loss: 0.3624880 Vali Loss: 0.2611945 Test Loss: 0.2428364
Validation loss decreased (0.321924 --> 0.261194).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 3.7710649967193604
Epoch: 8, Steps: 79 | Train Loss: 0.3060094 Vali Loss: 0.2081569 Test Loss: 0.1976707
Validation loss decreased (0.261194 --> 0.208157).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 3.798631429672241
Epoch: 9, Steps: 79 | Train Loss: 0.2664976 Vali Loss: 0.1777617 Test Loss: 0.1741427
Validation loss decreased (0.208157 --> 0.177762).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 3.7659151554107666
Epoch: 10, Steps: 79 | Train Loss: 0.2445249 Vali Loss: 0.1647871 Test Loss: 0.1619572
Validation loss decreased (0.177762 --> 0.164787).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 3.977571964263916
Epoch: 11, Steps: 79 | Train Loss: 0.2284556 Vali Loss: 0.1540500 Test Loss: 0.1546255
Validation loss decreased (0.164787 --> 0.154050).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 3.952981472015381
Epoch: 12, Steps: 79 | Train Loss: 0.2196918 Vali Loss: 0.1412187 Test Loss: 0.1459142
Validation loss decreased (0.154050 --> 0.141219).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 3.9889607429504395
Epoch: 13, Steps: 79 | Train Loss: 0.2103064 Vali Loss: 0.1405290 Test Loss: 0.1432499
Validation loss decreased (0.141219 --> 0.140529).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 4.090447187423706
Epoch: 14, Steps: 79 | Train Loss: 0.2042233 Vali Loss: 0.1367072 Test Loss: 0.1409719
Validation loss decreased (0.140529 --> 0.136707).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 4.014435529708862
Epoch: 15, Steps: 79 | Train Loss: 0.1986422 Vali Loss: 0.1386693 Test Loss: 0.1464661
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 4.046790599822998
Epoch: 16, Steps: 79 | Train Loss: 0.1924921 Vali Loss: 0.1272184 Test Loss: 0.1381897
Validation loss decreased (0.136707 --> 0.127218).  Saving model ...
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 4.038172006607056
Epoch: 17, Steps: 79 | Train Loss: 0.1883959 Vali Loss: 0.1300441 Test Loss: 0.1324646
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 3.9684057235717773
Epoch: 18, Steps: 79 | Train Loss: 0.1852162 Vali Loss: 0.1281231 Test Loss: 0.1314971
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 4.007623672485352
Epoch: 19, Steps: 79 | Train Loss: 0.1799509 Vali Loss: 0.1186484 Test Loss: 0.1279932
Validation loss decreased (0.127218 --> 0.118648).  Saving model ...
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 4.090399980545044
Epoch: 20, Steps: 79 | Train Loss: 0.1755932 Vali Loss: 0.1191808 Test Loss: 0.1252463
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 5.0824134349823
Epoch: 21, Steps: 79 | Train Loss: 0.1718407 Vali Loss: 0.1226300 Test Loss: 0.1227763
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 5.908601522445679
Epoch: 22, Steps: 79 | Train Loss: 0.1679408 Vali Loss: 0.1182044 Test Loss: 0.1191069
Validation loss decreased (0.118648 --> 0.118204).  Saving model ...
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 5.274129629135132
Epoch: 23, Steps: 79 | Train Loss: 0.1656725 Vali Loss: 0.1145992 Test Loss: 0.1179039
Validation loss decreased (0.118204 --> 0.114599).  Saving model ...
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 5.950662851333618
Epoch: 24, Steps: 79 | Train Loss: 0.1632169 Vali Loss: 0.1099649 Test Loss: 0.1153873
Validation loss decreased (0.114599 --> 0.109965).  Saving model ...
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 5.369112014770508
Epoch: 25, Steps: 79 | Train Loss: 0.1602992 Vali Loss: 0.1152199 Test Loss: 0.1156220
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 6.1032984256744385
Epoch: 26, Steps: 79 | Train Loss: 0.1580264 Vali Loss: 0.1137585 Test Loss: 0.1222110
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 5.783767461776733
Epoch: 27, Steps: 79 | Train Loss: 0.1572757 Vali Loss: 0.1141017 Test Loss: 0.1132972
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 6.305491924285889
Epoch: 28, Steps: 79 | Train Loss: 0.1534170 Vali Loss: 0.1098492 Test Loss: 0.1107518
Validation loss decreased (0.109965 --> 0.109849).  Saving model ...
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 5.835722923278809
Epoch: 29, Steps: 79 | Train Loss: 0.1528498 Vali Loss: 0.1077806 Test Loss: 0.1114211
Validation loss decreased (0.109849 --> 0.107781).  Saving model ...
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 6.360395669937134
Epoch: 30, Steps: 79 | Train Loss: 0.1498506 Vali Loss: 0.1157668 Test Loss: 0.1129102
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 6.197193145751953
Epoch: 31, Steps: 79 | Train Loss: 0.1482200 Vali Loss: 0.1088478 Test Loss: 0.1104962
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 5.8862175941467285
Epoch: 32, Steps: 79 | Train Loss: 0.1464222 Vali Loss: 0.1128957 Test Loss: 0.1070113
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 5.412735223770142
Epoch: 33, Steps: 79 | Train Loss: 0.1450820 Vali Loss: 0.1110912 Test Loss: 0.1078738
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 5.782731771469116
Epoch: 34, Steps: 79 | Train Loss: 0.1437188 Vali Loss: 0.1093497 Test Loss: 0.1055277
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 5.645054340362549
Epoch: 35, Steps: 79 | Train Loss: 0.1419855 Vali Loss: 0.1038442 Test Loss: 0.1061744
Validation loss decreased (0.107781 --> 0.103844).  Saving model ...
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 6.253247261047363
Epoch: 36, Steps: 79 | Train Loss: 0.1411380 Vali Loss: 0.1184993 Test Loss: 0.1076953
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 5.760176420211792
Epoch: 37, Steps: 79 | Train Loss: 0.1388705 Vali Loss: 0.1046082 Test Loss: 0.1083070
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 5.5389485359191895
Epoch: 38, Steps: 79 | Train Loss: 0.1378247 Vali Loss: 0.1113903 Test Loss: 0.1042111
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 5.5815300941467285
Epoch: 39, Steps: 79 | Train Loss: 0.1361953 Vali Loss: 0.1096504 Test Loss: 0.1051079
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 6.136449337005615
Epoch: 40, Steps: 79 | Train Loss: 0.1342632 Vali Loss: 0.1082003 Test Loss: 0.1066443
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 5.366307735443115
Epoch: 41, Steps: 79 | Train Loss: 0.1335643 Vali Loss: 0.1108839 Test Loss: 0.1063221
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009402
Epoch: 42 cost time: 5.140797853469849
Epoch: 42, Steps: 79 | Train Loss: 0.1320483 Vali Loss: 0.1212467 Test Loss: 0.1072963
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0009291
Epoch: 43 cost time: 6.250396251678467
Epoch: 43, Steps: 79 | Train Loss: 0.1303974 Vali Loss: 0.1049072 Test Loss: 0.1047775
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0009171
Epoch: 44 cost time: 4.933865070343018
Epoch: 44, Steps: 79 | Train Loss: 0.1300044 Vali Loss: 0.1134147 Test Loss: 0.1053947
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0009043
Epoch: 45 cost time: 5.770269155502319
Epoch: 45, Steps: 79 | Train Loss: 0.1276556 Vali Loss: 0.1134902 Test Loss: 0.1009574
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0008907
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.24552065134048462, mae:0.3855481445789337, rmse:0.4955004155635834, mape:0.013776922598481178, mspe:0.0003156140155624598, rse:0.34152868390083313, r2_score:0.8725938876685644, acc:0.9862230774015188
corr: [37.406273 37.351677 37.331856 37.324703 37.375046 37.438747 37.479465
 37.428383 37.423527 37.36266  37.391083 37.418026 37.441067 37.463314
 37.496323 37.56543  37.543552 37.558506 37.50051  37.48684  37.47385
 37.48458  37.36018  37.306465 37.264862 37.25747  37.327152 37.396477
 37.44676  37.421043 37.415024 37.390972 37.453804 37.5383   37.605495
 37.64284  37.688854 37.74624  37.72727  37.73207  37.684788 37.679295
 37.63051  37.588825 37.55233  37.515568 37.50432  37.49504  37.56458
 37.579174 37.59369  37.527184 37.49203  37.454895 37.48064  37.543552
 37.582703 37.62267  37.681274 37.75937  37.747192 37.7573   37.729084
 37.702866 37.642097 37.591446 37.444572 37.373783 37.325756 37.28176
 37.315426 37.326927 37.343807 37.291153 37.25917  37.20848  37.19603
 37.244858 37.269054 37.26987  37.308212 37.36273  37.30636  37.295666
 37.265923 37.233734 37.2051   37.159897 36.97682  36.902126 36.867203
 36.847504 36.8754   36.93259  36.964764 36.9248   36.900803 36.87559
 36.8995   36.917103 36.960308 36.973724 37.025486 37.08548  37.06179
 37.07242  37.035713 37.028725 36.98663  36.948627 36.80842  36.776314
 36.73043  36.708195 36.74047  36.78415  36.79416  36.74105  36.70561
 36.67808  36.70394  36.723564 36.740963 36.71733  36.729122 36.79532
 36.787773 36.7951   36.72555  36.69919  36.647984 36.572216 36.417072
 36.375465 36.32297  36.30384  36.338173 36.393833 36.42838  36.393353
 36.366432 36.32528  36.33442  36.379784 36.416138 36.427074 36.46659
 36.532475 36.517475 36.493107 36.45609  36.469646 36.44308  36.397636
 36.215252 36.196808 36.15928  36.13247  36.16496  36.202274 36.211403
 36.177044 36.138294 36.130684 36.166718 36.22021  36.286854 36.305862
 36.365387 36.459454 36.474907 36.49376  36.45552  36.436726 36.43024
 36.422962 36.10025  36.08227  36.085148 36.101254 36.159805 36.21507
 36.21135  36.171474 36.15259  36.124107 36.131268 36.175377 36.21043
 36.228336 36.27124  36.384903 36.407055 36.4376   36.417492 36.4409
 36.43101  36.41853  36.153423 36.137196 36.116684 36.114445 36.15007
 36.18831  36.181503 36.125664 36.110542 36.081657 36.122295 36.175495
 36.2397   36.2545   36.298126 36.3975   36.411648 36.462845 36.47994
 36.48672  36.456852 36.433704 36.072906 36.01173  35.968662 35.973656
 36.009495 36.03937  36.038116 35.974773 35.949318 35.92289  35.93693
 35.982124 36.02698  36.045116 36.086742 36.167336 36.189014 36.240494
 36.254383 36.253803 36.226208 36.18977  35.938744 35.893276 35.879612
 35.86076  35.8689   35.892258 35.884743 35.81498  35.761154 35.72011
 35.734566 35.81334  35.890503 35.92463  36.019695 36.163597 36.221424
 36.29756  36.29504  36.32978  36.303856 36.266415 35.935375 35.863976
 35.83178  35.80148  35.794952 35.813896 35.7937   35.71194  35.667973
 35.61854  35.639275 35.67168  35.74143  35.78054  35.861565 35.962887
 35.977234 36.016808 36.017326 36.055256 36.03255  36.006878 35.931995
 35.86758  35.851143 35.80843  35.821213 35.836357 35.80941  35.72675
 35.690758 35.68105  35.702732 35.747105 35.793106 35.83746  35.918648
 36.017982 36.02202  36.071297 36.074226 36.09569  36.07125  36.04298
 36.000576 35.89765  35.838337 35.764477 35.759495 35.783787 35.77891
 35.72091  35.68169  35.643402 35.65451  35.709576 35.759705 35.790886
 35.85348  35.971664 35.98037  36.046608 36.046047 36.064644 36.037025
 36.00805  35.889523 35.82496  35.766994 35.73025  35.775368 35.8056
 35.795727 35.74129  35.723427 35.734585 35.810074 35.885456 35.978603
 36.033012 36.12855  36.2756   36.298645 36.356594 36.396885 36.425224
 36.429546 36.426456 36.19128  36.1302   36.109055 36.083    36.138424
 36.219746 36.272057 36.26377  36.29528  36.34873  36.449585 36.563534
 36.68051 ]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 4.664720773696899
Epoch: 1, Steps: 79 | Train Loss: 1.0295126 Vali Loss: 0.8929916 Test Loss: 0.8024861
Validation loss decreased (inf --> 0.892992).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 5.226455450057983
Epoch: 2, Steps: 79 | Train Loss: 0.8924061 Vali Loss: 0.7746704 Test Loss: 0.6997432
Validation loss decreased (0.892992 --> 0.774670).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 4.797095775604248
Epoch: 3, Steps: 79 | Train Loss: 0.7631540 Vali Loss: 0.5510152 Test Loss: 0.5049983
Validation loss decreased (0.774670 --> 0.551015).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 4.410863637924194
Epoch: 4, Steps: 79 | Train Loss: 0.5526823 Vali Loss: 0.3751808 Test Loss: 0.3557686
Validation loss decreased (0.551015 --> 0.375181).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 5.208239316940308
Epoch: 5, Steps: 79 | Train Loss: 0.4538118 Vali Loss: 0.3359946 Test Loss: 0.3176307
Validation loss decreased (0.375181 --> 0.335995).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 4.814218997955322
Epoch: 6, Steps: 79 | Train Loss: 0.4062348 Vali Loss: 0.3097185 Test Loss: 0.2808742
Validation loss decreased (0.335995 --> 0.309719).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 5.41948127746582
Epoch: 7, Steps: 79 | Train Loss: 0.3675056 Vali Loss: 0.2719311 Test Loss: 0.2581395
Validation loss decreased (0.309719 --> 0.271931).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 4.832690000534058
Epoch: 8, Steps: 79 | Train Loss: 0.3293993 Vali Loss: 0.2400407 Test Loss: 0.2258330
Validation loss decreased (0.271931 --> 0.240041).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 4.740936279296875
Epoch: 9, Steps: 79 | Train Loss: 0.2835057 Vali Loss: 0.1860795 Test Loss: 0.1776965
Validation loss decreased (0.240041 --> 0.186079).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 5.212255477905273
Epoch: 10, Steps: 79 | Train Loss: 0.2448862 Vali Loss: 0.1567189 Test Loss: 0.1562526
Validation loss decreased (0.186079 --> 0.156719).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 4.862020254135132
Epoch: 11, Steps: 79 | Train Loss: 0.2265120 Vali Loss: 0.1477031 Test Loss: 0.1476335
Validation loss decreased (0.156719 --> 0.147703).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 5.048464775085449
Epoch: 12, Steps: 79 | Train Loss: 0.2154048 Vali Loss: 0.1398968 Test Loss: 0.1428623
Validation loss decreased (0.147703 --> 0.139897).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 4.615761995315552
Epoch: 13, Steps: 79 | Train Loss: 0.2083628 Vali Loss: 0.1365868 Test Loss: 0.1388428
Validation loss decreased (0.139897 --> 0.136587).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 4.656078815460205
Epoch: 14, Steps: 79 | Train Loss: 0.2010505 Vali Loss: 0.1393465 Test Loss: 0.1413830
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 4.6106486320495605
Epoch: 15, Steps: 79 | Train Loss: 0.1976183 Vali Loss: 0.1313170 Test Loss: 0.1333846
Validation loss decreased (0.136587 --> 0.131317).  Saving model ...
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 4.782958030700684
Epoch: 16, Steps: 79 | Train Loss: 0.1912909 Vali Loss: 0.1294975 Test Loss: 0.1297058
Validation loss decreased (0.131317 --> 0.129497).  Saving model ...
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 5.437559366226196
Epoch: 17, Steps: 79 | Train Loss: 0.1874393 Vali Loss: 0.1268801 Test Loss: 0.1290646
Validation loss decreased (0.129497 --> 0.126880).  Saving model ...
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 4.325919151306152
Epoch: 18, Steps: 79 | Train Loss: 0.1829631 Vali Loss: 0.1271245 Test Loss: 0.1299263
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 4.923486232757568
Epoch: 19, Steps: 79 | Train Loss: 0.1787137 Vali Loss: 0.1273618 Test Loss: 0.1240531
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 4.7652428150177
Epoch: 20, Steps: 79 | Train Loss: 0.1743536 Vali Loss: 0.1224969 Test Loss: 0.1242211
Validation loss decreased (0.126880 --> 0.122497).  Saving model ...
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 5.460967063903809
Epoch: 21, Steps: 79 | Train Loss: 0.1713478 Vali Loss: 0.1269433 Test Loss: 0.1247074
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 5.064647912979126
Epoch: 22, Steps: 79 | Train Loss: 0.1685979 Vali Loss: 0.1157820 Test Loss: 0.1193577
Validation loss decreased (0.122497 --> 0.115782).  Saving model ...
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 4.482394218444824
Epoch: 23, Steps: 79 | Train Loss: 0.1656054 Vali Loss: 0.1165342 Test Loss: 0.1189314
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 5.440795660018921
Epoch: 24, Steps: 79 | Train Loss: 0.1625383 Vali Loss: 0.1137637 Test Loss: 0.1156286
Validation loss decreased (0.115782 --> 0.113764).  Saving model ...
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 4.582345485687256
Epoch: 25, Steps: 79 | Train Loss: 0.1594757 Vali Loss: 0.1225453 Test Loss: 0.1192433
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 4.7656049728393555
Epoch: 26, Steps: 79 | Train Loss: 0.1576072 Vali Loss: 0.1131268 Test Loss: 0.1154046
Validation loss decreased (0.113764 --> 0.113127).  Saving model ...
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 4.744086027145386
Epoch: 27, Steps: 79 | Train Loss: 0.1557369 Vali Loss: 0.1139968 Test Loss: 0.1137382
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 5.569436073303223
Epoch: 28, Steps: 79 | Train Loss: 0.1528481 Vali Loss: 0.1094867 Test Loss: 0.1179256
Validation loss decreased (0.113127 --> 0.109487).  Saving model ...
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 5.339023113250732
Epoch: 29, Steps: 79 | Train Loss: 0.1523090 Vali Loss: 0.1106780 Test Loss: 0.1155517
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 5.818885326385498
Epoch: 30, Steps: 79 | Train Loss: 0.1485863 Vali Loss: 0.1138890 Test Loss: 0.1085313
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 5.210673093795776
Epoch: 31, Steps: 79 | Train Loss: 0.1478391 Vali Loss: 0.1161786 Test Loss: 0.1096009
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 4.677754878997803
Epoch: 32, Steps: 79 | Train Loss: 0.1462744 Vali Loss: 0.1158179 Test Loss: 0.1099975
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 5.072237491607666
Epoch: 33, Steps: 79 | Train Loss: 0.1432376 Vali Loss: 0.1169396 Test Loss: 0.1116089
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 4.634299278259277
Epoch: 34, Steps: 79 | Train Loss: 0.1413108 Vali Loss: 0.1162206 Test Loss: 0.1088705
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 5.139261484146118
Epoch: 35, Steps: 79 | Train Loss: 0.1406052 Vali Loss: 0.1080503 Test Loss: 0.1089404
Validation loss decreased (0.109487 --> 0.108050).  Saving model ...
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 5.05661940574646
Epoch: 36, Steps: 79 | Train Loss: 0.1390534 Vali Loss: 0.1159460 Test Loss: 0.1064895
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 5.315739154815674
Epoch: 37, Steps: 79 | Train Loss: 0.1365243 Vali Loss: 0.1195498 Test Loss: 0.1039674
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 5.491465091705322
Epoch: 38, Steps: 79 | Train Loss: 0.1360091 Vali Loss: 0.1101131 Test Loss: 0.1034244
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 5.033072233200073
Epoch: 39, Steps: 79 | Train Loss: 0.1340090 Vali Loss: 0.1064473 Test Loss: 0.1022838
Validation loss decreased (0.108050 --> 0.106447).  Saving model ...
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 5.454325437545776
Epoch: 40, Steps: 79 | Train Loss: 0.1325138 Vali Loss: 0.1121389 Test Loss: 0.1038077
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 4.237983226776123
Epoch: 41, Steps: 79 | Train Loss: 0.1305742 Vali Loss: 0.1098850 Test Loss: 0.1001760
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009402
Epoch: 42 cost time: 5.2876808643341064
Epoch: 42, Steps: 79 | Train Loss: 0.1288987 Vali Loss: 0.1113695 Test Loss: 0.1012023
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009291
Epoch: 43 cost time: 4.622114896774292
Epoch: 43, Steps: 79 | Train Loss: 0.1281571 Vali Loss: 0.1168063 Test Loss: 0.1050534
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009171
Epoch: 44 cost time: 5.177251100540161
Epoch: 44, Steps: 79 | Train Loss: 0.1269442 Vali Loss: 0.1130321 Test Loss: 0.1047006
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009043
Epoch: 45 cost time: 5.222357273101807
Epoch: 45, Steps: 79 | Train Loss: 0.1263631 Vali Loss: 0.1062954 Test Loss: 0.1001101
Validation loss decreased (0.106447 --> 0.106295).  Saving model ...
Adjusting learning rate to: 0.0008907
Epoch: 46 cost time: 5.451678514480591
Epoch: 46, Steps: 79 | Train Loss: 0.1247563 Vali Loss: 0.1123968 Test Loss: 0.1025543
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008763
Epoch: 47 cost time: 5.833513021469116
Epoch: 47, Steps: 79 | Train Loss: 0.1243104 Vali Loss: 0.1156851 Test Loss: 0.1014162
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0008612
Epoch: 48 cost time: 4.382415771484375
Epoch: 48, Steps: 79 | Train Loss: 0.1227405 Vali Loss: 0.1094846 Test Loss: 0.0980135
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0008453
Epoch: 49 cost time: 5.392924070358276
Epoch: 49, Steps: 79 | Train Loss: 0.1215720 Vali Loss: 0.1053096 Test Loss: 0.0978484
Validation loss decreased (0.106295 --> 0.105310).  Saving model ...
Adjusting learning rate to: 0.0008288
Epoch: 50 cost time: 5.26192307472229
Epoch: 50, Steps: 79 | Train Loss: 0.1211968 Vali Loss: 0.1054287 Test Loss: 0.0992109
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008115
Epoch: 51 cost time: 4.727236032485962
Epoch: 51, Steps: 79 | Train Loss: 0.1204619 Vali Loss: 0.1146705 Test Loss: 0.1044438
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0007937
Epoch: 52 cost time: 5.485839605331421
Epoch: 52, Steps: 79 | Train Loss: 0.1195155 Vali Loss: 0.1079644 Test Loss: 0.0997993
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0007752
Epoch: 53 cost time: 5.0978639125823975
Epoch: 53, Steps: 79 | Train Loss: 0.1185521 Vali Loss: 0.1108069 Test Loss: 0.1006714
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0007562
Epoch: 54 cost time: 4.864208221435547
Epoch: 54, Steps: 79 | Train Loss: 0.1176994 Vali Loss: 0.1134827 Test Loss: 0.1002259
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0007367
Epoch: 55 cost time: 5.161885738372803
Epoch: 55, Steps: 79 | Train Loss: 0.1165268 Vali Loss: 0.1080353 Test Loss: 0.1016109
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0007167
Epoch: 56 cost time: 5.286581754684448
Epoch: 56, Steps: 79 | Train Loss: 0.1161372 Vali Loss: 0.1108711 Test Loss: 0.1009176
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0006963
Epoch: 57 cost time: 4.28975510597229
Epoch: 57, Steps: 79 | Train Loss: 0.1149359 Vali Loss: 0.1097321 Test Loss: 0.0997622
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0006754
Epoch: 58 cost time: 4.850932359695435
Epoch: 58, Steps: 79 | Train Loss: 0.1149579 Vali Loss: 0.1140613 Test Loss: 0.1008366
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0006542
Epoch: 59 cost time: 4.720012187957764
Epoch: 59, Steps: 79 | Train Loss: 0.1140724 Vali Loss: 0.1092237 Test Loss: 0.0990160
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0006327
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.22497619688510895, mae:0.3693285882472992, rmse:0.4743165671825409, mape:0.013212247751653194, mspe:0.00029105119756422937, rse:0.32692751288414, r2_score:0.8836995276767098, acc:0.9867877522483468
corr: [37.809475 37.77455  37.748676 37.730633 37.721634 37.688923 37.669693
 37.755356 37.72742  37.6991   37.72258  37.72527  37.6941   37.682922
 37.617413 37.635967 37.621517 37.62035  37.6066   37.61815  37.62325
 37.630722 37.59179  37.567318 37.662476 37.608555 37.563538 37.55303
 37.514263 37.48733  37.445217 37.494843 37.487823 37.473232 37.475468
 37.487793 37.480694 37.476463 37.45885  37.484867 37.471943 37.505863
 37.527775 37.539425 37.55401  37.5633   37.528847 37.526436 37.60828
 37.547604 37.481304 37.464363 37.456856 37.433834 37.38749  37.437912
 37.42442  37.419773 37.45195  37.471203 37.45337  37.456493 37.41427
 37.427547 37.3889   37.393593 37.415993 37.418842 37.403805 37.41827
 37.404354 37.401505 37.395264 37.30409  37.262222 37.22679  37.177437
 37.137955 37.09211  37.105984 37.071865 37.056183 37.05084  37.040936
 36.998158 37.004204 36.967564 36.967663 36.944836 36.91805  36.936214
 36.928844 36.891598 36.890774 36.87511  36.866226 37.084293 37.011635
 36.953075 36.917103 36.876617 36.831863 36.77445  36.80608  36.77964
 36.758286 36.75265  36.746773 36.721382 36.71981  36.706707 36.724796
 36.702015 36.697887 36.72346  36.710644 36.66228  36.659138 36.631332
 36.619247 36.786957 36.716747 36.66583  36.596165 36.528008 36.447704
 36.4199   36.474895 36.438778 36.41577  36.412342 36.403442 36.400085
 36.4075   36.378845 36.389694 36.376022 36.3979   36.440998 36.460808
 36.451057 36.461082 36.437008 36.401566 36.575356 36.518353 36.461662
 36.424133 36.380527 36.324398 36.263695 36.2854   36.252995 36.239418
 36.236122 36.242027 36.258095 36.25827  36.23434  36.25816  36.24466
 36.276096 36.31747  36.325394 36.31476  36.339737 36.356934 36.37962
 36.332996 36.31751  36.275173 36.27231  36.238865 36.2161   36.21165
 36.255314 36.242367 36.240784 36.27614  36.31291  36.338875 36.371155
 36.38893  36.43538  36.416073 36.42482  36.451656 36.463047 36.46982
 36.487946 36.46999  36.486847 36.506264 36.450035 36.406796 36.363922
 36.34698  36.308964 36.279305 36.32005  36.30543  36.29938  36.319023
 36.34387  36.34652  36.383595 36.39107  36.417507 36.43857  36.464527
 36.508717 36.51713  36.5155   36.54991  36.56243  36.58182  36.490944
 36.451187 36.411316 36.38358  36.379417 36.36954  36.36012  36.38621
 36.39338  36.42155  36.446785 36.49851  36.51038  36.52157  36.520008
 36.53669  36.529503 36.534023 36.555195 36.563686 36.55167  36.596737
 36.629562 36.635563 36.464394 36.43139  36.419765 36.40181  36.37228
 36.358124 36.33733  36.377613 36.361404 36.354916 36.356964 36.373272
 36.382782 36.38101  36.38761  36.40548  36.404533 36.415207 36.479156
 36.50308  36.486275 36.51881  36.53129  36.551044 36.38015  36.3286
 36.31318  36.310665 36.289326 36.26317  36.241993 36.290165 36.300774
 36.288692 36.292835 36.30749  36.297752 36.30347  36.29559  36.340805
 36.34472  36.36214  36.378693 36.401703 36.433872 36.478367 36.48612
 36.53074  36.59139  36.520836 36.472137 36.45177  36.41406  36.379932
 36.356438 36.364834 36.329525 36.32182  36.318836 36.305805 36.282204
 36.249203 36.21688  36.214146 36.17464  36.172916 36.20852  36.226948
 36.232193 36.267612 36.299313 36.336994 36.563236 36.49393  36.46423
 36.454334 36.432175 36.3976   36.374027 36.399105 36.357727 36.346504
 36.362137 36.32872  36.293938 36.26959  36.22272  36.201477 36.14319
 36.173298 36.194057 36.207344 36.227543 36.28379  36.308002 36.35395
 36.609085 36.562428 36.52079  36.51528  36.517914 36.505665 36.494656
 36.52367  36.508812 36.53483  36.525295 36.540455 36.53704  36.531216
 36.495758 36.513756 36.496517 36.518967 36.571426 36.605392 36.64616
 36.701775 36.727417 36.766415 37.42798  37.416336 37.413994 37.45513
 37.45682 ]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 4.6110944747924805
Epoch: 1, Steps: 79 | Train Loss: 1.0209685 Vali Loss: 0.8865803 Test Loss: 0.8092218
Validation loss decreased (inf --> 0.886580).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 5.017605781555176
Epoch: 2, Steps: 79 | Train Loss: 0.8827241 Vali Loss: 0.7672253 Test Loss: 0.6762983
Validation loss decreased (0.886580 --> 0.767225).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 4.6268768310546875
Epoch: 3, Steps: 79 | Train Loss: 0.7366167 Vali Loss: 0.5294160 Test Loss: 0.4564366
Validation loss decreased (0.767225 --> 0.529416).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 5.535292863845825
Epoch: 4, Steps: 79 | Train Loss: 0.5236726 Vali Loss: 0.3442226 Test Loss: 0.3276077
Validation loss decreased (0.529416 --> 0.344223).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 4.657641649246216
Epoch: 5, Steps: 79 | Train Loss: 0.4307439 Vali Loss: 0.3207396 Test Loss: 0.3081966
Validation loss decreased (0.344223 --> 0.320740).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 5.067225217819214
Epoch: 6, Steps: 79 | Train Loss: 0.3911762 Vali Loss: 0.2951069 Test Loss: 0.2776592
Validation loss decreased (0.320740 --> 0.295107).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 4.648145437240601
Epoch: 7, Steps: 79 | Train Loss: 0.3518180 Vali Loss: 0.2466464 Test Loss: 0.2369776
Validation loss decreased (0.295107 --> 0.246646).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 4.609041452407837
Epoch: 8, Steps: 79 | Train Loss: 0.3043731 Vali Loss: 0.2161603 Test Loss: 0.1981410
Validation loss decreased (0.246646 --> 0.216160).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 4.858302116394043
Epoch: 9, Steps: 79 | Train Loss: 0.2589698 Vali Loss: 0.1679157 Test Loss: 0.1657900
Validation loss decreased (0.216160 --> 0.167916).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 4.411499500274658
Epoch: 10, Steps: 79 | Train Loss: 0.2357664 Vali Loss: 0.1475029 Test Loss: 0.1508238
Validation loss decreased (0.167916 --> 0.147503).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 4.870525598526001
Epoch: 11, Steps: 79 | Train Loss: 0.2190189 Vali Loss: 0.1437910 Test Loss: 0.1438024
Validation loss decreased (0.147503 --> 0.143791).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 4.927228689193726
Epoch: 12, Steps: 79 | Train Loss: 0.2092922 Vali Loss: 0.1404487 Test Loss: 0.1435775
Validation loss decreased (0.143791 --> 0.140449).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 5.389288425445557
Epoch: 13, Steps: 79 | Train Loss: 0.1997770 Vali Loss: 0.1350940 Test Loss: 0.1368759
Validation loss decreased (0.140449 --> 0.135094).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 4.618137836456299
Epoch: 14, Steps: 79 | Train Loss: 0.1948655 Vali Loss: 0.1299746 Test Loss: 0.1340283
Validation loss decreased (0.135094 --> 0.129975).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 4.429378986358643
Epoch: 15, Steps: 79 | Train Loss: 0.1898851 Vali Loss: 0.1312004 Test Loss: 0.1314012
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 5.51984167098999
Epoch: 16, Steps: 79 | Train Loss: 0.1848870 Vali Loss: 0.1318651 Test Loss: 0.1307881
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 4.904563665390015
Epoch: 17, Steps: 79 | Train Loss: 0.1809329 Vali Loss: 0.1256586 Test Loss: 0.1270374
Validation loss decreased (0.129975 --> 0.125659).  Saving model ...
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 4.967579364776611
Epoch: 18, Steps: 79 | Train Loss: 0.1769851 Vali Loss: 0.1271376 Test Loss: 0.1282953
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 5.074241876602173
Epoch: 19, Steps: 79 | Train Loss: 0.1738711 Vali Loss: 0.1267005 Test Loss: 0.1244110
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 5.424320459365845
Epoch: 20, Steps: 79 | Train Loss: 0.1688295 Vali Loss: 0.1264144 Test Loss: 0.1239827
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 4.386596202850342
Epoch: 21, Steps: 79 | Train Loss: 0.1670468 Vali Loss: 0.1235557 Test Loss: 0.1214573
Validation loss decreased (0.125659 --> 0.123556).  Saving model ...
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 5.465636253356934
Epoch: 22, Steps: 79 | Train Loss: 0.1633236 Vali Loss: 0.1163903 Test Loss: 0.1183906
Validation loss decreased (0.123556 --> 0.116390).  Saving model ...
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 4.64269495010376
Epoch: 23, Steps: 79 | Train Loss: 0.1607026 Vali Loss: 0.1174858 Test Loss: 0.1232484
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 4.841164588928223
Epoch: 24, Steps: 79 | Train Loss: 0.1578374 Vali Loss: 0.1159816 Test Loss: 0.1151409
Validation loss decreased (0.116390 --> 0.115982).  Saving model ...
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 4.981860160827637
Epoch: 25, Steps: 79 | Train Loss: 0.1549086 Vali Loss: 0.1182533 Test Loss: 0.1203281
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 5.3806257247924805
Epoch: 26, Steps: 79 | Train Loss: 0.1535867 Vali Loss: 0.1104715 Test Loss: 0.1109331
Validation loss decreased (0.115982 --> 0.110471).  Saving model ...
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 5.038200855255127
Epoch: 27, Steps: 79 | Train Loss: 0.1507478 Vali Loss: 0.1135136 Test Loss: 0.1160212
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 4.996913909912109
Epoch: 28, Steps: 79 | Train Loss: 0.1480873 Vali Loss: 0.1167394 Test Loss: 0.1130824
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 4.919032096862793
Epoch: 29, Steps: 79 | Train Loss: 0.1463106 Vali Loss: 0.1151457 Test Loss: 0.1095102
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 5.42590594291687
Epoch: 30, Steps: 79 | Train Loss: 0.1437658 Vali Loss: 0.1156276 Test Loss: 0.1098415
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 4.685576677322388
Epoch: 31, Steps: 79 | Train Loss: 0.1423914 Vali Loss: 0.1102863 Test Loss: 0.1048524
Validation loss decreased (0.110471 --> 0.110286).  Saving model ...
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 4.900029897689819
Epoch: 32, Steps: 79 | Train Loss: 0.1399577 Vali Loss: 0.1114660 Test Loss: 0.1037004
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 5.209053039550781
Epoch: 33, Steps: 79 | Train Loss: 0.1385951 Vali Loss: 0.1100474 Test Loss: 0.1065199
Validation loss decreased (0.110286 --> 0.110047).  Saving model ...
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 4.458943843841553
Epoch: 34, Steps: 79 | Train Loss: 0.1360356 Vali Loss: 0.1189851 Test Loss: 0.1088069
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 4.842674016952515
Epoch: 35, Steps: 79 | Train Loss: 0.1356335 Vali Loss: 0.1162087 Test Loss: 0.1095686
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 5.594065189361572
Epoch: 36, Steps: 79 | Train Loss: 0.1334731 Vali Loss: 0.1082193 Test Loss: 0.1037752
Validation loss decreased (0.110047 --> 0.108219).  Saving model ...
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 5.693882942199707
Epoch: 37, Steps: 79 | Train Loss: 0.1319424 Vali Loss: 0.1116351 Test Loss: 0.1015951
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 4.776233196258545
Epoch: 38, Steps: 79 | Train Loss: 0.1299772 Vali Loss: 0.1071805 Test Loss: 0.1019127
Validation loss decreased (0.108219 --> 0.107180).  Saving model ...
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 4.430166721343994
Epoch: 39, Steps: 79 | Train Loss: 0.1287097 Vali Loss: 0.1072237 Test Loss: 0.0994316
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 5.544865608215332
Epoch: 40, Steps: 79 | Train Loss: 0.1284788 Vali Loss: 0.1159571 Test Loss: 0.1046010
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 5.736375570297241
Epoch: 41, Steps: 79 | Train Loss: 0.1262421 Vali Loss: 0.1083216 Test Loss: 0.1041318
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009402
Epoch: 42 cost time: 4.8570685386657715
Epoch: 42, Steps: 79 | Train Loss: 0.1249037 Vali Loss: 0.1139877 Test Loss: 0.1039614
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009291
Epoch: 43 cost time: 4.623403310775757
Epoch: 43, Steps: 79 | Train Loss: 0.1242278 Vali Loss: 0.1087204 Test Loss: 0.0983324
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009171
Epoch: 44 cost time: 5.174927473068237
Epoch: 44, Steps: 79 | Train Loss: 0.1233027 Vali Loss: 0.1111016 Test Loss: 0.1021699
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009043
Epoch: 45 cost time: 4.745417833328247
Epoch: 45, Steps: 79 | Train Loss: 0.1218789 Vali Loss: 0.1058693 Test Loss: 0.0994388
Validation loss decreased (0.107180 --> 0.105869).  Saving model ...
Adjusting learning rate to: 0.0008907
Epoch: 46 cost time: 4.90734076499939
Epoch: 46, Steps: 79 | Train Loss: 0.1212258 Vali Loss: 0.1112699 Test Loss: 0.1010854
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008763
Epoch: 47 cost time: 5.081998825073242
Epoch: 47, Steps: 79 | Train Loss: 0.1198106 Vali Loss: 0.1102384 Test Loss: 0.0995550
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0008612
Epoch: 48 cost time: 4.643700122833252
Epoch: 48, Steps: 79 | Train Loss: 0.1180869 Vali Loss: 0.1017870 Test Loss: 0.0979400
Validation loss decreased (0.105869 --> 0.101787).  Saving model ...
Adjusting learning rate to: 0.0008453
Epoch: 49 cost time: 4.8403730392456055
Epoch: 49, Steps: 79 | Train Loss: 0.1175562 Vali Loss: 0.1092845 Test Loss: 0.0975330
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008288
Epoch: 50 cost time: 4.345616340637207
Epoch: 50, Steps: 79 | Train Loss: 0.1173933 Vali Loss: 0.1068128 Test Loss: 0.0991200
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0008115
Epoch: 51 cost time: 5.218286514282227
Epoch: 51, Steps: 79 | Train Loss: 0.1156110 Vali Loss: 0.1105025 Test Loss: 0.1007174
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0007937
Epoch: 52 cost time: 4.672465085983276
Epoch: 52, Steps: 79 | Train Loss: 0.1150723 Vali Loss: 0.1140160 Test Loss: 0.1020969
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0007752
Epoch: 53 cost time: 4.790575742721558
Epoch: 53, Steps: 79 | Train Loss: 0.1136848 Vali Loss: 0.1123726 Test Loss: 0.0986553
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0007562
Epoch: 54 cost time: 5.10748291015625
Epoch: 54, Steps: 79 | Train Loss: 0.1122055 Vali Loss: 0.1082622 Test Loss: 0.0969526
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0007367
Epoch: 55 cost time: 4.629640340805054
Epoch: 55, Steps: 79 | Train Loss: 0.1118026 Vali Loss: 0.1136587 Test Loss: 0.0999657
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0007167
Epoch: 56 cost time: 4.890832185745239
Epoch: 56, Steps: 79 | Train Loss: 0.1109343 Vali Loss: 0.1066864 Test Loss: 0.0971275
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0006963
Epoch: 57 cost time: 4.383956670761108
Epoch: 57, Steps: 79 | Train Loss: 0.1100298 Vali Loss: 0.1048262 Test Loss: 0.0994235
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0006754
Epoch: 58 cost time: 5.003032684326172
Epoch: 58, Steps: 79 | Train Loss: 0.1099880 Vali Loss: 0.1068095 Test Loss: 0.0995011
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0006542
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.2251308113336563, mae:0.369992733001709, rmse:0.4744795262813568, mape:0.013211636804044247, mspe:0.0002885271096602082, rse:0.32703983783721924, r2_score:0.8854233976457638, acc:0.9867883631959558
corr: [37.92701  37.901745 37.917057 37.92733  37.932373 37.942646 37.943584
 37.98774  37.990406 37.955544 37.8647   37.793537 37.768444 37.707146
 37.686165 37.661873 37.63753  37.61797  37.596313 37.572674 37.57358
 37.571766 37.585983 37.64796  37.646465 37.628357 37.929493 37.856518
 37.84803  37.820988 37.79527  37.794064 37.775417 37.822987 37.80915
 37.763386 37.707806 37.641582 37.598732 37.524998 37.4928   37.447113
 37.399044 37.423576 37.436638 37.449154 37.444443 37.408283 37.454514
 37.57199  37.604164 37.586536 38.027462 37.92936  37.902203 37.8671
 37.83116  37.815006 37.78257  37.77408  37.73559  37.688736 37.60738
 37.51276  37.458054 37.366215 37.304947 37.256393 37.179214 37.1314
 37.104176 37.071808 37.07011  37.0183   37.020042 37.0945   37.12018
 37.08494  37.635353 37.50698  37.445007 37.399265 37.366665 37.3516
 37.3165   37.310593 37.320583 37.29554  37.213    37.119926 37.05414
 36.932613 36.895386 36.836994 36.778713 36.73351  36.689632 36.65883
 36.640324 36.611526 36.60896  36.64043  36.687397 36.687595 37.270153
 37.199898 37.187153 37.135685 37.09242  37.0776   37.03459  37.025574
 37.01033  36.974106 36.89405  36.822086 36.790306 36.692253 36.634525
 36.570126 36.489803 36.434113 36.41429  36.39576  36.367737 36.335575
 36.31932  36.371758 36.364876 36.314342 36.906467 36.802094 36.748917
 36.700977 36.66829  36.674915 36.654373 36.64686  36.617916 36.594547
 36.529903 36.450207 36.40282  36.30597  36.29058  36.250076 36.194298
 36.16421  36.179977 36.144703 36.115257 36.06194  36.058792 36.120087
 36.13365  36.082638 36.60558  36.502434 36.476246 36.44106  36.426487
 36.445343 36.44268  36.440502 36.429638 36.402954 36.341812 36.27768
 36.243637 36.1848   36.195686 36.15342  36.11439  36.11818  36.116413
 36.132908 36.126984 36.07952  36.07932  36.12245  36.164932 36.14522
 36.54524  36.468628 36.465084 36.462852 36.470947 36.468735 36.464687
 36.469555 36.44526  36.401775 36.33689  36.27388  36.254623 36.199642
 36.2027   36.182747 36.165623 36.162224 36.16137  36.15443  36.13454
 36.099525 36.084526 36.135952 36.155025 36.132572 36.503704 36.42618
 36.39217  36.36449  36.356167 36.380188 36.368595 36.364193 36.333946
 36.304867 36.24552  36.193096 36.202023 36.15798  36.142467 36.120415
 36.105377 36.104073 36.122093 36.13542  36.12917  36.111897 36.12596
 36.175198 36.193195 36.18737  36.53858  36.444973 36.424873 36.386024
 36.33592  36.32936  36.3167   36.32093  36.33483  36.32342  36.282673
 36.23321  36.200256 36.134247 36.11826  36.08592  36.037453 36.037468
 36.01588  36.022877 36.007164 35.98627  36.016933 36.108063 36.14071
 36.14276  36.589077 36.49522  36.475372 36.455406 36.47794  36.495342
 36.48152  36.476624 36.461246 36.439354 36.39574  36.333878 36.308205
 36.2517   36.2273   36.188965 36.154793 36.166817 36.145992 36.151745
 36.172462 36.14613  36.15867  36.22248  36.23852  36.2278   36.644905
 36.54377  36.50463  36.47332  36.460083 36.44391  36.40553  36.39376
 36.398808 36.38951  36.36252  36.313583 36.289085 36.219803 36.179897
 36.130585 36.076973 36.06824  36.081524 36.077946 36.092712 36.052242
 36.047787 36.13227  36.154743 36.135437 36.70802  36.637283 36.610435
 36.597664 36.574932 36.58356  36.571724 36.555088 36.560116 36.55188
 36.493313 36.41785  36.38055  36.321243 36.28899  36.257248 36.228092
 36.234455 36.261932 36.30029  36.33043  36.327354 36.384556 36.467667
 36.527996 36.547718 36.985077 36.901993 36.874123 36.85208  36.844013
 36.85414  36.84844  36.878304 36.879997 36.86951  36.836514 36.827995
 36.830173 36.78582  36.787594 36.76861  36.772293 36.83474  36.873165
 36.909004 36.971832 37.019382 37.099415 37.221966 37.299835 37.34126
 37.807625]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 4.852985858917236
Epoch: 1, Steps: 79 | Train Loss: 1.0115564 Vali Loss: 0.8656508 Test Loss: 0.7968244
Validation loss decreased (inf --> 0.865651).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 5.0789196491241455
Epoch: 2, Steps: 79 | Train Loss: 0.8798928 Vali Loss: 0.7617069 Test Loss: 0.6979055
Validation loss decreased (0.865651 --> 0.761707).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 3.609480381011963
Epoch: 3, Steps: 79 | Train Loss: 0.7538562 Vali Loss: 0.5684850 Test Loss: 0.4898399
Validation loss decreased (0.761707 --> 0.568485).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 3.753380298614502
Epoch: 4, Steps: 79 | Train Loss: 0.5605811 Vali Loss: 0.4378434 Test Loss: 0.4148781
Validation loss decreased (0.568485 --> 0.437843).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 3.6228678226470947
Epoch: 5, Steps: 79 | Train Loss: 0.4608383 Vali Loss: 0.3391310 Test Loss: 0.3241641
Validation loss decreased (0.437843 --> 0.339131).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 3.5713205337524414
Epoch: 6, Steps: 79 | Train Loss: 0.4038566 Vali Loss: 0.3072723 Test Loss: 0.2889613
Validation loss decreased (0.339131 --> 0.307272).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 3.7481689453125
Epoch: 7, Steps: 79 | Train Loss: 0.3634499 Vali Loss: 0.2744196 Test Loss: 0.2556413
Validation loss decreased (0.307272 --> 0.274420).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 3.5763988494873047
Epoch: 8, Steps: 79 | Train Loss: 0.3269752 Vali Loss: 0.2399193 Test Loss: 0.2230092
Validation loss decreased (0.274420 --> 0.239919).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 3.7709484100341797
Epoch: 9, Steps: 79 | Train Loss: 0.2849663 Vali Loss: 0.2032473 Test Loss: 0.1883515
Validation loss decreased (0.239919 --> 0.203247).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 3.732311964035034
Epoch: 10, Steps: 79 | Train Loss: 0.2508823 Vali Loss: 0.1632508 Test Loss: 0.1585298
Validation loss decreased (0.203247 --> 0.163251).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 3.783564805984497
Epoch: 11, Steps: 79 | Train Loss: 0.2298296 Vali Loss: 0.1494464 Test Loss: 0.1487850
Validation loss decreased (0.163251 --> 0.149446).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 3.874476432800293
Epoch: 12, Steps: 79 | Train Loss: 0.2162307 Vali Loss: 0.1402371 Test Loss: 0.1385321
Validation loss decreased (0.149446 --> 0.140237).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 3.710394859313965
Epoch: 13, Steps: 79 | Train Loss: 0.2059061 Vali Loss: 0.1374148 Test Loss: 0.1394183
Validation loss decreased (0.140237 --> 0.137415).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 3.7116599082946777
Epoch: 14, Steps: 79 | Train Loss: 0.1977329 Vali Loss: 0.1390570 Test Loss: 0.1362389
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 3.7662243843078613
Epoch: 15, Steps: 79 | Train Loss: 0.1940861 Vali Loss: 0.1423053 Test Loss: 0.1376079
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 3.7137463092803955
Epoch: 16, Steps: 79 | Train Loss: 0.1888462 Vali Loss: 0.1351310 Test Loss: 0.1321549
Validation loss decreased (0.137415 --> 0.135131).  Saving model ...
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 3.682002544403076
Epoch: 17, Steps: 79 | Train Loss: 0.1837365 Vali Loss: 0.1272896 Test Loss: 0.1309199
Validation loss decreased (0.135131 --> 0.127290).  Saving model ...
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 3.6745760440826416
Epoch: 18, Steps: 79 | Train Loss: 0.1788823 Vali Loss: 0.1256323 Test Loss: 0.1277075
Validation loss decreased (0.127290 --> 0.125632).  Saving model ...
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 3.6712746620178223
Epoch: 19, Steps: 79 | Train Loss: 0.1750178 Vali Loss: 0.1221995 Test Loss: 0.1241982
Validation loss decreased (0.125632 --> 0.122199).  Saving model ...
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 3.5767791271209717
Epoch: 20, Steps: 79 | Train Loss: 0.1705692 Vali Loss: 0.1263598 Test Loss: 0.1242527
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 3.7058589458465576
Epoch: 21, Steps: 79 | Train Loss: 0.1675339 Vali Loss: 0.1186022 Test Loss: 0.1243373
Validation loss decreased (0.122199 --> 0.118602).  Saving model ...
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 3.67233943939209
Epoch: 22, Steps: 79 | Train Loss: 0.1650298 Vali Loss: 0.1211459 Test Loss: 0.1169439
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 3.751600980758667
Epoch: 23, Steps: 79 | Train Loss: 0.1605827 Vali Loss: 0.1121677 Test Loss: 0.1125330
Validation loss decreased (0.118602 --> 0.112168).  Saving model ...
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 3.6418814659118652
Epoch: 24, Steps: 79 | Train Loss: 0.1579473 Vali Loss: 0.1160099 Test Loss: 0.1114274
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 3.5484442710876465
Epoch: 25, Steps: 79 | Train Loss: 0.1553150 Vali Loss: 0.1198880 Test Loss: 0.1147576
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 3.6185803413391113
Epoch: 26, Steps: 79 | Train Loss: 0.1533191 Vali Loss: 0.1161718 Test Loss: 0.1149160
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 3.649653434753418
Epoch: 27, Steps: 79 | Train Loss: 0.1517363 Vali Loss: 0.1118844 Test Loss: 0.1115840
Validation loss decreased (0.112168 --> 0.111884).  Saving model ...
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 3.778569221496582
Epoch: 28, Steps: 79 | Train Loss: 0.1477696 Vali Loss: 0.1147578 Test Loss: 0.1083359
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 3.6070051193237305
Epoch: 29, Steps: 79 | Train Loss: 0.1459347 Vali Loss: 0.1091076 Test Loss: 0.1069980
Validation loss decreased (0.111884 --> 0.109108).  Saving model ...
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 3.6775503158569336
Epoch: 30, Steps: 79 | Train Loss: 0.1450108 Vali Loss: 0.1044269 Test Loss: 0.1065201
Validation loss decreased (0.109108 --> 0.104427).  Saving model ...
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 3.6529312133789062
Epoch: 31, Steps: 79 | Train Loss: 0.1428900 Vali Loss: 0.1142490 Test Loss: 0.1075783
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 3.672276258468628
Epoch: 32, Steps: 79 | Train Loss: 0.1418079 Vali Loss: 0.1132979 Test Loss: 0.1038635
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 3.651714324951172
Epoch: 33, Steps: 79 | Train Loss: 0.1400899 Vali Loss: 0.1124102 Test Loss: 0.1023192
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 3.6880850791931152
Epoch: 34, Steps: 79 | Train Loss: 0.1393080 Vali Loss: 0.1103018 Test Loss: 0.1030891
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 3.7417290210723877
Epoch: 35, Steps: 79 | Train Loss: 0.1373704 Vali Loss: 0.1097173 Test Loss: 0.1023013
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 3.7912023067474365
Epoch: 36, Steps: 79 | Train Loss: 0.1353312 Vali Loss: 0.1090711 Test Loss: 0.1007214
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 3.6796741485595703
Epoch: 37, Steps: 79 | Train Loss: 0.1340864 Vali Loss: 0.1048565 Test Loss: 0.1031224
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 3.5751171112060547
Epoch: 38, Steps: 79 | Train Loss: 0.1320585 Vali Loss: 0.1077980 Test Loss: 0.0987982
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 3.861987829208374
Epoch: 39, Steps: 79 | Train Loss: 0.1320174 Vali Loss: 0.1051999 Test Loss: 0.0987139
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 3.627253293991089
Epoch: 40, Steps: 79 | Train Loss: 0.1303467 Vali Loss: 0.1077109 Test Loss: 0.0988791
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0009504
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.26215308904647827, mae:0.39982637763023376, rmse:0.5120089054107666, mape:0.014269912615418434, mspe:0.0003356324159540236, rse:0.35290732979774475, r2_score:0.866179505947534, acc:0.9857300873845816
corr: [38.009644 37.871346 37.882946 37.86585  37.92321  37.949352 37.892292
 37.92822  37.94286  37.96157  37.998676 38.042976 38.05959  38.06119
 38.089718 38.10181  38.078518 38.028175 37.95981  37.895607 37.833378
 37.82204  37.72527  37.73611  37.74037  37.736008 37.776093 37.84782
 38.10991  37.913044 37.886192 37.87142  37.890022 37.92791  37.905552
 37.96233  37.97015  37.97837  38.026604 38.086258 38.126797 38.14475
 38.187286 38.186188 38.185947 38.142994 38.10777  38.073    38.04296
 38.02586  37.931484 37.90599  37.90095  37.91995  37.936245 37.937817
 38.219482 38.055286 38.044533 38.048706 38.032417 37.972588 37.88885
 37.925785 37.919216 37.889557 37.881966 37.899643 37.930706 37.945152
 37.93919  37.907818 37.877193 37.83949  37.775196 37.73647  37.688194
 37.656055 37.5621   37.476192 37.459896 37.44411  37.44242  37.470654
 37.526352 37.335407 37.288548 37.242077 37.24231  37.184856 37.06179
 37.110046 37.115532 37.12878  37.1383   37.16728  37.22208  37.26348
 37.281338 37.278137 37.305637 37.30887  37.26954  37.2213   37.194862
 37.16869  37.11143  37.057728 37.059956 37.088917 37.101334 37.10125
 36.90647  36.793358 36.812973 36.7802   36.79229  36.77612  36.69985
 36.732388 36.717484 36.72162  36.739403 36.79064  36.83724  36.86798
 36.89958  36.927685 36.933277 36.890774 36.828503 36.78327  36.76817
 36.742035 36.633434 36.577312 36.603287 36.63122  36.619377 36.59932
 36.55933  36.464474 36.46853  36.43412  36.44397  36.45263  36.37559
 36.427147 36.435623 36.46354  36.491222 36.526623 36.590015 36.580856
 36.58112  36.59801  36.59492  36.52826  36.463295 36.444473 36.44404
 36.423706 36.37709  36.349945 36.388397 36.40725  36.434677 36.43537
 36.358562 36.27901  36.275066 36.261932 36.25826  36.264023 36.26266
 36.369343 36.375134 36.421627 36.469025 36.50967  36.555546 36.580574
 36.61739  36.652206 36.650696 36.616653 36.603405 36.587    36.579166
 36.57945  36.52825  36.489307 36.500923 36.502174 36.508457 36.513046
 36.27896  36.210266 36.216694 36.22374  36.269424 36.30421  36.269146
 36.31035  36.313576 36.33232  36.372463 36.419216 36.459415 36.46126
 36.540344 36.54911  36.51654  36.477848 36.426487 36.398518 36.38275
 36.390556 36.375706 36.362286 36.384594 36.3975   36.432716 36.465572
 36.27329  36.169495 36.152195 36.15648  36.19313  36.22681  36.191483
 36.24789  36.25015  36.29325  36.342148 36.381924 36.429977 36.418446
 36.46921  36.501553 36.464367 36.411915 36.3839   36.405926 36.420124
 36.427208 36.3956   36.375103 36.429367 36.478386 36.530285 36.555542
 36.217052 36.131317 36.145794 36.11447  36.115547 36.114254 36.05093
 36.080353 36.042507 36.102478 36.150543 36.203823 36.28775  36.303123
 36.354332 36.386444 36.393085 36.36835  36.36336  36.370453 36.353203
 36.366386 36.338387 36.33494  36.338753 36.33048  36.367138 36.410427
 36.18012  36.065388 36.077057 36.05802  36.08512  36.094578 36.038406
 36.082714 36.06915  36.079185 36.115963 36.148815 36.188316 36.223587
 36.25386  36.27427  36.28645  36.2785   36.255688 36.22963  36.215168
 36.257263 36.230152 36.18881  36.19131  36.236397 36.321552 36.401627
 36.199287 36.0646   36.049442 36.061077 36.088406 36.097378 36.04656
 36.11929  36.12188  36.133644 36.160275 36.199295 36.23084  36.191032
 36.2763   36.295425 36.311733 36.281948 36.21668  36.217304 36.1928
 36.202343 36.1624   36.168434 36.164806 36.20122  36.289276 36.378304
 36.663185 36.53688  36.53467  36.552605 36.568825 36.54887  36.514526
 36.550667 36.530624 36.543545 36.5792   36.630566 36.67749  36.719006
 36.7575   36.812164 36.8424   36.842846 36.810516 36.81596  36.809513
 36.826786 36.811768 36.87307  36.896767 36.963085 37.07754  37.156525
 37.28019 ]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 3.6535322666168213
Epoch: 1, Steps: 79 | Train Loss: 1.0060244 Vali Loss: 0.8560172 Test Loss: 0.7980161
Validation loss decreased (inf --> 0.856017).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 3.694991111755371
Epoch: 2, Steps: 79 | Train Loss: 0.8690854 Vali Loss: 0.7631201 Test Loss: 0.6793881
Validation loss decreased (0.856017 --> 0.763120).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 3.779003143310547
Epoch: 3, Steps: 79 | Train Loss: 0.7087987 Vali Loss: 0.5318861 Test Loss: 0.4734197
Validation loss decreased (0.763120 --> 0.531886).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 3.733945369720459
Epoch: 4, Steps: 79 | Train Loss: 0.5550395 Vali Loss: 0.4445405 Test Loss: 0.4132890
Validation loss decreased (0.531886 --> 0.444541).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 3.73404860496521
Epoch: 5, Steps: 79 | Train Loss: 0.4712104 Vali Loss: 0.3675269 Test Loss: 0.3482932
Validation loss decreased (0.444541 --> 0.367527).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 3.577922821044922
Epoch: 6, Steps: 79 | Train Loss: 0.4091037 Vali Loss: 0.3195997 Test Loss: 0.2899073
Validation loss decreased (0.367527 --> 0.319600).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 3.857928514480591
Epoch: 7, Steps: 79 | Train Loss: 0.3662410 Vali Loss: 0.2877446 Test Loss: 0.2692948
Validation loss decreased (0.319600 --> 0.287745).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 3.5999813079833984
Epoch: 8, Steps: 79 | Train Loss: 0.3406315 Vali Loss: 0.2590287 Test Loss: 0.2387346
Validation loss decreased (0.287745 --> 0.259029).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 3.4868760108947754
Epoch: 9, Steps: 79 | Train Loss: 0.3068326 Vali Loss: 0.2263564 Test Loss: 0.2046082
Validation loss decreased (0.259029 --> 0.226356).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 3.742532730102539
Epoch: 10, Steps: 79 | Train Loss: 0.2675483 Vali Loss: 0.1910732 Test Loss: 0.1718089
Validation loss decreased (0.226356 --> 0.191073).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 3.5359649658203125
Epoch: 11, Steps: 79 | Train Loss: 0.2367901 Vali Loss: 0.1506148 Test Loss: 0.1524831
Validation loss decreased (0.191073 --> 0.150615).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 3.607264757156372
Epoch: 12, Steps: 79 | Train Loss: 0.2233833 Vali Loss: 0.1494029 Test Loss: 0.1459138
Validation loss decreased (0.150615 --> 0.149403).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 3.6263489723205566
Epoch: 13, Steps: 79 | Train Loss: 0.2140882 Vali Loss: 0.1416287 Test Loss: 0.1425216
Validation loss decreased (0.149403 --> 0.141629).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 3.6061172485351562
Epoch: 14, Steps: 79 | Train Loss: 0.2061782 Vali Loss: 0.1381390 Test Loss: 0.1387940
Validation loss decreased (0.141629 --> 0.138139).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 3.776782989501953
Epoch: 15, Steps: 79 | Train Loss: 0.1995570 Vali Loss: 0.1394081 Test Loss: 0.1341576
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 3.570150852203369
Epoch: 16, Steps: 79 | Train Loss: 0.1937848 Vali Loss: 0.1451044 Test Loss: 0.1357850
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 3.5933921337127686
Epoch: 17, Steps: 79 | Train Loss: 0.1889271 Vali Loss: 0.1302916 Test Loss: 0.1315180
Validation loss decreased (0.138139 --> 0.130292).  Saving model ...
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 3.645742893218994
Epoch: 18, Steps: 79 | Train Loss: 0.1855840 Vali Loss: 0.1423390 Test Loss: 0.1300515
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 3.661088228225708
Epoch: 19, Steps: 79 | Train Loss: 0.1809999 Vali Loss: 0.1316680 Test Loss: 0.1289070
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 3.5572965145111084
Epoch: 20, Steps: 79 | Train Loss: 0.1767992 Vali Loss: 0.1244083 Test Loss: 0.1245858
Validation loss decreased (0.130292 --> 0.124408).  Saving model ...
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 3.61722731590271
Epoch: 21, Steps: 79 | Train Loss: 0.1741224 Vali Loss: 0.1220957 Test Loss: 0.1231582
Validation loss decreased (0.124408 --> 0.122096).  Saving model ...
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 3.671268939971924
Epoch: 22, Steps: 79 | Train Loss: 0.1696120 Vali Loss: 0.1221418 Test Loss: 0.1231542
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 3.672854423522949
Epoch: 23, Steps: 79 | Train Loss: 0.1660848 Vali Loss: 0.1146980 Test Loss: 0.1261418
Validation loss decreased (0.122096 --> 0.114698).  Saving model ...
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 3.5981979370117188
Epoch: 24, Steps: 79 | Train Loss: 0.1638270 Vali Loss: 0.1163526 Test Loss: 0.1202387
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 3.7519404888153076
Epoch: 25, Steps: 79 | Train Loss: 0.1612322 Vali Loss: 0.1184012 Test Loss: 0.1226978
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 3.5724334716796875
Epoch: 26, Steps: 79 | Train Loss: 0.1589764 Vali Loss: 0.1172600 Test Loss: 0.1205812
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 3.7485909461975098
Epoch: 27, Steps: 79 | Train Loss: 0.1564453 Vali Loss: 0.1191429 Test Loss: 0.1157854
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 3.783888578414917
Epoch: 28, Steps: 79 | Train Loss: 0.1526221 Vali Loss: 0.1083320 Test Loss: 0.1119215
Validation loss decreased (0.114698 --> 0.108332).  Saving model ...
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 3.6604678630828857
Epoch: 29, Steps: 79 | Train Loss: 0.1514926 Vali Loss: 0.1151246 Test Loss: 0.1128734
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 3.673872947692871
Epoch: 30, Steps: 79 | Train Loss: 0.1484929 Vali Loss: 0.1124185 Test Loss: 0.1114868
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 3.696674108505249
Epoch: 31, Steps: 79 | Train Loss: 0.1464302 Vali Loss: 0.1134970 Test Loss: 0.1107334
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 3.6813290119171143
Epoch: 32, Steps: 79 | Train Loss: 0.1451406 Vali Loss: 0.1108700 Test Loss: 0.1086132
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 3.6572797298431396
Epoch: 33, Steps: 79 | Train Loss: 0.1435384 Vali Loss: 0.1123408 Test Loss: 0.1059809
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 3.6241798400878906
Epoch: 34, Steps: 79 | Train Loss: 0.1417829 Vali Loss: 0.1145341 Test Loss: 0.1068961
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 3.6538755893707275
Epoch: 35, Steps: 79 | Train Loss: 0.1412947 Vali Loss: 0.1097893 Test Loss: 0.1047093
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 3.6788206100463867
Epoch: 36, Steps: 79 | Train Loss: 0.1389986 Vali Loss: 0.1090683 Test Loss: 0.1029377
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 3.5259149074554443
Epoch: 37, Steps: 79 | Train Loss: 0.1371544 Vali Loss: 0.1095646 Test Loss: 0.1098405
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 3.5643227100372314
Epoch: 38, Steps: 79 | Train Loss: 0.1354423 Vali Loss: 0.1119871 Test Loss: 0.1064821
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0009680
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.2800340950489044, mae:0.4138067960739136, rmse:0.529182493686676, mape:0.0148040521889925, mspe:0.0003616743488237262, rse:0.3647443950176239, r2_score:0.8574037584711964, acc:0.9851959478110075
corr: [38.039787 37.975815 37.980732 38.064217 38.099953 38.122906 38.11918
 38.112034 38.107784 38.118145 38.182583 38.22344  38.223316 38.17663
 38.176155 38.177216 38.179024 38.159794 38.171528 38.16321  38.19018
 38.257854 38.224606 38.2303   38.22469  38.31241  38.372272 38.412815
 38.454296 38.47627  37.93015  37.91601  37.92909  38.006783 38.02334
 38.062668 38.05204  38.056168 38.027542 38.023006 38.076405 38.11659
 38.10776  38.107437 38.086433 38.119534 38.158356 38.168743 38.163513
 38.123665 38.122124 38.175755 38.144924 38.167404 38.18559  38.208347
 38.240353 38.25618  38.278    38.23309  37.74028  37.702778 37.708794
 37.74285  37.772728 37.7767   37.742954 37.692654 37.642117 37.585648
 37.679497 37.70185  37.68417  37.694695 37.626312 37.66692  37.67405
 37.648495 37.62797  37.558254 37.484936 37.466152 37.41621  37.421486
 37.434216 37.45397  37.50127  37.531437 37.5781   37.60236  36.93828
 36.949993 36.94506  36.962025 37.012203 37.04609  37.08863  37.089294
 37.088764 37.06116  37.12831  37.147934 37.182373 37.206673 37.159702
 37.235207 37.256596 37.254925 37.24497  37.21186  37.111996 37.13712
 37.077003 37.063053 37.085297 37.115723 37.14163  37.185276 37.219067
 37.24364  36.687542 36.678165 36.67624  36.691216 36.731583 36.78372
 36.795757 36.802666 36.81053  36.779053 36.82951  36.85366  36.8272
 36.7989   36.748905 36.793167 36.80587  36.797844 36.820168 36.810547
 36.78872  36.82989  36.77994  36.777313 36.825035 36.85242  36.882645
 36.89898  36.96054  36.944183 36.293423 36.29972  36.3526   36.412186
 36.508167 36.571808 36.60406  36.626537 36.622356 36.601242 36.707867
 36.746693 36.746788 36.768017 36.774845 36.82495  36.825417 36.837193
 36.852543 36.84612  36.820065 36.87871  36.880737 36.901134 36.89465
 36.958942 37.020824 37.014427 37.080875 37.08062  36.630383 36.63323
 36.636787 36.67206  36.747234 36.804462 36.834785 36.851654 36.862442
 36.878956 36.947647 36.972744 36.955994 36.977413 36.992    37.006294
 37.016594 36.999775 36.993473 36.97974  36.939053 36.956444 36.91612
 36.873283 36.883606 36.916126 36.930656 36.917034 36.971375 36.95337
 36.663864 36.641537 36.651608 36.6592   36.6952   36.700787 36.741608
 36.785576 36.81498  36.81446  36.918083 36.99009  36.97964  37.019043
 37.02202  37.051582 37.031887 36.992256 36.97376  36.963375 36.93715
 36.981056 36.956432 36.946777 36.961266 36.966846 36.97298  36.94619
 36.973248 36.92462  36.497982 36.489758 36.50691  36.544907 36.616768
 36.6707   36.69541  36.695396 36.73902  36.726273 36.82254  36.85262
 36.848503 36.842663 36.808556 36.855362 36.851486 36.8309   36.818687
 36.796722 36.7599   36.823383 36.803646 36.80577  36.79777  36.82034
 36.898968 36.91846  36.956715 36.92345  36.635536 36.609955 36.590202
 36.598663 36.680996 36.73771  36.782417 36.73973  36.75336  36.724976
 36.813892 36.84946  36.84181  36.818943 36.821167 36.82499  36.78012
 36.770348 36.724277 36.677273 36.666985 36.695087 36.70421  36.70656
 36.71281  36.760025 36.832817 36.840397 36.89889  36.883144 36.4733
 36.465813 36.48335  36.49934  36.587334 36.67283  36.684494 36.668396
 36.68282  36.66792  36.764587 36.79431  36.805733 36.743675 36.752434
 36.783936 36.734432 36.697872 36.652515 36.619583 36.617146 36.652355
 36.652344 36.66078  36.71857  36.735687 36.81163  36.82288  36.893227
 36.84898  36.662056 36.6489   36.696056 36.7706   36.84058  36.900402
 36.92423  36.950256 36.950504 36.956116 37.05471  37.06737  37.056667
 37.05177  37.093197 37.095665 37.149284 37.182865 37.21473  37.24192
 37.27753  37.354168 37.363586 37.374172 37.41258  37.524094 37.64546
 37.658596 37.738632 37.75929  37.513046 37.534832 37.589397 37.72846
 37.790005]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 3.607259511947632
Epoch: 1, Steps: 79 | Train Loss: 1.0060174 Vali Loss: 0.8553032 Test Loss: 0.7895296
Validation loss decreased (inf --> 0.855303).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 3.667248487472534
Epoch: 2, Steps: 79 | Train Loss: 0.8599445 Vali Loss: 0.7204694 Test Loss: 0.6640494
Validation loss decreased (0.855303 --> 0.720469).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 3.6600348949432373
Epoch: 3, Steps: 79 | Train Loss: 0.7300209 Vali Loss: 0.5707229 Test Loss: 0.4993414
Validation loss decreased (0.720469 --> 0.570723).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 3.59189510345459
Epoch: 4, Steps: 79 | Train Loss: 0.5352689 Vali Loss: 0.3416167 Test Loss: 0.3219858
Validation loss decreased (0.570723 --> 0.341617).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 3.5685319900512695
Epoch: 5, Steps: 79 | Train Loss: 0.4266155 Vali Loss: 0.3179606 Test Loss: 0.2995785
Validation loss decreased (0.341617 --> 0.317961).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 3.5720598697662354
Epoch: 6, Steps: 79 | Train Loss: 0.3797072 Vali Loss: 0.2909685 Test Loss: 0.2687937
Validation loss decreased (0.317961 --> 0.290968).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 3.534630298614502
Epoch: 7, Steps: 79 | Train Loss: 0.3478314 Vali Loss: 0.2557267 Test Loss: 0.2338113
Validation loss decreased (0.290968 --> 0.255727).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 3.660620927810669
Epoch: 8, Steps: 79 | Train Loss: 0.3113444 Vali Loss: 0.2261771 Test Loss: 0.2059775
Validation loss decreased (0.255727 --> 0.226177).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 3.7606284618377686
Epoch: 9, Steps: 79 | Train Loss: 0.2705685 Vali Loss: 0.1879313 Test Loss: 0.1715557
Validation loss decreased (0.226177 --> 0.187931).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 3.643174648284912
Epoch: 10, Steps: 79 | Train Loss: 0.2415819 Vali Loss: 0.1665121 Test Loss: 0.1515164
Validation loss decreased (0.187931 --> 0.166512).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 3.779158592224121
Epoch: 11, Steps: 79 | Train Loss: 0.2263107 Vali Loss: 0.1506539 Test Loss: 0.1420578
Validation loss decreased (0.166512 --> 0.150654).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 3.7022182941436768
Epoch: 12, Steps: 79 | Train Loss: 0.2145170 Vali Loss: 0.1390725 Test Loss: 0.1378516
Validation loss decreased (0.150654 --> 0.139072).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 3.523876190185547
Epoch: 13, Steps: 79 | Train Loss: 0.2051850 Vali Loss: 0.1363743 Test Loss: 0.1346715
Validation loss decreased (0.139072 --> 0.136374).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 3.5405125617980957
Epoch: 14, Steps: 79 | Train Loss: 0.1964794 Vali Loss: 0.1357707 Test Loss: 0.1345831
Validation loss decreased (0.136374 --> 0.135771).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 3.460129737854004
Epoch: 15, Steps: 79 | Train Loss: 0.1915784 Vali Loss: 0.1330706 Test Loss: 0.1290525
Validation loss decreased (0.135771 --> 0.133071).  Saving model ...
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 3.681084156036377
Epoch: 16, Steps: 79 | Train Loss: 0.1842388 Vali Loss: 0.1232085 Test Loss: 0.1266528
Validation loss decreased (0.133071 --> 0.123209).  Saving model ...
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 3.6067068576812744
Epoch: 17, Steps: 79 | Train Loss: 0.1805436 Vali Loss: 0.1289807 Test Loss: 0.1265135
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 3.6544315814971924
Epoch: 18, Steps: 79 | Train Loss: 0.1752447 Vali Loss: 0.1188573 Test Loss: 0.1251948
Validation loss decreased (0.123209 --> 0.118857).  Saving model ...
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 3.5969462394714355
Epoch: 19, Steps: 79 | Train Loss: 0.1723214 Vali Loss: 0.1192800 Test Loss: 0.1213639
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 3.650378465652466
Epoch: 20, Steps: 79 | Train Loss: 0.1679254 Vali Loss: 0.1239435 Test Loss: 0.1176502
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 3.537846803665161
Epoch: 21, Steps: 79 | Train Loss: 0.1643614 Vali Loss: 0.1145569 Test Loss: 0.1169763
Validation loss decreased (0.118857 --> 0.114557).  Saving model ...
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 3.631843328475952
Epoch: 22, Steps: 79 | Train Loss: 0.1616820 Vali Loss: 0.1159954 Test Loss: 0.1177632
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 3.4956583976745605
Epoch: 23, Steps: 79 | Train Loss: 0.1579785 Vali Loss: 0.1170961 Test Loss: 0.1101758
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 3.615684986114502
Epoch: 24, Steps: 79 | Train Loss: 0.1552331 Vali Loss: 0.1141084 Test Loss: 0.1143786
Validation loss decreased (0.114557 --> 0.114108).  Saving model ...
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 3.6103663444519043
Epoch: 25, Steps: 79 | Train Loss: 0.1529465 Vali Loss: 0.1106269 Test Loss: 0.1105427
Validation loss decreased (0.114108 --> 0.110627).  Saving model ...
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 3.708822727203369
Epoch: 26, Steps: 79 | Train Loss: 0.1506631 Vali Loss: 0.1112281 Test Loss: 0.1080670
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 3.5292959213256836
Epoch: 27, Steps: 79 | Train Loss: 0.1491097 Vali Loss: 0.1140519 Test Loss: 0.1129359
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 3.6314659118652344
Epoch: 28, Steps: 79 | Train Loss: 0.1472160 Vali Loss: 0.1219294 Test Loss: 0.1085946
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 3.640873908996582
Epoch: 29, Steps: 79 | Train Loss: 0.1439433 Vali Loss: 0.1071141 Test Loss: 0.1033511
Validation loss decreased (0.110627 --> 0.107114).  Saving model ...
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 3.692760944366455
Epoch: 30, Steps: 79 | Train Loss: 0.1434379 Vali Loss: 0.1063340 Test Loss: 0.1040502
Validation loss decreased (0.107114 --> 0.106334).  Saving model ...
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 3.6332106590270996
Epoch: 31, Steps: 79 | Train Loss: 0.1406066 Vali Loss: 0.1171696 Test Loss: 0.1047960
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 3.6192283630371094
Epoch: 32, Steps: 79 | Train Loss: 0.1392889 Vali Loss: 0.1073209 Test Loss: 0.1061902
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 3.508002519607544
Epoch: 33, Steps: 79 | Train Loss: 0.1373228 Vali Loss: 0.1149116 Test Loss: 0.1019824
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 3.6229991912841797
Epoch: 34, Steps: 79 | Train Loss: 0.1365473 Vali Loss: 0.1065070 Test Loss: 0.1008671
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 3.6027443408966064
Epoch: 35, Steps: 79 | Train Loss: 0.1339096 Vali Loss: 0.1074842 Test Loss: 0.0997530
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 3.7557666301727295
Epoch: 36, Steps: 79 | Train Loss: 0.1327911 Vali Loss: 0.1035731 Test Loss: 0.0991818
Validation loss decreased (0.106334 --> 0.103573).  Saving model ...
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 3.554567575454712
Epoch: 37, Steps: 79 | Train Loss: 0.1316533 Vali Loss: 0.1017589 Test Loss: 0.0991161
Validation loss decreased (0.103573 --> 0.101759).  Saving model ...
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 3.570835590362549
Epoch: 38, Steps: 79 | Train Loss: 0.1308207 Vali Loss: 0.1056701 Test Loss: 0.0989659
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 3.59523868560791
Epoch: 39, Steps: 79 | Train Loss: 0.1284915 Vali Loss: 0.1115589 Test Loss: 0.0975413
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 3.555739641189575
Epoch: 40, Steps: 79 | Train Loss: 0.1280017 Vali Loss: 0.1052811 Test Loss: 0.0975176
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 3.57417368888855
Epoch: 41, Steps: 79 | Train Loss: 0.1261884 Vali Loss: 0.1087854 Test Loss: 0.1006882
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009402
Epoch: 42 cost time: 3.5026650428771973
Epoch: 42, Steps: 79 | Train Loss: 0.1252743 Vali Loss: 0.1028582 Test Loss: 0.1035570
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009291
Epoch: 43 cost time: 3.6614017486572266
Epoch: 43, Steps: 79 | Train Loss: 0.1247716 Vali Loss: 0.1064578 Test Loss: 0.0976108
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009171
Epoch: 44 cost time: 3.6114048957824707
Epoch: 44, Steps: 79 | Train Loss: 0.1242449 Vali Loss: 0.1047376 Test Loss: 0.1009441
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0009043
Epoch: 45 cost time: 3.6891543865203857
Epoch: 45, Steps: 79 | Train Loss: 0.1222747 Vali Loss: 0.1142797 Test Loss: 0.1003521
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0008907
Epoch: 46 cost time: 3.677635669708252
Epoch: 46, Steps: 79 | Train Loss: 0.1221779 Vali Loss: 0.1038717 Test Loss: 0.0981910
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0008763
Epoch: 47 cost time: 3.5639147758483887
Epoch: 47, Steps: 79 | Train Loss: 0.1205453 Vali Loss: 0.1121245 Test Loss: 0.0992421
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0008612
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.24626536667346954, mae:0.3881900906562805, rmse:0.49625131487846375, mape:0.013880075886845589, mspe:0.00031701140687800944, rse:0.34204626083374023, r2_score:0.8645836423374138, acc:0.9861199241131544
corr: [38.417084 38.411682 38.400803 38.338806 38.323536 38.253624 38.130417
 38.055386 37.98991  37.934048 37.814774 37.73364  37.72931  37.72496
 37.620235 37.59095  37.577595 37.647537 37.68078  37.703857 37.746998
 37.813248 37.74397  37.71453  37.661316 37.627472 37.709034 37.700336
 37.706978 37.71493  37.72487  37.82232  37.833553 38.32587  38.268444
 38.227455 38.14629  38.086178 38.0233   37.929935 37.888218 37.85461
 37.824905 37.750248 37.709713 37.68357  37.66446  37.575886 37.58082
 37.593243 37.60699  37.61522  37.605812 37.601387 37.63894  37.606876
 37.585823 37.52929  37.502262 37.566563 37.592304 37.608913 37.624077
 37.621277 37.64571  37.630177 38.026012 37.971188 37.90397  37.790302
 37.703796 37.625317 37.529346 37.49282  37.45254  37.415066 37.3085
 37.240475 37.218204 37.19782  37.102966 37.09354  37.064297 37.05598
 37.036724 37.014435 37.043514 37.05879  36.996487 36.97999  36.94979
 36.921352 36.976906 36.95837  36.980804 36.983265 36.9642   36.998253
 37.02625  37.486324 37.434616 37.37346  37.290356 37.21307  37.14334
 37.089516 37.092033 37.09835  37.07507  37.013203 36.983353 36.972645
 36.97674  36.90366  36.89938  36.877804 36.87602  36.8585   36.83564
 36.815998 36.84166  36.770966 36.7272   36.663227 36.63892  36.679962
 36.664387 36.687084 36.655872 36.61368  36.621994 36.596382 37.116035
 37.038948 36.979465 36.878353 36.822334 36.78565  36.68933  36.64231
 36.627995 36.58535  36.53288  36.481266 36.496975 36.514595 36.464054
 36.465736 36.46831  36.49276  36.493347 36.45718  36.44247  36.445877
 36.395332 36.333313 36.271336 36.225147 36.221745 36.20086  36.181545
 36.16311  36.119514 36.160664 36.14761  36.61971  36.58422  36.56299
 36.520664 36.48897  36.46189  36.39697  36.389122 36.41716  36.411674
 36.389996 36.399406 36.449383 36.466297 36.431408 36.440014 36.446518
 36.49532  36.50048  36.46697  36.467316 36.49488  36.449753 36.419754
 36.347527 36.290928 36.295277 36.294605 36.318462 36.338627 36.306538
 36.346863 36.351112 36.735447 36.692757 36.65686  36.6127   36.58661
 36.549618 36.510277 36.504684 36.51282  36.496338 36.48208  36.47741
 36.486115 36.49841  36.460228 36.468784 36.45402  36.47898  36.488052
 36.450516 36.44173  36.428036 36.368813 36.325054 36.282646 36.28404
 36.324696 36.34616  36.377365 36.395226 36.361397 36.389324 36.386654
 36.4594   36.421497 36.39895  36.324974 36.293118 36.28252  36.246872
 36.23483  36.226013 36.238285 36.229427 36.227554 36.257553 36.315155
 36.3179   36.342842 36.332325 36.359314 36.400173 36.397827 36.387188
 36.39469  36.343853 36.30174  36.265007 36.241108 36.29626  36.292458
 36.3082   36.355286 36.356228 36.410236 36.42968  36.57851  36.536076
 36.518402 36.44894  36.405907 36.389862 36.350735 36.33723  36.334732
 36.334724 36.307278 36.315308 36.36493  36.387352 36.364834 36.399673
 36.3938   36.416725 36.46362  36.46923  36.51175  36.546146 36.51368
 36.47806  36.4176   36.397907 36.46119  36.456318 36.462345 36.474464
 36.479042 36.56836  36.601917 36.640507 36.60144  36.59651  36.560192
 36.558853 36.533813 36.481228 36.45909  36.44014  36.431942 36.38914
 36.356674 36.38171  36.39258  36.3588   36.373684 36.379208 36.45186
 36.521923 36.518883 36.553932 36.59606  36.559143 36.527115 36.46933
 36.431915 36.45304  36.424206 36.451233 36.477364 36.473614 36.541164
 36.57119  36.785137 36.744442 36.735134 36.69903  36.663254 36.60686
 36.556393 36.521523 36.52302  36.532284 36.530624 36.543674 36.598442
 36.643147 36.621574 36.629356 36.627792 36.71493  36.77883  36.78332
 36.782066 36.852737 36.781986 36.77701  36.776215 36.740215 36.83573
 36.860737 36.9262   37.03336  37.0944   37.22537  37.325405 38.035267
 38.0338  ]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 3.661189317703247
Epoch: 1, Steps: 79 | Train Loss: 1.0072118 Vali Loss: 0.8636544 Test Loss: 0.7954751
Validation loss decreased (inf --> 0.863654).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 3.777203321456909
Epoch: 2, Steps: 79 | Train Loss: 0.8753885 Vali Loss: 0.7607917 Test Loss: 0.6870545
Validation loss decreased (0.863654 --> 0.760792).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 3.46095609664917
Epoch: 3, Steps: 79 | Train Loss: 0.7481364 Vali Loss: 0.5642887 Test Loss: 0.5021803
Validation loss decreased (0.760792 --> 0.564289).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 3.5880441665649414
Epoch: 4, Steps: 79 | Train Loss: 0.5571949 Vali Loss: 0.3988909 Test Loss: 0.3679789
Validation loss decreased (0.564289 --> 0.398891).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 3.5501370429992676
Epoch: 5, Steps: 79 | Train Loss: 0.4539592 Vali Loss: 0.3541054 Test Loss: 0.3296855
Validation loss decreased (0.398891 --> 0.354105).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 3.551797866821289
Epoch: 6, Steps: 79 | Train Loss: 0.4033698 Vali Loss: 0.3003025 Test Loss: 0.2860371
Validation loss decreased (0.354105 --> 0.300302).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 3.637720823287964
Epoch: 7, Steps: 79 | Train Loss: 0.3649682 Vali Loss: 0.2676761 Test Loss: 0.2477783
Validation loss decreased (0.300302 --> 0.267676).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 3.703421115875244
Epoch: 8, Steps: 79 | Train Loss: 0.3325376 Vali Loss: 0.2455295 Test Loss: 0.2252311
Validation loss decreased (0.267676 --> 0.245529).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 3.5587778091430664
Epoch: 9, Steps: 79 | Train Loss: 0.2993151 Vali Loss: 0.2129051 Test Loss: 0.1934766
Validation loss decreased (0.245529 --> 0.212905).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 3.5720407962799072
Epoch: 10, Steps: 79 | Train Loss: 0.2621345 Vali Loss: 0.1768477 Test Loss: 0.1691696
Validation loss decreased (0.212905 --> 0.176848).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 3.466930866241455
Epoch: 11, Steps: 79 | Train Loss: 0.2403885 Vali Loss: 0.1672096 Test Loss: 0.1596014
Validation loss decreased (0.176848 --> 0.167210).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 3.624915599822998
Epoch: 12, Steps: 79 | Train Loss: 0.2243423 Vali Loss: 0.1586871 Test Loss: 0.1515769
Validation loss decreased (0.167210 --> 0.158687).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 3.498771905899048
Epoch: 13, Steps: 79 | Train Loss: 0.2143455 Vali Loss: 0.1428384 Test Loss: 0.1450224
Validation loss decreased (0.158687 --> 0.142838).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 3.4820339679718018
Epoch: 14, Steps: 79 | Train Loss: 0.2054903 Vali Loss: 0.1414771 Test Loss: 0.1393783
Validation loss decreased (0.142838 --> 0.141477).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 3.5094876289367676
Epoch: 15, Steps: 79 | Train Loss: 0.1951978 Vali Loss: 0.1303229 Test Loss: 0.1347790
Validation loss decreased (0.141477 --> 0.130323).  Saving model ...
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 3.5183193683624268
Epoch: 16, Steps: 79 | Train Loss: 0.1906632 Vali Loss: 0.1315455 Test Loss: 0.1300372
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 3.532557964324951
Epoch: 17, Steps: 79 | Train Loss: 0.1848980 Vali Loss: 0.1269494 Test Loss: 0.1280559
Validation loss decreased (0.130323 --> 0.126949).  Saving model ...
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 3.57596755027771
Epoch: 18, Steps: 79 | Train Loss: 0.1796955 Vali Loss: 0.1232611 Test Loss: 0.1262838
Validation loss decreased (0.126949 --> 0.123261).  Saving model ...
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 3.521991729736328
Epoch: 19, Steps: 79 | Train Loss: 0.1769341 Vali Loss: 0.1265910 Test Loss: 0.1239887
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 3.49369740486145
Epoch: 20, Steps: 79 | Train Loss: 0.1729521 Vali Loss: 0.1158249 Test Loss: 0.1192751
Validation loss decreased (0.123261 --> 0.115825).  Saving model ...
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 3.7091262340545654
Epoch: 21, Steps: 79 | Train Loss: 0.1699153 Vali Loss: 0.1164907 Test Loss: 0.1182209
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 3.5297188758850098
Epoch: 22, Steps: 79 | Train Loss: 0.1650155 Vali Loss: 0.1159121 Test Loss: 0.1176785
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 3.4606666564941406
Epoch: 23, Steps: 79 | Train Loss: 0.1628906 Vali Loss: 0.1159098 Test Loss: 0.1152021
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 3.6220033168792725
Epoch: 24, Steps: 79 | Train Loss: 0.1588021 Vali Loss: 0.1108411 Test Loss: 0.1149886
Validation loss decreased (0.115825 --> 0.110841).  Saving model ...
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 3.524930477142334
Epoch: 25, Steps: 79 | Train Loss: 0.1574655 Vali Loss: 0.1207099 Test Loss: 0.1156095
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 3.626845359802246
Epoch: 26, Steps: 79 | Train Loss: 0.1531952 Vali Loss: 0.1050414 Test Loss: 0.1069230
Validation loss decreased (0.110841 --> 0.105041).  Saving model ...
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 3.589995861053467
Epoch: 27, Steps: 79 | Train Loss: 0.1512919 Vali Loss: 0.1065737 Test Loss: 0.1106059
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 3.443720817565918
Epoch: 28, Steps: 79 | Train Loss: 0.1492666 Vali Loss: 0.1053672 Test Loss: 0.1071298
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 3.609127998352051
Epoch: 29, Steps: 79 | Train Loss: 0.1475714 Vali Loss: 0.1127426 Test Loss: 0.1065307
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 3.4794185161590576
Epoch: 30, Steps: 79 | Train Loss: 0.1457893 Vali Loss: 0.1089235 Test Loss: 0.1068785
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 3.5934500694274902
Epoch: 31, Steps: 79 | Train Loss: 0.1440243 Vali Loss: 0.1075886 Test Loss: 0.1039622
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 3.586388111114502
Epoch: 32, Steps: 79 | Train Loss: 0.1429742 Vali Loss: 0.1084570 Test Loss: 0.1040881
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 3.5833170413970947
Epoch: 33, Steps: 79 | Train Loss: 0.1410194 Vali Loss: 0.1026756 Test Loss: 0.1063396
Validation loss decreased (0.105041 --> 0.102676).  Saving model ...
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 3.5473275184631348
Epoch: 34, Steps: 79 | Train Loss: 0.1394092 Vali Loss: 0.1052314 Test Loss: 0.1031202
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 3.831869602203369
Epoch: 35, Steps: 79 | Train Loss: 0.1372668 Vali Loss: 0.1037781 Test Loss: 0.1028001
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 3.5006356239318848
Epoch: 36, Steps: 79 | Train Loss: 0.1363780 Vali Loss: 0.1069294 Test Loss: 0.1034769
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 3.6279962062835693
Epoch: 37, Steps: 79 | Train Loss: 0.1345248 Vali Loss: 0.1027135 Test Loss: 0.1049406
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 3.6309916973114014
Epoch: 38, Steps: 79 | Train Loss: 0.1337869 Vali Loss: 0.1065771 Test Loss: 0.1042907
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 3.517853260040283
Epoch: 39, Steps: 79 | Train Loss: 0.1323868 Vali Loss: 0.1000464 Test Loss: 0.1041900
Validation loss decreased (0.102676 --> 0.100046).  Saving model ...
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 3.4933156967163086
Epoch: 40, Steps: 79 | Train Loss: 0.1321133 Vali Loss: 0.1099707 Test Loss: 0.1024498
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 3.5555708408355713
Epoch: 41, Steps: 79 | Train Loss: 0.1301437 Vali Loss: 0.1093695 Test Loss: 0.1009884
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009402
Epoch: 42 cost time: 3.7410550117492676
Epoch: 42, Steps: 79 | Train Loss: 0.1293836 Vali Loss: 0.1085908 Test Loss: 0.1015800
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009291
Epoch: 43 cost time: 3.7484679222106934
Epoch: 43, Steps: 79 | Train Loss: 0.1286732 Vali Loss: 0.1098208 Test Loss: 0.1034019
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009171
Epoch: 44 cost time: 3.6044039726257324
Epoch: 44, Steps: 79 | Train Loss: 0.1268786 Vali Loss: 0.1034483 Test Loss: 0.1035211
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009043
Epoch: 45 cost time: 3.622436046600342
Epoch: 45, Steps: 79 | Train Loss: 0.1267683 Vali Loss: 0.1052492 Test Loss: 0.1019347
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0008907
Epoch: 46 cost time: 3.6026623249053955
Epoch: 46, Steps: 79 | Train Loss: 0.1252511 Vali Loss: 0.1151191 Test Loss: 0.1052598
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0008763
Epoch: 47 cost time: 3.6621532440185547
Epoch: 47, Steps: 79 | Train Loss: 0.1241200 Vali Loss: 0.1083962 Test Loss: 0.1045114
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0008612
Epoch: 48 cost time: 3.57759952545166
Epoch: 48, Steps: 79 | Train Loss: 0.1241302 Vali Loss: 0.1150448 Test Loss: 0.1028266
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0008453
Epoch: 49 cost time: 3.651592254638672
Epoch: 49, Steps: 79 | Train Loss: 0.1221225 Vali Loss: 0.1089549 Test Loss: 0.1017628
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0008288
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.2565876543521881, mae:0.3950667083263397, rmse:0.506544828414917, mape:0.014100824482738972, mspe:0.0003280179516877979, rse:0.3491411805152893, r2_score:0.8565607104638038, acc:0.985899175517261
corr: [38.143097 38.140602 38.09596  38.01577  37.900433 37.768772 37.656345
 37.586758 37.503967 37.474846 37.398685 37.374157 37.291477 37.274143
 37.282654 37.360893 37.45372  37.57543  37.682438 37.714474 37.718884
 37.72335  37.698265 37.693317 37.691486 37.760323 37.79176  37.8127
 37.821754 37.821167 37.887688 37.90591  37.95008  37.95899  37.966835
 37.99557  37.93454  37.910892 37.87898  37.819828 37.734333 37.6683
 37.62666  37.583385 37.511444 37.489082 37.445866 37.42728  37.34306
 37.355953 37.368412 37.403004 37.47241  37.528442 37.62212  37.615105
 37.602295 37.614185 37.564728 37.548702 37.549034 37.58876  37.59875
 37.618217 37.651993 37.65011  37.672897 37.697395 37.762196 37.77207
 37.77883  37.770412 37.7926   37.712196 37.599335 37.497383 37.39848
 37.31625  37.190796 37.091484 36.97517  36.90909  36.830296 36.807693
 36.73301  36.739532 36.76498  36.81635  36.855488 36.891563 36.93533
 36.900726 36.8853   36.88197  36.828835 36.804367 36.78231  36.826015
 36.8608   36.91302  36.93311  36.951008 37.022644 37.083996 37.162636
 37.235386 37.281788 37.317455 37.227932 37.1428   37.029053 36.91829
 36.81345  36.754295 36.6862   36.614727 36.537617 36.481407 36.398693
 36.397415 36.340763 36.355034 36.394363 36.46095  36.52523  36.55698
 36.59309  36.59875  36.596508 36.57921  36.53283  36.493458 36.480732
 36.531555 36.5516   36.549953 36.542267 36.535637 36.575    36.628677
 36.665073 36.688145 36.719254 36.724823 36.707752 36.674175 36.60803
 36.545074 36.466034 36.424854 36.35914  36.299606 36.256348 36.19976
 36.10258  36.074543 36.00733  36.01732  36.054436 36.10107  36.127274
 36.1857   36.257114 36.23889  36.25159  36.299744 36.29219  36.311615
 36.33462  36.399937 36.420116 36.450035 36.465565 36.491528 36.54194
 36.579346 36.63437  36.673363 36.707397 36.75215  36.446903 36.407818
 36.347603 36.29027  36.247192 36.21377  36.191044 36.16243  36.121323
 36.106464 36.060177 36.07149  36.04235  36.067963 36.107243 36.17632
 36.252945 36.271194 36.33408  36.335766 36.35341  36.391582 36.369946
 36.35579  36.364246 36.434017 36.46581  36.494823 36.508884 36.543938
 36.606327 36.664833 36.721104 36.77187  36.832493 36.837532 36.445915
 36.381325 36.302532 36.247234 36.18537  36.146603 36.084396 36.032646
 35.98787  35.954487 35.917217 35.907513 35.84813  35.867123 35.936962
 36.01871  36.085297 36.15604  36.216843 36.196552 36.197372 36.203716
 36.177616 36.156567 36.184227 36.2669   36.271473 36.298504 36.2986
 36.309822 36.36062  36.405636 36.49465  36.54571  36.609013 36.653965
 36.47957  36.422985 36.36218  36.276096 36.20121  36.166412 36.076305
 36.005726 35.939915 35.9096   35.860752 35.84853  35.80283  35.852028
 35.92264  35.997395 36.072887 36.141525 36.246067 36.255222 36.25194
 36.265747 36.222397 36.20709  36.256886 36.279808 36.306686 36.332264
 36.371265 36.42445  36.51946  36.586906 36.658455 36.702908 36.769753
 36.81054  36.57595  36.563892 36.51931  36.427055 36.353626 36.310608
 36.23427  36.15354  36.06864  36.028614 35.9688   35.969917 35.92297
 35.967915 36.017704 36.07479  36.152534 36.216614 36.317356 36.31539
 36.333664 36.353275 36.336582 36.304775 36.300922 36.37586  36.401188
 36.423794 36.49464  36.521843 36.609257 36.648552 36.688484 36.72661
 36.765553 36.814625 36.93253  36.919598 36.886307 36.804142 36.74694
 36.7112   36.645325 36.576134 36.51047  36.473194 36.45741  36.481087
 36.483917 36.533833 36.591576 36.693623 36.747356 36.796513 36.875607
 36.863678 36.862995 36.837124 36.77984  36.731564 36.73557  36.790638
 36.824062 36.87058  36.923077 36.99701  37.060856 37.105736 37.19606
 37.25589  37.303516 37.338966 37.57013  37.613705 37.632603 37.62589
 37.62697 ]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 3.516164541244507
Epoch: 1, Steps: 79 | Train Loss: 0.9904419 Vali Loss: 0.8429366 Test Loss: 0.7885787
Validation loss decreased (inf --> 0.842937).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 3.5212507247924805
Epoch: 2, Steps: 79 | Train Loss: 0.8522535 Vali Loss: 0.7030211 Test Loss: 0.6467447
Validation loss decreased (0.842937 --> 0.703021).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 3.4304516315460205
Epoch: 3, Steps: 79 | Train Loss: 0.7160268 Vali Loss: 0.5801573 Test Loss: 0.5153110
Validation loss decreased (0.703021 --> 0.580157).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 3.4096271991729736
Epoch: 4, Steps: 79 | Train Loss: 0.5421786 Vali Loss: 0.3539991 Test Loss: 0.3357155
Validation loss decreased (0.580157 --> 0.353999).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 3.700721025466919
Epoch: 5, Steps: 79 | Train Loss: 0.4277004 Vali Loss: 0.3114535 Test Loss: 0.2933934
Validation loss decreased (0.353999 --> 0.311453).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 3.3933491706848145
Epoch: 6, Steps: 79 | Train Loss: 0.3815460 Vali Loss: 0.2698711 Test Loss: 0.2616101
Validation loss decreased (0.311453 --> 0.269871).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 3.4982595443725586
Epoch: 7, Steps: 79 | Train Loss: 0.3426748 Vali Loss: 0.2413052 Test Loss: 0.2236041
Validation loss decreased (0.269871 --> 0.241305).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 3.5196950435638428
Epoch: 8, Steps: 79 | Train Loss: 0.3060130 Vali Loss: 0.2161894 Test Loss: 0.1951308
Validation loss decreased (0.241305 --> 0.216189).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 3.4533469676971436
Epoch: 9, Steps: 79 | Train Loss: 0.2824795 Vali Loss: 0.1927621 Test Loss: 0.1816884
Validation loss decreased (0.216189 --> 0.192762).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 3.674652099609375
Epoch: 10, Steps: 79 | Train Loss: 0.2508600 Vali Loss: 0.1720163 Test Loss: 0.1558427
Validation loss decreased (0.192762 --> 0.172016).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 3.3619213104248047
Epoch: 11, Steps: 79 | Train Loss: 0.2295136 Vali Loss: 0.1568162 Test Loss: 0.1445605
Validation loss decreased (0.172016 --> 0.156816).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 3.429226875305176
Epoch: 12, Steps: 79 | Train Loss: 0.2169418 Vali Loss: 0.1414420 Test Loss: 0.1369692
Validation loss decreased (0.156816 --> 0.141442).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 3.4236156940460205
Epoch: 13, Steps: 79 | Train Loss: 0.2055428 Vali Loss: 0.1374852 Test Loss: 0.1331387
Validation loss decreased (0.141442 --> 0.137485).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 3.517660140991211
Epoch: 14, Steps: 79 | Train Loss: 0.1961209 Vali Loss: 0.1353909 Test Loss: 0.1342244
Validation loss decreased (0.137485 --> 0.135391).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 3.541919231414795
Epoch: 15, Steps: 79 | Train Loss: 0.1904779 Vali Loss: 0.1257559 Test Loss: 0.1254639
Validation loss decreased (0.135391 --> 0.125756).  Saving model ...
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 3.483126401901245
Epoch: 16, Steps: 79 | Train Loss: 0.1844894 Vali Loss: 0.1207993 Test Loss: 0.1221072
Validation loss decreased (0.125756 --> 0.120799).  Saving model ...
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 3.398531675338745
Epoch: 17, Steps: 79 | Train Loss: 0.1772648 Vali Loss: 0.1173256 Test Loss: 0.1189914
Validation loss decreased (0.120799 --> 0.117326).  Saving model ...
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 3.444572925567627
Epoch: 18, Steps: 79 | Train Loss: 0.1728040 Vali Loss: 0.1230854 Test Loss: 0.1217338
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 3.3586337566375732
Epoch: 19, Steps: 79 | Train Loss: 0.1685748 Vali Loss: 0.1133006 Test Loss: 0.1135296
Validation loss decreased (0.117326 --> 0.113301).  Saving model ...
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 3.356166124343872
Epoch: 20, Steps: 79 | Train Loss: 0.1643672 Vali Loss: 0.1099273 Test Loss: 0.1156533
Validation loss decreased (0.113301 --> 0.109927).  Saving model ...
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 3.5187172889709473
Epoch: 21, Steps: 79 | Train Loss: 0.1588356 Vali Loss: 0.1081702 Test Loss: 0.1111588
Validation loss decreased (0.109927 --> 0.108170).  Saving model ...
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 3.464707612991333
Epoch: 22, Steps: 79 | Train Loss: 0.1571912 Vali Loss: 0.1152724 Test Loss: 0.1092118
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 3.349252223968506
Epoch: 23, Steps: 79 | Train Loss: 0.1556929 Vali Loss: 0.1158503 Test Loss: 0.1078394
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 3.3693108558654785
Epoch: 24, Steps: 79 | Train Loss: 0.1534698 Vali Loss: 0.1118370 Test Loss: 0.1075044
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 3.495818853378296
Epoch: 25, Steps: 79 | Train Loss: 0.1504202 Vali Loss: 0.1166145 Test Loss: 0.1062817
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 3.482290267944336
Epoch: 26, Steps: 79 | Train Loss: 0.1482989 Vali Loss: 0.1128796 Test Loss: 0.1086815
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 3.4801206588745117
Epoch: 27, Steps: 79 | Train Loss: 0.1468716 Vali Loss: 0.1070824 Test Loss: 0.1012120
Validation loss decreased (0.108170 --> 0.107082).  Saving model ...
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 3.3510866165161133
Epoch: 28, Steps: 79 | Train Loss: 0.1449272 Vali Loss: 0.1087701 Test Loss: 0.1061022
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 3.450623035430908
Epoch: 29, Steps: 79 | Train Loss: 0.1428642 Vali Loss: 0.1129668 Test Loss: 0.1043475
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 3.5461013317108154
Epoch: 30, Steps: 79 | Train Loss: 0.1405827 Vali Loss: 0.1085757 Test Loss: 0.1013469
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 3.501694917678833
Epoch: 31, Steps: 79 | Train Loss: 0.1396007 Vali Loss: 0.0967833 Test Loss: 0.1008044
Validation loss decreased (0.107082 --> 0.096783).  Saving model ...
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 3.4066922664642334
Epoch: 32, Steps: 79 | Train Loss: 0.1385496 Vali Loss: 0.1094518 Test Loss: 0.1031123
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 3.5146567821502686
Epoch: 33, Steps: 79 | Train Loss: 0.1355142 Vali Loss: 0.1094433 Test Loss: 0.0987246
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 3.477734088897705
Epoch: 34, Steps: 79 | Train Loss: 0.1346318 Vali Loss: 0.1008645 Test Loss: 0.0996923
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 3.3665876388549805
Epoch: 35, Steps: 79 | Train Loss: 0.1334618 Vali Loss: 0.0991108 Test Loss: 0.0965109
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 3.472252130508423
Epoch: 36, Steps: 79 | Train Loss: 0.1330492 Vali Loss: 0.1053371 Test Loss: 0.0964143
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 3.632646083831787
Epoch: 37, Steps: 79 | Train Loss: 0.1317048 Vali Loss: 0.1041114 Test Loss: 0.0955092
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 3.545823335647583
Epoch: 38, Steps: 79 | Train Loss: 0.1297120 Vali Loss: 0.1006302 Test Loss: 0.0995352
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 3.4356532096862793
Epoch: 39, Steps: 79 | Train Loss: 0.1295336 Vali Loss: 0.1064800 Test Loss: 0.0986870
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 3.3305230140686035
Epoch: 40, Steps: 79 | Train Loss: 0.1288061 Vali Loss: 0.1100734 Test Loss: 0.0995448
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 3.379612922668457
Epoch: 41, Steps: 79 | Train Loss: 0.1270499 Vali Loss: 0.1110066 Test Loss: 0.0993325
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0009402
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.25539830327033997, mae:0.3959946930408478, rmse:0.5053694844245911, mape:0.014183232560753822, mspe:0.0003312408225610852, rse:0.3483310341835022, r2_score:0.8580232197584389, acc:0.9858167674392462
corr: [37.989883 37.876297 37.888058 37.845875 37.76329  37.724113 37.7034
 37.712723 37.706352 37.6019   37.53932  37.52078  37.52641  37.47582
 37.387356 37.37398  37.4514   37.494747 37.438183 37.416084 37.417778
 37.471466 37.439846 37.488274 37.47968  37.50493  37.487167 37.457764
 37.453552 37.51263  37.557354 37.514103 37.499382 37.544178 37.538494
 37.593277 37.67074  37.650543 37.677998 37.729748 38.08458  38.012707
 38.01601  38.00157  37.929386 37.905445 37.84535  37.83282  37.8177
 37.788387 37.73933  37.688572 37.66964  37.63803  37.53905  37.48165
 37.531166 37.53539  37.4856   37.407703 37.40937  37.444904 37.432976
 37.465923 37.46592  37.45549  37.43329  37.39595  37.395264 37.425446
 37.409466 37.334316 37.312386 37.341732 37.293842 37.28996  37.357403
 37.352104 37.341633 37.35579  37.95145  37.824646 37.751003 37.68892
 37.577984 37.490047 37.425995 37.383827 37.317005 37.26483  37.21013
 37.17943  37.14567  37.089447 37.007416 36.955418 36.99443  36.99629
 36.906757 36.861202 36.86366  36.901012 36.892735 36.90886  36.92728
 36.91762  36.874428 36.816097 36.806118 36.833954 36.845947 36.784477
 36.769783 36.798386 36.77536  36.773678 36.79843  36.771202 36.738613
 36.73383  37.44973  37.36208  37.304768 37.209255 37.117844 37.09657
 37.082985 37.072857 37.031166 36.959194 36.885685 36.841118 36.82061
 36.78886  36.684795 36.617783 36.621914 36.60486  36.50868  36.44452
 36.442883 36.469017 36.490097 36.48039  36.468914 36.478695 36.45804
 36.407658 36.38928  36.371395 36.409367 36.358513 36.34802  36.375443
 36.37415  36.406178 36.44132  36.425476 36.420815 36.43218  36.993484
 36.93524  36.893986 36.843136 36.793606 36.782856 36.76982  36.763927
 36.747673 36.718143 36.69423  36.69037  36.69662  36.668224 36.576336
 36.53665  36.526764 36.53373  36.49374  36.475395 36.47535  36.48641
 36.497616 36.558094 36.575546 36.594635 36.592842 36.58466  36.624325
 36.66384  36.62634  36.54771  36.53532  36.556236 36.53273  36.54317
 36.58534  36.593616 36.62286  36.625347 36.936333 36.866367 36.826706
 36.803085 36.755337 36.731342 36.718876 36.726337 36.742695 36.72813
 36.67144  36.63685  36.628242 36.57336  36.46982  36.420822 36.433044
 36.456696 36.397453 36.337627 36.32936  36.36834  36.406628 36.431473
 36.45685  36.47155  36.463486 36.445362 36.475063 36.50389  36.513336
 36.48122  36.47608  36.473705 36.45324  36.45783  36.5092   36.50544
 36.53665  36.5521   36.66262  36.587894 36.588665 36.551888 36.45597
 36.43676  36.4293   36.411602 36.387184 36.37321  36.357037 36.34221
 36.358467 36.299503 36.20405  36.13392  36.15882  36.163685 36.09999
 36.073845 36.079823 36.169785 36.213028 36.29128  36.347507 36.403767
 36.428013 36.44847  36.462997 36.528564 36.552296 36.488533 36.467854
 36.495693 36.44626  36.43887  36.470997 36.450897 36.468235 36.49803
 36.636932 36.560932 36.54973  36.526474 36.429806 36.360844 36.348473
 36.379032 36.314392 36.22036  36.222923 36.258556 36.27269  36.234673
 36.168064 36.10864  36.13294  36.16004  36.10916  36.0705   36.07442
 36.11342  36.12388  36.19757  36.222374 36.261494 36.245373 36.253937
 36.257816 36.29581  36.31444  36.243958 36.229984 36.27629  36.252335
 36.287956 36.371986 36.379086 36.41475  36.463585 36.572063 36.480644
 36.48572  36.440014 36.353985 36.27927  36.245052 36.253166 36.24039
 36.207893 36.197983 36.233574 36.257805 36.1904   36.123337 36.12185
 36.197853 36.237167 36.143116 36.1066   36.135025 36.197784 36.1995
 36.23309  36.259624 36.33594  36.365154 36.39261  36.424374 36.516273
 36.571484 36.55533  36.59593  36.656567 36.651474 36.69443  36.831696
 36.88123  36.920147 37.0244   37.479134 37.419144 37.44572  37.437336
 37.415173]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 3.5838942527770996
Epoch: 1, Steps: 79 | Train Loss: 0.9813378 Vali Loss: 0.8375186 Test Loss: 0.7728830
Validation loss decreased (inf --> 0.837519).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 3.377811908721924
Epoch: 2, Steps: 79 | Train Loss: 0.8384126 Vali Loss: 0.7079276 Test Loss: 0.6550825
Validation loss decreased (0.837519 --> 0.707928).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 3.502492904663086
Epoch: 3, Steps: 79 | Train Loss: 0.7219425 Vali Loss: 0.5638163 Test Loss: 0.4934990
Validation loss decreased (0.707928 --> 0.563816).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 3.45196533203125
Epoch: 4, Steps: 79 | Train Loss: 0.5425277 Vali Loss: 0.3708669 Test Loss: 0.3474431
Validation loss decreased (0.563816 --> 0.370867).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 3.5524563789367676
Epoch: 5, Steps: 79 | Train Loss: 0.4510877 Vali Loss: 0.3354225 Test Loss: 0.3186104
Validation loss decreased (0.370867 --> 0.335422).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 3.4682836532592773
Epoch: 6, Steps: 79 | Train Loss: 0.4027790 Vali Loss: 0.3099011 Test Loss: 0.2771882
Validation loss decreased (0.335422 --> 0.309901).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 3.5557925701141357
Epoch: 7, Steps: 79 | Train Loss: 0.3570635 Vali Loss: 0.2497024 Test Loss: 0.2376758
Validation loss decreased (0.309901 --> 0.249702).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 3.4341375827789307
Epoch: 8, Steps: 79 | Train Loss: 0.3223455 Vali Loss: 0.2420966 Test Loss: 0.2192090
Validation loss decreased (0.249702 --> 0.242097).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 3.652898073196411
Epoch: 9, Steps: 79 | Train Loss: 0.2894034 Vali Loss: 0.1952741 Test Loss: 0.1846305
Validation loss decreased (0.242097 --> 0.195274).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 3.6154255867004395
Epoch: 10, Steps: 79 | Train Loss: 0.2582651 Vali Loss: 0.1694528 Test Loss: 0.1656294
Validation loss decreased (0.195274 --> 0.169453).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 3.6276299953460693
Epoch: 11, Steps: 79 | Train Loss: 0.2369796 Vali Loss: 0.1550863 Test Loss: 0.1529176
Validation loss decreased (0.169453 --> 0.155086).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 3.5342111587524414
Epoch: 12, Steps: 79 | Train Loss: 0.2229850 Vali Loss: 0.1464405 Test Loss: 0.1477419
Validation loss decreased (0.155086 --> 0.146440).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 3.3822555541992188
Epoch: 13, Steps: 79 | Train Loss: 0.2109341 Vali Loss: 0.1400213 Test Loss: 0.1379560
Validation loss decreased (0.146440 --> 0.140021).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 3.5161023139953613
Epoch: 14, Steps: 79 | Train Loss: 0.2011629 Vali Loss: 0.1238191 Test Loss: 0.1286928
Validation loss decreased (0.140021 --> 0.123819).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 3.4985222816467285
Epoch: 15, Steps: 79 | Train Loss: 0.1924713 Vali Loss: 0.1306743 Test Loss: 0.1357875
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 3.566939353942871
Epoch: 16, Steps: 79 | Train Loss: 0.1869570 Vali Loss: 0.1275658 Test Loss: 0.1293108
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 3.5302505493164062
Epoch: 17, Steps: 79 | Train Loss: 0.1794238 Vali Loss: 0.1209223 Test Loss: 0.1246180
Validation loss decreased (0.123819 --> 0.120922).  Saving model ...
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 3.448195219039917
Epoch: 18, Steps: 79 | Train Loss: 0.1772089 Vali Loss: 0.1179010 Test Loss: 0.1196635
Validation loss decreased (0.120922 --> 0.117901).  Saving model ...
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 3.4756970405578613
Epoch: 19, Steps: 79 | Train Loss: 0.1706032 Vali Loss: 0.1168756 Test Loss: 0.1169598
Validation loss decreased (0.117901 --> 0.116876).  Saving model ...
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 3.408608913421631
Epoch: 20, Steps: 79 | Train Loss: 0.1667244 Vali Loss: 0.1167698 Test Loss: 0.1159787
Validation loss decreased (0.116876 --> 0.116770).  Saving model ...
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 3.706260919570923
Epoch: 21, Steps: 79 | Train Loss: 0.1636598 Vali Loss: 0.1129252 Test Loss: 0.1117149
Validation loss decreased (0.116770 --> 0.112925).  Saving model ...
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 3.6236751079559326
Epoch: 22, Steps: 79 | Train Loss: 0.1619149 Vali Loss: 0.1123965 Test Loss: 0.1141909
Validation loss decreased (0.112925 --> 0.112397).  Saving model ...
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 3.5962533950805664
Epoch: 23, Steps: 79 | Train Loss: 0.1584558 Vali Loss: 0.1115822 Test Loss: 0.1111701
Validation loss decreased (0.112397 --> 0.111582).  Saving model ...
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 3.578777313232422
Epoch: 24, Steps: 79 | Train Loss: 0.1564631 Vali Loss: 0.1122681 Test Loss: 0.1103477
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 3.4826467037200928
Epoch: 25, Steps: 79 | Train Loss: 0.1532472 Vali Loss: 0.1078314 Test Loss: 0.1083519
Validation loss decreased (0.111582 --> 0.107831).  Saving model ...
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 3.641508102416992
Epoch: 26, Steps: 79 | Train Loss: 0.1526845 Vali Loss: 0.1137417 Test Loss: 0.1088748
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 3.544982671737671
Epoch: 27, Steps: 79 | Train Loss: 0.1489139 Vali Loss: 0.1159214 Test Loss: 0.1082620
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 3.3526570796966553
Epoch: 28, Steps: 79 | Train Loss: 0.1479983 Vali Loss: 0.1056784 Test Loss: 0.1041687
Validation loss decreased (0.107831 --> 0.105678).  Saving model ...
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 3.6561546325683594
Epoch: 29, Steps: 79 | Train Loss: 0.1455504 Vali Loss: 0.1051093 Test Loss: 0.1026437
Validation loss decreased (0.105678 --> 0.105109).  Saving model ...
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 3.48471736907959
Epoch: 30, Steps: 79 | Train Loss: 0.1448321 Vali Loss: 0.1113753 Test Loss: 0.1067715
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 3.5241401195526123
Epoch: 31, Steps: 79 | Train Loss: 0.1425178 Vali Loss: 0.1042628 Test Loss: 0.1025495
Validation loss decreased (0.105109 --> 0.104263).  Saving model ...
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 3.3963918685913086
Epoch: 32, Steps: 79 | Train Loss: 0.1414206 Vali Loss: 0.1105144 Test Loss: 0.1020400
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 3.6413516998291016
Epoch: 33, Steps: 79 | Train Loss: 0.1401900 Vali Loss: 0.1227404 Test Loss: 0.1091643
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 3.600341320037842
Epoch: 34, Steps: 79 | Train Loss: 0.1389164 Vali Loss: 0.1139137 Test Loss: 0.1051820
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 3.7141642570495605
Epoch: 35, Steps: 79 | Train Loss: 0.1375168 Vali Loss: 0.1067464 Test Loss: 0.1048550
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 3.6375491619110107
Epoch: 36, Steps: 79 | Train Loss: 0.1356397 Vali Loss: 0.1052623 Test Loss: 0.1017970
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 3.7104079723358154
Epoch: 37, Steps: 79 | Train Loss: 0.1351261 Vali Loss: 0.1097571 Test Loss: 0.1038167
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 3.554546356201172
Epoch: 38, Steps: 79 | Train Loss: 0.1337932 Vali Loss: 0.1058977 Test Loss: 0.1015321
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 3.6089165210723877
Epoch: 39, Steps: 79 | Train Loss: 0.1318390 Vali Loss: 0.1061523 Test Loss: 0.1013805
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 3.541585922241211
Epoch: 40, Steps: 79 | Train Loss: 0.1313824 Vali Loss: 0.1071198 Test Loss: 0.1013036
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 3.6135456562042236
Epoch: 41, Steps: 79 | Train Loss: 0.1307049 Vali Loss: 0.1062483 Test Loss: 0.1012689
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0009402
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.2747516930103302, mae:0.40983277559280396, rmse:0.5241675972938538, mape:0.014683137647807598, mspe:0.00035709235817193985, rse:0.3612878620624542, r2_score:0.8506866896049241, acc:0.9853168623521924
corr: [38.44963  38.383724 38.343758 38.324047 38.316895 38.336163 38.343372
 38.309273 38.33627  38.349495 38.33236  38.2423   38.161983 38.089516
 38.055595 37.958256 37.918053 37.846535 37.82494  37.81862  37.865948
 37.90452  37.903137 37.842472 37.79098  37.751545 37.768833 37.740227
 37.63849  37.616863 37.64555  37.640484 37.63275  37.638935 37.652233
 37.711166 37.767853 37.84978  37.905304 37.93478  37.992462 37.97332
 37.93415  37.92871  37.985043 38.299564 38.244694 38.196003 38.16449
 38.146027 38.092625 38.065582 38.032604 38.003357 38.018078 38.00921
 37.926685 37.846317 37.800114 37.797558 37.734592 37.683426 37.605015
 37.537758 37.503887 37.506256 37.496147 37.48869  37.48351  37.441727
 37.372734 37.360596 37.336838 37.265167 37.234497 37.196342 37.114124
 37.057983 37.043343 36.99467  36.988445 37.002766 37.01112  37.03635
 37.050575 37.068695 37.009853 36.98249  36.98593  37.034092 37.521507
 37.458652 37.408623 37.36623  37.367317 37.36093  37.3321   37.28864
 37.25939  37.284103 37.31312  37.2209   37.149403 37.095966 37.048763
 36.9941   36.903553 36.79188  36.715168 36.66871  36.66041  36.64984
 36.62354  36.58537  36.52428  36.54126  36.547455 36.53969  36.482224
 36.472954 36.498596 36.472145 36.441917 36.446175 36.43401  36.44157
 36.465847 36.495335 36.51361  36.54779  36.536106 36.481136 36.414963
 36.46742  36.531757 36.8725   36.808468 36.73664  36.664394 36.6575
 36.645615 36.59419  36.551006 36.524612 36.541428 36.58164  36.522858
 36.48142  36.49898  36.51908  36.47327  36.418484 36.383846 36.344555
 36.294132 36.307987 36.35713  36.3915   36.392048 36.3306   36.298836
 36.329082 36.328117 36.259026 36.236725 36.25491  36.25621  36.260498
 36.261955 36.29989  36.31588  36.370674 36.402847 36.425346 36.457684
 36.486675 36.459267 36.466053 36.53044  36.62617  36.569443 36.539093
 36.515232 36.509193 36.557972 36.570595 36.533253 36.536194 36.529945
 36.558655 36.61053  36.572735 36.512787 36.452747 36.432217 36.419926
 36.344276 36.288925 36.259724 36.255592 36.315563 36.341476 36.406197
 36.426434 36.38659  36.370247 36.39335  36.41214  36.365055 36.354866
 36.355392 36.336548 36.345387 36.405216 36.476643 36.47923  36.535812
 36.574825 36.632324 36.65951  36.695232 36.680885 36.66072  36.668465
 36.703953 36.55495  36.482117 36.44451  36.438576 36.47454  36.48666
 36.45601  36.451027 36.464516 36.484406 36.541183 36.499264 36.43973
 36.43199  36.403606 36.364132 36.28273  36.209248 36.178375 36.191345
 36.2431   36.3088   36.334198 36.375683 36.299366 36.270943 36.253452
 36.252365 36.199364 36.194588 36.189945 36.19119  36.20249  36.21285
 36.243164 36.276096 36.333916 36.364162 36.41667  36.44257  36.50681
 36.50866  36.506897 36.54255  36.60307  36.571907 36.49667  36.439213
 36.409435 36.4493   36.455364 36.448864 36.444027 36.447342 36.46673
 36.505966 36.478718 36.462406 36.422764 36.40349  36.35006  36.292095
 36.222355 36.134663 36.123512 36.14341  36.17799  36.193527 36.249992
 36.258095 36.278    36.307095 36.307156 36.29711  36.33584  36.36601
 36.343216 36.350338 36.4013   36.459198 36.47887  36.538624 36.610504
 36.65037  36.692997 36.739124 36.735695 36.739532 36.787952 36.89401
 36.809853 36.742985 36.725155 36.734543 36.804276 36.83014  36.854977
 36.865425 36.910873 36.933533 37.00654  37.00408  37.00183  36.997116
 36.98367  36.97213  36.944992 36.920444 36.86766  36.830566 36.870754
 36.941086 36.990616 37.026016 36.990833 36.990788 37.036568 37.0463
 36.986103 36.996685 37.014645 36.99818  37.016087 37.04748  37.142174
 37.160995 37.20365  37.284016 37.38453  37.406013 37.491825 37.531864
 37.593735 37.66749  37.76928  37.75159  37.777008 37.839886 37.918594
 37.993683]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 3.4892141819000244
Epoch: 1, Steps: 79 | Train Loss: 0.9400972 Vali Loss: 0.8431834 Test Loss: 0.7684864
Validation loss decreased (inf --> 0.843183).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 3.5705623626708984
Epoch: 2, Steps: 79 | Train Loss: 0.8228820 Vali Loss: 0.7222254 Test Loss: 0.6574529
Validation loss decreased (0.843183 --> 0.722225).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 3.444658041000366
Epoch: 3, Steps: 79 | Train Loss: 0.7137520 Vali Loss: 0.5786587 Test Loss: 0.5194331
Validation loss decreased (0.722225 --> 0.578659).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 3.349440336227417
Epoch: 4, Steps: 79 | Train Loss: 0.5566239 Vali Loss: 0.4133119 Test Loss: 0.3702707
Validation loss decreased (0.578659 --> 0.413312).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 3.374300003051758
Epoch: 5, Steps: 79 | Train Loss: 0.4666011 Vali Loss: 0.3288082 Test Loss: 0.3191889
Validation loss decreased (0.413312 --> 0.328808).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 3.4559485912323
Epoch: 6, Steps: 79 | Train Loss: 0.4063042 Vali Loss: 0.2773386 Test Loss: 0.2704829
Validation loss decreased (0.328808 --> 0.277339).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 3.5255286693573
Epoch: 7, Steps: 79 | Train Loss: 0.3568755 Vali Loss: 0.2495981 Test Loss: 0.2314184
Validation loss decreased (0.277339 --> 0.249598).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 3.3043761253356934
Epoch: 8, Steps: 79 | Train Loss: 0.3192607 Vali Loss: 0.2222374 Test Loss: 0.2104662
Validation loss decreased (0.249598 --> 0.222237).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 3.5248987674713135
Epoch: 9, Steps: 79 | Train Loss: 0.2943176 Vali Loss: 0.2117129 Test Loss: 0.1981920
Validation loss decreased (0.222237 --> 0.211713).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 3.5888671875
Epoch: 10, Steps: 79 | Train Loss: 0.2762061 Vali Loss: 0.1959772 Test Loss: 0.1855280
Validation loss decreased (0.211713 --> 0.195977).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 3.3888590335845947
Epoch: 11, Steps: 79 | Train Loss: 0.2577315 Vali Loss: 0.1758166 Test Loss: 0.1678404
Validation loss decreased (0.195977 --> 0.175817).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 3.572006940841675
Epoch: 12, Steps: 79 | Train Loss: 0.2408479 Vali Loss: 0.1623135 Test Loss: 0.1543463
Validation loss decreased (0.175817 --> 0.162313).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 3.4918134212493896
Epoch: 13, Steps: 79 | Train Loss: 0.2242150 Vali Loss: 0.1453295 Test Loss: 0.1479001
Validation loss decreased (0.162313 --> 0.145330).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 3.3119826316833496
Epoch: 14, Steps: 79 | Train Loss: 0.2104694 Vali Loss: 0.1378610 Test Loss: 0.1375095
Validation loss decreased (0.145330 --> 0.137861).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 3.454411506652832
Epoch: 15, Steps: 79 | Train Loss: 0.1999748 Vali Loss: 0.1345587 Test Loss: 0.1350577
Validation loss decreased (0.137861 --> 0.134559).  Saving model ...
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 3.8852484226226807
Epoch: 16, Steps: 79 | Train Loss: 0.1891131 Vali Loss: 0.1299358 Test Loss: 0.1333515
Validation loss decreased (0.134559 --> 0.129936).  Saving model ...
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 3.665010929107666
Epoch: 17, Steps: 79 | Train Loss: 0.1832833 Vali Loss: 0.1211435 Test Loss: 0.1267130
Validation loss decreased (0.129936 --> 0.121143).  Saving model ...
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 3.705310583114624
Epoch: 18, Steps: 79 | Train Loss: 0.1779480 Vali Loss: 0.1260175 Test Loss: 0.1228923
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 3.5716054439544678
Epoch: 19, Steps: 79 | Train Loss: 0.1739956 Vali Loss: 0.1122629 Test Loss: 0.1208340
Validation loss decreased (0.121143 --> 0.112263).  Saving model ...
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 3.5347487926483154
Epoch: 20, Steps: 79 | Train Loss: 0.1705179 Vali Loss: 0.1255989 Test Loss: 0.1226486
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 3.776264190673828
Epoch: 21, Steps: 79 | Train Loss: 0.1674736 Vali Loss: 0.1158495 Test Loss: 0.1178220
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 3.328673839569092
Epoch: 22, Steps: 79 | Train Loss: 0.1642119 Vali Loss: 0.1217410 Test Loss: 0.1176085
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 3.4133949279785156
Epoch: 23, Steps: 79 | Train Loss: 0.1622327 Vali Loss: 0.1182606 Test Loss: 0.1163045
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 3.553701400756836
Epoch: 24, Steps: 79 | Train Loss: 0.1613201 Vali Loss: 0.1075039 Test Loss: 0.1139484
Validation loss decreased (0.112263 --> 0.107504).  Saving model ...
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 3.5534167289733887
Epoch: 25, Steps: 79 | Train Loss: 0.1578740 Vali Loss: 0.1225160 Test Loss: 0.1152645
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 3.3531341552734375
Epoch: 26, Steps: 79 | Train Loss: 0.1558787 Vali Loss: 0.1091062 Test Loss: 0.1132247
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 3.463721513748169
Epoch: 27, Steps: 79 | Train Loss: 0.1542154 Vali Loss: 0.1161485 Test Loss: 0.1116747
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 3.5410614013671875
Epoch: 28, Steps: 79 | Train Loss: 0.1508619 Vali Loss: 0.1123917 Test Loss: 0.1105139
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 3.430847644805908
Epoch: 29, Steps: 79 | Train Loss: 0.1492299 Vali Loss: 0.1104232 Test Loss: 0.1095313
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 3.420295476913452
Epoch: 30, Steps: 79 | Train Loss: 0.1482019 Vali Loss: 0.1183354 Test Loss: 0.1151998
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 3.5775110721588135
Epoch: 31, Steps: 79 | Train Loss: 0.1466489 Vali Loss: 0.1139540 Test Loss: 0.1133589
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 3.464411735534668
Epoch: 32, Steps: 79 | Train Loss: 0.1455717 Vali Loss: 0.1097204 Test Loss: 0.1076691
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 3.61857533454895
Epoch: 33, Steps: 79 | Train Loss: 0.1432511 Vali Loss: 0.1068035 Test Loss: 0.1038776
Validation loss decreased (0.107504 --> 0.106803).  Saving model ...
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 3.596972703933716
Epoch: 34, Steps: 79 | Train Loss: 0.1414209 Vali Loss: 0.1034399 Test Loss: 0.1055947
Validation loss decreased (0.106803 --> 0.103440).  Saving model ...
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 3.599973678588867
Epoch: 35, Steps: 79 | Train Loss: 0.1417575 Vali Loss: 0.1075907 Test Loss: 0.1086097
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 3.6139352321624756
Epoch: 36, Steps: 79 | Train Loss: 0.1404684 Vali Loss: 0.1063849 Test Loss: 0.1037602
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 3.480504035949707
Epoch: 37, Steps: 79 | Train Loss: 0.1396861 Vali Loss: 0.1118977 Test Loss: 0.1053595
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 3.458557605743408
Epoch: 38, Steps: 79 | Train Loss: 0.1380093 Vali Loss: 0.1090871 Test Loss: 0.1064609
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 3.479677438735962
Epoch: 39, Steps: 79 | Train Loss: 0.1360916 Vali Loss: 0.1075280 Test Loss: 0.1063846
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 3.615926742553711
Epoch: 40, Steps: 79 | Train Loss: 0.1349980 Vali Loss: 0.1198902 Test Loss: 0.1096854
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 3.431579113006592
Epoch: 41, Steps: 79 | Train Loss: 0.1350856 Vali Loss: 0.1096950 Test Loss: 0.1050992
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0009402
Epoch: 42 cost time: 3.5649445056915283
Epoch: 42, Steps: 79 | Train Loss: 0.1336544 Vali Loss: 0.1083828 Test Loss: 0.1037988
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0009291
Epoch: 43 cost time: 3.738696813583374
Epoch: 43, Steps: 79 | Train Loss: 0.1327070 Vali Loss: 0.1072642 Test Loss: 0.1024679
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0009171
Epoch: 44 cost time: 3.5472609996795654
Epoch: 44, Steps: 79 | Train Loss: 0.1322784 Vali Loss: 0.1096507 Test Loss: 0.1065426
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0009043
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.2739778757095337, mae:0.40665480494499207, rmse:0.5234289765357971, mape:0.014558251015841961, mspe:0.00035566388396546245, rse:0.36077871918678284, r2_score:0.8554566643610889, acc:0.985441748984158
corr: [37.8536   37.86527  37.84665  37.83088  37.81338  37.834545 37.84146
 37.781048 37.741734 37.677734 37.610565 37.655003 37.682087 37.71255
 37.754005 37.675945 37.655327 37.68752  37.675735 37.686073 37.63527
 37.605698 37.61804  37.68325  37.68749  37.658817 37.68528  37.734425
 37.67266  37.64403  37.67983  37.648724 37.579193 37.54159  37.549385
 37.52149  37.47534  37.508602 37.544357 37.573483 37.5962   37.6487
 37.75997  37.788128 37.856083 37.864357 37.897327 37.971645 38.046444
 38.115643 38.141094 38.165035 37.994    37.96545  37.90324  37.86032
 37.79001  37.763245 37.714787 37.630966 37.579926 37.482517 37.402317
 37.40899  37.421806 37.432987 37.43718  37.38734  37.353985 37.369198
 37.345318 37.357365 37.315487 37.25942  37.206974 37.262142 37.252678
 37.24543  37.244797 37.23951  37.161224 37.102665 37.102516 37.04135
 36.951286 36.876656 36.837364 36.79405  36.750935 36.75608  36.76013
 36.75437  36.72212  36.7692   36.850048 36.916958 36.952324 36.951317
 37.00283  37.08286  37.155254 37.21556  37.235516 37.264286 37.113297
 37.1075   37.049847 37.0148   36.961884 36.934227 36.899673 36.843437
 36.777126 36.692417 36.642315 36.654446 36.64799  36.62929  36.630066
 36.554718 36.516796 36.506783 36.474014 36.526936 36.509678 36.471756
 36.478916 36.563126 36.558754 36.533962 36.555687 36.578754 36.50584
 36.441154 36.44668  36.428772 36.352306 36.310997 36.301044 36.280033
 36.230434 36.234394 36.227215 36.228813 36.218227 36.238045 36.286953
 36.33571  36.387177 36.39971  36.436516 36.49532  36.52426  36.589394
 36.645027 36.682312 36.25768  36.26682  36.237835 36.24395  36.171883
 36.159603 36.155064 36.10407  36.068977 36.051872 35.97823  36.008514
 36.031975 36.033245 36.06659  36.03574  36.02472  36.072716 36.03942
 36.063988 36.08271  36.06454  36.063778 36.130016 36.13751  36.132492
 36.174984 36.23584  36.189007 36.14465  36.13837  36.100468 36.05352
 36.01937  36.01788  36.001568 35.97049  36.002995 36.041058 35.999474
 35.96528  35.97783  36.05001  36.091984 36.12869  36.176426 36.224735
 36.337788 36.41421  36.50101  36.579784 36.63963  36.26614  36.26597
 36.235855 36.222805 36.181667 36.16452  36.13867  36.06571  36.03188
 35.969986 35.914276 35.941994 35.96322  35.99254  36.005947 35.98341
 35.975548 36.00547  36.00857  36.03746  36.036777 35.997658 35.974617
 36.040073 36.05574  36.02903  36.0509   36.110157 36.065296 36.01059
 36.006508 35.981075 35.96411  35.9543   35.939045 35.917484 35.881573
 35.89856  35.910072 35.861362 35.832386 35.877373 35.929127 35.96888
 36.005703 36.02108  36.070206 36.16237  36.25161  36.354137 36.446358
 36.509464 36.144638 36.18484  36.13799  36.106075 36.056873 36.054523
 36.10433  36.062984 36.030754 35.998795 35.994934 36.052402 36.107307
 36.11703  36.15223  36.12468  36.11704  36.117416 36.085873 36.099396
 36.07526  36.07956  36.106926 36.181797 36.184963 36.173832 36.19691
 36.248154 36.222794 36.188564 36.189075 36.16436  36.102882 36.05623
 36.035862 35.978664 35.944496 35.95566  35.96701  35.95354  35.937386
 35.97956  36.072533 36.130093 36.197247 36.245346 36.323177 36.507828
 36.607445 36.72027  36.798523 36.855267 36.39242  36.402103 36.378815
 36.3486   36.30339  36.28992  36.292847 36.2402   36.216236 36.207466
 36.1874   36.245804 36.309875 36.393723 36.427578 36.36989  36.375187
 36.405018 36.398323 36.425934 36.412083 36.43577  36.45672  36.577137
 36.59556  36.57107  36.589287 36.64122  36.604397 36.56657  36.579185
 36.56514  36.535194 36.547455 36.535564 36.50481  36.49293  36.50741
 36.523113 36.504578 36.504906 36.589363 36.665897 36.76446  36.84895
 36.90124  37.020294 37.2281   37.361874 37.48701  37.593086 37.686348
 37.332256]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 3.615689992904663
Epoch: 1, Steps: 79 | Train Loss: 0.9565949 Vali Loss: 0.8142206 Test Loss: 0.7525452
Validation loss decreased (inf --> 0.814221).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 3.5843183994293213
Epoch: 2, Steps: 79 | Train Loss: 0.8189300 Vali Loss: 0.6852159 Test Loss: 0.6247318
Validation loss decreased (0.814221 --> 0.685216).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 3.5403058528900146
Epoch: 3, Steps: 79 | Train Loss: 0.6925949 Vali Loss: 0.5666415 Test Loss: 0.4972310
Validation loss decreased (0.685216 --> 0.566642).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 3.5795257091522217
Epoch: 4, Steps: 79 | Train Loss: 0.5535527 Vali Loss: 0.3995785 Test Loss: 0.3672788
Validation loss decreased (0.566642 --> 0.399579).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 3.503316640853882
Epoch: 5, Steps: 79 | Train Loss: 0.4651525 Vali Loss: 0.3215055 Test Loss: 0.3075138
Validation loss decreased (0.399579 --> 0.321506).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 3.488537549972534
Epoch: 6, Steps: 79 | Train Loss: 0.3947641 Vali Loss: 0.2686253 Test Loss: 0.2581653
Validation loss decreased (0.321506 --> 0.268625).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 3.3597822189331055
Epoch: 7, Steps: 79 | Train Loss: 0.3408406 Vali Loss: 0.2193575 Test Loss: 0.2120929
Validation loss decreased (0.268625 --> 0.219358).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 3.6597328186035156
Epoch: 8, Steps: 79 | Train Loss: 0.3041484 Vali Loss: 0.2040690 Test Loss: 0.1916029
Validation loss decreased (0.219358 --> 0.204069).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 3.72379994392395
Epoch: 9, Steps: 79 | Train Loss: 0.2761072 Vali Loss: 0.1788761 Test Loss: 0.1775317
Validation loss decreased (0.204069 --> 0.178876).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 3.50274395942688
Epoch: 10, Steps: 79 | Train Loss: 0.2554424 Vali Loss: 0.1682821 Test Loss: 0.1643529
Validation loss decreased (0.178876 --> 0.168282).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 3.6816787719726562
Epoch: 11, Steps: 79 | Train Loss: 0.2406896 Vali Loss: 0.1531969 Test Loss: 0.1553914
Validation loss decreased (0.168282 --> 0.153197).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 3.6644539833068848
Epoch: 12, Steps: 79 | Train Loss: 0.2263340 Vali Loss: 0.1528412 Test Loss: 0.1487839
Validation loss decreased (0.153197 --> 0.152841).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 3.646977424621582
Epoch: 13, Steps: 79 | Train Loss: 0.2183021 Vali Loss: 0.1514569 Test Loss: 0.1435139
Validation loss decreased (0.152841 --> 0.151457).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 3.836104154586792
Epoch: 14, Steps: 79 | Train Loss: 0.2061997 Vali Loss: 0.1431298 Test Loss: 0.1426893
Validation loss decreased (0.151457 --> 0.143130).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 3.5378880500793457
Epoch: 15, Steps: 79 | Train Loss: 0.1973981 Vali Loss: 0.1334153 Test Loss: 0.1322667
Validation loss decreased (0.143130 --> 0.133415).  Saving model ...
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 3.5746726989746094
Epoch: 16, Steps: 79 | Train Loss: 0.1916421 Vali Loss: 0.1264378 Test Loss: 0.1286484
Validation loss decreased (0.133415 --> 0.126438).  Saving model ...
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 3.381624221801758
Epoch: 17, Steps: 79 | Train Loss: 0.1825893 Vali Loss: 0.1209537 Test Loss: 0.1261203
Validation loss decreased (0.126438 --> 0.120954).  Saving model ...
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 3.4029839038848877
Epoch: 18, Steps: 79 | Train Loss: 0.1768690 Vali Loss: 0.1218829 Test Loss: 0.1229888
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 3.4397761821746826
Epoch: 19, Steps: 79 | Train Loss: 0.1712665 Vali Loss: 0.1122729 Test Loss: 0.1184474
Validation loss decreased (0.120954 --> 0.112273).  Saving model ...
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 3.5281102657318115
Epoch: 20, Steps: 79 | Train Loss: 0.1673552 Vali Loss: 0.1095273 Test Loss: 0.1176216
Validation loss decreased (0.112273 --> 0.109527).  Saving model ...
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 3.403926134109497
Epoch: 21, Steps: 79 | Train Loss: 0.1633763 Vali Loss: 0.1109910 Test Loss: 0.1124050
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 3.497450590133667
Epoch: 22, Steps: 79 | Train Loss: 0.1595736 Vali Loss: 0.1058615 Test Loss: 0.1136760
Validation loss decreased (0.109527 --> 0.105862).  Saving model ...
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 3.6326467990875244
Epoch: 23, Steps: 79 | Train Loss: 0.1573414 Vali Loss: 0.1105840 Test Loss: 0.1132658
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 3.6738922595977783
Epoch: 24, Steps: 79 | Train Loss: 0.1566002 Vali Loss: 0.1040741 Test Loss: 0.1092833
Validation loss decreased (0.105862 --> 0.104074).  Saving model ...
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 3.7200024127960205
Epoch: 25, Steps: 79 | Train Loss: 0.1533207 Vali Loss: 0.1064470 Test Loss: 0.1088141
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 3.511232614517212
Epoch: 26, Steps: 79 | Train Loss: 0.1516733 Vali Loss: 0.1101002 Test Loss: 0.1092138
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 3.6858248710632324
Epoch: 27, Steps: 79 | Train Loss: 0.1490137 Vali Loss: 0.1083592 Test Loss: 0.1093962
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 3.6870362758636475
Epoch: 28, Steps: 79 | Train Loss: 0.1471013 Vali Loss: 0.1110564 Test Loss: 0.1060412
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 3.7029738426208496
Epoch: 29, Steps: 79 | Train Loss: 0.1450366 Vali Loss: 0.1077225 Test Loss: 0.1057693
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 3.757847785949707
Epoch: 30, Steps: 79 | Train Loss: 0.1432844 Vali Loss: 0.1026774 Test Loss: 0.1049568
Validation loss decreased (0.104074 --> 0.102677).  Saving model ...
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 3.6910078525543213
Epoch: 31, Steps: 79 | Train Loss: 0.1412848 Vali Loss: 0.1071504 Test Loss: 0.1111541
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 3.553075075149536
Epoch: 32, Steps: 79 | Train Loss: 0.1402806 Vali Loss: 0.0998276 Test Loss: 0.1025451
Validation loss decreased (0.102677 --> 0.099828).  Saving model ...
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 3.453347682952881
Epoch: 33, Steps: 79 | Train Loss: 0.1393471 Vali Loss: 0.1095793 Test Loss: 0.1060881
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 3.4408280849456787
Epoch: 34, Steps: 79 | Train Loss: 0.1363438 Vali Loss: 0.1067503 Test Loss: 0.1016131
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 3.4832098484039307
Epoch: 35, Steps: 79 | Train Loss: 0.1354778 Vali Loss: 0.1029673 Test Loss: 0.1022114
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 3.536562442779541
Epoch: 36, Steps: 79 | Train Loss: 0.1349926 Vali Loss: 0.1022069 Test Loss: 0.1016037
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 3.684804677963257
Epoch: 37, Steps: 79 | Train Loss: 0.1339900 Vali Loss: 0.0997331 Test Loss: 0.0993946
Validation loss decreased (0.099828 --> 0.099733).  Saving model ...
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 3.8678088188171387
Epoch: 38, Steps: 79 | Train Loss: 0.1320966 Vali Loss: 0.1042277 Test Loss: 0.1029888
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 3.457176685333252
Epoch: 39, Steps: 79 | Train Loss: 0.1316506 Vali Loss: 0.1013317 Test Loss: 0.1010033
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 3.6074612140655518
Epoch: 40, Steps: 79 | Train Loss: 0.1301639 Vali Loss: 0.0999079 Test Loss: 0.1021571
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 3.6514320373535156
Epoch: 41, Steps: 79 | Train Loss: 0.1300041 Vali Loss: 0.1095446 Test Loss: 0.1049694
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009402
Epoch: 42 cost time: 3.508023262023926
Epoch: 42, Steps: 79 | Train Loss: 0.1276814 Vali Loss: 0.1017555 Test Loss: 0.1021215
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009291
Epoch: 43 cost time: 3.6789820194244385
Epoch: 43, Steps: 79 | Train Loss: 0.1277690 Vali Loss: 0.1062584 Test Loss: 0.0989349
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009171
Epoch: 44 cost time: 3.5151102542877197
Epoch: 44, Steps: 79 | Train Loss: 0.1273492 Vali Loss: 0.1025530 Test Loss: 0.1026766
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0009043
Epoch: 45 cost time: 3.6064112186431885
Epoch: 45, Steps: 79 | Train Loss: 0.1266030 Vali Loss: 0.1067624 Test Loss: 0.1026420
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0008907
Epoch: 46 cost time: 3.461024522781372
Epoch: 46, Steps: 79 | Train Loss: 0.1243347 Vali Loss: 0.1018677 Test Loss: 0.1009284
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0008763
Epoch: 47 cost time: 3.50488018989563
Epoch: 47, Steps: 79 | Train Loss: 0.1239935 Vali Loss: 0.1001235 Test Loss: 0.1023523
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0008612
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.27266204357147217, mae:0.40598195791244507, rmse:0.5221704840660095, mape:0.014528635889291763, mspe:0.0003531239344738424, rse:0.35991135239601135, r2_score:0.849016698297695, acc:0.9854713641107082
corr: [38.02541  38.069317 38.089474 38.04643  38.009613 37.958183 37.912563
 37.78457  37.710278 37.583702 37.47267  37.470654 37.357346 37.27495
 37.20086  37.200325 37.17179  37.125523 37.031864 37.008564 37.01105
 37.021492 37.05582  37.067074 37.118073 37.18143  37.240074 37.35772
 37.449272 37.47414  37.557095 37.59078  37.623257 37.65766  37.748066
 37.78313  37.77072  37.759636 37.76656  37.8135   37.886177 37.843277
 37.82398  37.864532 37.891304 37.88362  37.868385 37.826393 37.81887
 37.780285 37.807625 37.83105  37.824066 37.794514 37.779778 37.81652
 37.92337  37.956337 37.953243 37.982872 37.993774 38.006443 37.95507
 37.85868  37.78613  37.75948  37.7168   37.579887 37.463173 37.33478
 37.21701  37.2308   37.13247  37.054802 36.98203  36.962692 36.899464
 36.86858  36.785404 36.728344 36.670086 36.6132   36.584362 36.577583
 36.611294 36.646973 36.68402  36.738434 36.732338 36.729076 36.77256
 36.791714 36.81411  36.82939  36.86898  36.829247 36.756554 36.716602
 36.699913 36.73989  36.824852 36.782024 36.737976 36.742943 36.778606
 36.797554 36.78249  36.764217 36.749313 36.711456 36.724625 36.732273
 36.701523 36.66565  36.66088  36.687153 36.78392  36.793774 36.775112
 36.786335 36.999134 36.984627 36.96298  36.885292 36.84034  36.836826
 36.772087 36.690063 36.64634  36.54584  36.44912  36.43618  36.35855
 36.32309  36.258514 36.271797 36.258667 36.20815  36.127556 36.07841
 36.04237  36.03253  36.044155 36.03072  36.071198 36.101814 36.15851
 36.211517 36.208534 36.24501  36.267925 36.27102  36.27291  36.26136
 36.29846  36.23415  36.166325 36.1266   36.123806 36.152943 36.20497
 36.1777   36.154793 36.20183  36.25141  36.243385 36.231052 36.187637
 36.17572  36.168373 36.192444 36.218487 36.23756  36.254097 36.23686
 36.256073 36.327656 36.357124 36.349636 36.37569  36.64131  36.650562
 36.67367  36.652634 36.612144 36.60465  36.56789  36.466904 36.405624
 36.293972 36.211773 36.17313  36.095844 36.03293  35.969902 35.972115
 35.943085 35.8988   35.833317 35.829205 35.807392 35.758453 35.760654
 35.739944 35.78051  35.800476 35.862904 35.949844 35.96108  35.9945
 36.062347 36.06443  36.11289  36.142212 36.18783  36.173264 36.114883
 36.115147 36.091488 36.111107 36.16152  36.165966 36.1631   36.16449
 36.17158  36.180695 36.159996 36.18107  36.15782  36.12426  36.130867
 36.1486   36.114285 36.111843 36.081783 36.088364 36.164257 36.16203
 36.12524  36.17041  36.610264 36.626133 36.61486  36.572388 36.49509
 36.452488 36.41021  36.31716  36.24563  36.170254 36.101124 36.093826
 36.011177 35.920555 35.880882 35.88428  35.82569  35.78947  35.718075
 35.698822 35.676132 35.694607 35.711563 35.701847 35.733162 35.76921
 35.86777  35.942406 35.981045 36.014748 36.09408  36.15433  36.204414
 36.270237 36.31185  36.3121   36.27489  36.275555 36.30754  36.32234
 36.395123 36.394283 36.368576 36.376648 36.406044 36.405838 36.38826
 36.37443  36.35336  36.319122 36.30414  36.32721  36.308212 36.262497
 36.24773  36.269608 36.3565   36.36192  36.331516 36.357693 36.627304
 36.648033 36.664917 36.641205 36.568356 36.550613 36.526146 36.41668
 36.34836  36.213013 36.120087 36.113735 36.047253 35.949665 35.900608
 35.93714  35.91409  35.85586  35.793343 35.79256  35.831814 35.84504
 35.86832  35.874046 35.933025 36.003532 36.11312  36.200706 36.24088
 36.29694  36.3137   36.375954 36.43787  36.508377 36.59988  36.638676
 36.65698  36.69261  36.71203  36.81394  36.92002  36.888863 36.91397
 36.96665  37.03308  37.061142 37.066784 37.109962 37.166492 37.208416
 37.216194 37.255184 37.273857 37.312473 37.375298 37.468548 37.600952
 37.649403 37.683987 37.76635  37.697136 37.704884 37.67067  37.647804
 37.626682]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 3.568542242050171
Epoch: 1, Steps: 79 | Train Loss: 1.0914166 Vali Loss: 0.9349639 Test Loss: 0.8580631
Validation loss decreased (inf --> 0.934964).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 3.5068657398223877
Epoch: 2, Steps: 79 | Train Loss: 0.9097607 Vali Loss: 0.7250634 Test Loss: 0.6768885
Validation loss decreased (0.934964 --> 0.725063).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 3.7193527221679688
Epoch: 3, Steps: 79 | Train Loss: 0.7431105 Vali Loss: 0.5815555 Test Loss: 0.5298349
Validation loss decreased (0.725063 --> 0.581555).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 3.531028985977173
Epoch: 4, Steps: 79 | Train Loss: 0.6135986 Vali Loss: 0.4171117 Test Loss: 0.3962603
Validation loss decreased (0.581555 --> 0.417112).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 3.6796438694000244
Epoch: 5, Steps: 79 | Train Loss: 0.4937891 Vali Loss: 0.2912040 Test Loss: 0.2842317
Validation loss decreased (0.417112 --> 0.291204).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 3.5709307193756104
Epoch: 6, Steps: 79 | Train Loss: 0.4082399 Vali Loss: 0.2451507 Test Loss: 0.2381699
Validation loss decreased (0.291204 --> 0.245151).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 3.5517749786376953
Epoch: 7, Steps: 79 | Train Loss: 0.3576640 Vali Loss: 0.2064856 Test Loss: 0.2124962
Validation loss decreased (0.245151 --> 0.206486).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 3.64654803276062
Epoch: 8, Steps: 79 | Train Loss: 0.3197544 Vali Loss: 0.1783214 Test Loss: 0.1901491
Validation loss decreased (0.206486 --> 0.178321).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 3.610506057739258
Epoch: 9, Steps: 79 | Train Loss: 0.2920429 Vali Loss: 0.1612408 Test Loss: 0.1779476
Validation loss decreased (0.178321 --> 0.161241).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 3.735365390777588
Epoch: 10, Steps: 79 | Train Loss: 0.2705354 Vali Loss: 0.1546751 Test Loss: 0.1699175
Validation loss decreased (0.161241 --> 0.154675).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 3.5553743839263916
Epoch: 11, Steps: 79 | Train Loss: 0.2580288 Vali Loss: 0.1591821 Test Loss: 0.1708601
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 3.906231641769409
Epoch: 12, Steps: 79 | Train Loss: 0.2480905 Vali Loss: 0.1503571 Test Loss: 0.1595551
Validation loss decreased (0.154675 --> 0.150357).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 3.6447970867156982
Epoch: 13, Steps: 79 | Train Loss: 0.2391741 Vali Loss: 0.1428638 Test Loss: 0.1535413
Validation loss decreased (0.150357 --> 0.142864).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 3.7725987434387207
Epoch: 14, Steps: 79 | Train Loss: 0.2292736 Vali Loss: 0.1510676 Test Loss: 0.1520915
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 3.550523281097412
Epoch: 15, Steps: 79 | Train Loss: 0.2216228 Vali Loss: 0.1389261 Test Loss: 0.1474978
Validation loss decreased (0.142864 --> 0.138926).  Saving model ...
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 3.6370842456817627
Epoch: 16, Steps: 79 | Train Loss: 0.2123396 Vali Loss: 0.1340446 Test Loss: 0.1428105
Validation loss decreased (0.138926 --> 0.134045).  Saving model ...
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 3.6548163890838623
Epoch: 17, Steps: 79 | Train Loss: 0.2064113 Vali Loss: 0.1432128 Test Loss: 0.1417043
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 3.7083733081817627
Epoch: 18, Steps: 79 | Train Loss: 0.2018832 Vali Loss: 0.1317578 Test Loss: 0.1368956
Validation loss decreased (0.134045 --> 0.131758).  Saving model ...
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 3.6439123153686523
Epoch: 19, Steps: 79 | Train Loss: 0.1965060 Vali Loss: 0.1302695 Test Loss: 0.1335438
Validation loss decreased (0.131758 --> 0.130270).  Saving model ...
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 3.765179395675659
Epoch: 20, Steps: 79 | Train Loss: 0.1923470 Vali Loss: 0.1266541 Test Loss: 0.1319760
Validation loss decreased (0.130270 --> 0.126654).  Saving model ...
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 3.833252429962158
Epoch: 21, Steps: 79 | Train Loss: 0.1875314 Vali Loss: 0.1285593 Test Loss: 0.1330747
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 3.9609477519989014
Epoch: 22, Steps: 79 | Train Loss: 0.1858615 Vali Loss: 0.1275328 Test Loss: 0.1281248
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 3.6397340297698975
Epoch: 23, Steps: 79 | Train Loss: 0.1845504 Vali Loss: 0.1259113 Test Loss: 0.1292545
Validation loss decreased (0.126654 --> 0.125911).  Saving model ...
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 3.6173298358917236
Epoch: 24, Steps: 79 | Train Loss: 0.1800629 Vali Loss: 0.1268620 Test Loss: 0.1308741
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 3.681689500808716
Epoch: 25, Steps: 79 | Train Loss: 0.1769862 Vali Loss: 0.1294317 Test Loss: 0.1260871
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 3.66736102104187
Epoch: 26, Steps: 79 | Train Loss: 0.1743761 Vali Loss: 0.1192082 Test Loss: 0.1270689
Validation loss decreased (0.125911 --> 0.119208).  Saving model ...
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 3.5689005851745605
Epoch: 27, Steps: 79 | Train Loss: 0.1752264 Vali Loss: 0.1218147 Test Loss: 0.1246553
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 3.80926513671875
Epoch: 28, Steps: 79 | Train Loss: 0.1704530 Vali Loss: 0.1175668 Test Loss: 0.1248772
Validation loss decreased (0.119208 --> 0.117567).  Saving model ...
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 3.6244802474975586
Epoch: 29, Steps: 79 | Train Loss: 0.1695176 Vali Loss: 0.1265068 Test Loss: 0.1223953
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 3.5616626739501953
Epoch: 30, Steps: 79 | Train Loss: 0.1669499 Vali Loss: 0.1210216 Test Loss: 0.1232176
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 3.502333164215088
Epoch: 31, Steps: 79 | Train Loss: 0.1641101 Vali Loss: 0.1210495 Test Loss: 0.1280222
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 3.597592830657959
Epoch: 32, Steps: 79 | Train Loss: 0.1656510 Vali Loss: 0.1196102 Test Loss: 0.1200413
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 3.564364433288574
Epoch: 33, Steps: 79 | Train Loss: 0.1648200 Vali Loss: 0.1244753 Test Loss: 0.1197467
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 3.612710952758789
Epoch: 34, Steps: 79 | Train Loss: 0.1621875 Vali Loss: 0.1180087 Test Loss: 0.1227542
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 3.62142276763916
Epoch: 35, Steps: 79 | Train Loss: 0.1595664 Vali Loss: 0.1161944 Test Loss: 0.1195218
Validation loss decreased (0.117567 --> 0.116194).  Saving model ...
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 3.5003225803375244
Epoch: 36, Steps: 79 | Train Loss: 0.1595713 Vali Loss: 0.1189131 Test Loss: 0.1203621
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 3.6960701942443848
Epoch: 37, Steps: 79 | Train Loss: 0.1597262 Vali Loss: 0.1210596 Test Loss: 0.1196732
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 3.601963758468628
Epoch: 38, Steps: 79 | Train Loss: 0.1572975 Vali Loss: 0.1195877 Test Loss: 0.1188745
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 3.5454812049865723
Epoch: 39, Steps: 79 | Train Loss: 0.1551405 Vali Loss: 0.1144942 Test Loss: 0.1169452
Validation loss decreased (0.116194 --> 0.114494).  Saving model ...
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 3.5646660327911377
Epoch: 40, Steps: 79 | Train Loss: 0.1544222 Vali Loss: 0.1236079 Test Loss: 0.1184548
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 3.6131300926208496
Epoch: 41, Steps: 79 | Train Loss: 0.1532033 Vali Loss: 0.1205494 Test Loss: 0.1177103
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009402
Epoch: 42 cost time: 3.9403278827667236
Epoch: 42, Steps: 79 | Train Loss: 0.1517056 Vali Loss: 0.1187062 Test Loss: 0.1159960
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009291
Epoch: 43 cost time: 3.504178524017334
Epoch: 43, Steps: 79 | Train Loss: 0.1505726 Vali Loss: 0.1177663 Test Loss: 0.1163165
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009171
Epoch: 44 cost time: 3.6218464374542236
Epoch: 44, Steps: 79 | Train Loss: 0.1506953 Vali Loss: 0.1180396 Test Loss: 0.1175587
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009043
Epoch: 45 cost time: 3.721700668334961
Epoch: 45, Steps: 79 | Train Loss: 0.1491744 Vali Loss: 0.1185307 Test Loss: 0.1167311
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0008907
Epoch: 46 cost time: 3.832345724105835
Epoch: 46, Steps: 79 | Train Loss: 0.1478310 Vali Loss: 0.1190510 Test Loss: 0.1157504
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0008763
Epoch: 47 cost time: 3.5524163246154785
Epoch: 47, Steps: 79 | Train Loss: 0.1466385 Vali Loss: 0.1214540 Test Loss: 0.1185695
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0008612
Epoch: 48 cost time: 3.7167985439300537
Epoch: 48, Steps: 79 | Train Loss: 0.1469947 Vali Loss: 0.1176458 Test Loss: 0.1189999
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0008453
Epoch: 49 cost time: 3.6518046855926514
Epoch: 49, Steps: 79 | Train Loss: 0.1461603 Vali Loss: 0.1195212 Test Loss: 0.1165064
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0008288
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.5727773308753967, mae:0.5738809108734131, rmse:0.756820559501648, mape:0.020624933764338493, mspe:0.0007526316330768168, rse:0.52164626121521, r2_score:0.6303448214752617, acc:0.9793750662356615
corr: [36.8938   36.865334 36.795467 36.87945  36.633556 36.428566 36.24572
 36.136288 36.053127 35.83667  35.614956 35.487602 35.34891  35.25814
 35.129704 35.013012 35.011734 35.004574 35.26771  35.26106  35.161304
 35.198227 35.2551   35.30482  35.2615   35.289536 35.29893  35.37649
 35.232384 35.277843 35.229065 35.055035 34.992924 34.955616 34.919575
 35.02042  35.134617 35.074856 35.27003  35.133198 34.97446  34.963257
 34.880398 34.920906 34.94953  34.902863 35.00803  35.18854  35.225647
 35.355747 35.496746 35.567127 35.58614  35.45065  35.437485 35.467518
 35.51254  35.500664 35.421814 35.373516 35.378304 35.27532  35.159004
 35.126236 34.973186 34.95107  34.866665 34.849483 34.782227 34.845127
 34.844738 34.85945  34.850296 36.738804 36.678505 36.59917  36.56001
 36.265625 36.064644 35.840645 35.740604 35.65014  35.49049  35.2731
 35.16937  35.089077 34.929554 34.773582 34.678837 34.66917  34.62404
 34.694702 34.58909  34.477825 34.506668 34.573696 34.50318  34.427174
 34.476086 34.48829  34.52794  34.39908  34.42147  34.34952  34.16304
 34.163425 34.05664  34.09749  34.10833  34.126434 34.024933 34.22543
 34.03333  33.912037 33.89313  33.71928  33.79689  33.86588  33.959637
 34.116146 34.263905 34.349274 34.45546  34.60807  34.70258  34.760506
 34.679134 34.68438  34.69945  34.66966  34.713673 34.675323 34.539375
 34.526474 34.36507  34.23478  34.122417 33.842205 33.806652 33.81981
 33.86727  33.800808 33.810226 33.88727  33.839893 33.878586 35.546577
 35.487595 35.415462 35.438457 35.214344 35.00265  34.791992 34.612602
 34.519005 34.28663  34.063553 34.126007 34.067955 34.009327 34.036686
 34.002323 33.98104  34.00139  34.204563 34.174225 34.070488 34.09595
 34.172295 34.167877 34.150955 34.14468  34.174828 34.280582 34.25263
 34.310448 34.377415 34.363342 34.27072  34.09522  34.034435 34.029884
 34.083214 33.922253 33.942833 33.750668 33.640728 33.639694 33.507973
 33.478836 33.451324 33.456112 33.51234  33.60536  33.649925 33.755657
 33.907154 33.947056 34.01902  34.048214 34.172558 34.20528  34.291904
 34.281593 34.248146 34.14901  34.142254 34.034313 33.814323 33.646706
 33.414516 33.51594  33.486134 33.420322 33.32201  33.213383 33.218464
 33.309902 33.2733   34.945087 34.958767 34.864162 34.926434 34.87868
 34.715942 34.541668 34.441357 34.36854  34.23433  34.114388 34.168716
 34.07831  33.95511  33.873924 33.867874 33.85966  33.852795 34.016796
 34.071033 33.99932  34.06262  34.02788  34.008453 33.96622  33.982597
 33.945786 33.98536  33.93328  34.044376 34.01742  33.77077  33.733887
 33.666252 33.584015 33.60079  33.617584 33.552235 33.73954  33.61584
 33.53032  33.513626 33.42034  33.53849  33.566647 33.582516 33.69725
 33.847874 33.995697 34.093254 34.21793  34.304844 34.34268  34.224503
 34.194126 34.152824 34.078922 33.973995 33.85294  33.742607 33.79206
 33.618027 33.49397  33.44772  33.318    33.32425  33.36909  33.41794
 33.29046  33.347145 33.292816 33.303917 33.31273  35.255245 35.227516
 35.17946  35.273224 35.17602  35.050873 34.91531  34.752666 34.668335
 34.566055 34.31438  34.25854  34.167885 34.120567 34.16348  34.12736
 34.088547 34.154305 34.428535 34.52019  34.56805  34.606224 34.663876
 34.686516 34.666733 34.722843 34.779194 34.888054 34.808125 34.747364
 34.68363  34.59114  34.610817 34.43548  34.36372  34.333824 34.398636
 34.26757  34.37696  34.220314 34.125195 34.106224 33.980865 34.043007
 34.071484 34.13016  34.24334  34.38291  34.47691  34.55942  34.70622
 34.798717 34.899345 34.858406 34.972355 35.030266 35.089237 35.051193
 34.972588 34.899258 34.928974 34.822044 34.725826 34.639828 34.354923
 34.3003   34.276867 34.235523 34.12818  34.191288 34.229923 34.22648
 34.08429 ]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 3.6402390003204346
Epoch: 1, Steps: 79 | Train Loss: 0.8955170 Vali Loss: 0.7590708 Test Loss: 0.6937062
Validation loss decreased (inf --> 0.759071).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 3.572920322418213
Epoch: 2, Steps: 79 | Train Loss: 0.7630342 Vali Loss: 0.6250384 Test Loss: 0.5753955
Validation loss decreased (0.759071 --> 0.625038).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 3.534245729446411
Epoch: 3, Steps: 79 | Train Loss: 0.6494603 Vali Loss: 0.5064922 Test Loss: 0.4700043
Validation loss decreased (0.625038 --> 0.506492).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 3.550217866897583
Epoch: 4, Steps: 79 | Train Loss: 0.5282738 Vali Loss: 0.3390649 Test Loss: 0.3267966
Validation loss decreased (0.506492 --> 0.339065).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 3.6486783027648926
Epoch: 5, Steps: 79 | Train Loss: 0.4043639 Vali Loss: 0.2351328 Test Loss: 0.2315439
Validation loss decreased (0.339065 --> 0.235133).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 3.665395498275757
Epoch: 6, Steps: 79 | Train Loss: 0.3369948 Vali Loss: 0.1854417 Test Loss: 0.1932356
Validation loss decreased (0.235133 --> 0.185442).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 3.6385364532470703
Epoch: 7, Steps: 79 | Train Loss: 0.2968120 Vali Loss: 0.1635516 Test Loss: 0.1759062
Validation loss decreased (0.185442 --> 0.163552).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 3.5696728229522705
Epoch: 8, Steps: 79 | Train Loss: 0.2652840 Vali Loss: 0.1424735 Test Loss: 0.1612057
Validation loss decreased (0.163552 --> 0.142474).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 3.710021734237671
Epoch: 9, Steps: 79 | Train Loss: 0.2434995 Vali Loss: 0.1424141 Test Loss: 0.1546857
Validation loss decreased (0.142474 --> 0.142414).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 3.551520347595215
Epoch: 10, Steps: 79 | Train Loss: 0.2294932 Vali Loss: 0.1348548 Test Loss: 0.1491758
Validation loss decreased (0.142414 --> 0.134855).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 3.658677816390991
Epoch: 11, Steps: 79 | Train Loss: 0.2165317 Vali Loss: 0.1310349 Test Loss: 0.1450951
Validation loss decreased (0.134855 --> 0.131035).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 3.5814404487609863
Epoch: 12, Steps: 79 | Train Loss: 0.2062984 Vali Loss: 0.1252199 Test Loss: 0.1415608
Validation loss decreased (0.131035 --> 0.125220).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 3.567711353302002
Epoch: 13, Steps: 79 | Train Loss: 0.1956729 Vali Loss: 0.1313507 Test Loss: 0.1396891
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 3.663796901702881
Epoch: 14, Steps: 79 | Train Loss: 0.1915665 Vali Loss: 0.1281114 Test Loss: 0.1371135
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 3.6637179851531982
Epoch: 15, Steps: 79 | Train Loss: 0.1856102 Vali Loss: 0.1168067 Test Loss: 0.1282536
Validation loss decreased (0.125220 --> 0.116807).  Saving model ...
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 3.6472415924072266
Epoch: 16, Steps: 79 | Train Loss: 0.1795675 Vali Loss: 0.1131463 Test Loss: 0.1241687
Validation loss decreased (0.116807 --> 0.113146).  Saving model ...
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 3.6648175716400146
Epoch: 17, Steps: 79 | Train Loss: 0.1768572 Vali Loss: 0.1086644 Test Loss: 0.1202569
Validation loss decreased (0.113146 --> 0.108664).  Saving model ...
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 3.6936261653900146
Epoch: 18, Steps: 79 | Train Loss: 0.1749652 Vali Loss: 0.1121238 Test Loss: 0.1204497
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 3.690709352493286
Epoch: 19, Steps: 79 | Train Loss: 0.1674812 Vali Loss: 0.1060467 Test Loss: 0.1139852
Validation loss decreased (0.108664 --> 0.106047).  Saving model ...
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 3.4422435760498047
Epoch: 20, Steps: 79 | Train Loss: 0.1645314 Vali Loss: 0.1079969 Test Loss: 0.1149163
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 3.547685146331787
Epoch: 21, Steps: 79 | Train Loss: 0.1591134 Vali Loss: 0.1058423 Test Loss: 0.1120865
Validation loss decreased (0.106047 --> 0.105842).  Saving model ...
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 3.7999930381774902
Epoch: 22, Steps: 79 | Train Loss: 0.1569271 Vali Loss: 0.1017681 Test Loss: 0.1110759
Validation loss decreased (0.105842 --> 0.101768).  Saving model ...
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 3.85956072807312
Epoch: 23, Steps: 79 | Train Loss: 0.1545239 Vali Loss: 0.1052362 Test Loss: 0.1086449
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 3.9540936946868896
Epoch: 24, Steps: 79 | Train Loss: 0.1523794 Vali Loss: 0.1029655 Test Loss: 0.1099272
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 3.6614227294921875
Epoch: 25, Steps: 79 | Train Loss: 0.1507545 Vali Loss: 0.1029081 Test Loss: 0.1082002
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 3.519925355911255
Epoch: 26, Steps: 79 | Train Loss: 0.1496670 Vali Loss: 0.1030979 Test Loss: 0.1086663
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 3.670957088470459
Epoch: 27, Steps: 79 | Train Loss: 0.1449880 Vali Loss: 0.1007072 Test Loss: 0.1053787
Validation loss decreased (0.101768 --> 0.100707).  Saving model ...
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 3.953988552093506
Epoch: 28, Steps: 79 | Train Loss: 0.1431924 Vali Loss: 0.1012613 Test Loss: 0.1080241
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 3.688436508178711
Epoch: 29, Steps: 79 | Train Loss: 0.1433861 Vali Loss: 0.1007904 Test Loss: 0.1034962
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 3.611023187637329
Epoch: 30, Steps: 79 | Train Loss: 0.1428779 Vali Loss: 0.0989755 Test Loss: 0.1069161
Validation loss decreased (0.100707 --> 0.098975).  Saving model ...
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 3.5617058277130127
Epoch: 31, Steps: 79 | Train Loss: 0.1395954 Vali Loss: 0.0975127 Test Loss: 0.1039041
Validation loss decreased (0.098975 --> 0.097513).  Saving model ...
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 3.617220163345337
Epoch: 32, Steps: 79 | Train Loss: 0.1363724 Vali Loss: 0.1047705 Test Loss: 0.1058826
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 3.518578290939331
Epoch: 33, Steps: 79 | Train Loss: 0.1367554 Vali Loss: 0.1006571 Test Loss: 0.1014066
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 3.7339115142822266
Epoch: 34, Steps: 79 | Train Loss: 0.1345202 Vali Loss: 0.1022640 Test Loss: 0.1066892
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 3.6038718223571777
Epoch: 35, Steps: 79 | Train Loss: 0.1327958 Vali Loss: 0.0971234 Test Loss: 0.1029007
Validation loss decreased (0.097513 --> 0.097123).  Saving model ...
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 3.6343905925750732
Epoch: 36, Steps: 79 | Train Loss: 0.1313938 Vali Loss: 0.0972955 Test Loss: 0.0996200
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 3.7433998584747314
Epoch: 37, Steps: 79 | Train Loss: 0.1314758 Vali Loss: 0.1046929 Test Loss: 0.1042205
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 3.823601245880127
Epoch: 38, Steps: 79 | Train Loss: 0.1315778 Vali Loss: 0.0926307 Test Loss: 0.1029973
Validation loss decreased (0.097123 --> 0.092631).  Saving model ...
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 3.8408498764038086
Epoch: 39, Steps: 79 | Train Loss: 0.1288062 Vali Loss: 0.0966216 Test Loss: 0.0988951
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 3.6142234802246094
Epoch: 40, Steps: 79 | Train Loss: 0.1280133 Vali Loss: 0.0963261 Test Loss: 0.0996836
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 3.8068714141845703
Epoch: 41, Steps: 79 | Train Loss: 0.1271547 Vali Loss: 0.0917215 Test Loss: 0.0999957
Validation loss decreased (0.092631 --> 0.091722).  Saving model ...
Adjusting learning rate to: 0.0009402
Epoch: 42 cost time: 3.70210599899292
Epoch: 42, Steps: 79 | Train Loss: 0.1265812 Vali Loss: 0.0943689 Test Loss: 0.1001974
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009291
Epoch: 43 cost time: 3.9495136737823486
Epoch: 43, Steps: 79 | Train Loss: 0.1247620 Vali Loss: 0.0983033 Test Loss: 0.0975796
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009171
Epoch: 44 cost time: 3.960797071456909
Epoch: 44, Steps: 79 | Train Loss: 0.1256938 Vali Loss: 0.0985335 Test Loss: 0.1002512
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009043
Epoch: 45 cost time: 3.8251993656158447
Epoch: 45, Steps: 79 | Train Loss: 0.1255771 Vali Loss: 0.0983216 Test Loss: 0.0982606
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0008907
Epoch: 46 cost time: 4.000813722610474
Epoch: 46, Steps: 79 | Train Loss: 0.1209148 Vali Loss: 0.0938503 Test Loss: 0.0989154
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0008763
Epoch: 47 cost time: 3.8489909172058105
Epoch: 47, Steps: 79 | Train Loss: 0.1220451 Vali Loss: 0.0956322 Test Loss: 0.0962374
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0008612
Epoch: 48 cost time: 3.6660549640655518
Epoch: 48, Steps: 79 | Train Loss: 0.1233098 Vali Loss: 0.0923803 Test Loss: 0.0982994
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0008453
Epoch: 49 cost time: 3.8620128631591797
Epoch: 49, Steps: 79 | Train Loss: 0.1235598 Vali Loss: 0.0937767 Test Loss: 0.0982081
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0008288
Epoch: 50 cost time: 3.6897683143615723
Epoch: 50, Steps: 79 | Train Loss: 0.1198460 Vali Loss: 0.0940342 Test Loss: 0.0995279
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0008115
Epoch: 51 cost time: 3.9406368732452393
Epoch: 51, Steps: 79 | Train Loss: 0.1191788 Vali Loss: 0.0989373 Test Loss: 0.1000328
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0007937
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.29473572969436646, mae:0.4202299118041992, rmse:0.5428956747055054, mape:0.015025141648948193, mspe:0.0003815698146354407, rse:0.3741963803768158, r2_score:0.842502097876266, acc:0.9849748583510518
corr: [38.551388 38.458263 38.451965 38.43631  38.396946 38.35092  38.314037
 38.302055 38.268917 38.221657 38.172855 38.147137 38.101288 38.007317
 37.98525  37.9803   37.99562  37.974678 37.955475 37.926044 37.932663
 37.941887 37.93062  37.900795 37.86596  37.84909  37.77433  37.743767
 37.754627 37.761112 37.801807 37.783497 37.77022  37.772636 37.763374
 37.73029  37.733902 37.724274 37.66566  37.62193  37.586224 37.58564
 37.57339  37.561966 37.516296 37.478848 37.41582  37.387363 37.314426
 37.240288 37.233467 37.191505 37.22222  37.153698 37.139454 37.148643
 37.10961  37.075108 37.045982 37.11098  37.1227   37.12759  37.12671
 37.105766 37.061005 37.068714 37.06772  37.070267 37.05557  37.044373
 36.971653 36.94278  36.99857  36.954395 36.915356 36.922203 36.863644
 36.91736  36.876934 36.897907 36.923904 36.951366 37.01235  37.036003
 37.052727 37.009743 36.969376 36.944786 36.912743 36.909454 36.917774
 37.847775 37.810005 37.82546  37.83492  37.771126 37.71298  37.678192
 37.635445 37.593105 37.55745  37.48439  37.4688   37.38914  37.28102
 37.228634 37.2075   37.174824 37.131115 37.088078 37.04313  36.989304
 36.963055 36.91392  36.86217  36.81628  36.774067 36.6839   36.651302
 36.64142  36.635334 36.627403 36.568336 36.54809  36.54044  36.519497
 36.47482  36.43327  36.366566 36.25626  36.244526 36.193363 36.158943
 36.14416  36.10797  36.0023   35.96925  35.929203 35.91029  35.862453
 35.804905 35.797707 35.787113 35.858707 35.83458  35.80788  35.839622
 35.816185 35.770058 35.733223 35.754917 35.73916  35.721798 35.692505
 35.677437 35.638084 35.63837  35.690655 35.65693  35.624687 35.6264
 35.57428  35.55957  35.64489  35.63713  35.616886 35.65247  35.63564
 35.693245 35.654778 35.62853  35.673466 35.737606 35.819386 35.897274
 35.954247 35.993355 35.9782   35.985794 35.99624  36.034885 36.09184
 36.86285  36.842407 36.8836   36.88211  36.87543  36.867012 36.877686
 36.859715 36.82666  36.805294 36.736923 36.69129  36.67204  36.617428
 36.590508 36.566933 36.568253 36.52945  36.50456  36.481297 36.472965
 36.483543 36.482742 36.453102 36.44536  36.422516 36.35225  36.36605
 36.402786 36.407516 36.40818  36.39345  36.4135   36.424656 36.40377
 36.40284  36.38158  36.393284 36.32044  36.285027 36.226357 36.198257
 36.135326 36.10924  36.03442  35.974304 35.92172  35.906734 35.836945
 35.792625 35.82146  35.829918 35.874863 35.842216 35.860794 35.86242
 35.841957 35.81979  35.80203  35.87206  35.86838  35.838234 35.810368
 35.76675  35.714085 35.70969  35.76657  35.77113  35.735718 35.775402
 35.722496 35.689938 35.77993  35.745068 35.718155 35.757507 35.738262
 35.781166 35.707466 35.74115  35.814915 35.885456 35.97169  36.021694
 36.100033 36.127323 36.10821  36.107204 36.15331  36.1746   36.18966
 36.938133 36.8931   36.927784 36.91981  36.862705 36.81868  36.776356
 36.739372 36.71306  36.68979  36.627033 36.60065  36.566963 36.53488
 36.519943 36.48275  36.4655   36.441322 36.44733  36.443596 36.460754
 36.504982 36.49795  36.4814   36.497402 36.50622  36.459858 36.468533
 36.50175  36.543777 36.586296 36.56041  36.59483  36.57902  36.557545
 36.52     36.49008  36.472645 36.394653 36.35932  36.330067 36.311436
 36.268032 36.23383  36.188087 36.157967 36.12123  36.096764 36.041172
 36.03935  36.079784 36.08043  36.135723 36.098312 36.086533 36.088898
 36.085064 36.03471  35.996216 36.045177 36.045418 36.053547 36.081646
 36.060806 36.031116 36.035336 36.100075 36.120354 36.11652  36.17962
 36.143677 36.161205 36.247383 36.22765  36.271732 36.34391  36.350075
 36.45951  36.452015 36.48576  36.5817   36.68006  36.790386 36.91277
 37.009335 37.060925 37.100914 37.129333 37.15255  37.211216 37.255417
 36.06686 ]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 3.8065404891967773
Epoch: 1, Steps: 79 | Train Loss: 0.8385543 Vali Loss: 0.7120972 Test Loss: 0.6668320
Validation loss decreased (inf --> 0.712097).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 3.978419303894043
Epoch: 2, Steps: 79 | Train Loss: 0.7240826 Vali Loss: 0.6052971 Test Loss: 0.5659966
Validation loss decreased (0.712097 --> 0.605297).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 3.7539963722229004
Epoch: 3, Steps: 79 | Train Loss: 0.6219285 Vali Loss: 0.5228911 Test Loss: 0.4720789
Validation loss decreased (0.605297 --> 0.522891).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 4.010915994644165
Epoch: 4, Steps: 79 | Train Loss: 0.5420911 Vali Loss: 0.3899040 Test Loss: 0.3519238
Validation loss decreased (0.522891 --> 0.389904).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 4.07635760307312
Epoch: 5, Steps: 79 | Train Loss: 0.4165778 Vali Loss: 0.2356813 Test Loss: 0.2272684
Validation loss decreased (0.389904 --> 0.235681).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 3.8844199180603027
Epoch: 6, Steps: 79 | Train Loss: 0.3303246 Vali Loss: 0.1871668 Test Loss: 0.1894958
Validation loss decreased (0.235681 --> 0.187167).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 3.8160243034362793
Epoch: 7, Steps: 79 | Train Loss: 0.2891747 Vali Loss: 0.1595213 Test Loss: 0.1712800
Validation loss decreased (0.187167 --> 0.159521).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 3.728827476501465
Epoch: 8, Steps: 79 | Train Loss: 0.2645580 Vali Loss: 0.1496821 Test Loss: 0.1637387
Validation loss decreased (0.159521 --> 0.149682).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 4.09957218170166
Epoch: 9, Steps: 79 | Train Loss: 0.2430087 Vali Loss: 0.1322230 Test Loss: 0.1517604
Validation loss decreased (0.149682 --> 0.132223).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 3.986996650695801
Epoch: 10, Steps: 79 | Train Loss: 0.2231456 Vali Loss: 0.1348504 Test Loss: 0.1485013
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 3.886659622192383
Epoch: 11, Steps: 79 | Train Loss: 0.2087347 Vali Loss: 0.1311393 Test Loss: 0.1401607
Validation loss decreased (0.132223 --> 0.131139).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 3.633793354034424
Epoch: 12, Steps: 79 | Train Loss: 0.2007294 Vali Loss: 0.1249076 Test Loss: 0.1398048
Validation loss decreased (0.131139 --> 0.124908).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 3.870438575744629
Epoch: 13, Steps: 79 | Train Loss: 0.1922631 Vali Loss: 0.1227919 Test Loss: 0.1335448
Validation loss decreased (0.124908 --> 0.122792).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 3.8390421867370605
Epoch: 14, Steps: 79 | Train Loss: 0.1895307 Vali Loss: 0.1181978 Test Loss: 0.1370071
Validation loss decreased (0.122792 --> 0.118198).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 3.8379952907562256
Epoch: 15, Steps: 79 | Train Loss: 0.1836508 Vali Loss: 0.1184538 Test Loss: 0.1303457
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 3.7390716075897217
Epoch: 16, Steps: 79 | Train Loss: 0.1807268 Vali Loss: 0.1170606 Test Loss: 0.1308025
Validation loss decreased (0.118198 --> 0.117061).  Saving model ...
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 3.9317610263824463
Epoch: 17, Steps: 79 | Train Loss: 0.1769264 Vali Loss: 0.1126877 Test Loss: 0.1301161
Validation loss decreased (0.117061 --> 0.112688).  Saving model ...
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 3.956486225128174
Epoch: 18, Steps: 79 | Train Loss: 0.1724553 Vali Loss: 0.1103553 Test Loss: 0.1241235
Validation loss decreased (0.112688 --> 0.110355).  Saving model ...
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 4.1491076946258545
Epoch: 19, Steps: 79 | Train Loss: 0.1686268 Vali Loss: 0.1081987 Test Loss: 0.1261524
Validation loss decreased (0.110355 --> 0.108199).  Saving model ...
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 3.690950393676758
Epoch: 20, Steps: 79 | Train Loss: 0.1657706 Vali Loss: 0.1077119 Test Loss: 0.1208800
Validation loss decreased (0.108199 --> 0.107712).  Saving model ...
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 3.6427509784698486
Epoch: 21, Steps: 79 | Train Loss: 0.1628772 Vali Loss: 0.1079485 Test Loss: 0.1164739
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 3.799508810043335
Epoch: 22, Steps: 79 | Train Loss: 0.1602075 Vali Loss: 0.1090749 Test Loss: 0.1146845
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 3.786661386489868
Epoch: 23, Steps: 79 | Train Loss: 0.1585489 Vali Loss: 0.1050922 Test Loss: 0.1170191
Validation loss decreased (0.107712 --> 0.105092).  Saving model ...
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 3.753878116607666
Epoch: 24, Steps: 79 | Train Loss: 0.1567404 Vali Loss: 0.1060885 Test Loss: 0.1123983
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 3.626260280609131
Epoch: 25, Steps: 79 | Train Loss: 0.1542917 Vali Loss: 0.1051550 Test Loss: 0.1107970
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 4.056257009506226
Epoch: 26, Steps: 79 | Train Loss: 0.1516631 Vali Loss: 0.1033411 Test Loss: 0.1097089
Validation loss decreased (0.105092 --> 0.103341).  Saving model ...
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 3.989849328994751
Epoch: 27, Steps: 79 | Train Loss: 0.1503422 Vali Loss: 0.0979073 Test Loss: 0.1101958
Validation loss decreased (0.103341 --> 0.097907).  Saving model ...
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 3.840426445007324
Epoch: 28, Steps: 79 | Train Loss: 0.1466806 Vali Loss: 0.0973715 Test Loss: 0.1074035
Validation loss decreased (0.097907 --> 0.097371).  Saving model ...
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 3.9312288761138916
Epoch: 29, Steps: 79 | Train Loss: 0.1455437 Vali Loss: 0.0976435 Test Loss: 0.1097417
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 3.8226521015167236
Epoch: 30, Steps: 79 | Train Loss: 0.1434000 Vali Loss: 0.0997215 Test Loss: 0.1054191
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 3.929110527038574
Epoch: 31, Steps: 79 | Train Loss: 0.1401510 Vali Loss: 0.0951014 Test Loss: 0.1060339
Validation loss decreased (0.097371 --> 0.095101).  Saving model ...
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 3.9231865406036377
Epoch: 32, Steps: 79 | Train Loss: 0.1394260 Vali Loss: 0.0934155 Test Loss: 0.1043167
Validation loss decreased (0.095101 --> 0.093415).  Saving model ...
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 3.813063383102417
Epoch: 33, Steps: 79 | Train Loss: 0.1359499 Vali Loss: 0.0953491 Test Loss: 0.1018255
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 3.778003454208374
Epoch: 34, Steps: 79 | Train Loss: 0.1347212 Vali Loss: 0.0945584 Test Loss: 0.1016059
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 3.7305939197540283
Epoch: 35, Steps: 79 | Train Loss: 0.1346781 Vali Loss: 0.0967945 Test Loss: 0.1015316
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 3.8087549209594727
Epoch: 36, Steps: 79 | Train Loss: 0.1340564 Vali Loss: 0.0993435 Test Loss: 0.1023648
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 3.8375043869018555
Epoch: 37, Steps: 79 | Train Loss: 0.1319221 Vali Loss: 0.0982654 Test Loss: 0.1024545
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 3.9065613746643066
Epoch: 38, Steps: 79 | Train Loss: 0.1313323 Vali Loss: 0.0961938 Test Loss: 0.1013806
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 3.87129282951355
Epoch: 39, Steps: 79 | Train Loss: 0.1305118 Vali Loss: 0.0960655 Test Loss: 0.1004604
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 3.709423065185547
Epoch: 40, Steps: 79 | Train Loss: 0.1286507 Vali Loss: 0.1001854 Test Loss: 0.1040606
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 3.739617109298706
Epoch: 41, Steps: 79 | Train Loss: 0.1269861 Vali Loss: 0.0952547 Test Loss: 0.0995181
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0009402
Epoch: 42 cost time: 4.041970252990723
Epoch: 42, Steps: 79 | Train Loss: 0.1266361 Vali Loss: 0.0957275 Test Loss: 0.0992899
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0009291
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.3248107433319092, mae:0.4463086724281311, rmse:0.5699216723442078, mape:0.015964508056640625, mspe:0.00042026941082440317, rse:0.39282432198524475, r2_score:0.8162150387757987, acc:0.9840354919433594
corr: [38.18035  38.22723  38.198326 38.213192 38.13311  38.08129  38.05895
 37.974686 37.927547 38.044674 38.061325 37.93235  37.944157 37.962536
 37.90855  37.92576  37.905846 37.95285  37.92227  37.931892 37.988003
 38.033672 38.033546 38.00513  38.049038 38.12192  38.151905 38.18476
 38.153828 38.143227 38.097523 38.097733 38.14519  38.192257 38.223866
 38.19686  38.236404 38.20374  38.19708  38.178787 38.24932  38.259724
 38.278893 38.277485 38.323406 38.324104 38.31966  38.304787 38.34541
 38.35814  38.336334 38.38246  38.350773 38.35325  38.322174 38.365555
 38.33345  38.33692  38.259987 38.16428  38.194786 38.20045  38.212875
 38.154472 38.05828  38.041153 38.0156   37.932    37.872387 37.85835
 37.80776  37.77583  37.7142   37.643272 37.553043 37.509815 37.525326
 37.484016 37.46405  37.481583 37.412853 37.360752 37.27138  37.172417
 37.13823  37.018257 36.990067 37.0339   37.08869  37.04635  37.01521
 36.97544  36.94324  36.899975 36.88951  36.882637 36.84569  36.811726
 36.74514  36.772312 36.745728 36.771473 36.759716 36.793118 36.801594
 36.83947  36.907146 36.952553 36.95536  36.914204 36.96272  36.93071
 36.961727 36.960754 36.95173  36.93772  36.968224 36.96353  36.94294
 36.902958 36.926414 37.196    37.17109  37.200413 37.184578 37.03533
 36.95617  36.94787  36.84689  36.78054  36.8271   36.80765  36.681225
 36.700047 36.676647 36.632313 36.64137  36.637928 36.63341  36.58534
 36.61085  36.662983 36.705135 36.70892  36.677246 36.72953  36.767162
 36.770866 36.781956 36.759396 36.78253  36.766037 36.75619  36.761852
 36.75912  36.76182  36.72718  36.749546 36.718422 36.688156 36.689095
 36.75633  36.78139  36.815205 36.831043 36.858948 36.888428 36.895245
 36.882576 36.979332 36.998    37.007565 37.027393 37.06503  37.0774
 37.009815 37.043266 37.006245 37.049885 36.976845 36.9141   36.91609
 36.916378 36.984283 36.921146 36.85213  36.85707  36.82133  36.777264
 36.751125 36.76585  36.732994 36.699226 36.628548 36.58624  36.521408
 36.465687 36.40474  36.357964 36.32173  36.307777 36.272743 36.266823
 36.202965 36.150448 36.145435 36.05068  35.99513  36.063675 36.120003
 36.12376  36.131145 36.10606  36.049885 36.006924 35.99877  36.02518
 35.99363  35.97145  35.93582  35.953426 35.95944  35.97296  35.94239
 36.01109  36.02537  36.10446  36.177494 36.21641  36.279034 36.275093
 36.312496 36.26196  36.31776  36.324825 36.34689  36.368477 36.381603
 36.38438  36.334312 36.34068  36.40866  36.502235 36.51744  36.616863
 36.663456 36.569958 36.51527  36.53075  36.476727 36.465195 36.51594
 36.530903 36.45137  36.467358 36.496517 36.46284  36.487015 36.532173
 36.557728 36.53293  36.560722 36.62915  36.677513 36.661175 36.64976
 36.6993   36.73145  36.73826  36.74515  36.74991  36.743973 36.744724
 36.751457 36.786797 36.781506 36.825657 36.827305 36.896904 36.891712
 36.859566 36.870205 36.937374 36.93318  36.970116 36.99669  37.0758
 37.145912 37.16367  37.153717 37.226685 37.248383 37.29224  37.30762
 37.367672 37.38956  37.3483   37.433823 37.374485 37.393967 37.292206
 37.24209  37.24378  37.245537 37.302193 37.27144  37.220886 37.220177
 37.126564 37.115993 37.077877 37.09078  37.049053 37.037876 36.97213
 36.958153 36.89429  36.84971  36.81623  36.809246 36.790077 36.784306
 36.774254 36.767456 36.710922 36.67432  36.640312 36.56652  36.552
 36.617153 36.632366 36.6185   36.58765  36.58765  36.52536  36.534733
 36.55859  36.56337  36.57323  36.549507 36.523636 36.565422 36.551598
 36.61895  36.620132 36.69284  36.723545 36.838726 36.92173  36.9647
 37.05824  37.07908  37.153328 37.155083 37.205776 37.251648 37.31993
 37.330254 37.3748   37.39682  37.400852 37.403946 37.572872 36.07715
 35.900623]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 4.168001413345337
Epoch: 1, Steps: 79 | Train Loss: 0.7597013 Vali Loss: 0.6594287 Test Loss: 0.6065653
Validation loss decreased (inf --> 0.659429).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 3.9550156593322754
Epoch: 2, Steps: 79 | Train Loss: 0.6760835 Vali Loss: 0.6200011 Test Loss: 0.5684483
Validation loss decreased (0.659429 --> 0.620001).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 3.9025840759277344
Epoch: 3, Steps: 79 | Train Loss: 0.6177163 Vali Loss: 0.4801990 Test Loss: 0.4691061
Validation loss decreased (0.620001 --> 0.480199).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 3.883362293243408
Epoch: 4, Steps: 79 | Train Loss: 0.5036731 Vali Loss: 0.3893804 Test Loss: 0.3458382
Validation loss decreased (0.480199 --> 0.389380).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 3.8681232929229736
Epoch: 5, Steps: 79 | Train Loss: 0.3953822 Vali Loss: 0.2366886 Test Loss: 0.2380962
Validation loss decreased (0.389380 --> 0.236689).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 3.97947096824646
Epoch: 6, Steps: 79 | Train Loss: 0.3227837 Vali Loss: 0.2055314 Test Loss: 0.2003772
Validation loss decreased (0.236689 --> 0.205531).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 3.88558292388916
Epoch: 7, Steps: 79 | Train Loss: 0.2819108 Vali Loss: 0.1636078 Test Loss: 0.1713508
Validation loss decreased (0.205531 --> 0.163608).  Saving model ...
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 4.040941476821899
Epoch: 8, Steps: 79 | Train Loss: 0.2536790 Vali Loss: 0.1477726 Test Loss: 0.1552089
Validation loss decreased (0.163608 --> 0.147773).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 3.824101448059082
Epoch: 9, Steps: 79 | Train Loss: 0.2291229 Vali Loss: 0.1362404 Test Loss: 0.1524165
Validation loss decreased (0.147773 --> 0.136240).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 3.8739280700683594
Epoch: 10, Steps: 79 | Train Loss: 0.2127827 Vali Loss: 0.1334540 Test Loss: 0.1447384
Validation loss decreased (0.136240 --> 0.133454).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 3.980759382247925
Epoch: 11, Steps: 79 | Train Loss: 0.2006429 Vali Loss: 0.1308232 Test Loss: 0.1380228
Validation loss decreased (0.133454 --> 0.130823).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 3.7379369735717773
Epoch: 12, Steps: 79 | Train Loss: 0.1924052 Vali Loss: 0.1239190 Test Loss: 0.1333624
Validation loss decreased (0.130823 --> 0.123919).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 3.9160749912261963
Epoch: 13, Steps: 79 | Train Loss: 0.1856688 Vali Loss: 0.1203027 Test Loss: 0.1331847
Validation loss decreased (0.123919 --> 0.120303).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 3.871270179748535
Epoch: 14, Steps: 79 | Train Loss: 0.1802509 Vali Loss: 0.1191162 Test Loss: 0.1331394
Validation loss decreased (0.120303 --> 0.119116).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 4.038655757904053
Epoch: 15, Steps: 79 | Train Loss: 0.1780783 Vali Loss: 0.1197816 Test Loss: 0.1301263
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 4.00630259513855
Epoch: 16, Steps: 79 | Train Loss: 0.1734823 Vali Loss: 0.1129298 Test Loss: 0.1299748
Validation loss decreased (0.119116 --> 0.112930).  Saving model ...
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 4.077998876571655
Epoch: 17, Steps: 79 | Train Loss: 0.1728746 Vali Loss: 0.1158698 Test Loss: 0.1288845
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 3.973942518234253
Epoch: 18, Steps: 79 | Train Loss: 0.1661058 Vali Loss: 0.1106804 Test Loss: 0.1247710
Validation loss decreased (0.112930 --> 0.110680).  Saving model ...
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 3.8067660331726074
Epoch: 19, Steps: 79 | Train Loss: 0.1653789 Vali Loss: 0.1122477 Test Loss: 0.1261756
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 4.036481857299805
Epoch: 20, Steps: 79 | Train Loss: 0.1638080 Vali Loss: 0.1101856 Test Loss: 0.1234901
Validation loss decreased (0.110680 --> 0.110186).  Saving model ...
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 4.127546072006226
Epoch: 21, Steps: 79 | Train Loss: 0.1633697 Vali Loss: 0.1129986 Test Loss: 0.1259403
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 4.009623050689697
Epoch: 22, Steps: 79 | Train Loss: 0.1612061 Vali Loss: 0.1073493 Test Loss: 0.1217000
Validation loss decreased (0.110186 --> 0.107349).  Saving model ...
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 3.8881847858428955
Epoch: 23, Steps: 79 | Train Loss: 0.1592454 Vali Loss: 0.1040487 Test Loss: 0.1205765
Validation loss decreased (0.107349 --> 0.104049).  Saving model ...
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 4.005012035369873
Epoch: 24, Steps: 79 | Train Loss: 0.1566885 Vali Loss: 0.1062628 Test Loss: 0.1226462
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 3.7991583347320557
Epoch: 25, Steps: 79 | Train Loss: 0.1584796 Vali Loss: 0.1074646 Test Loss: 0.1223078
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009360
Epoch: 26 cost time: 4.153400421142578
Epoch: 26, Steps: 79 | Train Loss: 0.1568479 Vali Loss: 0.1059230 Test Loss: 0.1179070
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009587
Epoch: 27 cost time: 3.8282206058502197
Epoch: 27, Steps: 79 | Train Loss: 0.1561582 Vali Loss: 0.1046883 Test Loss: 0.1189829
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009767
Epoch: 28 cost time: 3.9367711544036865
Epoch: 28, Steps: 79 | Train Loss: 0.1514415 Vali Loss: 0.1005082 Test Loss: 0.1165890
Validation loss decreased (0.104049 --> 0.100508).  Saving model ...
Adjusting learning rate to: 0.0009896
Epoch: 29 cost time: 4.101694583892822
Epoch: 29, Steps: 79 | Train Loss: 0.1523513 Vali Loss: 0.1000496 Test Loss: 0.1167413
Validation loss decreased (0.100508 --> 0.100050).  Saving model ...
Adjusting learning rate to: 0.0009974
Epoch: 30 cost time: 4.03816032409668
Epoch: 30, Steps: 79 | Train Loss: 0.1489966 Vali Loss: 0.1016053 Test Loss: 0.1152540
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0010000
Epoch: 31 cost time: 3.8935391902923584
Epoch: 31, Steps: 79 | Train Loss: 0.1485366 Vali Loss: 0.1038320 Test Loss: 0.1142750
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009995
Epoch: 32 cost time: 3.8897294998168945
Epoch: 32, Steps: 79 | Train Loss: 0.1490173 Vali Loss: 0.0996369 Test Loss: 0.1142927
Validation loss decreased (0.100050 --> 0.099637).  Saving model ...
Adjusting learning rate to: 0.0009980
Epoch: 33 cost time: 3.9058942794799805
Epoch: 33, Steps: 79 | Train Loss: 0.1493609 Vali Loss: 0.1001248 Test Loss: 0.1159773
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009954
Epoch: 34 cost time: 3.895357131958008
Epoch: 34, Steps: 79 | Train Loss: 0.1498830 Vali Loss: 0.1003133 Test Loss: 0.1150918
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009919
Epoch: 35 cost time: 3.9189631938934326
Epoch: 35, Steps: 79 | Train Loss: 0.1465335 Vali Loss: 0.1054955 Test Loss: 0.1130293
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009874
Epoch: 36 cost time: 3.8041019439697266
Epoch: 36, Steps: 79 | Train Loss: 0.1475022 Vali Loss: 0.0999522 Test Loss: 0.1142809
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009819
Epoch: 37 cost time: 4.062532901763916
Epoch: 37, Steps: 79 | Train Loss: 0.1433447 Vali Loss: 0.1010155 Test Loss: 0.1132500
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009754
Epoch: 38 cost time: 4.088048219680786
Epoch: 38, Steps: 79 | Train Loss: 0.1444059 Vali Loss: 0.0967187 Test Loss: 0.1129266
Validation loss decreased (0.099637 --> 0.096719).  Saving model ...
Adjusting learning rate to: 0.0009680
Epoch: 39 cost time: 3.7028939723968506
Epoch: 39, Steps: 79 | Train Loss: 0.1451631 Vali Loss: 0.1006275 Test Loss: 0.1153779
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0009597
Epoch: 40 cost time: 3.860382080078125
Epoch: 40, Steps: 79 | Train Loss: 0.1421300 Vali Loss: 0.0994625 Test Loss: 0.1128853
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0009504
Epoch: 41 cost time: 3.8729443550109863
Epoch: 41, Steps: 79 | Train Loss: 0.1444431 Vali Loss: 0.0989089 Test Loss: 0.1124897
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0009402
Epoch: 42 cost time: 3.755230665206909
Epoch: 42, Steps: 79 | Train Loss: 0.1411620 Vali Loss: 0.0989898 Test Loss: 0.1123797
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0009291
Epoch: 43 cost time: 4.0934367179870605
Epoch: 43, Steps: 79 | Train Loss: 0.1415674 Vali Loss: 0.0995481 Test Loss: 0.1122004
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0009171
Epoch: 44 cost time: 4.012296438217163
Epoch: 44, Steps: 79 | Train Loss: 0.1402626 Vali Loss: 0.0967194 Test Loss: 0.1121350
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0009043
Epoch: 45 cost time: 4.258329153060913
Epoch: 45, Steps: 79 | Train Loss: 0.1379626 Vali Loss: 0.1010260 Test Loss: 0.1075038
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0008907
Epoch: 46 cost time: 4.017066478729248
Epoch: 46, Steps: 79 | Train Loss: 0.1382137 Vali Loss: 0.0973382 Test Loss: 0.1072092
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0008763
Epoch: 47 cost time: 3.8175761699676514
Epoch: 47, Steps: 79 | Train Loss: 0.1390063 Vali Loss: 0.1065474 Test Loss: 0.1126105
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0008612
Epoch: 48 cost time: 4.118200063705444
Epoch: 48, Steps: 79 | Train Loss: 0.1419958 Vali Loss: 0.1026655 Test Loss: 0.1082364
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0008453
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.37570127844810486, mae:0.4789288341999054, rmse:0.6129447817802429, mape:0.017154091969132423, mspe:0.0004884529625996947, rse:0.42247840762138367, r2_score:0.7881475153042237, acc:0.9828459080308676
corr: [38.062645 38.545376 38.519936 38.545635 38.495167 38.495556 38.51554
 38.51329  38.48363  38.429592 38.417713 38.41377  38.39243  38.354626
 38.354828 38.331432 38.35328  38.335213 38.309475 38.278877 38.308144
 38.328865 38.328876 38.354073 38.34065  38.3659   38.40626  38.40054
 38.359005 38.32912  38.26261  38.304985 38.318924 38.32603  38.31199
 38.316006 38.299007 38.327682 38.323822 38.31686  38.284286 38.358982
 38.450066 38.417274 38.395298 38.38671  38.364548 38.333435 38.26812
 38.298744 38.264282 38.232224 38.143696 38.11121  38.212276 38.22496
 38.221123 38.23741  38.189198 38.239525 38.238503 38.213047 38.18567
 38.21722  38.187195 38.195396 38.20077  38.19072  38.2035   38.189564
 38.120438 38.086433 38.08419  38.08601  38.11466  38.146248 38.141953
 38.09959  38.091583 38.113026 38.11042  38.071884 37.985783 37.938927
 37.933804 37.90709  37.923203 37.931736 37.943066 37.876392 37.84414
 37.851616 37.81925  37.78785  37.76081  37.765846 37.729805 37.731163
 37.700886 37.695854 37.73733  37.68895  37.66025  37.61602  37.576523
 37.546455 37.527416 37.471172 37.41887  37.388695 37.384808 37.35692
 37.293255 37.24349  37.173977 37.168293 37.18859  37.158398 37.076473
 37.019302 36.974922 36.873978 36.864296 36.814697 36.764828 36.73912
 36.72835  36.670723 36.606697 36.539906 36.525723 36.578426 36.51817
 36.47434  36.412693 36.438244 36.442425 36.41715  36.402752 36.426907
 36.447437 36.438343 36.40807  36.309387 36.289524 36.25875  36.283787
 36.318264 36.35032  36.361645 36.311344 36.31431  36.312298 36.32035
 36.346573 36.356136 36.352226 36.26883  36.206165 36.24604  36.26821
 36.355648 36.336292 36.302925 36.282146 36.33421  36.277874 36.18856
 36.19525  36.239426 36.220966 36.20658  36.234684 36.17405  36.22438
 36.244797 36.250523 36.25052  36.182514 36.19809  36.2019   36.21325
 36.40576  36.790154 36.745388 36.73615  36.708244 36.71625  36.662067
 36.669064 36.66549  36.64516  36.68628  36.671413 36.647095 36.614067
 36.61085  36.574936 36.58627  36.602642 36.601334 36.60673  36.629883
 36.634457 36.646755 36.66969  36.669563 36.661964 36.6485   36.671642
 36.64332  36.626102 36.559727 36.61017  36.609158 36.66187  36.66934
 36.665257 36.654053 36.632656 36.59703  36.56916  36.567642 36.583054
 36.62634  36.645706 36.679432 36.671665 36.660954 36.681313 36.642303
 36.661816 36.67822  36.64249  36.56414  36.553734 36.576862 36.56116
 36.581512 36.595253 36.55488  36.596184 36.629642 36.621788 36.640133
 36.652645 36.62055  36.628944 36.68259  36.693405 36.71063  36.734074
 36.743393 36.729618 36.71649  36.706314 36.77322  36.79584  36.809875
 36.825462 36.8209   36.87189  36.894703 36.898357 36.87476  36.875786
 36.859684 36.866207 36.904423 36.945934 36.981686 36.94848  36.952263
 36.978645 37.000607 36.997646 37.016514 37.053787 37.055637 37.09069
 37.106144 37.105167 37.12029  37.07827  37.064224 37.060978 37.04239
 37.03001  37.053596 37.0361   37.040302 37.06863  37.12226  37.12843
 37.09935  37.049545 36.992035 36.982155 36.9773   36.902985 36.823612
 36.807323 36.78789  36.699226 36.701984 36.645676 36.617638 36.63476
 36.637447 36.62953  36.58512  36.5243   36.51696  36.582474 36.550514
 36.546883 36.47611  36.538403 36.591442 36.592606 36.60814  36.609486
 36.629696 36.681858 36.707123 36.65919  36.67245  36.672043 36.74302
 36.790047 36.826088 36.822643 36.799725 36.831047 36.86641  36.86248
 36.916782 36.95147  36.940662 36.89606  36.865376 36.920177 36.92875
 37.019684 37.027287 37.04475  37.065838 37.143787 37.1147   37.06798
 37.112545 37.17467  37.170555 37.216568 37.274776 37.268215 37.360165
 37.420345 37.523235 37.573338 37.530636 37.553745 37.579136 37.610527
 16.335394]
Use GPU: cuda:0
>>>>>>>start training : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10118
val 1499
test 2996
Patience count starts from 1 epoch
Using OneCycleLR learning rate adjustment
Starting pretraining
Epoch: 1 cost time: 4.182397127151489
Epoch: 1, Steps: 79 | Train Loss: 1.2105310 Vali Loss: 1.0215073 Test Loss: 0.9697845
Validation loss decreased (inf --> 1.021507).  Saving model ...
Adjusting learning rate to: 0.0000426
Epoch: 2 cost time: 3.918409585952759
Epoch: 2, Steps: 79 | Train Loss: 1.0350278 Vali Loss: 0.7791297 Test Loss: 0.7916772
Validation loss decreased (1.021507 --> 0.779130).  Saving model ...
Adjusting learning rate to: 0.0000505
Epoch: 3 cost time: 4.01247501373291
Epoch: 3, Steps: 79 | Train Loss: 0.9514106 Vali Loss: 0.6187908 Test Loss: 0.6231086
Validation loss decreased (0.779130 --> 0.618791).  Saving model ...
Adjusting learning rate to: 0.0000635
Epoch: 4 cost time: 3.6111414432525635
Epoch: 4, Steps: 79 | Train Loss: 0.7249861 Vali Loss: 0.4935037 Test Loss: 0.4747576
Validation loss decreased (0.618791 --> 0.493504).  Saving model ...
Adjusting learning rate to: 0.0000815
Epoch: 5 cost time: 3.7126057147979736
Epoch: 5, Steps: 79 | Train Loss: 0.5868322 Vali Loss: 0.3738031 Test Loss: 0.3673175
Validation loss decreased (0.493504 --> 0.373803).  Saving model ...
Adjusting learning rate to: 0.0001044
Epoch: 6 cost time: 3.754129409790039
Epoch: 6, Steps: 79 | Train Loss: 0.4953989 Vali Loss: 0.2789930 Test Loss: 0.2974299
Validation loss decreased (0.373803 --> 0.278993).  Saving model ...
Adjusting learning rate to: 0.0001317
Epoch: 7 cost time: 3.7297117710113525
Epoch: 7, Steps: 79 | Train Loss: 0.4249244 Vali Loss: 0.2830882 Test Loss: 0.2680871
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0001634
Epoch: 8 cost time: 3.87441349029541
Epoch: 8, Steps: 79 | Train Loss: 0.4066596 Vali Loss: 0.2515724 Test Loss: 0.2528406
Validation loss decreased (0.278993 --> 0.251572).  Saving model ...
Adjusting learning rate to: 0.0001989
Epoch: 9 cost time: 3.7604238986968994
Epoch: 9, Steps: 79 | Train Loss: 0.3707367 Vali Loss: 0.2405017 Test Loss: 0.2400026
Validation loss decreased (0.251572 --> 0.240502).  Saving model ...
Adjusting learning rate to: 0.0002380
Epoch: 10 cost time: 3.9080698490142822
Epoch: 10, Steps: 79 | Train Loss: 0.3726857 Vali Loss: 0.2076903 Test Loss: 0.2295150
Validation loss decreased (0.240502 --> 0.207690).  Saving model ...
Adjusting learning rate to: 0.0002802
Epoch: 11 cost time: 3.7863335609436035
Epoch: 11, Steps: 79 | Train Loss: 0.3612671 Vali Loss: 0.1809862 Test Loss: 0.1995865
Validation loss decreased (0.207690 --> 0.180986).  Saving model ...
Adjusting learning rate to: 0.0003250
Epoch: 12 cost time: 3.671311616897583
Epoch: 12, Steps: 79 | Train Loss: 0.3174309 Vali Loss: 0.1794251 Test Loss: 0.1884544
Validation loss decreased (0.180986 --> 0.179425).  Saving model ...
Adjusting learning rate to: 0.0003719
Epoch: 13 cost time: 3.7014527320861816
Epoch: 13, Steps: 79 | Train Loss: 0.3188195 Vali Loss: 0.1497541 Test Loss: 0.1731661
Validation loss decreased (0.179425 --> 0.149754).  Saving model ...
Adjusting learning rate to: 0.0004205
Epoch: 14 cost time: 3.8313913345336914
Epoch: 14, Steps: 79 | Train Loss: 0.2939045 Vali Loss: 0.1343835 Test Loss: 0.1920939
Validation loss decreased (0.149754 --> 0.134383).  Saving model ...
Adjusting learning rate to: 0.0004701
Epoch: 15 cost time: 3.7897937297821045
Epoch: 15, Steps: 79 | Train Loss: 0.3121029 Vali Loss: 0.1327934 Test Loss: 0.1721793
Validation loss decreased (0.134383 --> 0.132793).  Saving model ...
Adjusting learning rate to: 0.0005203
Epoch: 16 cost time: 3.9299888610839844
Epoch: 16, Steps: 79 | Train Loss: 0.3159494 Vali Loss: 0.1894855 Test Loss: 0.2129350
EarlyStopping counter: 1 out of 10
Adjusting learning rate to: 0.0005705
Epoch: 17 cost time: 3.722813129425049
Epoch: 17, Steps: 79 | Train Loss: 0.3088779 Vali Loss: 0.1975587 Test Loss: 0.2179444
EarlyStopping counter: 2 out of 10
Adjusting learning rate to: 0.0006202
Epoch: 18 cost time: 3.8584396839141846
Epoch: 18, Steps: 79 | Train Loss: 0.3620811 Vali Loss: 0.2151474 Test Loss: 0.2647999
EarlyStopping counter: 3 out of 10
Adjusting learning rate to: 0.0006687
Epoch: 19 cost time: 3.77266263961792
Epoch: 19, Steps: 79 | Train Loss: 0.3901925 Vali Loss: 0.5083694 Test Loss: 0.5294798
EarlyStopping counter: 4 out of 10
Adjusting learning rate to: 0.0007156
Epoch: 20 cost time: 3.796555995941162
Epoch: 20, Steps: 79 | Train Loss: 0.5564827 Vali Loss: 0.3251728 Test Loss: 0.2986256
EarlyStopping counter: 5 out of 10
Adjusting learning rate to: 0.0007604
Epoch: 21 cost time: 3.7452633380889893
Epoch: 21, Steps: 79 | Train Loss: 0.3771498 Vali Loss: 0.2567340 Test Loss: 0.2381297
EarlyStopping counter: 6 out of 10
Adjusting learning rate to: 0.0008025
Epoch: 22 cost time: 3.757802963256836
Epoch: 22, Steps: 79 | Train Loss: 0.3723666 Vali Loss: 0.2251503 Test Loss: 0.2435437
EarlyStopping counter: 7 out of 10
Adjusting learning rate to: 0.0008415
Epoch: 23 cost time: 3.707322359085083
Epoch: 23, Steps: 79 | Train Loss: 0.4363206 Vali Loss: 0.2299911 Test Loss: 0.2794843
EarlyStopping counter: 8 out of 10
Adjusting learning rate to: 0.0008770
Epoch: 24 cost time: 3.786872625350952
Epoch: 24, Steps: 79 | Train Loss: 0.3989602 Vali Loss: 0.3290811 Test Loss: 0.3807188
EarlyStopping counter: 9 out of 10
Adjusting learning rate to: 0.0009086
Epoch: 25 cost time: 3.833505868911743
Epoch: 25, Steps: 79 | Train Loss: 0.3972405 Vali Loss: 0.1964777 Test Loss: 0.2166677
EarlyStopping counter: 10 out of 10
Adjusting learning rate to: 0.0009360
Early stopping
>>>>>>>testing : omdata1_test1_MSPT_SSL_custom_ftMS_sl365_ll0_pl0_dm128_nh8_el2_dl1_df256_fc1_ebtimeF_dtTrue_20240615_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2996
loading pretrain model weight
test shape: (2996, 1, 365, 1) (2996, 1, 365, 1)
test shape: (2996, 365, 1) (2996, 365, 1)
mse:0.378891259431839, mae:0.4835236370563507, rmse:0.6155414581298828, mape:0.017269324511289597, mspe:0.00048682064516469836, rse:0.4242681860923767, r2_score:0.7784633605225072, acc:0.9827306754887104
corr: [38.012024 37.781467 38.70663  38.09992  38.235767 38.470802 37.84717
 37.910732 38.544662 37.85303  38.171444 37.803986 37.882675 37.92577
 37.276424 38.023483 37.78323  38.15162  38.34165  38.164787 38.204273
 38.1058   38.29381  38.49224  38.34087  37.767323 38.244156 38.01077
 38.14489  37.948963 38.30654  38.253265 37.793694 37.675827 38.328117
 38.413467 38.05099  38.206707 38.69093  38.354652 37.903557 38.011677
 38.235294 38.897213 38.726215 38.372505 38.16191  38.61139  38.521683
 38.43437  38.063545 38.324116 38.416443 38.470333 38.469093 38.56193
 37.983665 38.1755   37.8206   38.18952  38.464363 37.573597 37.811928
 38.008675 37.65291  38.147015 38.094128 38.00629  37.896034 38.208836
 38.12206  37.92543  37.460308 38.07452  37.988613 37.67733  37.74658
 37.95608  37.632202 37.716763 37.9356   37.369118 37.64724  37.450188
 37.93219  37.32744  37.490173 37.604588 37.37633  36.61061  36.745045
 37.228024 37.08423  36.934586 37.48733  37.202805 37.243874 37.016933
 36.797737 36.961697 36.911625 36.604668 36.924942 37.243362 37.06673
 37.103184 37.370667 37.20694  36.888527 36.93216  36.78863  37.049736
 37.018013 36.74197  36.93483  36.897465 36.89189  36.248432 36.775787
 36.65853  36.57913  36.50021  36.42804  35.863003 36.36248  36.1312
 36.499035 36.096836 36.39509  35.566227 36.268963 36.178585 36.446095
 36.3769   36.308674 35.91235  35.858162 36.091423 35.42779  36.019573
 35.76212  35.78299  35.91942  35.696873 35.902206 35.98073  35.896065
 35.64791  35.73491  35.435883 35.639236 35.47317  35.429604 35.506424
 35.343    35.736813 35.756855 35.9095   35.54188  35.618046 36.254555
 35.428295 35.761433 35.76241  35.68548  35.748962 35.64407  35.72266
 35.7846   35.31666  36.197323 36.095055 35.78802  35.20052  35.9231
 35.733116 35.699795 35.72011  35.195004 36.25985  36.1985   35.462482
 35.511486 35.97184  35.44666  36.1799   35.355473 35.72843  35.74012
 35.872925 35.523586 35.678394 35.664387 35.83177  35.807697 35.784653
 35.797913 35.69968  35.368187 35.891426 35.80663  35.941326 35.97622
 35.232254 35.83484  35.08599  35.033043 35.707005 35.689224 35.70941
 35.585186 35.65899  36.043728 36.046833 36.30719  35.56815  35.82192
 35.994923 36.07948  35.677902 36.29536  35.91548  36.120052 36.01992
 35.817898 36.041275 36.014126 36.467503 36.251602 35.869877 35.776035
 36.10295  35.10617  35.727757 35.780273 36.084003 36.1118   35.663624
 36.15499  35.986095 35.63476  35.72805  35.829926 35.568623 36.008194
 35.808563 34.636246 35.441605 35.621    36.021378 35.97999  35.62574
 34.928875 35.589005 35.08017  34.948616 35.76089  36.13565  36.272533
 35.550404 35.04453  36.381893 36.05777  35.879795 35.784645 35.470837
 35.994076 35.90396  35.41875  36.115665 36.278507 35.613846 36.222893
 36.01625  35.643265 35.918076 36.023594 35.93775  36.176785 35.68628
 35.950962 35.61986  35.79399  36.008038 35.657463 35.628353 35.448215
 35.626614 35.414444 35.06954  35.89747  35.350815 33.90508  35.05427
 34.71088  35.1668   35.432896 35.232677 35.957264 35.86141  35.87315
 35.36885  36.168835 35.883404 35.63342  35.921703 35.923965 35.769188
 35.14587  35.757072 35.74837  35.922672 35.427254 35.281624 34.934425
 35.21328  35.141266 35.199203 35.330082 35.3712   35.17494  34.756123
 34.283295 35.54587  35.17128  35.583355 35.584698 35.39383  35.63224
 35.695618 35.65861  35.52116  36.380615 36.23083  36.29099  36.126408
 35.817425 36.242878 35.95904  36.365467 35.77508  36.29793  36.122837
 36.383205 36.577213 36.45965  36.438038 36.50791  36.62031  36.687233
 36.616985 36.654995 36.282303 36.484447 35.677208 37.13469  36.76204
 36.596233 36.399372 36.36472  35.91871  36.33806  35.925995 35.7932
 36.01004 ]
